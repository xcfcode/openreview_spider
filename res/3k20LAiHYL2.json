{"notes": [{"id": "3k20LAiHYL2", "original": "cUeOv8m1Nin", "number": 2398, "cdate": 1601308264582, "ddate": null, "tcdate": 1601308264582, "tmdate": 1616077554990, "tddate": null, "forum": "3k20LAiHYL2", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ovr0xOEXQv", "original": null, "number": 1, "cdate": 1610040376480, "ddate": null, "tcdate": 1610040376480, "tmdate": 1610473968767, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper presents two self-supervised learning objectives that can be used as intermediate pre-training tasks to refine the T5 sequence-to-sequence model between pre-training and task fine-tuning. It shows that, at small to moderate model sizes, adding this step significantly improves performance on commonsense-oriented target tasks.\n\nPros:\n- This appears to be a fairly straightforward improvement in self-supervised learning in NLP, with fairly extensive experiments.\n\nCons:\n\n- This model isn't trained at the same extremely large scales (10B+-words) as state-of-the-art models, and it performs significantly below the state of the art. It's not clear that the released model represents a useful model for any application as-is, and while it's likely, it's not proven that the ideas in this paper would still be useful at larger scales.\n- Given that, it seems like the most likely audience for this work is other developers of pretrained models in NLP, which makes the fit to a general ML conference less clear.\n- The framing around 'concepts' and, more importantly, the model name 'concept-aware LM', gives the unwarranted impression that the new model handles 'concepts' in a way that T5 doesn't. It is not reasonable to use the word 'concept' to refer to specific parts of speech in your title (even if you later explain that), and whether your model handles concepts in a categorically different way from T5 would take a substantial analysis to show, which doesn't seem to be present. I don't think this paper is up to ICLR's standards with the current name, and urge the authors to change it.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040376467, "tmdate": 1610473968749, "id": "ICLR.cc/2021/Conference/Paper2398/-/Decision"}}}, {"id": "GFaBPjbgCWz", "original": null, "number": 2, "cdate": 1603856713544, "ddate": null, "tcdate": 1603856713544, "tmdate": 1606801960521, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review", "content": {"title": "Interesting work", "review": "Summary:\nThis paper proposes the \u201cConcept Aware Language Model\u201d (CALM) --- a pre-trained T5 Transformer is trained on self-supervised intermediate tasks to learn relational commonsense knowledge, before fine-tuning on downstream tasks. The intermediate tasks include (1) concept to sentence (c2s) generation - given a list of permuted concepts (verbs and nouns), generate the target sequence, (2) concept order recovery (cor) - given a sequence with the order of concepts (verbs and nouns) shuffled, generate the original sequence, (3) given a sequence and it\u2019s perturbed version (concepts shuffled), generate the sequence (classification but as a generation task). Training is first done in a multi-task fashion, followed by a stage of training that uses generated outputs from (1) and (2) for (3).\n\nThe goal of the paper is to show that carefully designed objectives for self-supervised intermediate task training add relational commonsense knowledge which helps improve model performance on downstream commonsense reasoning tasks. \n\nStrengths:\n- S1 - The idea of self-supervised intermediate task training is an exciting one especially in the form of adding reasoning capabilities that BERT, GPT, T5 etc. might not be acquiring during the large-scale pre-training phase.   \n- S2 -  As shown by results in Table 1 on 5 commonsense reasoning tasks, the intermediate task training proposed in this work improves performance over and above the T5 model (and variants including with salient span masking for concepts).  \n- S3 - The experiments involved averaging across 3 random seeds and the authors have reported confidence intervals.   \n\nWeaknesses and questions:\n- W1 - One missing baseline is T5 trained to unshuffle entire sequences - given an input with the tokens shuffled, generate the original sequence. This would show how much value c2s and cor are really adding. The current T5 baselines are all trained purely for infilling which seems a bit unfair compared to CALM which is generating entire sequences.  \n- W2 - Given a T5 model that can score sequences (maybe after training on the autoencoding objective), would it score \u201capples grow on trees\u201d higher than \u201ctrees grow on apples\u201d? If yes, then the model seems to already exhibit this style of reasoning. Would it score \u201capples grow on trees\u201d and \u201capples grow in the ground\u201d similarly? The distinction here is between sequences that are non-grammatical or unlikely to ever appear versus sequences that may have appeared (eg: \u201cpotatoes grow in the ground\u201d). Presently, (a) it\u2019s unclear if the designed objectives are providing commonsense reasoning above something the model can know from autoregressive language model scoring, and (b) it appears that the objectives are not designed to add relational commonsense knowledge of the sort where we know apples don\u2019t grow in the ground.   \n- W3 - c2s is designed specifically to do well on CommonGen. Are the gains on this task smaller than what you might have expected? If yes, why isn\u2019t it helping more?   \n- W4 - Figure 4 needs error bars, the dev sets are really small and it\u2019s hard to interpret which differences are significant.  \n\n\n\n__UPDATE__\n\nThanks for the incredibly detailed response! I've raised my score to a 8.  \nI do in general quite like the paper, and the responses here are thought-provoking. I'm not sure I'm totally convinced by the WSC results comparing CALM the classifier to T5 the sequence scorer. Not sure if it's an apples to apples comparison...but I'm not sure there's a straightforward setup for this, and perhaps it starts to get beyond the scope of what's being presented here. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097309, "tmdate": 1606915765869, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2398/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review"}}}, {"id": "pa09VDWRzTS", "original": null, "number": 2, "cdate": 1605467648131, "ddate": null, "tcdate": 1605467648131, "tmdate": 1606279620031, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to All Reviewers / Revision Details (1/2)", "comment": "Hi all. We\u2019ve revised and updated the draft according to your valuable reviews. The updates are summarized as follows:\n- Included results of CALM-large which is based on T5-large. (See Table 2 in the revised draft or NewTable-1 in this comment)\n- Included results of CALM (BART-base), which is obtained by applying our method on BART-base. The experiments with BART-large are currently running. (See Table 9 in the revised draft or NewTable-2 in this comment)\n- Included results of ablation study analyzing the choice of using either nouns or verbs as concepts. (See Table 10 in the revised draft or NewTable-3 in this comment)\n- Included experiments on LAMA probes  (Petroni et al.,2019)  and KILT  (Petroni et al.,2020), two sets of benchmark datasets for knowledge-intensive tasks (See Table 6 in the revised draft or NewTable-4 in this comment).\n- Included results of several popular base size pre-trained language models in Table 1 of the revised draft and conducted a statistical significance test. Results show that CALM significantly outperforms T5-base, BERT-base, ERNIE, and KnowBERT with a p-value < 0.01.\n- We conducted a human evaluation of CommonGEN predictions.\n- We\u2019ve polished the writing and figures in the paper.\n- **Nov 24 paper updates** The paper is updated with all the additional experiments we have conducted during the rebuttal phase.\n\nFirst of all, after the submission deadline, we have continually pre-trained CALM-large with T5-large as the backbone model and now the results are available (see Fig 2 in the revised paper):\n\n- Results of discriminative tasks\n|     method                  | #parameters      |    CSQA    |    OBQA   |   PIQA   |    aNLI    |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|     T5-large                 |      774M                | 69.81  |  61.40  |  72.19   | 75.54 |\n|     CALM-large\t       |      774M                | 71.31  |  66.00  |  75.11   | 77.12 |\n|     BERT-large            |      345M                | 57.06  |  60.04  |  67.08   | 66.75 |\n|     RoBERTa-large     |      345M                | 71.81  |  63.90  |  76.90   | 82.35 |\n|     SOTA                      |      11B                   | 79.1    |  87.2    |  90.13   | 89.70 |\n\n- Results on CommonGEN\n|method                                 | # parameters    |  BLEU4|  METEOR | CIDEr | SPICE |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|BART                                      |  406M                 |  26.30  |  30.90       | 13.92  | 30.60 | \n|T5-large                                 |   774M                |  28.60  |  30.10       | 14.96  | 31.60 |\n|CALM-large\t                       |   774M                |  29.50  |  31.90       | 15.61  | **33.20** |\n|KG-BART (SOTA, with KG)   |  406M                 |  **30.90**  |  **32.40**       | **16.83**  | 32.70 | \n- NewTable 1\n\nWe find that CALM-large **significantly improves upon T5-large with p-value < 0.01 on all datasets**. This confirms that our approach is effective for larger models, which may help resolve the concern of R3 and R4. Also, CALM-large significantly outperforms BERT-large while performs slightly worse than RoBERTa, which is optimized with more gradient updates and cannot be used for NLG tasks. On CommonGen, CALM-large yields state-of-the-art performance on the SPICE metric, which correlates best with human evaluation according to the CommonGEN paper.\n\n- Results with BART-base as backbone model (139M parameters)\n|method            | CSQA | OBQA  |   PIQA  | aNLI   |  CommonGEN                 | \n| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: |\n|BART-base      | 56.31| 58.30 |  67.53|59.85 |  25.10/29.50/13.16/30.20  | \n|CALM-bart\t | **58.22**| **59.10** |  **69.40**|**61.28** |  **26.40**/**29.90**/**13.71**/**31.10**  | \n- NewTable 2\n\nWe can see that our approach consistently and significantly (with **p-value < 0.01**) improves BART-base on all datasets. This result shows that our method is versatile to different pre-trained models."}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "GxZR3qvtTQA", "original": null, "number": 20, "cdate": 1606277244334, "ddate": null, "tcdate": 1606277244334, "tmdate": 1606277244334, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "jVQb_qIeQ1_", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Very Informative Rebuttal!", "comment": "Thank you for the detailed response and large number of new reported experiments!\n\nI have raised my score to 7, because I believe your  new experiments show encouraging results as model size grows and you have clarified some of my misunderstanding in your response. I discuss each issue below:\n\n> New Results\n\nI agree that the your results with the addition of larger models and more datasets show that CALM has a significant, if somewhat moderate effect.\n\n> CommonGEN Performance\n\nThank you for clarifying what you think the gains are here\u2014I agree that CALM is significantly more parameter efficient. Re-reading the CommonGen paper I agree that SPICE is the most important metric and also that your reimplementation of T5 far outperforms theirs, which makes this more impressive. Thanks for walking me through this!\n\n> \u201cConcepts\u201d as a Term\n\nI am still critical of your use of the term \u201cconcept\u201d\u2014but I do appreciate your updates and think this is more in line with general definitions of the community. My personal view is that \u201cconcept\u201d is a very vague idea so to use it as motivation might be fine, but using it as a term here is a bit of an over-claim. That said, I do think this falls within the variation of usage in the community.\n\n> Data Effectiveness / Specificity of Training Regime\n\nI am still unconvinced that we have a meaningful way to distinguish between the setup of the CALM objectives and the datasets that NLU benchmarks tend to use\u2014but solving this problem is out of the scope of this paper. I do not think that the data curve as shown makes it clear whether the CALM objective essentially calibrated the models for the dataset in advance of finetuning or encodes \u201cdeeper knowledge\u201d. I would warn against making too many claims about the knowledge in a model, which is still very much a blackbox, but I don\u2019t think not being able to show this deeper knowledge is present can be considered a fault of this paper.\n\n> Nouns & Verbs\n\nThank you for clarifying this in the update!\n\nAll in all, I now much more convinced that CALM adds information over current pre-training regimes. As I mentioned at the beginning of this response, I have raised my score for this paper to a 7 and recommend acceptance!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "HD2ykaLN-zh", "original": null, "number": 3, "cdate": 1603862125014, "ddate": null, "tcdate": 1603862125014, "tmdate": 1606277082143, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review", "content": {"title": "Interesting Proposed Objectives, Weak Results", "review": "This paper suggestsan intermediate training regime that can be used between pretraining and the end-task finetuning. The authors suggest that their method captures more commonsense knowledge by being focused on capturing knowledge about \u201cconcepts\u201d. Four different denoising objectives\u2014two generative and two discriminative\u2014are proposed and described in detail, with various possible ways of optimizing for all four. Experimental results show improvements over both the base T5 model and the large T5 model. The proposed method achieves SoTA results on CommonGen with slightly more than half the parameters of the current SoTA model.  Ablations show the necessity of applying a 2-step intermediate training scheme with mixed training followed by joint training.  CALM shows better results with less data than the base model. \n\nStrengths:\n- Unifying generative and contrastive training is an important and interesting goal.\n- The objectives suggested are cheap to compute and seem to increase the signal available in the data.\n- Extensive results show improvements over a base model and a larger model across a range of tasks.\n- Performance with relatively little finetuning data are encouraging.\n\nWeaknesses:\n- Somewhat weaker results on some CommonGen metrics are disappointing.\n- Using \u201cconcept\u201d to stand in for verbs and nouns is somewhat confusing.\n\nAfter reading other reviews and the authors\u2019 responses to all of the reviewers, I recommend this paper by accepted\u2014extensive results show that the CALM objectives offer more signal from data than current pretraining methods.\n\nThis paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model. These objectives certainly improve over the original T5 base _and_ larges models that are used as initializations, and especially outperform the base model in the low-data regime. The authors use objectives which capture both generative and discriminative information, which some have suggested contain mutually beneficial signal but have not been unified in a single training method.\n\nBelow are two paragraphs from my original review. The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly. Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model. I therefore believe that these results show merit.\n> However, in every task except CommonGEN the authors do not discuss any methods that are even close to the state of the art. For CSQA, the best number in this paper is 63.32 vs. 79.5 on the current leaderboard. Similar numbers are true for the rest of the tasks: 60.90 vs. 87 for OBQA, 71.01 vs. 90 for PIQA, and 63.20 vs.  89.70 for aNLI. I do not believe that SoTA results are necessary to write a good paper, and indeed the obsession our field has with SoTA is unhealthy. Yet, it is difficult for me to trust that the effects in this paper will generalize to better performing models without further evidence: what if the CALM intermediate objectives only help with mistakes that larger models do not make in the first place? \n\n> On the generative task CALM performs closer to SOTA, but it improves only slightly on T5. This is especially disappointing as the objectives introduced _directly_ match the task in CommonGEN, making this intermediate training a form of noisy training data rather than pretraining.\n\nI still feel that the authors\u2019 use of \u201cconcept\u201d and \u201ccommonsense\u201d is vague, when their method can be defined more clearly with more mundane terminology. In practice, the authors use nouns and verbs as their concepts, which is fine in terms of pretraining objectives, but surely does not capture the generality of concepts. The authors have somewhat clarified in this in their updated version.\n\nFinally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning. One way this can be seen is  that the slopes of the T5 and CALM lines are very similar after an initial \u201cbump\u201d which T5 likely needs to calibrate to the new distribution. This makes claims of learning \u201ccommonsense\u201d hard to verify, though I do agree that _something_ relevant to solving these problems is clearly being learned.\n\nAltogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools? I recommend this paper for acceptance, though I encourage the authors to revise their paper to make this the focus of the story, rather than the vaguely defined notion of \u201cconcept\u201d.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097309, "tmdate": 1606915765869, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2398/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review"}}}, {"id": "CHBMLoj7qBm", "original": null, "number": 19, "cdate": 1606194068662, "ddate": null, "tcdate": 1606194068662, "tmdate": 1606195180391, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "C0gWvfzRko", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 2 Cont", "comment": ">Weakness 2: \n>\n>Given a T5 model that can score sequences (maybe after training on the autoencoding objective), would it score \u201capples grow on trees\u201d higher than \u201ctrees grow on apples\u201d? If yes, then the model seems to already exhibit this style of reasoning. Would it score \u201capples grow on trees\u201d and \u201capples grow in the ground\u201d similarly? The distinction here is between sequences that are non-grammatical or unlikely to ever appear versus sequences that may have appeared (eg: \u201cpotatoes grow in the ground\u201d). Presently, (a) it\u2019s unclear if the designed objectives are providing commonsense reasoning above something the model can know from autoregressive language model scoring, and (b) it appears that the objectives are not designed to add relational commonsense knowledge of the sort where we know apples don\u2019t grow in the ground. \n\nIn addition to the previous response, we also conducted an experiment using T5-base and CALM-base to identify original vs. concept-perturbed sentences. We find that CALM-base achieves over 5% accuracy improvement upon CALM-base. This confirms that, while our proposed objectives are not designed to introduce \u201cadd new relational knowledge\u201d to the model \u2014 since we\u2019re only perturbing existing sentences, they make a pre-trained acquired relational commonsense knowledge in texts more effectively.\n\nAs for concept set generalization, we appreciate this idea and will leave it as future work.\n\n>Weakness 3: \n>\n>c2s is designed specifically to do well on CommonGen. Are the gains on this task smaller than what you might have expected? If yes, why isn\u2019t it helping more?\n\n- Additional Response: \nIn addition, we have conducted experiments with larger backbone models and the results on CommonGEN are as follows:\n\n Results on CommonGEN\n|method                                 | # parameters    |  BLEU4|  METEOR | CIDEr | SPICE |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|BART                                      |  406M                 |  26.30  |  30.90       | 13.92  | 30.60 | \n|T5-large                                 |   774M                |  28.60  |  30.10       | 14.96  | 31.60 |\n|CALM-large\t                       |   774M                |  29.50  |  31.90       | 15.61  | **33.20** |\n|KG-BART (SOTA, with KG)   |  406M                 |  **30.90**  |  **32.40**       | **16.83**  | 32.70 | \n\nWe can see that CALM-large consistently outperforms T5-large by a large margin. It also achieves state-of-the-art performance on the SPICE metric, which is the most important metric for CommonGEN because it correlates best with human evaluation according to the original paper of CommonGEN. \n\nWe have also conducted a human evaluation of CommonGEN predictions.\nWe asked three annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.\nHere's the detail:\n- Number of test sentences: 50\n- Binary selection by majority voting\n- Kappa score (inter-annotator agreement) : 0.73\n- CALM-base : 0.6 / T5-base : 0.4\n\nWe can find that CALM predictions usually more reasonable than T5.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "ags4CT4hOY", "original": null, "number": 3, "cdate": 1605468201408, "ddate": null, "tcdate": 1605468201408, "tmdate": 1606194396512, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "pa09VDWRzTS", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to All Reviewers / Revision Details (2/2)", "comment": "- Results with Noun / Verb as a concept\n\n|method     | CSQA  | OBQA  |   PIQA  | aNLI   |  CommonGEN      | \n| -----------   | :-------: | :--------: | :-------: | :-------: | :-------: |  \n| CALM              | **63.32** | **60.90** |  **71.01**|**63.20** |  **26.40**/**31.40**/**13.88**/**33.00** |  \n| CALM-noun\t |  62.45 | 59.40 |  69.05|61.55 |  25.70/31.20/13.17/32.60  |  \n| CALM-verb\t | 62.51| 59.10 |  69.24|61.40 |  25.60/31.20/13.24/32.60  | \n\n- NewTable 3\n\nWe can see that using either nouns-only or verbs-only as concepts for our approach leads to a substantial performance drop. This supports our choice of using both nouns and verbs as concepts.\n\n\n- LAMA Probe\n| method    | MRR  | Precision@50| Precision@10| Precision@1|\n| ----------- | :-------: | :--------: | :-------: | :-------: |\n|T5-base   |11.53  |  38.52             |     21.60         |         5.93      |\n|CALM\t    | **12.09** |  **39.69**             |     **22.53**         |     **6.46** |\n\n- KILT tasks (acc)\n|method | FEVER (fact verification )| AY2 (entity linking) | \n| ----------- | :-------: | :--------: |\n|T5-base| 76.65                              | 74.97                      |\n|CALM   | **77.44**                               | **77.24**        |\n\n- NewTable 4\n\nWe can see CALM consistently outperforms the T5-base model across a set of knowledge-related tasks. \n\nFor human evaluation, We asked three annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.\nHere's the detail:\n- Number of test sentences: 50\n- Binary selection by majority voting\n- Kappa score (inter-annotator agreement) : 0.73\n- CALM-base : 0.6 / T5-base : 0.4\n\n==============================\n\nReference: \\\nPetroni et al.,2019 Language Models as Knowledge Bases?, EMNLP 2019\\\nPetroni et al.,2020 Kilt: a benchmark for knowledge intensive language tasks, arxiv\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "xLd9tWV12BS", "original": null, "number": 8, "cdate": 1605469117695, "ddate": null, "tcdate": 1605469117695, "tmdate": 1606194261801, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "LsBF7AEEFQX", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (2/2)", "comment": ">Weakness 2: \n>\n>The objectives used are essentially reframings of the rules used to generate CommonGEN, and yet scores on CommonGEN are not very impressive.\n\nFirst, only the C2S objective is similar to that used to generate CommonGEN while the other objectives, including COR, the contrastive objective, and the joint-training framework are different from the rules for constructing CommonGEN and their usefulness is confirmed through ablation study and different versions of the CALM model. Second, actually, the performance gain of CALM upon T5-base is more significant than it looks like. Specifically, CALM achieves state-of-the-art performance on the SPICE metric, which is the most important metric for CommonGEN because it correlates best with human evaluation according to the original paper of CommonGEN. For BLEU and CIDEr, the improvement of CALM upon T5-base is almost half of that yielded by T5-large, which has nearly 4x parameters.  In addition, experiments with larger backbone models show that CALM-large consistently outperforms T5-large by a large margin. \n\nIn addition, we conducted a human evaluation of CommonGEN predictions.\nWe asked three annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.\nHere's the detail:\n- Number of test sentences: 50\n- Binary selection by majority voting\n- Kappa score (inter-annotator agreement) : 0.73\n- CALM-base : 0.6 / T5-base : 0.4\n\nWe can find that CALM predictions usually more reasonable than T5.\n\n>Weakness 3: \n>\n>The notion of \u201cconcepts\u201d remains vague throughout the paper as do claims regarding commonsense.\n\nWe have refined most of our claims in the revised version per your suggestion. Specifically, we defined the concepts to be nouns and verbs extracted by Spacy POS tagger. And our claim about commonsense is that our training objective encourages relational and compositional commonsense reasoning and help pack more commonsense knowledge into the parameter of a pre-trained model.\n\n>Concerns: \n>\n>The CALM intermediate objectives share many properties with all of these datasets and are actually calibrating the model to the kind of correlations they should expect to predict in advance of fine-tuning. One way this can be seen is that the slopes of the T5 and CALM lines are very similar after an initial \u201cbump\u201d which T5 likely needs to calibrate to the new distribution. \n\nIt is true that CALM intermediate objectives share many properties with CommonGEN. For other discriminative tasks, the shared properties is that both of them require the model to reason with concepts, and this is an essential part of commonsense reasoning. The learning curve shows that with CALM intermediate objectives, the model needs less training data to achieve a good performance and suggests CALM may encode more commonsense knowledge in advance. It is common that the slope of different methods with more data are similar for data size-performance curves because the performance tends to saturate with more data.\n\n>Question: \n>\n>Are nouns and verbs distinguished between in the list of words given to the model? On page 3 it\u2019s implied that verbs come first, but is there a delimiter between the verb list and the noun list?\n\nIn C2S, Nouns and Verbs are not distinguished and jointly shuffled, as shown by the permute function in Eq (1). In COR, To ensure grammatical correctness of shuffled sentences, Nouns and Verbs are distinguished because nouns are only shuffled with nouns and vice versa, as indicated by the concept-permute function. The notation on Page 3 is just for simplicity and there\u2019s no delimiter between the verb list and the noun list. We\u2019ve made it clear in the revised version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "-DT75R36Spx", "original": null, "number": 18, "cdate": 1606120564550, "ddate": null, "tcdate": 1606120564550, "tmdate": 1606120564550, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "C0gWvfzRko", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Reminder for the discussion", "comment": "Dear Reviewer 2,\n\nWe want to send you a friendly reminder for the discussion.\nHere are the things that we have added and resolved by your valuable feedback!\n\n- We show results of **deshuffling**. The result shows that performance after fine-tuning sentence de-shuffling is lower than original T5 except for CommonGEN.\n- We add discussion of our designed objectives in terms of commonsense knowledge.\n- We add discussion of CommonGen results. We argue that the performance gain of CALM upon T5-base is more significant than it looks like.\n\nMoreover, we added additional experiments of applying our methods into large models and other pre-trained language models, and also more ablation studies.\n\nWe thank you again for your valuable comments, and we would appreciate it if you could review our improvement and give us another round of feedback.\n\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "8SBUE54dNR5", "original": null, "number": 17, "cdate": 1606118586894, "ddate": null, "tcdate": 1606118586894, "tmdate": 1606118586894, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "elLy1OjESse", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Reminder for the discussion", "comment": "Dear Reviewer 4,\n\nWe want to send you a friendly reminder for the discussion.\nHere are the things that we have added and resolved by your valuable feedback!\n\n- We include results of **CALM-large** which is based on T5-large. The results show improvement.\n- We include results of **CALM(BART)**, which is by applying our method on BART. The results show the effectiveness of our method on other pertained language models.\n- To check the **effectiveness of noun and verb phrases as concepts**, we conducted an additional experiment with only concepts from ConceptNet. The results show that noun and verb phrases as concepts is more effective than concepts from ConceptNet.\n\nWe thank you again for your valuable comments, and we would appreciate it if you could review our improvement and give us another round of feedback.\n\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "ePJMLI-Q-Bn", "original": null, "number": 1, "cdate": 1603572229557, "ddate": null, "tcdate": 1603572229557, "tmdate": 1606109002937, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review", "content": {"title": "Nice approach to incoporate commonsense into large models but needs more evaluation", "review": "#### Summary\nThis paper addresses the issue of incorporating commonsense into large pretrained language models. They propose a pretraining method that does not leverage knowledge graphs but rather use approaches which involve corrupting the input sentence in various ways to recover the original sentence. This methodology is used to try to bake in commonsense into models. Results are shown on both discriminative and generative common sense datasets\n\n#### Novelty and clarity\nThe training procedure of corrupting inputs to retrieve outputs is not new but the use on commonsense tasks does seem novel and also is an interesting approach. The paper was very clear to read and the technical aspects were well described.\n\n#### Strengths\n(1) The use of a self-supervised approach is great because it requires no annotation and the training procedure is simple. It is also described well\n(2) The variety of baselines used is good and comparison against models larger than the proposed model is interesting to see.\n(3) The use of generated sentences to improve language models on hard areas like commonsense and offensiveness is a great idea as it can help make the model more robust\n\n\n#### Weaknesses\n(1) There should be a more comprehensive set of results completed to see how much improvement this model has. Mainly on the CommonGen class there should be some manual evaluation done similar in the original paper to see if the outputted sentences make sense and that the improvement in automatic metrics is carried over into human evaluation. \n(2) A key aspect to look into is the robustness of this model. In the C2S approach the concepts were shuffled to generate the correct sentence. During inference time if the concepts were shuffled in a different manner would the model still be able to generate the correct sentences? There was three random seeds used but as was said \"the performance is sensitive to different random seeds.\" which seems that the model isn't as robust to newly seen inputs\n\n#### Detailed Comments\nIf spacy was also used for POS tagging along with tokenization this should be made clear. Also for every sentence was there 3 nouns/verbs extracted? \nOne thing I'm unclear about is in Table 2 why is the \"Our T5-Base\" better than the \"T5-Base\" above? Is this T5 with additional epochs? I think this should be made clear. Additionally I wouldn't say and \"is only slightly worse than KG-BART.\" It seems a lot worse especially on BLEU and CIDER. It is nice to see a smaller model is beating a larger model on some metrics\n\"The difference between CALM and CALM (Joint) is that the former is initialized by the CALM(Mix).\" Did you mean to say latter instead of former? Also I don't see CALM (Join) in the table. I'm assuming this is CALM without Mix warmup\n\n\n#### Questions for the Authors\n1) How did you ensure shuffling the sentences still has grammatical correctness? A sentence like \"Running I am\" is not grammatically correct.\n2) Instead of a POS tagger why did you not use an NER extractor? Also wouldn't swapping different fruits into sentences like replacing \"Apples grow on trees\" with \"Watermelons grow on trees\" help with robustness\n3) And what point is the generative model good enough that it doesn't help to create distractor sentences\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097309, "tmdate": 1606915765869, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2398/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review"}}}, {"id": "jVQb_qIeQ1_", "original": null, "number": 13, "cdate": 1605833910016, "ddate": null, "tcdate": 1605833910016, "tmdate": 1605833910016, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "xLd9tWV12BS", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Look Forward to Hearing From You ", "comment": "Dear Reviewer 3,\n\nWe appreciate your valuable feedback and comments. We could improve our paper in a good way with your great feedback!!\n\nWe want to send you a friendly reminder that the first phase of the response period is completed, and want to hear back from you about our response.\n\nIf you could let us know any other concerns that we could not address in the response, we'd be happy to talk about it! Thanks again for your time, and your response is eagerly awaited.\n\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "elLy1OjESse", "original": null, "number": 12, "cdate": 1605833854876, "ddate": null, "tcdate": 1605833854876, "tmdate": 1605833854876, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "eDrEdwjZlZ0", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Look Forward to Hearing From You", "comment": "Dear Reviewer 4,\n\nWe appreciate your valuable feedback and comments.\nWe could improve our paper in a good way with your great feedback!!\n\nWe want to send you a friendly reminder that the first phase of the response period is completed, and want to hear back from you about our response.\n\nIf you could let us know any other concerns that we could not address in the response, we'd be happy to talk about it!\nThanks again for your time, and your response is eagerly awaited.\n\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "tcgpZXXGRl5", "original": null, "number": 4, "cdate": 1605468557786, "ddate": null, "tcdate": 1605468557786, "tmdate": 1605833282356, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "ePJMLI-Q-Bn", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (1/2)", "comment": "Thanks for your supportive review and valuable feedback!\n\n### Response to weakness:\n\n>Weakness 1: \n>\n>There should be a more comprehensive set of results completed to see how much improvement this model has. Mainly on the CommonGen class there should be some manual evaluation done similar in the original paper to see if the outputted sentences make sense and that the improvement in automatic metrics is carried over into human evaluation.\n\nThis is a good suggestion! To make the empirical results more convincing, we have conducted a set of additional experiments. Specifically, we have investigated whether our CALM model packs more concept-centric knowledge compared to the original T5-base, we conducted two well-recognized knowledge probing analysis of compared models: Language Model Analysis (LAMA) probe (Petroni et al.,2019) and Knowledge Intensive Language Task (KILT) (Petroni et al., 2020). The results are as follows (presented in Table 6 in the updated draft):\n\n- LAMA Probe\n|method    | MRR  | Precision@50| Precision@10| Precision@1|\n| ----------- | :-------: | :--------: | :-------: | :-------: |\n|T5-base   |11.53  |  38.52             |     21.60         |         5.93      |\n|CALM\t    | **12.09** |  **39.69**        |     **22.53**         |        **6.46**       |\t\n\n- KILT tasks (acc)\n|method | FEVER (fact verification )| AY2 (entity linking) | \n| ----------- | :-------: | :--------: |\n|T5-base|           76.65                     | 74.97                      |\n|CALM   |           **77.44**                     | **77.24**                    |\n\nWe can see CALM consistently outperforms the T5-base model across a set of knowledge-related tasks. \n\nWe have also conducted experiments with T5-large as a larger backbone model and find our method still improves the T5-large model by a consistent margin compared to that with the base-size backbone (CALM-large vs T5-large in Table 2):\n\n|method            | CSQA | OBQA  |   PIQA  | aNLI   |  CommonGEN                   |\n| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: |\n|T5-large           | 69.81  |  61.40  |  72.19   | 75.54 |  25.10/31.90/12.54/32.90 |\n|CALM-large\t  | **70.71**  |  **66.00**  |  **74.59**   | **76.96** |  **26.70**/**32.00**/**14.01**/**33.30** |\n\nAs for the CommonGen dataset, we\u2019ve included some qualitative examples in the draft that show sentences generated by CALM make more sense than that generated by T5-base. We\u2019re currently doing a human evaluation per your suggestion and will circle back in two or three days.\n\n**New results for Human Evaluation:**\nWe conducted a human evaluation of CommonGEN predictions.\nWe asked three annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.\nHere's the detail:\n- Number of test sentences: 50\n- Binary selection by majority voting\n- Kappa score (inter-annotator agreement) : 0.73\n- CALM-base : 0.6 / T5-base : 0.4\n\nWe can find that CALM predictions usually more reasonable than T5.\n\n>Weakness 2: \n>\n>A key aspect to look into is the robustness of this model. In the C2S approach the concepts were shuffled to generate the correct sentence. During inference time if the concepts were shuffled in a different manner would the model still be able to generate the correct sentences? There was three random seeds used but as was said \"the performance is sensitive to different random seeds.\" which seems that the model isn't as robust to newly seen inputs.\n\nSince the concepts are shuffled randomly and the corpus for pre-training is relatively large, we believe it will be robust to different orders for shuffling. As for the sensitivities of performance, it is not because the input is shuffled differently, instead, we suspect it is because the training sets of some datasets are small, which corresponds to the findings in (Dodge et al. 2020).\n\n==============================\\\nReference: \\\nPetroni et al.,2019 Language Models as Knowledge Bases? EMNLP 2019\\\nPetroni et al.,2020 Kilt: a benchmark for knowledge intensive language tasks arxiv\\\nDodge et al. 2020 Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "eDrEdwjZlZ0", "original": null, "number": 11, "cdate": 1605469616765, "ddate": null, "tcdate": 1605469616765, "tmdate": 1605754771693, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "lqCL5zL6oxW", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (3/3)", "comment": ">Concern 1: \n>\n>I am not convinced that the generated corrupted sentences are in general grammatically correct, as stated in Sec. 2.1,\n\nA large portion of the generated corrupted sentences are grammatically correct because nouns are only shuffled with nouns and verbs are shuffled with verbs. It is true that some of them are not grammatically correct because of issues in verb forms. However, we do not have to ensure the sentence to be ALWAYS grammatically correct because our objective aims to denoise even from corrupted sentences and we believe that the majority of noise in the corrupted sentences is the order and relation between concepts.\n\n>Concern 2: \n>\n>I do not see a strong connection between completing COR and compositional reasoning, as stated in Sec. 2.1.\n\nThanks for pointing it out. The COR task is more related to relational reasoning because it requires the model to correct the order between different concepts, which involves understanding and reasoning the relation between different concepts. It is the C2S task that is related to compositional reasoning because it requires the model to compose sentences with different compositions of concepts, according to (Lin et al. 2020). In the revised draft, we have corrected the claim. \n\n>Concern 3: \n>\n>The way of getting distractor sentences appears ad-hoc, may need further justification.\n\nGenerating distractors for contrastive learning in a GAN-style is related to previous work for language model pre-training like ELECTRA, which uses a masked language model to generate distractor tokens. This choice makes the distractor hard to distinguish and makes the training more informative. In addition, we chose to use the sentences generated by the model itself as distractors because the model exploits the knowledge that is already packed in its parameters to generate them. Therefore, training the model to distinguish them will force the model to acquire new commonsense knowledge that it lacks. In this way, the model is trained to iteratively improve upon itself in a self-play fashion. We think it may be a good choice and further investigation can be left for future work.\n\n>Concern 4: \n>\n>Y in equation (5) and (6) needs explanation\n\nWe\u2019ve added explanations in the revised version. Thanks for pointing this out!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "F91d7jcrsuo", "original": null, "number": 5, "cdate": 1605468693500, "ddate": null, "tcdate": 1605468693500, "tmdate": 1605724750117, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "tcgpZXXGRl5", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (2/2)", "comment": "### Response to detailed comments:\n\n- We\u2019ve made it clear that we used Spacy for both tokenization and POS tagging. \n- The T5-base (our implementation) uses different hyperparameter settings that suit better to base-size models compared to that - reported in the leaderboard. \n- We have corrected the claim about performance comparison with KG-BART.\n- We have made it clear by using the terms CALM(w/o mix warmup) thoroughly.\n\n### Response to questions:\n\n>Q1: \n>\n>How did you ensure shuffling the sentences still has grammatical correctness? A sentence like \"Running I am\" is not grammatically correct.\n\nAs described in the paper, we only shuffle concepts between the same category (i.e. shuffle nouns with nouns and verbs with verbs), so if the sentence is \u201capple grows on the tree\u201d, the corrupted sentence will be \u201ctree grows on the apple\u201d rather than \u201cgrows tree on the apple\u201d. This ensures most shuffled sentences grammatically correct.\n\n>Q2: \n>\n>Instead of a POS tagger why did you not use an NER extractor? Also wouldn't swapping different fruits into sentences like replacing \"Apples grow on trees\" with \"Watermelons grow on trees\" help with robustness\n\nThat\u2019s a good point! Actually, most entities can be extracted by POS tagging from the PROPNOUN tag. Therefore we did not use NER extractor since we do not need to know whether the entities are names or places. This saves some computation. Replacing concepts with noisy concepts that are similar to the original one is a good point for introducing extra noise signals. We are currently running experiments to verify this and will post the results during discussion or update them in the final version if it has not been done within the discussion period.\n\n>Q3: \n>\n>And what point is the generative model good enough that it doesn't help to create distractor sentences\n\nIn the joint training framework, the model exploits the knowledge that is already packed in its parameters to generate sentences. Even if the output sentences are of good quality, they may still differ from real sentences, and training the discriminator to distinguish them forces the model to acquire new commonsense knowledge. Therefore, the model is trained to iteratively improve upon itself in a self-play fashion.  In addition, generating distractors for contrastive learning in a GAN-style is related to previous work for language model pre-training like ELECTRA, which uses a masked language model to generate distractor tokens. This choice makes the distractor hard to distinguish and makes the training more informative.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "C0gWvfzRko", "original": null, "number": 6, "cdate": 1605468844081, "ddate": null, "tcdate": 1605468844081, "tmdate": 1605724645956, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "GFaBPjbgCWz", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for your supportive review and valuable feedback!\n\n>Weakness 1: \n>\n>One missing baseline is T5 trained to unshuffle entire sequences - given input with the tokens shuffled, generate the original sequence. This would show how much value c2s and cor are really adding. The current T5 baselines are all trained purely for infilling which seems a bit unfair compared to CALM which is generating entire sequences.\n\nTraining a text-to-text transformer with all tokens in the input shuffled and generating the original sentence is explored in the original T5 paper as a variant and the performance is significantly worse (see the Deshuffling variant in Table 4 in https://arxiv.org/pdf/1910.10683.pdf). We suspect this is because the task may be too difficult to train. We also conducted a variant of fine-tuning T5-base with this objective. The results are shown below:\n\n|method     | CSQA | OBQA  |   PIQA  | aNLI   |  CommonGEN                   |\n| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: |\n|T5-base       | 61.88  |  58.20  |  68.14   | 61.10 |  24.90/31.20/12.99/32.40 |\n|T5-base + deshuffling | 61.32  |  57.40  |  67.75   | 60.70 |  25.10/31.20/13.21/32.50 |\n|CALM | **63.32**  |  **60.90**  |  **71.01**   | **63.20** |  **26.40**/**31.40**/**13.88**/**33.00** |\n\nWe can see that the performance after fine-tuning sentence de-shuffling is lower than the original T5 except for CommonGEN which is similar to this task and performs significantly worse than CALM on all datasets. This confirms the effectiveness of our concept-centric objectives.\n\n>Weakness 2: \n>\n>Given a T5 model that can score sequences (maybe after training on the autoencoding objective), would it score \u201capples grow on trees\u201d higher than \u201ctrees grow on apples\u201d? If yes, then the model seems to already exhibit this style of reasoning. Would it score \u201capples grow on trees\u201d and \u201capples grow in the ground\u201d similarly? The distinction here is between sequences that are non-grammatical or unlikely to ever appear versus sequences that may have appeared (eg: \u201cpotatoes grow in the ground\u201d). Presently, (a) it\u2019s unclear if the designed objectives are providing commonsense reasoning above something the model can know from autoregressive language model scoring, and (b) it appears that the objectives are not designed to add relational commonsense knowledge of the sort where we know apples don\u2019t grow in the ground. \n\nGreat points! For the first question, according to (Trinh et al. 2018), a language model can help solve the Winograd Schema Challenge dataset and therefore possess this kind of knowledge, and that\u2019s why pre-training can improve the performance on commonsense-related datasets. However, as stated in the paper, these kinds of knowledge are implicitly learned during traditional pre-training and our method encourages the model to learn this knowledge more effectively. To further demonstrate this, we have conducted an experiment where we use CALM and T5-base to solve the Winograd Schema Challenge in an unsupervised setting by using the discrimination objective (prefix) of CALM to distinguish which sentence is correct and use T5-base to score the sentences respectively. We find that CALM outperforms T5-base by over 3 points on WSC, demonstrating our objectives provide more commonsense knowledge for the model.\n\nFor the second question, sentences in the training data are typically longer than the example and there may be many distractor concepts like the ground in the input concept-set. In this scenario, the model is encouraged to learn this kind of knowledge. Also, as suggested by Reviewer 1,  replacing concepts with noisy concepts that are similar to the original one is a great point for introducing extra noise signals. We are currently running experiments to verify this and will post the results during discussion or update them in the final version if it has not been done within the discussion period.\n\n>Weakness 3: \n>\n>c2s is designed specifically to do well on CommonGen. Are the gains on this task smaller than what you might have expected? If yes, why isn\u2019t it helping more?\n\nActually, the performance gain of CALM upon T5-base is more significant than it looks like. For BLEU and CIDEr, the improvement of CALM upon T5-base is almost half of that yielded by T5-large, which has nearly 4x parameters. Also, a possible reason is that CALM is trained in a multi-task fashion instead of C2S only. \n\n>Weakness 4: \n>\n>Figure 4 needs error bars, the dev sets are really small and it\u2019s hard to interpret which differences are significant.\n\nThe results in Figure 4 are not on dev sets but on test sets, which are larger. We\u2019ll add the error bar to make it more convincing. Thanks for your suggestion.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "LsBF7AEEFQX", "original": null, "number": 7, "cdate": 1605469029413, "ddate": null, "tcdate": 1605469029413, "tmdate": 1605724490362, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "HD2ykaLN-zh", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (1/2)", "comment": "Thanks for your insightful review and valuable feedback!\n\n>Weakness 1: \n>\n>The scores reported by the base & proposed models are so far below SoTA it is unclear where these techniques will generalize to models that are likely to be used.\n\nThank you for the comment! We admit that the performance of base-size CALM and T5 are far from SOTA numbers. However, we believe that the performance comparison of base size pre-trained models is meaningful because for SOTA models that contains 11B parameters:(1) the computational cost for continually pre-training them with our method is very computationally expensive, and (2) they are unlikely to be used in real-world applications because T5-3B and T5-11B model can\u2019t be fitted in the GPU memory for inference even batch size set as 1 and the inference latency would be unsatisfactory. This is also suggested by the fact that the most popular (in terms of download times at huggingface\u2019s model hub) pre-trained models are mostly base size models.\n\nTo test whether our proposed method can generalize to larger models, we continually pre-trained CALM-large with the T5-large model as the backbone after the submission deadline. The result comparison between CALM-large and T5-large is shown as follows (also see Table 2 in the revised draft).\n\n- Results of discriminative tasks\n|     method                  | #parameters      |    CSQA    |    OBQA   |   PIQA   |    aNLI    |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|     T5-large                 |      774M                | 69.81  |  61.40  |  72.19   | 75.54 |\n|     CALM-large\t       |      774M                | 71.31  |  66.00  |  75.11   | 77.12 |\n|     BERT-large            |      345M                | 57.06  |  60.04  |  67.08   | 66.75 |\n|     RoBERTa-large     |      345M                | 71.81  |  63.90  |  76.90   | 82.35 |\n|     SOTA                      |      11B                   | 79.1    |  87.2    |  90.13   | 89.70 |\n\n- Results on CommonGEN\n|method                                 | # parameters    |  BLEU4|  METEOR | CIDEr | SPICE |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|BART                                      |  406M                 |  26.30  |  30.90       | 13.92  | 30.60 | \n|T5-large                                 |   774M                |  28.60  |  30.10       | 14.96  | 31.60 |\n|CALM-large\t                       |   774M                |  29.50  |  31.90       | 15.61  | **33.20** |\n|KG-BART (SOTA, with KG)   |  406M                 |  **30.90**  |  **32.40**       | **16.83**  | 32.70 | \n\nWe can see that **CALM-large consistently improves upon T5-large on all tested datasets (statistically significantly with p-value < 0.01)**. The improvement is consistent with the improvement of CALM-base upon T5-base. This shows that our method can hopefully generalize well to even larger models like T5-3B and T5-11B.\n\nWe have also tested the effectiveness of CALM on other tasks and datasets to make the empirical results more convincing. Specifically, we have investigated whether our CALM model packs more concept-centric knowledge compared to the original T5-base, we conducted two well-recognized knowledge probing analysis of compared models: Language Model Analysis (LAMA) probe (Petroni et al.,2019) and Knowledge Intensive Language Task (KILT) (Petroni et al., 2020). The results are as follows (presented in Table 6 in the updated draft):\n\n- LAMA Probe\n|method    | MRR  | Precision@50| Precision@10| Precision@1|\n| ----------- | :-------: | :--------: | :-------: | :-------: |\n|T5-base   |11.53  |  38.52             |     21.60         |         5.93      |\n|CALM\t    | **12.09** |  **39.69**        |     **22.53**         |        **6.46**       |\t\n\n- KILT tasks (acc)\n|method | FEVER (fact verification )| AY2 (entity linking) | \n| ----------- | :-------: | :--------: |\n|T5-base|           76.65                     | 74.97                      |\n|CALM   |           **77.44**                     | **77.24**                    |\n\nWe can see CALM consistently outperforms the T5-base model across a set of knowledge-related tasks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "lqCL5zL6oxW", "original": null, "number": 10, "cdate": 1605469502764, "ddate": null, "tcdate": 1605469502764, "tmdate": 1605724355076, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "qC8UVJc3Uy", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (2/3)", "comment": "As for other pre-trained models, we have continually pre-trained BART-base models with our objectives and tested its performance on different datasets. The results are shown as follows. We can see that our approach consistently and significantly (with p-value < 0.01) improves BART-base on all datasets. This result shows that our method is versatile to different pre-trained models.\n\n- Results with BART-base as backbone model (139M parameters)\n|method            | CSQA | OBQA  |   PIQA  | aNLI   |  CommonGEN                 | \n| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: |\n|BART-base      | 56.31| 58.30 |  67.53|59.85 |  25.10/29.50/13.16/30.20  | \n|CALM-bart\t | **58.22**| **59.10** |  **69.40**|**61.28** |  **26.40**/**29.90**/**13.71**/**31.10**  | \n\n>Weakness 2: \n>\n>Noun and verb phrases extracted from sentences are not always concepts. Masking-out certain words in the input is similar to the idea of removing non-content words from input. A deeper analysis of the proposed method would have been nice to understand which part is effective in the new task, keeping content words or not using mask-out, what if only concepts from a knowledge base are kept instead of content words?\n\nThat\u2019s a good point. We have tested the variant for the C2S objective where only concepts from ConceptNet are kept. The performance on CSQA, and PIQA is lower than the version used in our paper, as shown in the Table below:\n\n|method                                                 |CSQA |   PIQA  |\n| ----------- | :-------: | :--------: | \n|CALM-C2S (ours: nouns + verbs)                 | **62.24**| **68.75** |\n|CALM-C2S (concepts in ConceptNet)  |61.88 | 68.29  |\n\nWe can see that our method outperforms this variant. We find that when only keeping concepts in ConceptNet, the input concepts are fewer and make the input information not enough for recovering the original sentence. Specifically, for more than 40% of the sentences, the concept-set extracted by ConceptNet is smaller than that of all content words by at least 2 concepts.\n\nAlso, we have conducted an ablation study about using either verbs or nouns as concepts. The results are shown as follows:\n\n|method            | CSQA | OBQA  |   PIQA  | aNLI   |  CommonGEN                   | \n| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: |\n|CALM              | **63.32**| **60.90** |  **71.01**|**63.20** |  **26.40**/**31.40**/**13.88**/**33.00**  |\n|CALM-noun\t | 62.45| 59.40 |  69.05|61.55 |  25.70/31.20/13.17/32.60  |\n|CALM-verb\t | 62.51| 59.10 |  69.24|61.40 |  25.60/31.20/13.24/32.60  |\n\nWe can see that using either nouns-only or verbs-only as concepts for our approach leads to a substantial performance drop. This supports our choice of using both nouns and verbs as concepts.\nIn addition, our objectives are not merely about \u201cmasking\u201d and denoising, but they compose sentences with concepts.\n\n>Weakness 3: \n>\n>The CALM model proposed in this work performs worse than the SOTA models in three out of four metrics on CommonGen, despite it uses less model parameters. What if the proposed tasks are applied on T5-Large?\n\nWe have conducted experiments on T5-large and the results are shown in response 1. With T5-large, CALM-large\u2019s performance is comparable with the SOTA model (KG-BART).  Also, KG-BART exploits an external knowledge base for fine-tuning and inference. In contrast, our method does rely on any external KB and make it easier to use in practice. In addition, CALM-large outperforms SOTA on the SPICE metric, which is the most important metric on CommonGEN because it correlates the best with human evaluation, according to section 5.1 in the original paper of CommonGEN (Lin et al. 2020).\n\n==============================\\\nReference:\\\nCommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning  Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi and Xiang Ren  In Findings of EMNLP 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "qC8UVJc3Uy", "original": null, "number": 9, "cdate": 1605469292494, "ddate": null, "tcdate": 1605469292494, "tmdate": 1605724309787, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "0AAGc4nty-t", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (1/3)", "comment": "Thanks for your insightful review and valuable feedback!\n\n>Weakness 1: \n>\n>The key concern about the paper is the lack of rigorous experimentation to study the effectiveness of the two self-supervised learning tasks. First, the methods are only compared with T5-base related methods on the four commonsense classification tasks. The leaderboard of commonsense QA shows that more than 20 systems report an accuracy higher than 63.32, which is the best configuration of the proposed method. Second, the proposed tasks are applied only to T5. I am wondering if it is effective on the other pre-trained language models. Third, the performance improvement on the classification tasks appears marginal. Statistical tests are desirable to show if such improvements are significant.\n\nIt is true that the performance of base-size CALM and T5 is far from SOTA. However, it is notable that the SOTA models have 11B parameters, which is around 50 times more than our base size models.\n\nTo test whether our proposed method can generalize to larger models, we continually pre-trained CALM-large with the T5-large model as the backbone after the submission deadline. The result comparison between CALM-large and T5-large is shown as follows (also see Table 2 in the revised draft).\n\n- Results of discriminative tasks\n|     method                  | #parameters      |    CSQA    |    OBQA   |   PIQA   |    aNLI    |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|     T5-large                 |      774M                | 69.81  |  61.40  |  72.19   | 75.54 |\n|     CALM-large\t       |      774M                | 71.31  |  66.00  |  75.11   | 77.12 |\n|     BERT-large            |      345M                | 57.06  |  60.04  |  67.08   | 66.75 |\n|     RoBERTa-large     |      345M                | 71.81  |  63.90  |  76.90   | 82.35 |\n|     SOTA                      |      11B                   | 79.1    |  87.2    |  90.13   | 89.70 |\n\n- Results on CommonGEN\n|method                                 | # parameters    |  BLEU4|  METEOR | CIDEr | SPICE |\n| ---------------------------   |   :-------:   |   :--------:   | :-------: | :-------: | :-------: |\n|BART                                      |  406M                 |  26.30  |  30.90       | 13.92  | 30.60 | \n|T5-large                                 |   774M                |  28.60  |  30.10       | 14.96  | 31.60 |\n|CALM-large\t                       |   774M                |  29.50  |  31.90       | 15.61  | **33.20** |\n|KG-BART (SOTA, with KG)   |  406M                 |  **30.90**  |  **32.40**       | **16.83**  | 32.70 | \n\nWe can see that CALM-large consistently improves upon T5-large on all tested datasets (statistical significantly with p-value < 0.01). The improvement is consistent with the improvement of CALM-base upon T5-base. This shows that our method can hopefully generalize well to even larger models like T5-3B and T5-11B.\n\nIn addition, we have conducted statistical tests and find **the improvement of CALM (base size) upon T5-base to be significant with p-value < 0.01 on all classification datasets**. Also, we believe that the performance comparison of base size pre-trained models is meaningful because for SOTA models that contains 11B parameters: (1) the computational cost for continually pre-training them with our method is very computationally expensive (it will take around one month on 8xV100s), and (2) they are unlikely to be used in real-world applications because T5-3B and T5-11B model can\u2019t be fitted in the GPU memory for inference even batch size set as 1 and the inference latency would be unsatisfactory. This is also suggested by the fact that the most popular (in terms of download times at huggingface\u2019s model hub) pre-trained models are mostly base size models.\n\nWe have also tested the effectiveness of CALM on other tasks and datasets to make the empirical results more convincing (See General Response NewTable 4). We can see CALM consistently outperforms the T5-base model across a set of knowledge-related tasks.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3k20LAiHYL2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2398/Authors|ICLR.cc/2021/Conference/Paper2398/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848870, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Comment"}}}, {"id": "0AAGc4nty-t", "original": null, "number": 4, "cdate": 1604211950836, "ddate": null, "tcdate": 1604211950836, "tmdate": 1605024220833, "tddate": null, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "invitation": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review", "content": {"title": "The paper suggests an interesting redirection for commonsense reasoning, but lacks of rigorous experimentation to justify the contributions.", "review": "This paper proposes two self-supervised pre-training tasks to further pre-train a pre-trained language model for commonsense reasoning. The first task is called concept-to-sentence generation, which reconstructs input sentences from noun/verb phrases extracted from the input. The second task is called concept order recovering, which predicts original sentences after shuffling the order of noun/verb phrases in input sentences. Experimental results show that the pre-trained language models fine-tuned with the two proposed tasks can lead to improvement on five commonsense reasoning benchmark datasets.\n\nStrengths: \n\n+ The idea of teaching language models through self-supervised learning tasks is neat.\n\n+ The performance of the proposed methods on few training examples looks great.\n\n+ The results section is well structured. There are ablation studies on the training objectives of each proposed task as well as a comparison of generated sentences.\n\n \nConcerns: \n\n- The key concern about the paper is the lack of rigorous experimentation to study the effectiveness of the two self-supervised learning tasks. First, the methods are only compared with T5-base related methods on the four commonsense classification tasks. The leaderboard of commonsense QA shows that more than 20 systems report an accuracy higher than 63.32, which is the best configuration of the proposed method. Second, the proposed tasks are applied only to T5. I am wondering if it is effective on the other pre-trained language models. Third, the performance improvement on the classification tasks appears marginal. Statistical tests are desirable to show if such improvements are significant.\n \n- Noun and verb phrases extracted from sentences are not always concepts. Masking-out certain words in the input is similar to the idea of removing non-content words from input. A deeper analysis of the proposed method would have been nice to understand which part is effective in the new task, keeping content words or not using mask-out, what if only concepts from a knowledge base is kept instead of content words? \n \n-  The CALM model proposed in this work performs worse than the SOTA models in three out of four metrics on CommonGen, despite it uses less model parameters. What if the proposed tasks are applied on T5-Large?\n\nMinor comments: \n\n* I am not convinced that the generated corrupted sentences are in general grammatically correct, as stated in Sec. 2.1, \n\n* I do not see a strong connection between completing COR and compositional reasoning, as stated in Sec. 2.1.\n\n* The way of getting distractor sentences appears ad-hoc, may need further justification.\n\n* Y in equation (5) and (6) needs explanation.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2398/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2398/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "authorids": ["~Wangchunshu_Zhou1", "~Dong-Ho_Lee1", "~Ravi_Kiran_Selvam1", "~Seyeon_Lee1", "~Xiang_Ren1"], "authors": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Xiang Ren"], "keywords": ["Language Model Pre-training", "Commonsense Reasoning", "Self-supervised Learning"], "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.", "one-sentence_summary": "We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|pretraining_texttotext_transformers_for_conceptcentric_common_sense", "supplementary_material": "/attachment/95c40476c9644ca56b6dc6cbf56ee5c2a64eb8fd.zip", "pdf": "/pdf/30f24a224a3d4133f7da640c76644f91a3d41f0a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021pretraining,\ntitle={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},\nauthor={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3k20LAiHYL2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3k20LAiHYL2", "replyto": "3k20LAiHYL2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097309, "tmdate": 1606915765869, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2398/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2398/-/Official_Review"}}}], "count": 22}