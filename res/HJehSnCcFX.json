{"notes": [{"id": "HJehSnCcFX", "original": "rye6KapqtQ", "number": 1582, "cdate": 1538088004477, "ddate": null, "tcdate": 1538088004477, "tmdate": 1545355391935, "tddate": null, "forum": "HJehSnCcFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1gSGx3xeV", "original": null, "number": 1, "cdate": 1544761356986, "ddate": null, "tcdate": 1544761356986, "tmdate": 1545354519103, "tddate": null, "forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Meta_Review", "content": {"metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review for neural Hawkes particle smoothing paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1582/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352784338, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352784338}}}, {"id": "HygtSCgqAQ", "original": null, "number": 12, "cdate": 1543274049304, "ddate": null, "tcdate": 1543274049304, "tmdate": 1543274049304, "tddate": null, "forum": "HJehSnCcFX", "replyto": "BJexOsOl0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "added short appendix G with new MNAR experiments", "comment": "We wrote:\n> We can also add experiments with c_k = 0.5 to cement the expository point.\n\nWe have now added these experiments, which constitute new Appendix G in the supplementary material.  \nIn these experiments, events are missing stochastically rather than deterministically.\nWe find that the method still works and has the same qualitative behavior. \n\n(We have renamed the missingness probability c_k to \\rho_k.  See the start of Appendix E for the notation.)\n\nThis setting is definitely MNAR.  A naive reader might protest that it appears to be MCAR, because whether an event is missing always has probability 0.5, independent of the type of the event.  However, this is the subtle point at issue.  It is MNAR because as we noted in the previous comment, \"the second factor of (3) ... decreases exponentially in the number of missing events |z|.\"  (If you're still not convinced, reread our \u201cpresentation of MAR and MNAR\u201d response.  We will work this into the final version of the paper, of course.)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "H1lDaiOgAm", "original": null, "number": 10, "cdate": 1542650815388, "ddate": null, "tcdate": 1542650815388, "tmdate": 1542837243378, "tddate": null, "forum": "HJehSnCcFX", "replyto": "Hyxi0YA_nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: experimental evaluation (1/4)", "comment": "Thanks for the suggestions:\n\n> The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  \n\nNote that we do show absolute performance on our downstream task (namely, imputation of missing events), via the axis labels in Figure 3.  Figure 3 also shows the impact of particle smoothing on this downstream task.\n\nOur Figure 2 also shows \u201cabsolute performance\u201d on the axes, measured as log q(z* | x) where q is the proposal distribution.  It\u2019s true that these numbers are hard to interpret.  Ideally we would compare them to log p(z* | x), since that would be the ideal proposal distribution.  But unfortunately it is intractable to compute that conditional probability: even for synthetic data we are only able to compute the joint probability p(x, z*).  Do you have any ideas?\n\n> Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.\n\nRegarding sample weights, we can report the effective sample size in the final version.  Our effective sample size is excellent for the synthetic datasets, about 20-25 for M=50 particles. On real datasets, ESS increased roughly as sqrt(M) as we varied M from 20 to 2000 in pilot experiments.  Unfortunately it was very low for M=50, the value of M that we used in our final experiments (ESS of 1.5 to 2.2 on average), yet we still got good imputation results on our task.  We might be able to raise the ESS by combining our method with multinomial resampling or local search.\n\nRegarding theoretical and wall-clock runtime, please see our separate response \u201cSubject: efficiency of Monte Carlo methods\u201d.  The TL;DR is that the runtime complexity is O(MI) where M is the number of particles and I is the number of observed events.  In practice, we generate the particles in parallel, leading to acceptable speeds of 300-400ms per event for the final method.  We can add this information to the final version of the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "BJexOsOl0m", "original": null, "number": 7, "cdate": 1542650727747, "ddate": null, "tcdate": 1542650727747, "tmdate": 1542808444291, "tddate": null, "forum": "HJehSnCcFX", "replyto": "Hyxi0YA_nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: MNAR data experiments (4/4)", "comment": "> We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown. \n\nSure, working with MNAR data is impossible without additional knowledge. But in our setting, we have that additional knowledge.\n\nThe problem with MNAR is that JOINTLY identifying p and p_miss is impossible. If you observe few 50-year-olds on your survey, you can't know (beyond your prior) whether that\u2019s because there are few 50-year-olds, or because 50-year-olds are very likely to omit their age. \n\nBut joint identification is unnecessary if either\n(1) one has separate knowledge of the missingness distribution p_miss\n(2) one has separate knowledge of the complete-data distribution p\n\nThat is: If we know at least one of the distributions, then we can still infer the other. Actually, both (1) and (2) hold in our present experiments.\n\nThe E step of EM uses the current guess of p and p_miss to infer the posterior distribution of the missing values. That posterior is uncontroversially defined by the simple Bayesian formula (3).\n\n(1) If p_miss is known and fixed, this gives a minor variant of ordinary EM. Ordinary EM makes the MAR assumption that the p_miss factor of (3) can be ignored. But we don't need to ignore p_miss if we actually know it! In our experiments, p_miss is MNAR but we do know it: we know that events of some types are always observed and events of other types are never observed. So, no problem!\n\n(2) Conversely, if p is known because we estimated it FROM SOME COMPLETE DATA, then we can use incomplete data to learn the MNAR missingness distribution p_miss. This setting even lets us learn a fancy missingness mechanism, e.g., some BiLSTM model that uses the context of an event to determine the probability of censoring it. \n\nWe relegated this EM discussion to Appendix F since it is not used in our experiments. Appendix F says: \u201cIn the more general MNAR scenario, we can extend the E-step to consider the not-at-random missingness mechanism (see equation (7b) below), but then we need both complete and incomplete sequences at training time in order to fit the parameters of the missingness mechanism (unless these parameters are already known) jointly with those of the neural Hawkes process.  ... we describe the methods and provide MCEM pseudocode.\u201d  \n\n> If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.\n\nAlas (as we mentioned in the \u201cpresentation of MAR and MNAR\u201d response), for missing data in event streams, nearly *every* setting is MNAR!  That is, the probability that z would be selected for censorship depends on the number and type of events in z. In particular, the second factor of (3) typically decreases exponentially in the number of missing events |z|, so it is not constant in z as required for MAR.\n\nIn particular, our experimental setting is MNAR in a sense described at the end of this response. Because it happens to be a special case of MNAR, it would be possible through a notational trick to gloss over the MNAR issue and not call the reader\u2019s attention to it. However, we thought this would be dangerous, so we would prefer to clarify this aspect of the exposition rather than deleting it. (We did relegate part of the discussion to an appendix.)\n\nWhy dangerous?  We imagine that a reader might try to apply our method to a fairly simple situation where each event of type k has independent probability c_k of being censored. We fear that the reader might carelessly omit the p_miss factor if we don\u2019t talk about it. However, that factor is necessary to avoid proposing too many missing events of those types that would NOT tend to be censored. \n\nE.g., proposing 100 missing events of type k means that p_miss includes a factor of c_k ^ 100. Thus, for c_k < 1 and especially for c_k << 1, the system should prefer -- other things equal -- to posit only 50 missing events. Intuitively, for 50 events to all have gone missing is not as improbable as for 100 events to have gone missing.\n\nIt\u2019s true that our reported experiments happen to have c_k = 1 (that is, events of type k are *deterministically* missing), so this exponential decay does not occur: c_k ^ 100 == c_k ^50. Nonetheless, to ensure that a future reader would handle the general case correctly, we prefer to give it some discussion. We can also add experiments with c_k = 0.5 to cement the expository point.\n\nEven our deterministic setting should still be regarded as MNAR, because c_k isn\u2019t *always* 1 in our experiments. Rather, c_k = 1 or 0 depending on k. Thus, our p_miss factor can be either 1 or 0 (making us MNAR). More precisely, p_miss = 0 if z includes events of a type k that would never go missing. This is the technical reason that our code never proposes such events \u2014 as explained at the bottom of page 14."}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "SJloisdeC7", "original": null, "number": 9, "cdate": 1542650787371, "ddate": null, "tcdate": 1542650787371, "tmdate": 1542650787371, "tddate": null, "forum": "HJehSnCcFX", "replyto": "Hyxi0YA_nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: clarity of notation (2/4)", "comment": "\nWe worked really hard on the exposition, including months of tinkering with the writing and getting feedback from colleagues.  Perhaps the subject matter is difficult, but we really worked to make it as clear as we could, and we stand by our presentational choices.\n\nYour specific objections seem to be a matter of taste -- but please recognize that they were intended to *improve* clarity.  We\u2019re happy to debate the best notation, but please don\u2019t reject a technical paper on this basis?  It\u2019s not as if our notation was careless or incomplete.  The use of \u201cComp\u201d, \u201cObs\u201d, and \u201cMiss\u201d as random variable names was supposed to be more mnemonic than C, O, and M.  The notation k@t denotes an event of type k at (\u201c@\u201d) time t.  This was supposed to be an improvement on Mei & Eisner\u2019s <k,t> notation because it distinguishes this kind of ordered pair from other kinds of ordered pairs (similar to the use of sigils in programming languages); it was suggested by a colleague.  \n\n> It's not clear what p (\"the data model\") and p_miss (\"the missingness mechanism\") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol. \n\nThis is spelled out carefully at the start of section 2 and around equation (1).  The generative story has two steps.  First, the data model p generates a complete event sequence Comp.  Then p_miss decides which of these events get revealed to the user.  \n\nObs = x is the resulting subsequence of revealed (observed) events, and Miss = z is the subsequence of unrevealed (missing) events.  In other words, the missingness mechanism partitions Comp into Obs and Miss.  p(x,z) is the joint probability of getting a particular complete event sequence x v z  *and* partitioning it into x and z.  We particularly needed the notation x because x is the sequence that our particle smoother reads from right to left.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "HyxZ9oOeAX", "original": null, "number": 8, "cdate": 1542650761204, "ddate": null, "tcdate": 1542650761204, "tmdate": 1542650761204, "tddate": null, "forum": "HJehSnCcFX", "replyto": "Hyxi0YA_nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: presentation of MAR and MNAR (3/4)", "comment": "\n> In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as \u2026\n\nThis is indeed a subtle point, one that we are proud to have handled correctly.  If you did not find our exposition clear, we will revise the camera-ready to lay out the issues more plainly.  Let\u2019s start in this response.\n\nLittle & Rubin\u2019s MCAR/MAR/MNAR taxonomy was meant for graphical models.  (Murphy\u2019s textbook just recapitulates this standard taxonomy.)  A graphical model has a fixed set of random variables, and the missingness mechanisms envisioned by Little & Rubin simply decide which of those variables to reveal.\n\nWe could have chosen to formulate our model in these terms, by using uncountably many random variables K_t where t ranges over the set of times.  K_t = k if there is an event of type k at time t, and otherwise K_t = 0.  Then a missing event corresponds to an unobserved variable K_t with value > 0. Values of 0 are never observed because we are never told that an event did *not* happen at time t.   Some values > 0 are observed and some are not.  Since the missingness of K_t depends on whether K_t > 0, this setting is ordinarily MNAR. \n\nHowever, we prefer to formulate our model in terms of the finite sequences that are generated or read by our LSTMs.  This improves the notation later in the paper.  \n\nFrom that point of view, unfortunately, the complete draws from p are not fixed-length vectors as in a graphical model: different draws from p can have different numbers of events.  This is why our notation does not use a simple \u201cmissingness vector\u201d of fixed finite length as in the standard notation.  A missing event is not a case of a variable whose value is missing (e.g., an event of unknown type): we don\u2019t even know whether the variable (event) exists in the first place!\n\nYet our treatment of MAR is the correct generalization of Little & Rubin\u2019s: namely, it\u2019s the case in which the second factor of (3) can be ignored.  (The ability to ignore that factor is precisely why anyone cares about the MAR case!)  This is discussed around equation (3) and in Appendix F.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "ryeByi_e0X", "original": null, "number": 6, "cdate": 1542650589190, "ddate": null, "tcdate": 1542650589190, "tmdate": 1542650589190, "tddate": null, "forum": "HJehSnCcFX", "replyto": "H1x57ne627", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: minor misunderstanding (1/5)", "comment": "\n> Experiments on synthetic datasets with 10 different initializations and two real datasets\n\nTo be precise, it\u2019s 10 completely different synthetic datasets.  Each dataset is drawn from a different distribution with randomly selected parameters.  Those distributions are not trained, so it\u2019s odd to speak of \u201cinitializations.\u201d\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "r1l6a9ueCm", "original": null, "number": 5, "cdate": 1542650564619, "ddate": null, "tcdate": 1542650564619, "tmdate": 1542650564619, "tddate": null, "forum": "HJehSnCcFX", "replyto": "H1x57ne627", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: distributions other than NHP (2/5)", "comment": "\n> The proposed technique is tightly connected to NHP \u2026 Can the proposed method also be applied to other processes?\n\nThanks for the question. Yes, certainly.  The main technique is to use particle filtering or smoothing to sample from the posterior over complete sequences.\n\nParticle filtering is applicable to any temporal point process where the number of events is finite with probability 1, and where it is tractable to compute (or estimate) the log-likelihood of a prefix of a complete sequence.\n\nTo extend this to particle smoothing, we developed a particular family of proposal distributions that is based on a continuous-time LSTM, as well as a method (Algorithm 1) for sampling proposals from such a distribution in the context of particle filtering.\n\nOur experiments use an NHP *model* of the complete sequence, together with a *proposal distribution* whose architecture happens to be almost identical to the NHP architecture (in mirror image, as it reads the future observed events from right to left).  However, *** our proposal distribution could also be used with other models ***: thus, we would also recommend it for particle smoothing of temporal point processes beyond just the NHP!\n\nThe job of the proposal distribution is to get a good fit to the model\u2019s complex posterior predictive distribution of the next event (which is defined by an integral over possible completions of the incompletely observed future).  A highly parameterized neural proposal distribution family like ours is designed to be flexible enough to do this, at least for non-pathological models. \n\nOne caveat: Our proposal distribution does also take the state of the original point process into account.  In our case, that is the state of the NHP (h(t) in equation (9).  If you were using a different point process, you would need to replace h(t) with some other sufficient statistic of the history H(t).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "HygEs9de0X", "original": null, "number": 4, "cdate": 1542650524140, "ddate": null, "tcdate": 1542650524140, "tmdate": 1542650524140, "tddate": null, "forum": "HJehSnCcFX", "replyto": "H1x57ne627", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: efficiency of Monte Carlo methods (3/5)", "comment": "\n> It turns out that the integral part of Equation 5 does not have an obvious analytical solution\n> under NHP. Then, we first need a set of  samples to approximate the likelihood evaluation.\n\nWell, this part of our method simply follows the algorithm given in the NHP paper (Mei & Eisner, NIPS 2017, sections B.2 and C.2), as we mention in our Appendix A (\u201cintegral computation\u201d).  \n\nComputationally it is not a problem.  A given run of particle smoothing begins by drawing O(I) time points from Uniform([0,T]), where I is the number of observed events.  All particles are evaluated using integrals that are estimated by evaluating the function at these time points.  \n\n(Using the same time points for all particles gives a paired comparison that reduces the variance of the normalized importance weights.  Note also that because we sample time points uniformly, longer intervals between imputed events will tend to contain more points, which is appropriate.)\n\n> Later, we also need to sample particles. \n\nOur GPU implementation (which we will release) parallelizes the outer loop over particles.  We sample 50 particles in parallel in these experiments, but we have tested with 1000 particles in parallel as well.  So this is not a real problem with a GPU.\n\n> I am not quite convinced the computational efficiency of this approach in real applications of practice.\n\nWe reported experiments that we performed to demonstrate the practicality.  \nOn average, drawing an ensemble of 50 particles takes \n5s per example on the synthetic datasets (average length 15 events)\n12s per example on the NYC taxi dataset (average length 32 events)\n100s per example on the elevator dataset (average length 313 events)\n\nThat is, 300-400 ms per event.  Such speeds are acceptable in many incomplete data applications, compared to the cost of collecting complete data.  Consider the applications on page 1 of the paper, all of which involve real-time decision making at a human timescale.\n\n> Also, there is no analysis either empirically or analytically about the impact of the \n> accumulative sampling errors on the inference performance. \n\nMei and Eisner (2017, Appendix C.2) found that rather few samples could be used to estimate the integral: even sampling at only I time points gave a standard deviation of log-likelihood that was on the order of 0.1% of absolute (Mei, p.c.).\n\nWhat kind of \u201caccumulative sampling errors\u201d are you concerned about?  Remember that our integral estimate is *unbiased*, and the particle filtering estimate is at least consistent.  (Although it is true that the normalized particle weights are distorted both by the finite number of particles and the variance in the integral estimates, the variance of the integral estimation decreases---rapidly---as O(1/n) where n is the # of sampled time points.)\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "HJeOuculCX", "original": null, "number": 3, "cdate": 1542650479917, "ddate": null, "tcdate": 1542650479917, "tmdate": 1542650479917, "tddate": null, "forum": "HJehSnCcFX", "replyto": "H1x57ne627", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: training the proposal distribution (4/5)", "comment": "\n> Furthermore, to learn the proposed distribution, the paper applies the REINFORCE algorithm\n> under the proposed distribution q. But REINFORCE is known for large variance issue. \n\nThis is a misunderstanding by the reviewer.  Following Lin & Eisner (2018), we use an interpolation of exclusive and inclusive KL divergence (equation (12)).\n\nREINFORCE corresponds to exclusive KL, which does have a variance issue.\n\nBut in practice, our tuned interpolation coefficient placed *all* the weight on inclusive KL, which has no variance issue.  (This fact is reported as \u201cbeta=1\u201d under equation (12).)  So our experiment effectively avoids REINFORCE altogether.  (Your comment may be the reason that beta=1 worked best for us, but see an alternative discussed below.  Note that Lin & Eisner found that beta < 1 worked best in their setting.)\n\n> Given that we already need lots of samples for the likelihood, it is unclear to me how \n> stable the algorithm could be in practice.\n\nWe\u2019re not sure what you mean here by \u201cstable.\u201d  Yes, we have a sampling-based method, but so do most people in the field right now!  As you know, stochastic gradient methods always make use of \u201clots of samples.\u201d  Remember that SGD works because the errors average out to 0 over many stochastic gradient steps.  (If you don\u2019t believe that, you should be rejecting all the deep learning papers that use SGD, right??)\n\nSGD methods succeed, both theoretically and practically, with even high-variance estimates of the batch gradient (e.g., where each stochastic estimate is derived from a *single* randomly chosen training example).  Thus, we should be fine with a noisy sampling-based gradient as long as it is *unbiased*.  \n\nOur Monte Carlo integral estimates (taken from Mei & Eisner 2017, Appendix B.2) are in fact unbiased.  And as a result, our stochastic gradient estimate is also unbiased, as required (assuming that the observed complete data are distributed according to p).   Why?  Since beta=1, our stochastic gradient is simply (10).  No particle filtering or smoothing is used to estimate (10), because we train it using observed complete data, as explained in the last long paragraph of section 3.2.1.  The only randomness is the integral over [0,T] (similar to the one in (5)) that is required to estimate the term log q(z | x) in (10) \u2026 and as just noted, this integral estimate is unbiased.\n\n(It is true that if beta were < 1, we would compute the exclusive KL gradient using particle filtering or smoothing with M particles, and this would introduce bias in the gradient.  Nonetheless, since the bias vanishes as M goes to infinity, it would be possible to restore a theoretical convergence guarantee by increasing M at an appropriate rate as SGD proceeds -- see Spall (2003), p. 107.)\n\nAs for whether the training algorithm could work \u201cin practice\u201d -- did you see the beautiful figure 2?  Our training method certainly appears to succeed \u201cin practice.\u201d  The trained proposal distribution is better on *virtually every example* in *12 different datasets*!  We as reviewers would be quite inclined to accept a paper with such clear results \u2026 \n\nFinally, recall that the paper has 3 algorithmic contributions: particle filtering, particle smoothing, and consensus decoding (as well as introducing a useful problem setting along with a well-thought-out evaluation metric).  Your question here about training the proposal distribution pertains only to particle smoothing.  Even if there were a problem here, the other contributions would stand.  But in fact, we see no problem here: we clearly demonstrate the value of training a proposal distribution.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "rJlf45ulRm", "original": null, "number": 2, "cdate": 1542650410451, "ddate": null, "tcdate": 1542650410451, "tmdate": 1542650410451, "tddate": null, "forum": "HJehSnCcFX", "replyto": "H1x57ne627", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: experimental evaluation (5/5)", "comment": "\n>  it is unfair to only compare the smoothing approach with the filtering baseline. \n\nWell, what other baseline do you think we should compare with?  There is not a lot of previous work on this problem.\n\nWe can see that Metropolis-Hastings would be a possible alternative, where the transition kernel proposes a single-event change (insert, delete, or move).  Unfortunately, this would be quite slow for a neural model like ours.  The reason is that a proposed change early in the sequence will affect the LSTM state and hence the probability of all subsequent events.  Thus, a single move takes O(length of proposed complete sequence) time to evaluate.  Furthermore the Markov chain may mix slowly because a move that changes only one event may often lead to an incoherent sequence that will be rejected.  The point of particle smoothing is essentially to avoid this kind of rejection by proposing a *coherent sequence of events* from an approximation q to the true posterior.  We can ensure that it is coherent because we build it up from left to right (taking the future into account).\n\nWe\u2019d be happy of course to propose Metropolis-Hastings as future work.  It could even build on our present work by using a variant of our current proposal distribution as the core of a Metropolis-Hastings kernel -- which would resample the latent events on a given *interval*.  However, we would be wary of developing this nontrivial extension within the current paper; it is not an established baseline and would take a few additional pages to develop.  The current submission already has too much material -- there are a lot of appendices, and the other reviewers seem to have found the submission to be overwhelming already.\n\nAnother good piece of future work would be particle Gibbs or other particle MCMC algorithms, which would also build on our present work.\n\n> sequential monte carlo approach often suffers from skewed particle issue where one particle gradually dominates all the other particles with no diversity.  \n\nThis is indeed a danger in SMC approaches.  But surely you don\u2019t think that all SMC papers should be rejected just because they use SMC?   There are several techniques in the SMC community for \u201crejuvenating\u201d a skewed ensemble, such as multinomial resampling, other forms of resampling, and the \u201cparticle cascade.\u201d  Any of these techniques could be combined with ours, and this is orthogonal to the technical contributions of our paper.\n\n> It is unclear how the proposed approach is able to handle this. \n\nIn fact, our particle smoothing method is also intended to alleviate this issue.  As you know, if we could achieve a perfect proposal distribution q that was proportional to p, then the particle weight p/q would be constant across all particles, completely eliminating the skew issue.  So our paper shows how to improve the proposal distribution.\n\nSpecifically, the reason that an SMC ensemble becomes skewed over time is that some of the proposed particles turn out to be less compatible with the future, and are reweighted to have a weight near 0.  Particle smoothing tries to incorporate the future into the proposal distribution so that this will not happen as badly.  \n\n> what people really care about is how different techniques can behave in real data to impute realistic missing events.\n\nWe certainly agree!  Which is why our section 4 (backed by appendices C-D, including Algorithm 2) gives a sophisticated method for doing exactly that.  Results from applying this method to impute missing events on real data are reported in section 5.2, including the carefully designed Figure 3.\n\nCould you please reread that material, and raise your score as appropriate to recognize the work that we did there?\n\nYou suggest Linderman et al. (2017) and Shelton et al. (2018) as if they would be appropriate baselines for this imputation task.  However, those papers only apply to Hawkes processes.  Please note that we did discuss them carefully in section 6.  \n\n(Specifically: Our particle filtering baseline is already the SAME as Linderman et al. (2017), just extended from the Hawkes process to the *neural* Hawkes process.  Shelton et al. (2018) use MCMC, but their MCMC algorithm takes advantage of special properties of the Hawkes process.  Unfortunately, those special properties no longer hold for the *neural* Hawkes process, which would therefore require a much slower MCMC algorithm, as noted above; we haven\u2019t tried that.)\n\n(You also suggest that Xu et al. (2017) is relevant.  We are happy to cite it in the final version, but note that that paper focuses on quite a different kind of missing data -- \u201cshort reads\u201d where a long sequence has been broken up and it is not known which pieces go together.  The first author of that paper agreed that his paper isn\u2019t directly comparable to our setting, when we corresponded with him before submission.) \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "B1g0JtueRX", "original": null, "number": 1, "cdate": 1542650085694, "ddate": null, "tcdate": 1542650085694, "tmdate": 1542650085694, "tddate": null, "forum": "HJehSnCcFX", "replyto": "SyxMi84Z6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "content": {"title": "Subject: response to short late review", "comment": "\n> It's difficult to follows.\n\nThanks for acknowledging the importance of the problem.  We are sorry to hear that you found the paper too difficult to read in the limited time that you had available to review it.  \n\nWe worked quite hard on the exposition.  If you have specific suggestions that could reduce the difficulty, we will be happy to consider them for the camera-ready version.\n\n> But it's a good paper and can be turned to a good paper for the next venue.\n\nThank you.  We agree that \u201cit\u2019s a good paper\u201d already. :)  You provide few comments about how it could be \u201cturned into a good paper for the next venue,\u201d so we are not sure of your reasons for wanting to delay its publication.\n\n> It would have helped if the authors made it clear why each part is chosen and clearly state what is the novelty and contributed of the paper to the field.\n\nYes, this is why we wrote section 6, \u201cDiscussion.\u201d  Could you please reread that section?  It begins: \u201cOur technical contribution is threefold,\u201d and goes on to clearly describe each contribution and its novelty and importance.\n\n> several existing and well developed approach: Neural Hawkes Process + particle smoothing + minimum bayes risk + alignment\n\nActually, we are further developing methods that are still in their infancy and are under current investigation.  NHP was first published in December 2017 and is being picked up by the community.  Neural methods for particle smoothing were first published in June 2018.  \n\nOur alignment method required developing a new metric and alignment algorithm (section 4 and Appendix C, including Algorithm 2).  These are not groundbreaking but they did require some thought.\n\nOur MBR method required developing a new approximate search method (section 4 including Theorem 1, and Appendix D including Algorithm 3).\n\nWe believe that the novel contributions of this paper are above threshold for publication in ICLR.  There is a lot of material in this paper.\n\nThe paper also includes strong experimental results that should be of interest to the ML community and that demonstrate the potential of our methods for applied work.  We provided extensive pseudocode and will release our implementation.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608187, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJehSnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1582/Authors|ICLR.cc/2019/Conference/Paper1582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers", "ICLR.cc/2019/Conference/Paper1582/Authors", "ICLR.cc/2019/Conference/Paper1582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608187}}}, {"id": "SyxMi84Z6Q", "original": null, "number": 3, "cdate": 1541650073629, "ddate": null, "tcdate": 1541650073629, "tmdate": 1541650073629, "tddate": null, "forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "content": {"title": "This paper proposes an algorithm for missing data problem in continuous time events data (ie, point processes) where both past and future events are helpful. ", "review": "This paper tackles a very important and practical problem in event stream planning. The problem is very interesting and the approach taken is standard.\n\nThe presentation of the paper is not clear enough. The notations and definitions and methods are presented in a complicated way. It's difficult to follows.\n\nFrom the contribution point of view the paper looks like to be a combination of several existing and well developed approach: Neural Hawkes Process + particle smoothing + minimum bayes risk + alignment. It's not very surprising to see these elements together. It would have helped if the authors made it clear why each part is chosen and clearly state what is the novelty and contributed of the paper to the field.\n\nThe paper in its current format is not ready for publication. But it's a good paper and can be turned to a good paper for the next venue.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "cdate": 1542234198858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977472, "tmdate": 1552335977472, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1x57ne627", "original": null, "number": 2, "cdate": 1541372961774, "ddate": null, "tcdate": 1541372961774, "tmdate": 1541533015411, "tddate": null, "forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "content": {"title": "Interesting problem with weak experimental evaluation", "review": "The authors propose a particle smoothing approach with an approximate minimum Bayes risk decoder to impute missing events in the Neural Hawkes Process (NHP). The main goal is to address the missing events problem in continuous-time event analysis, which is an important problem in practice. The core idea is within the framework of particle smoothing. \n\nTo formulate the posterior distribution of the missing event, the authors consider both the left-to-right past events and the right-to-left future events. The paper first applies the NHP to capture both the observed and inferred missing events to learn a representation of the past events, and then uses a similar NHP to learn the representation of the observed events from the future. Based on the two representations, it then formulates the intensity function of the missing events and uses the thinning algorithm to sample different particles. Based on the proposed distribution, the paper also considers to decode a single prediction achieving the Minimum Bayes Risk. Experiments on synthetic datasets with 10 different initializations and two real datasets show that the proposed smoothing approach is better than the filtering baseline. \n\nIn general, this paper considers an important problem which is under active research in literature recently. However, there are a few weaknesses of the paper that should be addressed. \n\n1. The proposed technique is tightly connected to NHP, which could limit the applicability of the approach to other temporal point processes. The essential idea is similar to Bi-LSTM to learn the representation from both ends of a sequence of asynchronous temporal events. There are several different ways to represent the inter-event time to feed into the network other than NHP. Can the proposed method also be applied to other processes?\n\n2. Within the particle filtering framework, each particle (hypothesis) is weighted by the likelihood of the sequence of observed events under that hypothesis. It turns out that the integral part of Equation 5 does not have an obvious analytical solution under NHP. Then, we first need a set of samples to approximate the likelihood evaluation. Later, we also need to sample particles. I am not quite convinced the computational efficiency of this approach in real applications of practice. Also, there is no analysis either empirically or analytically about the impact of the accumulative sampling errors on the inference performance. Furthermore, to learn the proposed distribution, the paper applies the REINFORCE algorithm under the proposed distribution q. But REINFORCE is known for large variance issue. Given that we already need lots of samples for the likelihood, it is unclear to me how stable the algorithm could be in practice.\n\n3. The experimental evaluation is weak. For particle filtering and smoothing, it is known that the filtering techniques are candidates for solving the smoothing problem but perform poorly when T is large. That's why it is necessary to develop more sophisticated strategies for good smoothing\nalgorithms. As a result, it is unfair to only compare the smoothing approach with the filtering baseline. \n\nActually, what people really care about is how different techniques can behave in real data to impute realistic missing events. From this perspective, I suggest to use the QQ-plot to evaluate the goodness of fitting on the synthetic dataset. For example, given a sequence of events generated from an independent temporal point process, we can randomly delete events, and then apply different techniques, including Linderman et al. (2017), Shelton et al.(2018), to impute missing events. Finally, we can compare the imputed sequence of events with the groundtruth. \n\nIn addition, sequential monte carlo approach often suffers from skewed particle issue where one particle gradually dominates all the other particles with no diversity. It is unclear how the proposed approach is able to handle this. \n\nOne missing related paper is \"Learning Hawkes Processes from Short Doubly-Censored Event Sequences\"\n\nSection 5.2 can be significantly strengthened if comparing with at least one of these approaches.\n\n4. The paper is fairly written. I had some trouble reading back and forth for understanding Figure 1 since it has long caption that is not self-contained. The annotation of Section 2 is also too heavy to quickly skim through to memorize. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "cdate": 1542234198858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977472, "tmdate": 1552335977472, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyxi0YA_nQ", "original": null, "number": 1, "cdate": 1541102035083, "ddate": null, "tcdate": 1541102035083, "tmdate": 1541533015171, "tddate": null, "forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "content": {"title": "Re: particle smoothing for neural Hawkes Processes", "review": "The paper presents an inference method (implicit distribution particle smoothing) for neural Hawkes processes that accounts for latent sequences of events that influence the observed trajectories.\n\nQuality\n+ The paper combines ideas from multiple areas of machine learning to tackle a challenging task of inference in multivariate continuous-time settings.\n- The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.\n\nClarity\n- notation is complex replete with symbols \"@\" and text in math formulas\n- It's not clear what p (\"the data model\") and p_miss (\"the missingness mechanism\") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol.  In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as: p(r|x_u, x_o) = p(r|x_o), r is a missingness vector, x_u is x unobserved, and x_o is x observed, then r corresponds to observation or not, whereas in the manuscript p_miss is on the values themselves, i.e. on the space where z={k_{i,j}@t_{i,j}} resides.  We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown.  If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.\n\nOriginality\n+ the work is rich, complex, original, and uses leading methods from multiple areas of ML.\n\nSignificance\n+ the significance of this work could be high, as it may provide a way to conduct difficult inference in an effective way to produce increasingly flexible modeling of trajectories amidst partial observation.\n- however the exposition (particularly the experiments) does not fully demonstrate this.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1582/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference of unobserved event streams with neural Hawkes particle smoothing", "abstract": "Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. When only some of the events are observed, we propose particle smoothing to infer the missing events. Particle smoothing is an extension of particle filtering in which proposed events are conditioned on the future as well as the past. For our setting, we develop a novel proposal distribution that is a type of continuous-time bidirectional LSTM. We use the sampled particles in an approximate minimum Bayes risk decoder that outputs a single low-risk prediction of the missing events. We experiment in multiple synthetic and real domains, modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner, 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events. In particular, particle smoothing consistently improves upon particle filtering, showing the benefit of training a bidirectional proposal distribution.", "keywords": [], "authorids": ["hmei@cs.jhu.edu", "ghq@pku.edu.cn", "jason@cs.jhu.edu"], "authors": ["Hongyuan Mei", "Guanghui Qin", "Jason Eisner"], "pdf": "/pdf/64087b4d3b5d3b59670fc1e78c69a75d6ec7eb25.pdf", "paperhash": "mei|inference_of_unobserved_event_streams_with_neural_hawkes_particle_smoothing", "_bibtex": "@misc{\nmei2019inference,\ntitle={Inference of unobserved event streams with neural Hawkes particle smoothing},\nauthor={Hongyuan Mei and Guanghui Qin and Jason Eisner},\nyear={2019},\nurl={https://openreview.net/forum?id=HJehSnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1582/Official_Review", "cdate": 1542234198858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJehSnCcFX", "replyto": "HJehSnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977472, "tmdate": 1552335977472, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}