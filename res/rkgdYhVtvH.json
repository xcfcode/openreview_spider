{"notes": [{"id": "rkgdYhVtvH", "original": "H1efoJPhIH", "number": 83, "cdate": 1569438847626, "ddate": null, "tcdate": 1569438847626, "tmdate": 1577168223366, "tddate": null, "forum": "rkgdYhVtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aUamXx1lkI", "original": null, "number": 1, "cdate": 1576798686983, "ddate": null, "tcdate": 1576798686983, "tmdate": 1576800948088, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Decision", "content": {"decision": "Reject", "comment": "The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in-depth.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722784, "tmdate": 1576800274154, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper83/-/Decision"}}}, {"id": "HJeq4HgrsS", "original": null, "number": 3, "cdate": 1573352754068, "ddate": null, "tcdate": 1573352754068, "tmdate": 1573352754068, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "ryxzFyxAFr", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer #2", "comment": "We appreciate reviewer\u2019s helpful and detailed feedback. The updates in our paper are marked in red. \n\n\n1. Adding an experiment showing how much the LPA impacts the results.\n\nWe appreciate reviewer\u2019s suggestion but in fact we have already performed it. In particular, in Figures 2 and 3 we vary the number of LPA iterations and the weight of LPA loss term $\\lambda$, which reveals the impact of LPA on model performance. However, this point was not well explained in the paper and we made changes where we added an experiment in which we vary the ratio of labeled nodes in LPA, and the result is shown in new Table 3 in the revised paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper83/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgdYhVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper83/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper83/Authors|ICLR.cc/2020/Conference/Paper83/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176647, "tmdate": 1576860557866, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment"}}}, {"id": "Bkg5pNerjH", "original": null, "number": 2, "cdate": 1573352642053, "ddate": null, "tcdate": 1573352642053, "tmdate": 1573352642053, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "SygJVJ60YH", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer #3", "comment": "We thank the reviewer for helpful and detailed feedback. Reviewer makes a number of helpful comments, which we addressed the feedback and included further clarifications in the paper to make the paper clearer and more understandable. The updates in our paper are marked in red.\n\n\n1. The proof of Theorem 2 is not clear.\n\nThank you for raising this point. We made changes to our paper and have expanded the proof of Theorem 2 by adding more explanations, so that the proof is easier to follow. We also added Figure 5 as an illustrating example for the proof of Lemma 2.\n\n\n2. Why \u201cy\u201d has to be reset at each iteration?\n\nThe reset step is part of the standard Label Propagation Algorithm (LPA), which was proposed by Zhu et al. 2005. The reason why this is a good idea is that LPA wants to persist labels of nodes which are labeled so that unlabeled nodes don\u2019t overpower the labeled ones as the initial labels would otherwise fade away. The reviewer makes a good point and we have updated the paper (introduction of LPA in Section 2.1) to make this point clearer.\n\n\n3. The improvement brought by the framework is marginal, even if it seems general.\n\nThe performance of our method exceeds almost all state of the art baselines on all datasets. Since the baselines like JK-Net are strong, and the accuracy results of baselines on most datasets are already very high (~0.9 or even higher), it is  not possible to surpass the baselines on these datasets by a very large margin. However, the improvement of our method is statistically significant and shows great results on standard benchmarks commonly used by the community.\n\n\n4. The training is transductive.\n\nThe reviewer is right that we implemented our method based on the code of Kipf\u2019s GCN, and therefore it is transductive. However, our method can be easily generalized to inductive case if we implement it in a way similar to GraphSAGE. Therefore, being transductive or inductive does not affect the novelty and impact of this work. We thank the reviewer for the comment and we have updated the manuscript to make it clearer that our results apply to both transductive as well as inductive settings.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper83/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgdYhVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper83/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper83/Authors|ICLR.cc/2020/Conference/Paper83/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176647, "tmdate": 1576860557866, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment"}}}, {"id": "S1gy3mlrir", "original": null, "number": 1, "cdate": 1573352358896, "ddate": null, "tcdate": 1573352358896, "tmdate": 1573352405307, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "rkgeaF11cr", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer #1", "comment": "We really appreciate reviewer\u2019s helpful and detailed feedback. Our response to raised questions is below. The updates in our paper are marked in red. Overall, we observe that reviewed makes a number of questions that can be easily clarified.\n\n\n1. What is the meaning of a L2 norms between features?\n\nIn Theorem 1, the term $||x_1 - x_2||_2$ is the Euclidean distance between two vectors $x_1$ and $x_2$, which are raw feature vectors for node $v_1$ and $v_2$ in the graph. The inequality $|M(x_1) - M(x_2)| \\leq L ||x_1 - x_2||_2$ in Theorem 1 is a standard $L$-Lipschitz constraint, meaning that the function $M$ is not too wiggly (i.e., if you move $x$ for distance $1$ then the change of $M(x)$ will not exceed $L$). Reviewer can learn more about the $L$-Lipschitz constraint in https://en.wikipedia.org/wiki/Lipschitz_continuity, which provides a good overview of the topic.\n\n\n2. How do you relate the claim of bag-of-words features used in experiments to your theoretical analysis?\n\nOur theoretical analysis is applicable to any form of node features, as long as node features can be represented as a vector of a fixed length. This is also by far the most common way to represent features/attributes of nodes in a graph.\n\nAs reviewer mentioned, in the datasets used in our experiments, the node features are bag-of-words features. We are a bit unsure why reviewer thinks this conflicts with theoretical analysis. Reviewer should note that Euclidean distance is still well defined on sparse multi-hot vectors, that is vectors where many components take a zero value and other components take non-zero values.\n\n\n3. How do you take a derivative with respect to a bag of words?\n\nBy bag-of-words features we mean a multi-hot vector encoding of which words appear in the document. For example, bag of words is a vector $[1, 0, 0, \u2026, 1, 0]$  which the length is the vocabulary size and position/dimension $i$ indicates whether word $i$ appears in the given document or not.\n\nTherefore, the derivatives are taken w.r.t. to a vector, as we defined in Definition 1. And the derivative of a vector $x$ w.r.t. another vector $y$ is then defined as a matrix, in which element $(i, j)$ is the derivative of $x[i]$ w.r.t. $y[j]$. In mathematics this is also known as the Jacobian matrix.\n\n\n4. The constraints on A* is not specified in Eq. 5 (Eq. 5 is now Eq. 6 in the revised paper).\n\nThis is a good point and we thank the reviewer for making it. We have added a footnote in our paper (above Eq. 6) further clarifying this point.\n\n\n5. The definition of y_hat is not specified in Eq. 5 (Eq. 5 is now Eq. 6 in the revised paper).\n\ny_hat is already defined in the last line of Theorem 3. To make the presentation clearer, we also modified our paper and introduced the definition of y_hat below Eq. 6. Please note that we added a superscript \u201clpa\u201d to y_hat in Eq. 6 in the revised paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper83/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgdYhVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper83/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper83/Authors|ICLR.cc/2020/Conference/Paper83/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176647, "tmdate": 1576860557866, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper83/Authors", "ICLR.cc/2020/Conference/Paper83/Reviewers", "ICLR.cc/2020/Conference/Paper83/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Comment"}}}, {"id": "SygJVJ60YH", "original": null, "number": 2, "cdate": 1571897127164, "ddate": null, "tcdate": 1571897127164, "tmdate": 1573060369372, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper proposed an unified model for Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. The results are given in terms of two theorems (whose proofs are in an appendix) which essentially state that the total label influence of nodes with a particular label \u201cl\u201d on a specific node is proportional to the probability that node is labelled as \u201cl\u201d by LPA. In practice, LPA acts as a regularizer to learn transformation matrices and edge weights simultaneously in GCN. By means of a simple joint loss (eq.8), the regularized training show that transductive learning with the joint model surpasses GCN/GNN baselines. \nWhile the proof of the first theorem is reasonable and seems correct (Taylor+ Schwarz inequality + L-Lip), the second left me a little puzzled, in particular wrt eq 16 and 17. Is it possible to add a graphical explanation or idea of the proof? Why \u201cy\u201d has to be reset at each iteration?\nUnfortunately, the improvement brought by the framework is marginal, even if it seems general. \nThe other limitation is that we have transductive training, but the authors are well aware about this.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575772516063, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper83/Reviewers"], "noninvitees": [], "tcdate": 1570237757327, "tmdate": 1575772516076, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Review"}}}, {"id": "ryxzFyxAFr", "original": null, "number": 1, "cdate": 1571843961795, "ddate": null, "tcdate": 1571843961795, "tmdate": 1572972640803, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. The motivation of this combination is supported by two analysis on the feature/label smoothing and feature/label influence. The proposed GCN-LPA framework utilizeds LPA to adjust the edge weight A* through the label information. Then, this edge weight A* is used to transfer the knowledge from label information to feature information for enhancing the representation learning in GCN. An end-to-end  solution is proposed by treating the LPA process as regularization. Overall, the idea of unifying GCNs and LAP in an end-to-end fashion is interesting. \n\nOne major concern is that from the experiment, it is unclear how much the LPA impacts the node classification. It will be more convincing if the performance comparison under different percentage of labeled samples (during LPA) is provided. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575772516063, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper83/Reviewers"], "noninvitees": [], "tcdate": 1570237757327, "tmdate": 1575772516076, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Review"}}}, {"id": "rkgeaF11cr", "original": null, "number": 3, "cdate": 1571908023807, "ddate": null, "tcdate": 1571908023807, "tmdate": 1572972640709, "tddate": null, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "invitation": "ICLR.cc/2020/Conference/Paper83/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication.\nThe theoretical part (secion 2) is hard to follow. For example, the authors introduce in 2.1 a mapping M from vertices to labels. Then in Theorem 1, this mapping M indeed maps features of the vertices to labels. But then waht is the meaning of a L2 norms between features? At this point, I had a look in the experiment section to see what features are considered in practice. In section 4.1, it is written for the citation networks: 'each node has a sparse bag-of-words feature vector' or in the coauthor networks: 'Node features represent paper keywords for each author's paper'. How do you relate these claims to your theoretical analysis? Things get even worse in section 2.3, where derivatives with respect to initial feature vector are taken. How do you take a derivative with respect to a bag of words?\nEquation (5) is not clear at all, we need to read the end of this section to understand that A^* is constrained to have the same support as the adjacency graph and computed as a function of the node features. The authors shuold also define clearly y_hat in (5)."}, "signatures": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper83/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unifying Graph Convolutional Neural Networks and Label Propagation", "authors": ["Hongwei Wang", "Jure Leskovec"], "authorids": ["wanghongwei55@gmail.com", "jure@cs.stanford.edu"], "keywords": ["graph convolutional neural networks", "label propagation", "node classification"], "TL;DR": "This paper studies theoretical relationships between Graph Convolutional Neural Networks (GCN) and Label Propagation Algorithm (LPA), then proposes an end-to-end model that unifies GCN and LPA for node classification.", "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.\n", "pdf": "/pdf/0094b00cbf899a537bf66ed4fb1b2c8dcf16890c.pdf", "paperhash": "wang|unifying_graph_convolutional_neural_networks_and_label_propagation", "original_pdf": "/attachment/dea321de7aab1dfaeb366d98985f05aefa933565.pdf", "_bibtex": "@misc{\nwang2020unifying,\ntitle={Unifying Graph Convolutional Neural Networks and Label Propagation},\nauthor={Hongwei Wang and Jure Leskovec},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgdYhVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgdYhVtvH", "replyto": "rkgdYhVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575772516063, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper83/Reviewers"], "noninvitees": [], "tcdate": 1570237757327, "tmdate": 1575772516076, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper83/-/Official_Review"}}}], "count": 8}