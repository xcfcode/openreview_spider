{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396659316, "tcdate": 1486396659316, "number": 1, "id": "SyigaGIdg", "invitation": "ICLR.cc/2017/conference/-/paper528/acceptance", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper is clearly written, and contains original observations on the properties of the neural GPU model. These observations are an important part of research, and sharing them (and code) will help the field move forward. However, these observations do not quite add up to a coherent story, nor is a particular set of hypotheses explored in depth. So the main problem with this paper is that it doesn't fit the 'one main idea' standard format of papers, making it hard to build on this work.\n \n The other big problem with this paper is the lack of comparison to similar architectures. There is lots of intuition given about why the NGPU should work better than other architectures in some situations, but not much empirical validation of this idea."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396659815, "id": "ICLR.cc/2017/conference/-/paper528/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396659815}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484853439153, "tcdate": 1478299586822, "number": 528, "id": "ryjp1c9xg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryjp1c9xg", "signatures": ["~Wojciech_Zaremba1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484847745402, "tcdate": 1484847745402, "number": 2, "id": "SkYY9_CUg", "invitation": "ICLR.cc/2017/conference/-/paper528/public/comment", "forum": "ryjp1c9xg", "replyto": "SyQmiKWVl", "signatures": ["~Wojciech_Zaremba1"], "readers": ["everyone"], "writers": ["~Wojciech_Zaremba1"], "content": {"title": "reply", "comment": "* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.\n\nWojciech: mostly agree. we have tried in the paper several extensions but they seem to work in narrow cases. These regard how to align data.\n\n* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).\n\nWojciech: We should add such comparison.\n\n* More experiments on other tasks would be nice, it is only tested on some toy tasks.\n* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.\n\nWojciech: Our results outperform everything that was previously presented in NeuralGPU paper.\n\n* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.\n\nEric: Except in Figure 2, we do not use gradient noise.  [It generally did not seem to have much effect.]\n\n* Misleading number of NTM computation steps. You write O(n) but it is actually variable.\n\nWojciech: The point is that it scales linearly with amount of data (n is the length of data). E.g. NTM doesn't perform quadratic number of computation steps.\n\nIn general, thanks for comments. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287535596, "id": "ICLR.cc/2017/conference/-/paper528/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryjp1c9xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper528/reviewers", "ICLR.cc/2017/conference/paper528/areachairs"], "cdate": 1485287535596}}}, {"tddate": null, "tmdate": 1484847428390, "tcdate": 1484847428390, "number": 1, "id": "HknBFORLx", "invitation": "ICLR.cc/2017/conference/-/paper528/public/comment", "forum": "ryjp1c9xg", "replyto": "S1PY_TAze", "signatures": ["~Wojciech_Zaremba1"], "readers": ["everyone"], "writers": ["~Wojciech_Zaremba1"], "content": {"title": "reply", "comment": "Eric:\n * Except in Figure 2, we do not use gradient noise.  [It generally\n   did not seem to have much effect.]\n\n * In Figure 3, we trained on up to length-20 numbers and tested on\n   length-100 numbers.\n\nWojciech:\nNone of the models like LSTM or RLNTM is able to generalize to x2 longer sequences. Same most likely holds for NTM, however it's extremely hard to replicate NTM model. This model sometimes generalizes to sequences x10 longer. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287535596, "id": "ICLR.cc/2017/conference/-/paper528/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryjp1c9xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper528/reviewers", "ICLR.cc/2017/conference/paper528/areachairs"], "cdate": 1485287535596}}}, {"tddate": null, "tmdate": 1482595844567, "tcdate": 1482595844567, "number": 2, "content": {"title": "-", "question": "-"}, "id": "HyTWRz3Ng", "invitation": "ICLR.cc/2017/conference/-/paper528/pre-review/question", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer1"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482595845093, "id": "ICLR.cc/2017/conference/-/paper528/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper528/AnonReviewer3", "ICLR.cc/2017/conference/paper528/AnonReviewer1"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482595845093}}}, {"tddate": null, "tmdate": 1482595830514, "tcdate": 1482595830514, "number": 3, "id": "Hk0gRzhNg", "invitation": "ICLR.cc/2017/conference/-/paper528/official/review", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer1"], "content": {"title": "scattered but interesting remarks don't quite add up to a publishable unit", "rating": "5: Marginally below acceptance threshold", "review": "Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don't add up to a quantum of progress in the study of what can be done with the Neural GPU model.\n\nMinor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories.\n\nBasic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 \u201cfilters\u201d in the model ?  If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3.\n\nThe preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.\n\nThe remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU.\n\nThe point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something).\n\nThe notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595831029, "id": "ICLR.cc/2017/conference/-/paper528/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper528/AnonReviewer3", "ICLR.cc/2017/conference/paper528/AnonReviewer5", "ICLR.cc/2017/conference/paper528/AnonReviewer1"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595831029}}}, {"tddate": null, "tmdate": 1481929662017, "tcdate": 1481929662017, "number": 2, "id": "ry8aXlGEg", "invitation": "ICLR.cc/2017/conference/-/paper528/official/review", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer5"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The authors investigate the neural GPU model introduced by Kaiser and Sutskever. In section 3 they claim its performance is due to the O(n^2) number of steps it can perform for each example. In the subsequent section they highlight the importance of curriculum training and empirically show that larger models generalize better. In section 5 they construct examples that reveal failure modes. In the last section they compare the performance given different input formats.\n\nThe paper is well written. It contains an exhaustive set of experiments which provide insight into the details of training the neural GPU model. It pushes the boundary of algorithms that can be learned further. On the other hand, the paper seems to lack a coherent message. It also fails to provide any insight into the how and why of the observations made (i.e. why curriculum training is essential and why certain failure modes exist).\n\nThe introduction contains several statements which should be qualified or explained further. As far as I am aware statistical learning theory does not guarantee that empirical risk minimization is consistent when the number of parameters is larger than the number of examples; the generalization performance depends on the VC dimension of the function space instead. Furthermore, the suggested link between adversarial examples and learning algorithms is tenuous, and references or a further explanation should be provided for the contentious statement that deep neural networks are able to match the performance of any parallel machine learning algorithm.\n\nThe authors argue that the neural GPU performs O(n^2) \u201csteps\u201d on each example, which allows it to learn algorithms with super-linear complexity such as multiplication. This analysis seems to overlook the parallel nature of the neural GPU architecture: Both addition and multiplication have O(log n) time complexity when parallelism is used (cf. a carry-lookahead adder and a Wallace tree respectively).\n\nIn section 4 the authors show that their larger models generalize better, which they argue is not self-evident. However, since both training and test error decrease it is likely that the smaller models are underfitting, in which case it is not counter-intuitive at all that a larger model would have better generalization error.\n\nIt is interesting to see that progressively decreasing the number of terms and increasing the radix of the number system works well as a learning curriculum, although it would be nice to have a stronger intuitive or theoretical justification for the latter.\n\nThe final section claims that neural GPUs are cellular automata. Further justification for this statement would be useful, since cellular automata are discrete models, and the equivalence between both models is in no way obvious. The relationship between global operations and changing the input format is circuitous.\n\nIn conclusion, the paper provides some useful insights into the neural GPU model, but does not introduce original extensions to the model and does not explain any fundamental limitations. Several statements require stronger substantiation.\n\nPro:\n\n* Well written\n* Exhaustive set of experiments\n* Learning algorithms with decimal representation\n* Available source code\n\nCons:\n\n* No coherent hypothesis/premise advanced\n* Two or three bold statements without explanation or references\n* Some unclarity in experimental details\n* Limited novelty and originality factor\n\nTypos: add minus in \u201cchance of carrying k digits is 10^k\u201d (section 5); remove \u201care\u201d from \u201cthe larger models with 512 filters are achieve\u201d (section 4); add \u201ca\u201d in \u201csuch model doesn\u2019t generalize\u201d (section 4).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595831029, "id": "ICLR.cc/2017/conference/-/paper528/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper528/AnonReviewer3", "ICLR.cc/2017/conference/paper528/AnonReviewer5", "ICLR.cc/2017/conference/paper528/AnonReviewer1"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595831029}}}, {"tddate": null, "tmdate": 1481902874709, "tcdate": 1481902874709, "number": 1, "id": "SyQmiKWVl", "invitation": "ICLR.cc/2017/conference/-/paper528/official/review", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "content": {"title": "interesting paper but requires some more work", "rating": "5: Marginally below acceptance threshold", "review": "The paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.\n\nPros:\n* Well written.\n* Many investigations.\n* Available source code.\n\nCons:\n* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.\n* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).\n* More experiments on other tasks would be nice, it is only tested on some toy tasks.\n* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.\n* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.\n* Misleading number of NTM computation steps. You write O(n) but it is actually variable.\n\nAfter the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595831029, "id": "ICLR.cc/2017/conference/-/paper528/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper528/AnonReviewer3", "ICLR.cc/2017/conference/paper528/AnonReviewer5", "ICLR.cc/2017/conference/paper528/AnonReviewer1"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595831029}}}, {"tddate": null, "tmdate": 1480674498763, "tcdate": 1480673406999, "number": 1, "id": "S1PY_TAze", "invitation": "ICLR.cc/2017/conference/-/paper528/pre-review/question", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "content": {"title": "extensions / improvements to NeuralGPU model, test data length, NTM steps", "question": "From the title (Extensions ... of the Neural GPU) and chapter 4 (Improvements to the Neural GPU), it sounds like you extend or improve the Neural GPU model. From the text, I understand that you don't modify the Neural GPU model but you use improved training techniques and an improved implementation for memory efficiency. I think you should make that more clear in the title. So, there is no modification to the Neural GPU model?\n\nFrom Figure 2, it seems that gradient noise could slightly help. Have you used that in your remaining experiments? What exactly have you used in your remaining experiments?\n\nFor some cases, you describe the length of train data sequences and test data sequences but I haven't found that information for example for the experiments in Figure 3 and maybe others. How long was it? How much longer was test compared to train?\n\nYou state that the NTM number of steps is O(n). But I think that is actually variable, right?\n\nYou don't have any comparisons to other related models, like Grid LSTM, NTM, Adaptive Computation Time, etc?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482595845093, "id": "ICLR.cc/2017/conference/-/paper528/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper528/AnonReviewer3", "ICLR.cc/2017/conference/paper528/AnonReviewer1"], "reply": {"forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482595845093}}}, {"tddate": null, "tmdate": 1480673001543, "tcdate": 1480673001539, "number": 1, "id": "B1bgDpCGl", "invitation": "ICLR.cc/2017/conference/-/paper528/official/comment", "forum": "ryjp1c9xg", "replyto": "ryjp1c9xg", "signatures": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper528/AnonReviewer3"], "content": {"title": "header typo: ICLR 2016", "comment": "I guess you should write ICRL 2017 there."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "pdf": "/pdf/f3863d4c42aacb77a38781a712a6f439219599ba.pdf", "paperhash": "price|extensions_and_limitations_of_the_neural_gpu", "keywords": [], "conflicts": ["nyu.edu", "facebook.com", "fb.com", "google.com", "openai.com", "cs.utexas.edu"], "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287535408, "id": "ICLR.cc/2017/conference/-/paper528/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryjp1c9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper528/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper528/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper528/reviewers", "ICLR.cc/2017/conference/paper528/areachairs"], "cdate": 1485287535408}}}], "count": 10}