{"notes": [{"id": "RuUdMAU-XbI", "original": "l3Lv8tDM68c", "number": 11, "cdate": 1601308010418, "ddate": null, "tcdate": 1601308010418, "tmdate": 1614985703445, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wwtJbs4awtZ", "original": null, "number": 1, "cdate": 1610040440977, "ddate": null, "tcdate": 1610040440977, "tmdate": 1610474041966, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The idea presented in the paper is interesting and has caught the attention of the reviewers. However there seem to be only a tepid support for acceptance with a reviewer championing rejection. \nThere is little novelty in the approach but empirical validation shows results that consistently improve over selected baselines. I am afraid that more evaluations would be needed at this stage to consider this work for acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040440961, "tmdate": 1610474041950, "id": "ICLR.cc/2021/Conference/Paper11/-/Decision"}}}, {"id": "mLzmYfChPw", "original": null, "number": 3, "cdate": 1603881076797, "ddate": null, "tcdate": 1603881076797, "tmdate": 1607351697058, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Review", "content": {"title": "Nice idea, but limited novelty and experimental validation", "review": "This work proposes a novel method, called Dynamic Graph Network (DG-Net), for optimizing the architecture of a neural network. Building on the previous work introduced by (Xie et al., 2019), the authors propose to consider the network as a complete directed acyclic graph (DAG). Then, the edge weights of the DAG are generated dynamically for each input of the network. At each node of the network, the authors introduce an extra-module, called router, to estimate the edge weights as function of the input features.\n\nThe proposed method addresses the problem of optimizing the connectivity of neural networks in an interesting way, where the architecture is not fixed but it depends on the input instances. Moreover, I think that a strong advantage of the proposed technique is that the optimization of the architecture comes with a negligible extra cost both in terms of parameters and computational complexity. Overall, the paper is well written and easy to follow.\n\nMy only serious concern is the degree of novelty with respect to (Yuan et al., 2020), which was published at ECCV 2020. The main difference seems to be that in the proposed method the graph is dynamic (i.e., it depends on the input instances), instead  in (Yuan et al., 2018) the graph is learned but fixed for all the input samples. In the experimental results, I would have expected a deeper ablation study on the importance of the dynamic graph, since this is the main contribution of the paper. Instead, there is only one experiment in the appendix (Table 6). Therefore, the impact of the dynamic graph in the performance of the proposed method is not clear and it is difficult to evaluate the importance of this contribution.\n\nOther comments:\n\n- In Sec. 3.1 the authors say that the ResNet architecture can be represented with a DAG where the set of edges is defined as E={(i,j)|j=i+1,i+2}. This is not true: if you unroll the definition of the ResNet architecture, as done in Eq. (4)-(6) in [1], and compare it with what you obtain using Eq. (1), it is easy to see that the two resulting functions are different.\n\n- The definition of the convolutional block is not clear, is it a ReLU-conv-BN triplet as in (Xie et al., 2019)? \n\n- The use of a DAG with edge weights for representing the architecture is not novel, it was already introduced in (Xie et al., 2019).\n\n- In Sec. 4.3, Table 5 shows a comparison with state-of-the-art NAS-based methods. DG-Net is implemented using RegNet-X and RegNet-Y as the basic architecture, however in Table 5 the performance of the basic architectures (without the dynamic graph optimization) is not reported, this would be useful to evaluate the gain provided by the optimization of the architecture.  \n\n\n[1] Veit et al., Residual Networks Behave like Ensembles of Relatively Shallow Networks, NIPS 2016\n\n###############################################################################################\n\nAfter the discussion period:\n\nI thank the authors for their responses and for updating the paper. The authors have added a deeper analysis on the impact of the dynamic graph, however I still believe that the novelty of the paper is a bit limited. I have slightly increased my score to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151657, "tmdate": 1606915784037, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper11/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Review"}}}, {"id": "OEZKq5tY-e", "original": null, "number": 1, "cdate": 1603706820582, "ddate": null, "tcdate": 1603706820582, "tmdate": 1606744075374, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Review", "content": {"title": "Missing related work", "review": "This paper presents a novel approach (DG-Net) to \u201cgenerate\u201d a dynamic structure for the neural network, by learning to predict and select the edges between computational nodes in an end-to-end manner. The method is based on a gating mechanism, applied on top of a fully connected graph (similar to the connectivity in a DenseNet), designed to control the quantity of information received from each previous layer. The experiments show consistent improvement in image classification (ImageNet) and object detection (COCO).\n\nPro:\n- I enjoy the interpretation of the \u201cweighted edges\u201d as a dynamic architecture, able not only to address a more general class of models but also to adapt to each input accordingly (using a second-order approach).\n- The method is simple, yet effective with the results on both tasks showing improvement at a low computational cost.\n- The paper is clearly written and easy to follow and understand.\n\nCons:\n- In my opinion, two approaches are very related to this paper: the Highway network [1] and the ablations performed in the ResNet paper [2]. In both cases, the intuition is a little bit different: improve the expressiveness by replacing the residual connection with something more powerful - an input-dependent gate. While I agree that this paper offers a more general framework, working on a densely connected graph instead of a limited subset of residual edges, (plus some other minor technical differences: map gates vs scalar gates), the connection to those papers should be clearly discussed in the related work. Also, it is interesting to notice that in [2] this kind of gating decreases the performance compared to the simpler summation, contrary to what we see in DG-Net.\n- This idea suggests an additional ablation study: constant-gating instead of dynamic ones (by setting the alpha scalar equals to 1). This is related to the static experiment in the Supplementary material, but more closely to the DenseNet architecture and it would clarify that the improvements come indeed from the learned structure and not from the ability to combine features from different levels.\n- It is not clear from the paper how the ResNet architecture is adapted to the dynamic setup. I guess the graph is fully connected on each stage (as long as the spatial dimension is preserved), but since the paragraph about the \u201cmulti-stage\u201d architectures comes only in section 3.1, that speaks about classical architectures, not the DG-Net, it is not very clear what densely connected means in the experiments.\n- Is any particular initialisation scheme used for the routers? Could it be important for the optimization to start with some parameters that lead to the original version of the model (initialize the parameters such that the residual connections start with alpha=1 and non-existent connections are close to 0)?  For example in [3] all the skip connections are initialised such that the extra module is ignored in the first iterations.\n\nMinors and observations:\n- Even if the focus of this work is the dynamic of the edges, it would be interesting to observe if the architecture learns to use all the computational nodes and only connect them differently than what we used to do, or if it rather prefers to drop nodes, thus using a lighter model in the end. (maybe a regularization term that encourages that would be useful for computational efficiency).  - A statistical analysis in this direction could reveal interesting ideas.\n- The formatting of Table 1 is hard to follow. At least a vertical line between Baselines and DG-Net would improve the readability.\n\n[1] Highway Networks, Srivastava et. al\n[2] Identity mappings in deep residual networks, He et. al, ECCV 2016\n[3] Non-local Neural Networks, Wang et. al, CVPR 2018\n\nMy main concern regards the connection to previous works that use gating to aggregate information from previous layers, which I see very related to the current work. However, the proposed method is more general and seeing these gates as a routing mechanism that allows the model to learn its own structure in a differentiable way is interesting and could lead to further, more advanced ways of doing this. So, with a clearer discussion of those previous methods and how the current approach differs from them, I lean towards acceptance.\n\n############# UPDATE #############\n\nI thank the authors for their response and for updating the manuscript according to our questions. I agree with the other reviewers that the novelty of this paper is quite limited. However, the idea of using a dynamic, learned graph from a general large search space of models is interesting, provides good empirical results on both image classification and object detection and the authors provide ablation for the new components that motivate the paper. I maintain my initial score: 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151657, "tmdate": 1606915784037, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper11/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Review"}}}, {"id": "hjBoiudtxt", "original": null, "number": 10, "cdate": 1606232760139, "ddate": null, "tcdate": 1606232760139, "tmdate": 1606232760139, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "d34iazgz45v", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "RE: Response to Reviewer2", "comment": "I thank the authors for their response and for updating the manuscript according to our questions. The additional experiments strength the results and clarify my concerns."}, "signatures": ["ICLR.cc/2021/Conference/Paper11/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "f89UyxcKPFV", "original": null, "number": 2, "cdate": 1605875648056, "ddate": null, "tcdate": 1605875648056, "tmdate": 1606226927165, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "0ZLdWwSnSdY", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "We thank the reviewers for their suggestions and comments.\n\n> Q1: What is the theoretical foundation of the method? Given the fully connected nature of the graph, how does back-propagation work in this case? \n\n-------\n\nR1: We have added the update rules in terms of back-propagation for DG-Net in section 3.5 of Eqn. (7~10). The parameters of the convolutional operations and routers can be optimized jointly in a differentiable way.\nNotably, the threshold for each router is also learnable through acting as $\\alpha \\cdot \\sigma(\\alpha - \\tau)$, whose gradient can be computed by $\\sum (\\frac{\\partial {L_t}}{\\partial {x^j}} \\odot \\frac{\\partial {f^j}}{\\partial {x^{j^\\prime}}} \\odot \\mathbf{x}^i) \\cdot \\frac{\\partial {\\psi}^j}{\\partial \\tau^j}$. It avoids careful selections of hyper-parameters for the thresholds in practice. We also visualize the actual learned connectivity for different input samples in Fig. 7. In the adjacency matrix, each element is generated by the corresponding router based upon inputs. The gradients w.r.t each element $\\alpha^{(i,j)}$ can be noted as $\\sum (\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{x}^j} \\odot \\frac{\\partial {f}^j}{\\partial \\mathbf{x}^{j^{\\prime}}} \\odot \\mathbf{x}^i)$.\n\n> Q2: In a sense, in fact, you have a DAG, how do you deal with cycles? When do you stop the back-propagation if you do not have a stacked architecture?\n\n-------\n\nR2: DG-Net discusses the network structure belonging to DAGs (Directed Acyclic Graph),  which do not have cycles. The graphs that include cycles are not included in this paper. \n\n> Q3: Could you please provide the confidence intervals for all the results you presented? In fact, it seems that the values related to your approach are better than existing techniques in some cases but they look very close?\n\n-------\n\nR3: DG-Nets achieve significant improvements compared with existing methods, especially in the large-scale datasets of ImageNet and COCO. Added repeat runs also prove the stability of DG-Net. The comparisons can be analyzed in 4 folds: \n\n* In Table 1, DG-Net surpasses baselines in large margins up to 1.61% in ImageNet under **the same training setting**. In Table 2, DG-Net also obtains up to 2.73% gains in COCO object detection. Due to the training cost in ImageNet, we report part confidence intervals of networks, including MobileNetv2-1.0 ($73.54\\pm 0.06$) resnet18 ($71.32\\pm 0.14$), resnet50 ($78.28\\pm 0.04$) through 5 repeat runs. Results show consistent improvements with a small variance. We will update Table 1 by adding confidence intervals.\n\n* In Table 3, DG-Net also outperforms InstaNAS (73.2% vs 71.9%) under similar latency constraints with small variance (0.06) through 5 repeat runs.\n\n* In Table 4, we conduct 5 repeat runs for graph generators of ER($71.34\\pm 0.40$), BA($71.16\\pm 0.34$), WS($72.26\\pm 0.27$) and DG-Net($73.52\\pm 0.05$). It can be seen that randwire suffers from randomness caused by their random graph generators. The same phenomenon can be observed from their paper in Fig. 3. DG-Net can achieve more stable results with small variance (0.05).\n\n* In Table 5, we add the comparison in search cost of different NAS methods. Since the architecture in DG-Net can be optimized in a differentiable manner, higher results can be obtained with less search cost.\n\n> Q4: Is the computational complexity justified? Also note: you probably need a larger number of samples to learn the additional adjacency graph. What is the trade-off? Given the gain in terms of performance, the actual additional complexity might not be completely justified.\n\n-------\n\nR4: The training configurations of all comparison experiments are the same as the baselines, which means the dynamic graph is optimized using the same number of samples with static networks. Moreover, DG-Net does not require significantly more FLOPs or trainable parameters as shown in Tables 1,2.\n\nWe want to emphasize that the performance improvements come from the appropriate connection method for each sample, not the computational complexity. And the results in Table 6 can prove this. For the network with $\\alpha$, all connections exist with larger computational cost. But DG-Nets can also outperform them in less computational cost with only critical connections.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "d34iazgz45v", "original": null, "number": 7, "cdate": 1605893571184, "ddate": null, "tcdate": 1605893571184, "tmdate": 1606221566976, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "OEZKq5tY-e", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer2- Part 2/2", "comment": "> Q5: Statistical and visualization analyses of the learned connectivity.\n\n----\n\nR5: Details are added in Appendix 6.5. To analyze the architecture representations, we visualize the learned connectivity through the adjacency matrices as noted in section 3.4. For the statistical analysis, we show the distribution of the mean of the weights of edges in Fig. 5 and the distribution of standard deviation in Fig. 6. To visually analyze the connections corresponding to different samples, we give the learned connectivity in Fig. 7. Some observations and analyses can be made:\n* From the statistical analysis in Fig. 5, the weights of connections between different nodes have obvious differences. The difference is related to the topological orders of the nodes and the stages located.\n* Statistically, in a graph, the output edges of the nodes in the front of topological orders have larger weights. This can be explained that for a node with the order of $i$, the generated $\\mathbf{x}_i$ can be received by node $j$ (where $j > i$). This causes the features generated by the front nodes to participate in aggregation as a downstream input. It makes the front nodes contribute more, which can be used to reallocate calculation resources in future work.\n* From Fig. 6, it can be seen that there exist discrepancies in weight changes for different edges with respect to input samples. The difference is also related to the topological orders of the nodes and the stages located.\n* Interestingly, in a stage, the edges of the nodes in the back of topological orders have a larger variance. Similarly, the weights of edges in deeper stages also have a larger variance. We speculate that it is related to the level of semantic information of features. Specifically, features generated by the deep layers have high-level semantic information and the correlation of samples is stronger than features with low-level information generated by the shallow layers.\n* In Fig. 7, it can be seen that for different samples, both the structure and the corresponding weights learned by routers are different. For some easy samples, part edges are masked, resulting in a lighter model. This benefits computational efficiency and can be studied further in future work.\n\n> Q6: The formatting of Table 1 is hard to follow. \n\n----\n\nR6: Table 1 has been re-drawn to improve readability."}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "m92gHQteQOR", "original": null, "number": 8, "cdate": 1605893631376, "ddate": null, "tcdate": 1605893631376, "tmdate": 1606221537747, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "OEZKq5tY-e", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer2- Part 1/2", "comment": "We appreciate the suggestions and comments proposed by the reviewers, which lead to further improvements.\n\n> Q1: Comparisons with Highway networks and Identical mapping.\n\n----\n\nR1: DG-Net is different from Highway networks and Identical mappings in the aspect of search space, modeling type of connections and the learned architectures.\n* Highway networks and ResNet are modified on the plain networks with additional residual connections. The information flow is limited by the number of residual connections. While DG-Net formulates the network into a complete graph, where connections/edges exist between any two layers/nodes. In this way, DG-Net provides more general search spaces that allow more flexible transmission of information flow. Notably, DG-Net overcomes challenges on forwarding computation in section 3.4.\n* In the Highway network, the information flow is determined by a transform gate and a carry gate. These two gates control how much of the output is produced by transforming the input and carrying it. The same definition is also used in the exclusive gating in Identical mapping. However, due to the properties of the complete graph where each node has a direct connection with preceding nodes, DG-Net only requires one transform gate while retaining the original information. Moreover, in the Highway network and the exclusive gating, the sum of the weights for the two gates are constrained to 1. In DG-Net, the weights of different edges are independent of each other. This can preserve the differences in features produced by different nodes.\n* Both Highway networks and identical mapping only change the weights of connections, DG-Net generates instance-aware weights as well as real architectures according to the thresholds. Details are given in Appendix 6.4.\n\nOne noteworthy phenomenon is that the conditional gating mechanism decreases the performance in Identical mapping, which is contrary to DG-Net. This can be explained in 2 folds:\n* Under the definition of the exclusive gating, the weight of transformation is defined by $g(x)$, and $1-g(x)$ for identical mapping. But in this case, $g(x)$ approaches 0 will suppress the transformation function. This can be alleviated in the manner of shortcut-only gating. DG-Net overcomes this through the above-mentioned differences.\n* We point out that, according to Eqn. (7) in their paper, that recursively applying the weights of shortcuts will cause information decay and gradient vanishing. This could be the reason why performance drops. In DG-Net, there are direct connections and corresponding weights between layers. And the weights are independent of each other. It avoids recursively applying caused by increasing depth and suppression among gates.\n\n> Q2: Clarification on the source of improvements.\n\n----\n\nR2: In DG-Net, features from different input nodes are aggregated using a weighted sum. And the channels for different nodes are the same in a stage. Different from DenseNet, this avoids misalignment of channels caused by the removal or addition of edges. If the DenseNet architecture is used, additional modules will be introduced for channel matching, whose computation cost is much larger than that of routers. For a fair comparison, we choose constant gating in Appendix 6.3.\n\nTo further analyze the source of the improvement, we do statistical and visualization analyses of the learned connectivity in Appendix 6.5. In Fig. 7, it can be seen that for different samples, both the structure and the corresponding weights learned are different. \n\n> Q3: Dynamic setup for the ResNet.\n\n----\n\nR3: Yes, the graph is fully connected in each stage to preserve the spatial dimension. This is the same as the static network. We will indicate this in the paper.\n\n> Q4: Different initialization schemes for routers.\n\n----\n\nR4: In the original paper, the routers are initialized with non-bias. We further conduct different initialization schemes with positive {3} and negative {-3} biases in Appendix 6.4. These initialization methods correspond to existing connections and non-existent connections in the first iterations. Experimental results are given in Fig. 4. It can be seen that the positive initialization of bias achieves lower training loss in the early training procedure and obtains higher validation accuracy of 78.28% (78.00% for the bias of 0, 77.68% for the bias of -3). This suggests that initializing the connections to existing is better than unbiased and non-existent types.\n\nOther related experiments are rerun with bias initialized with 3, resulting in general improvements in Tables 1, 2, 3, 4, 5, 6. "}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "EldKO57oxqi", "original": null, "number": 4, "cdate": 1605892934800, "ddate": null, "tcdate": 1605892934800, "tmdate": 1606221486618, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "Ls_Z0Ge92k", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer3 - Part 2/2", "comment": "> Q3: Memory requirements in optimization and inference.\n\n----\n\nR3: We give the formula of gradient update in section 3.5 of Eqn. (7~10). Compared with static networks, the main additional memory during training is required by the aggregated features $\\mathbf{x}^{j^\\prime}$ for the purpose of back-propagation. It is related to the number of nodes in a graph. And the used \\textit{addition} operation of feature aggregation can effectively save memory instead of concatenation. We also test the actual memory increase during optimization. For ResNet50, the static network requires ~6G memory and ~8G for the dynamic one. For ResNet-101, the static network needs ~9G memory and ~12G for the dynamic one. DG-Net roughly requires an additional 30% memory during optimization. For much larger networks, some techniques (e.g. checkpoints and model parallel) can be used for training.\n\nSince the additional memory requirements are caused by the gradient update, this additional memory is not needed during inference. For ResNet50, the memory requirements are 3242M(static) and 3512M(dynamic) during inference. For ResNet101, the memory requirements are 3280M(static) and 3621M(dynamic) during inference. The additional memory requirements are acceptable compared to the improvements, which allows the deployment of resource-limit devices."}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "KRgw27ODcWl", "original": null, "number": 5, "cdate": 1605893060926, "ddate": null, "tcdate": 1605893060926, "tmdate": 1606221466915, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "Ls_Z0Ge92k", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer3 - Part 1/2", "comment": "We thank the reviewers for their suggestions and comments, which can help obtain some additional insights.\n\n> Q1: Differences with RandWire.\n\n----\n\nR1: DG-Net is largely different from RandWire in the aspect of search method, definition of search space, modeling method, performance, and conclusions. It can be summarized in 5 folds: \n\n* RandWire aimed to find graph generators that can produce new families of random models for further selection. And the hyper-parameters for the graph generators and generated architectures are selected by trial-and-error by human designers. While DG-Net intends to optimize the connectivity in a differentiable way, which is more effective and task-related.\n* RandWire defines the search spaces according to the properties of the graph generator. Once the graph is generated, only the weights of edges are learned. While DG-Net initializes the search space as complete graphs, which contains all possible connections. During training, both the architecture and the weights of connections can be learned, resulting in much larger search spaces.\n* RandWire assigns each edge with a learnable weight. During inference, the weight is used for all samples. In DG-Net, the weights are generated based upon inputs, allowing more flexible connections. The proposed instance-aware connectivity also overcomes challenges on modeling and forward computation. A router along with the node is used to generate weights for the output edges, which is effective and efficient (discussed in Appendix 6.2). DG-Net requires the aggregation of features differently for each input sample. The proposed buffer mechanism (in section 3.4) successfully solves this without introducing excessive computation or time-consuming burden.\n* We compare the performance in Table 4. The mean and standard deviation of results are given through 5 repeat runs. DG-Net obtains higher performance (73.52 vs 71.34/71.16/72.26) under the small computation regime proposed in section 4.2 of their paper. More importantly, the result of DG-Net is more stable with a small deviation (0.05 vs 0.40/0.34/0.27). RandWire largely suffers from randomness even under the best setting in their paper. This phenomenon is consistent with Fig. 3 in their paper, where large variances exist under the same setting of graph generators.\n* RandWire claimed that random graphs generated by well-defined graph generators are good enough. While DG-Net demonstrates that the connectivity for each sample can be optimized, resulting in better performance than the static architectures.\n\n| wiring type| top-1 |\n| ---- | ---- |\n| ER(P=0.2) | $71.34\\pm 0.40$ |\n| BA(M=5) | $71.16\\pm 0.34$ |\n| WS(K=4, P=0.75) | $72.26\\pm 0.27$ |\n| DG-Net |$73.52\\pm 0.05$ |\n\n> Q2: Statistical and visualization analyses of the learned connectivity.\n\n----\n\nR2: Details are added in Appendix 6.5. To analyze the architecture representations, we visualize the learned connectivity through the adjacency matrices as noted in section 3.4. For the statistical analysis, we show the distribution of the mean of the weights of edges in Fig. 5 and the distribution of standard deviation in Fig. 6. To visually analyze the connections corresponding to different samples, we give the learned connectivity in Fig. 7. Some observations and analyses can be made:\n* From the statistical analysis in Fig. 5, the weights of connections between different nodes have obvious differences. The difference is related to the topological orders of the nodes and the stages located.\n* Statistically, in a graph, the output edges of the nodes in the front of topological orders have larger weights. This can be explained that for a node with the order of $i$, the generated $\\mathbf{x}_i$ can be received by node $j$ (where $j > i$). This causes the features generated by the front nodes to participate in aggregation as a downstream input. It makes the front nodes contribute more, which can be used to reallocate calculation resources in future work.\n* From Fig. 6, it can be seen that there exist discrepancies in weight changes for different edges with respect to input samples. The difference is also related to the topological orders of the nodes and the stages located.\n* Interestingly, in a stage, the edges of the nodes in the back of topological orders have a larger variance. Similarly, the weights of edges in deeper stages also have a larger variance. We speculate that it is related to the level of semantic information of features. Specifically, features generated by the deep layers have high-level semantic information and the correlation of samples is stronger than features with low-level information generated by the shallow layers.\n* In Fig. 7, it can be seen that for different samples, both the structure and the corresponding weights learned by routers are different. For some easy samples, part edges are masked, resulting in a lighter model. This benefits computational efficiency and can be studied further in future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "PSnEh6-4_ub", "original": null, "number": 3, "cdate": 1605887975115, "ddate": null, "tcdate": 1605887975115, "tmdate": 1606221420939, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "mLzmYfChPw", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "We thank the reviewers for their suggestions and comments, which can help improve the paper.\n\n> Q1: About the novelty and comparisons with (Yuan et al., 2020).\n\n----\n\nR1: Compared with (Yuan et al., 2020), DG-Net is different in 5 folds:\n\n* (Yuan et al., 2020) tried to learn a fixed topology for the network, whose method is searching for the weights of edges in a complete graph. During inference, the weights for different samples are the same. In DG-Net, the weights of edges are generated based upon inputs, allowing more flexible connectivity patterns.\n* Due to the above advantage, DG-Net owns a larger search space than (Yuan et al., 2020). Their method is limited by the size of search space and cannot achieve significant improvement in small spaces (in their paper section 4.1, 4.2). This limits the application on some classic networks where each stage only has 3~6 layers. DG-Net overcomes this shortcoming and has been verified on many architectures. DG-Net surpasses baselines in large margins up to 1.61% in ImageNet in Table 1.\n* The proposed instance-aware connectivity also overcomes challenges on modeling and forward computation. A router along with the node is used to generate weights for the output edges, which is effective and efficient (discussed in Appendix 6.2). DG-Net requires the aggregation of features differently for each input sample. The proposed buffer mechanism (in section 3.4) successfully solves this without introducing excessive computation or time-consuming burden.\n* Instead of only learning the weights of edges in (Yuan et al., 2020), DG-Net also adjusts the real architectures according to the learned thresholds, which can bring better acceleration. We also add the statistical and visualization analyses of the learned connectivity in Appendix 6.5, which bring some interesting observations and may lead to future work.\n* Last, DG-Net achieves obviously better experimental results than (Yuan et al., 2020). We update the ablation study in Appendix 6.3, which reflects the improvements brought by the dynamic graph itself. In classical networks of ResNet-18/50/101 and MobileNet-v2, the proposed dynamic graph outperforms static networks in large margins. Compared with networks with learnable weights of complete graphs, DG-Net also demonstrates the effectiveness of the instance-aware connectivity. \nAnd the results also prove that DG-Net overcomes the limitation of search spaces met in (Yuan et al., 2020).\n\n| backbone | method | top-1 | gain |\n| ---- | ---- | ---- | ---- |\n| ResNet-18 | baseline | 70.30 | - |\n| ResNet-18 | leanable weights | 70.51 | +0.21 |\n| ResNet-18 | dynamic graph | 71.32 | +1.02 |\n| ResNet-50 | baseline | 76.70 | - |\n| ResNet-50 | leanable weights | 77.00 | +0.30 |\n| ResNet-50 | dynamic graph | 78.28 | +1.58 |\n| ResNet-101 | baseline | 78.29 | - |\n| ResNet-101 | leanable weights | 78.64 | +0.35 |\n| ResNet-101 | dynamic graph | 79.90 | +1.61 |\n| MBNet-v2-1.0| baseline | 72.60 | - |\n| MBNet-v2-1.0| leanable weights | 72.86 | +0.26 |\n| MBNet-v2-1.0| dynamic graph | 73.54 | +0.94 |\n\n> Q2: DAG representation of ResNet.\n\n----\n\nR2: Thanks to the reviewer for pointing this out. We show the nature view of ResNet as E={(i,j)|j=i+1,i+2}. Under the unrolled type in (Veit et al., 2016), the representation can be denoted as the wiring pattern as DenseNet. We have corrected this in the paper.\n\n> Q3: The definition of the convolutional block is not clear, is it a ReLU-conv-BN triplet as in (Xie et al., 2019)?\n\n----\n\nR3: Yes. We follow the definition in (Xie et al., 2019) which allows the aggregation to receive both positive and negative activation, preventing the aggregated activation from being inflated in case of a large input degree. We will add this to the paper.\n\n> Q4: The use of a DAG with edge weights for representing the architecture is not novel, it was already introduced in (Xie et al., 2019).\n\n----\n\nR4: The difference is the way of modeling the weights of edges. And DG-Net is also largely different from randwire in motivation, the capacity of search space, performance, and conclusions. Details are given in the Response to Reviewer3 of R1.\n\n> Q5: The original performance of RegNet-X and RegNet-Y in Table 5.\n\n----\n\nR5: We add the original performance under the same training setting. For RegNet-X-600M, the accuracy is 75.03%. DG-Net obtains 0.81% improvement. For RegNet-Y-600M, the accuracy is 76.1%. DG-Net improves accuracy by 0.90%. Since the architectures are already the best in a search space with $10^{18}$ possible configurations, the results are considerable.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "unQg7n7EzHR", "original": null, "number": 9, "cdate": 1605953837415, "ddate": null, "tcdate": 1605953837415, "tmdate": 1605955031478, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment", "content": {"title": "Paper Update", "comment": "Thanks to all the reviewers for their constructive suggestions and comments. \n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed 6 aspects of works:\n* The gradient update rules are given in section 3.5 for the parameters of convolutional operations and routers. \n* Instead of applying the non-bias initialization scheme for the biases of routers, we initialize the biases with positive ones of 3, resulting in the existence of connections in the first iterations. Through an ablation study on different initialization schemes in Appendix 6.4, we find the positive one is better. So we retrain related experiments in ImageNet and COCO, results are generally improved and updated in Tables 1,2,3,4,5. \n* More detailed comparisons with different connectivity methods are given in Appendix 6.3, which further verify the improvements brought by dynamic graphs themselves.\n* Repeat runs are conducted in Tables 3,4. Results show consistent improvements with a small variance compared with related methods..\n* Detailed statistical and visualization analyses are given in Appendix 6.5, which demonstrate the distributions of the learned connectivity. And some interesting phenomena have been observed, which can inspire future work.\n* The formatting of tables has been re-drawn to improve readability."}, "signatures": ["ICLR.cc/2021/Conference/Paper11/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RuUdMAU-XbI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper11/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper11/Authors|ICLR.cc/2021/Conference/Paper11/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875212, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Comment"}}}, {"id": "Ls_Z0Ge92k", "original": null, "number": 2, "cdate": 1603814149555, "ddate": null, "tcdate": 1603814149555, "tmdate": 1605024777372, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Review", "content": {"title": "Interesting extension of randomly wired architectures", "review": "Pros:\n- Interesting extension of RandWire to learn better architectures\n- Good experimental results\n\nCons:\n- Idea could be seen as minor modification of RandWire\n- Doubts about memory requirements\n\nThe paper proposes an improve over the idea of randomly wired architectures [1] by exploiting a complete graph where edges are weighted by dynamically computed weights. Dynamically computing edge weights allows the network to optimize its topology. While this idea could be seen as a relatively small modification of [1], it is still very interesting and the results prove its effectiveness in commonly used tasks.\n\nIt would have been interesting to show an analysis of what the method is actually learning, e.g. if a particular topology emerges from the adjacency matrix, in order to provide more insights on why the method is effective.\n\nI also have a concern about memory requirements that I would like the authors to address. It is true that DG-Net does not require significantly more FLOPS or trainable parameters. However, you use a complete graph which means that it requires to store in memory O(L^2) activation tensors, i.e. the activation of each convolutional layer weighted by edge weight. This could become a limitation for larger networks. Note that this is also a problem for [1] but, in that case, the sparse connectivity mitigates the issue. \n\n\n[1] Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He.  Exploring randomly wired neural networks for image recognition.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151657, "tmdate": 1606915784037, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper11/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Review"}}}, {"id": "0ZLdWwSnSdY", "original": null, "number": 4, "cdate": 1603900777783, "ddate": null, "tcdate": 1603900777783, "tmdate": 1605024777241, "tddate": null, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "invitation": "ICLR.cc/2021/Conference/Paper11/-/Official_Review", "content": {"title": "A radical new architecture, but unfortunately the actual design is not fully convincing and based on strong theoretical foundations.", "review": "\nThe paper discusses a model for learning the architecure of a convolutional networks starting from a fully connected graph. The idea is to learn the adjacency graph of the model together with the weights of the networks.\n\nStrenghts:\n\n- The idea of thinking out-of-the-box by imagining new architectures is very attractive and interesting.\n\nWeaknesses:\n\n- The actual advantages in the model do not look apparent given the results in the evaluation section.\n\n- The theoretical foundation of this work is unclear. For example, it is unclear how the proposed solution will work in practice in terms of back-propagation.\n\n- The performance results show that the proposed method is characterized by performance close to those of existing methods.\n\n\nIn general, I really welcome this type of work: non-conventional, experimental and quite radical in terms of approach. However, unfortunately, the authors do not provide a convincing description of their approach. Unfortunately, it does not appear that the method is developed on a sufficiently strong theoretical basis. For example, it is unclear how back-propagation work in these circumstances when you don't have a stacked architecture.\n\nThe choice of the thresholds and the actual learning of the adjacency matrix is not described in sufficient detail.\n\nThe actual computation complexity and the trade-offs in terms of computational complexity/accuracy is unclear.\n\nIn the experimental results, the actual performance of the method appear very similar to the other methods. In some cases they might be the same since the confidence intervals are overlapping.\n\nQuestions:\n\n- What is the theoretical foundation of the method? Given the fully connected nature of the graph, how does back-propagation work in this case? In a sense, in fact, you have a DAG, how do you deal with cycles? When you do you stop the back-propagation if you do not have a stacked architecture?\n\n- Could you please provide the confidence intervals for all the results you presented? In fact, it seems that the values related to your approach are better than existing techniques in some cases but they look very close?\n\n- Is the computational complexity justified? Also note: you probably need a larger number of samples to learn the additional adjacency graph. What is the trade-off? Given the gain in terms of performance, the actual additional complexity might not be completely justified.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper11/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper11/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "authorids": ["~Kun_Yuan1", "~Quanquan_Li1", "~Dapeng_Chen4", "~Aojun_Zhou2", "~Junjie_Yan1"], "authors": ["Kun Yuan", "Quanquan Li", "Dapeng Chen", "Aojun Zhou", "Junjie Yan"], "keywords": ["dynamic network", "data-dependent", "complete graph"], "abstract": "One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \\textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.", "one-sentence_summary": "Dynamic Graph Networks promote the model capacity by performing instance-aware connectivity for neural networks.", "pdf": "/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|dynamic_graph_learning_instanceaware_connectivity_for_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LOPTWNM4p2", "_bibtex": "@misc{\nyuan2021dynamic,\ntitle={Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks},\nauthor={Kun Yuan and Quanquan Li and Dapeng Chen and Aojun Zhou and Junjie Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=RuUdMAU-XbI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RuUdMAU-XbI", "replyto": "RuUdMAU-XbI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper11/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151657, "tmdate": 1606915784037, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper11/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper11/-/Official_Review"}}}], "count": 14}