{"notes": [{"id": "pAj7zLJK05U", "original": "VUWPQtIEpb", "number": 2915, "cdate": 1601308323344, "ddate": null, "tcdate": 1601308323344, "tmdate": 1614985644015, "tddate": null, "forum": "pAj7zLJK05U", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gCxKzOZl2UF", "original": null, "number": 1, "cdate": 1610040518003, "ddate": null, "tcdate": 1610040518003, "tmdate": 1610474126387, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers liked the concept of the zero-day attack and yet raised different concerns about the other parts of the paper. In general, Reviewers wanted to see more thorough experimental evaluations (e.g., against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses. AC encourages authors to incorporate Reviewers' comments when preparing the paper for elsewhere."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040517990, "tmdate": 1610474126372, "id": "ICLR.cc/2021/Conference/Paper2915/-/Decision"}}}, {"id": "euiuHQe4h16", "original": null, "number": 7, "cdate": 1606197082148, "ddate": null, "tcdate": 1606197082148, "tmdate": 1606197082148, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "v4-wHTr_aZW", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Official_Comment", "content": {"title": "Response to Nicholas Carlini", "comment": "Thank you for pointing out the mistake in our current version. We make a mistake in plotting Table 2 and Table 6. The results in these tables are for detecting $l_{\\inf}$ attacks. As we state in the experimental setup, the $l_{\\inf}$ attacks considered are PGD, BIM, and FGSM rather than BB, CW, DF. (We use the same table format from Table 1 and forget to replace the attack algorithm name) The considered $l_{\\inf}$  attacks are not designed to find the decision boundary of a neural network.\nActually, for these attacks, our approach detection performance is not as good as detecting boundary-based attacks. We discussed why the performance drops in section 4.3.2. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2915/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pAj7zLJK05U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2915/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2915/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2915/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2915/Authors|ICLR.cc/2021/Conference/Paper2915/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Official_Comment"}}}, {"id": "v4-wHTr_aZW", "original": null, "number": 1, "cdate": 1605223453360, "ddate": null, "tcdate": 1605223453360, "tmdate": 1605223453360, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Public_Comment", "content": {"title": "Is it fundamental that adversarial examples are close to the decision boundary?", "comment": "All of the attacks studied are attacks explicitly designed to find the decision boundary of a neural network. There are other attacks without this objective--for example PGD from Madry et al. 2017 as mentioned in this paper. Does the proposed technique work at defending against these attacks, too? Table 1,2,5,6 all only use the boundary-finding attacks. Did I miss these results somewhere?"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicholas_Carlini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pAj7zLJK05U", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/Authors", "ICLR.cc/2021/Conference/Paper2915/Reviewers", "ICLR.cc/2021/Conference/Paper2915/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024958894, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Public_Comment"}}}, {"id": "CvENlRiR5Ve", "original": null, "number": 1, "cdate": 1603433597939, "ddate": null, "tcdate": 1603433597939, "tmdate": 1605024105216, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review", "content": {"title": "Insufficient experiments and unreasonable assumption ", "review": "This paper proposes to use a counterattack strategy to attack an input x, and calculate the distance between x and the crafted example x' as the detection metric. There are two main concerns about this paper:\n\n1. The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism. There is no guarantee on the effectiveness of AttackDist when the attacks do not follow assumed patterns.\n\n2. In experiments, there are only oblivious attacks, where the adversaries do not know the mechanism of AttackDist. For a defense method, it is necessary to carefully design a convinced adaptive attack and demonstrate the effectiveness of the defense under the adaptive attack.\n\nMinors:\nIn the introduction section, the authors claim that \"Existing adversarial defense techniques could be classified into two main categories: adversarial training and detection\". Actually, there are many other types of defenses including input processing, robust architecture, random smoothing, certified defenses, etc. Besides, existing adversarial training methods like FastAT can easily scale to ImageNet, running for several hours on a single GPU. The authors should be more updated on these related progresses.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2915/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086101, "tmdate": 1606915805166, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2915/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review"}}}, {"id": "XVgLpE5Sb35", "original": null, "number": 2, "cdate": 1603879918507, "ddate": null, "tcdate": 1603879918507, "tmdate": 1605024105152, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review", "content": {"title": "Review for Paper2915", "review": "This paper proposes a method to detect adversarial examples. The detection\nscheme is based on the observations that typical adversarial attacks generate\nadversarial examples on the decision boundary, so if we use a \"counter attack\"\non the adversarial example, it will be easy to change its label.\n\nA main weakness of this paper is that the proposed approach does not include an\nadaptive attack for evaluation.  If the proposed detection scheme is known to\nthe attacker, the attacker can still generate visually indistinguishable\nadversarial examples that the detector fails to detect. This can usually be\ndone by adding the detection objective to the loss function for attack.  Many\nheuristic adversarial example detections and defense methods have been broken\nby stronger and adaptive attacks [1,2], and the use of adaptive attacks is\ncrucial [3].\n\nAdditionally, although the paper claims to detect zero-day, or unknown attacks,\nin evaluation the selection of attacks are quite limited. For example, it only\nincludes gradient based attacks, but not decision based attacks or evolutional\nadversarial attacks.\n\nThe paper attempts to make several theoretical justifications, but these\ntheorems are too simple (e.g., based on direct application of triangle\ninequality) and do not significantly improve the contribution of this paper.\n\nAs a conclusion, I cannot support the acceptance of this paper because the\nnovelty of the proposed method is limited and evaluation is insufficient.\n\n\n[1] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420 (2018).\n\n[2] Carlini, Nicholas, and David Wagner. \"Adversarial examples are not easily detected: Bypassing ten detection methods.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017.\n\n[3] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705 (2019).\n\n[4] Brendel, Wieland, Jonas Rauber, and Matthias Bethge. \"Decision-based adversarial attacks: Reliable attacks against black-box machine learning models.\" arXiv preprint arXiv:1712.04248 (2017).\n\n[5] Cheng, Minhao, et al. \"Query-efficient hard-label black-box attack: An optimization-based approach.\" arXiv preprint arXiv:1807.04457 (2018).\n\n[6] Alzantot, Moustafa, et al. \"Genattack: Practical black-box attacks with gradient-free optimization.\" Proceedings of the Genetic and Evolutionary Computation Conference. 2019.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2915/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086101, "tmdate": 1606915805166, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2915/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review"}}}, {"id": "nv8uxq46l1A", "original": null, "number": 3, "cdate": 1604020600844, "ddate": null, "tcdate": 1604020600844, "tmdate": 1605024105093, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review", "content": {"title": "NA", "review": "Summary: this paper is about adversarial detection based on counter-attack. The main intuition is that the adversarial sample lies closer to the decision boundary and hence if we do counter-attack to the data, the perturbation is expected to be much smaller than the clean data.\n\nAlthough the idea sounds interesting, I have a few questions to be answered:\n1. in eq(1) you already defined $\\Delta$ as the minimum distortion, how come it is larger than $\\delta^*$ in eq(3)?\n2. you assumed the $\\| y-x \\|_p=D(x)\\sim N(\\mu, \\sigma^2)$, have you verified this empirically?\n3. since $D(x)$ is obtained with optimal perturbation, i.e. $\\|\\delta^*\\|$, how do you obtain the $\\mu$ and $\\sigma$?\n4. from theorem (3) it seems that this detection method works for advanced attack, which requires $r_2 < r_1$, what if the attacker is a bad algorithm causing adversarial samples far from the boundary?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2915/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086101, "tmdate": 1606915805166, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2915/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review"}}}, {"id": "MIAbWkKNoi", "original": null, "number": 4, "cdate": 1604259843192, "ddate": null, "tcdate": 1604259843192, "tmdate": 1605024105037, "tddate": null, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "invitation": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review", "content": {"title": "Confusion in the proof of lemma 1", "review": "Summary:\n\nThe authors propose a novel approach to detect adversarial examples by measuring the perturbation norm after a counterattack. They theoretically provide a certified detecting performance and empirically show that AttackDist can characterize zero-day adversarial samples.\nStrength:\n1. The paper is well-organized and easy to understand.\n2. Zero-day attacks are challenging but more practical than other adversarial samples detection problems. AttackDist can perform significantly better than the baselines to distinguish zero-day adv examples.\n3. The authors show experiment results across different attacks and adversarial distances; AttackDist is consistently better than baselines.\n \n\nQuestions:\nThe idea of this paper is interesting and i would like to raise my rating if the authors can clarify my questions.\n\n1. The proof of lemma 1 is the key to developing AttackDist. However, I am confused about 'Due to the continuous of f, \u2026'. My question is how you make sure there always exists the point P between x and x*, such that g(P)=0; what if the deep neural network output is bounded.\n2. Why not evaluate the BlackBox attacking approaches like ZOO and NATTACK[1,2]? Especially, NATTACK learns adversarial examples' distributions, and AttackDist is based on adversarial samples' distribution.\n3. I am curious about whether AttackDist still works for large margin-based classifiers; what if the deep neural network is optimized with large margin loss function.\n\n\n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017.\n[2] Li, Yandong, et al. \"Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks.\" arXiv preprint arXiv:1905.00441 (2019).", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2915/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2915/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack", "authorids": ["~Simin_Chen1", "~Zihe_Song1", "~Lei_Ma1", "~Cong_Liu2", "wei.yang@utdallas.edu"], "authors": ["Simin Chen", "Zihe Song", "Lei Ma", "Cong Liu", "Wei Yang"], "keywords": [], "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial attacks, which could produce adversarial samples that easily fool the state-of-the-art DNNs. The harmfulness of adversarial attacks calls for the defense mechanisms under fire. However, the relationship between adversarial attacks and defenses is like spear and shield. Whenever a defense method is proposed, a new attack would be followed to bypass the defense immediately. Devising a definitive defense against new attacks~(zero-day attacks) is proven to be challenging. We tackle this challenge by characterizing the intrinsic properties of adversarial samples, via measuring the norm of the perturbation after a counterattack. Our method is based on the idea that, from an optimization perspective, adversarial samples would be closer to the decision boundary; thus the perturbation to counterattack adversarial samples would be significantly smaller than normal cases. Motivated by this, we propose AttackDist, an attack-agnostic property to characterize adversarial samples. We first theoretically clarify under which condition AttackDist can provide a certified detecting performance, then show that a potential application of AttackDist is distinguishing zero-day adversarial examples without knowing the mechanisms of new attacks. As a proof-of-concept, we evaluate AttackDist on two widely used benchmarks. The evaluation results show that AttackDist can outperform the state-of-the-art detection measures by large margins in detecting zero-day adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|attackdist_characterizing_zeroday_adversarial_samples_by_counter_attack", "pdf": "/pdf/44b02f034a3259917a79f05861ebc5bd6d5f8570.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=c4JeF6aNP", "_bibtex": "@misc{\nchen2021attackdist,\ntitle={AttackDist: Characterizing Zero-day Adversarial Samples by Counter Attack},\nauthor={Simin Chen and Zihe Song and Lei Ma and Cong Liu and Wei Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pAj7zLJK05U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pAj7zLJK05U", "replyto": "pAj7zLJK05U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086101, "tmdate": 1606915805166, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2915/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2915/-/Official_Review"}}}], "count": 8}