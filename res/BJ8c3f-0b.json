{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519466048841, "tcdate": 1509137567295, "number": 967, "cdate": 1518730159923, "id": "BJ8c3f-0b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJ8c3f-0b", "original": "rkhHnGZ0W", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260093530, "tcdate": 1517249533898, "number": 292, "cdate": 1517249533884, "id": "SkIWEkaSM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This work develops importance weighted autoencoder-like training but with sequential Monte Carlo.  The paper is interesting, well written and the methods are very timely (there are two highly related concurrent papers -  Naesseth et al. and Maddison et al.).  Initially, the reviewers shared concerns about the technical details of the paper, but the authors appear to addressed those and two reviews have been raised accordingly.   There is one outlier review (two 7s and one 3).  The 3 is the least thorough and has the lowest confidence (2) so that review is being weighted accordingly.\n\nThis appears to be a timely and interesting paper that is interesting to the community and warrants publication at ICLR.\n\nPros:\n- Well written and clear\n- An interesting approach\n- Neat technical innovations\n- Generative deep models are of great interest to the community (e.g. Variational Autoencoders)\n\nCons:\n- Could include a better treatment of recent related literature\n- Leaves a variety of open questions about specific details (i.e. from the reviews)", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516096458140, "tcdate": 1516096428411, "number": 8, "cdate": 1516096428411, "id": "rkN3iHo4f", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "SybDOZMEz", "signatures": ["ICLR.cc/2018/Conference/Paper967/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/Authors"], "content": {"title": "Response to Response to author feedback", "comment": "Thank you for the further comments.\n\n%%% You argue that increased K ... %%%\n\n>>> The argument about increasing K being detrimental to optimizing q(z|x) is actually distinct to the bound potentially not being tight.  Increasing K can be detrimental because of undermining our ability to reliably estimate the gradients, but is not detrimental in the true gradient direction itself.  The reason the bound does not become tight is that the \u201cbest\u201d SMC proposal is generally not for q(x|y)=p(x|y). Thus is it still important to optimize q(x|y), it is just that this optimum is distinct to p(x|y).  Increasing K can still be detrimental to this process because of reducing the signal to noise ratio of the gradient estimates, thus harming our ability to learn the optimal SMC proposal.\n\n%%% You show that for finite time ... This relates to another ... %%%\n\n>>> We have run the experiment in the LGSSM example for more iterations (Figure 3 updated in latest revision) to characterize this further.  Our result now show that SMC-10 appears to be converging to a worse proposal that ALT (in terms of the marginal posterior), while SMC-1000 is still yet to converge.\nFrom the perspective of the theoretical limit of infinite computation in a generic stochastic gradient ascent scheme than our results on the SNR (which are the motivation for ALT) are inconsequential because they only relate to variance of the estimator. However, the assumptions required for this convergence are typically not satisfied for neural network training anyway, so the impact of the SNR may still be felt in the limit for real neural net training.  Moreover, this theoretical converge limit is typically very distinct to the point at which the training saturates - training can appear to have converged far more quickly than it truly has.\n\nAside to our SNR arguments, there are also differences in the proposal achieved by ALT in the limit because of the effects previously discussed about the optimal importance sampling and SMC proposals being distinct (note the optimal SMC proposal also varies with K).  It can be hard to assess which of these is \u201cbetter\u201d because there is no objective metric for what a good q is.  For example, the KL tends to over-prefer low variance q\u2019s for use as proposal distributions.  To try and investigate the relative metrics of different proposals, we have done an empirical investigation into training \\phi for a given \\theta as discussed next.\n\n%%% I think it would be very illustrative ... %%%\n\n>>> We have run additional experiments (included in a revision in Appendix E.1.) which confirm that if we want to optimize just \\phi, then running IWAE with low number of particles is the best. In the experiment in Appendix E.1., we sweep through all possibilities of train_algorithm x train_number_of_particles x test_algorithm x test_number_of_particles where\n    - train_algorithm = test_algorithm = {IS, SMC}\n    - train_number_of_particles = test_number_of_particles = {10, 100, 1000}\n\nIn the experiment, we confirm that using the IWAE objective with low number of particles results in a better \\phi (in terms of inference) than optimizing AESMC with higher number of particles. Investigating other axes of variation:\n    - Inference performance worsens when we increase K_train.\n    - Inference performance improves when we increase K_test.\n    - The worst inference performance happens when we train with SMC with a lot of particles and test with IS with few particles.\n    - The best inference performance happens when we train with IS with few particles and test with SMC with a lot of particles.\n\n%%% Could this also be related to the bias in the AESMC gradients? %%%\n\n>>> As explained in our earlier comment, we think this is mostly tangential to the fact that the optimal q(x|y) for SMC is not the marginal posterior.\n\n%%% In Figure 4 you make mention to max(IS, SMC), in the experiments which one has been picked in each case? Does the ALT algorithm tend to pick one over the other?%%%\n\n>>> We currently don\u2019t have data for this but will include it in a later revision.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "tmdate": 1516096179724, "tcdate": 1516096179724, "number": 7, "cdate": 1516096179724, "id": "r132qrjEG", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "Syi1Z34ZG", "signatures": ["ICLR.cc/2018/Conference/Paper967/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/Authors"], "content": {"title": "Thank you", "comment": "Thank you for your further consideration and bumping up your score."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515804788643, "tcdate": 1512517858764, "number": 3, "cdate": 1512517858764, "id": "Syi1Z34ZG", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "signatures": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Some very interesting ideas and empirical results. But there are a couple of issues.", "rating": "7: Good paper, accept", "review": "Update:\nOn further consideration (and reading the other reviews), I'm bumping my rating up to a 7. I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al. and Maddison et al. work).\n\n-----------\n\nThis paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling. Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE-style bounds (w.r.t. the inference parameters) grows with their accuracy. They therefore propose using a more-biased but lower-variance bound to train the inference parameters, and the more-accurate bound to train the generative model.\n\nOverall, I found this paper quite interesting. There are a few things I think could be cleared up, but this seems like good work (although I'm not totally up to date on the very recent literature in this area).\n\nSome comments:\n\n* Section 4: I found this argument extremely interesting. However, it\u2019s worth noting that your argument implies that you could get an O(1) SNR by averaging K noisy estimates of I_K. Rainforth et al. suggest this approach, as well as the approach of averaging K^2 noisy estimates, which the theory suggests may be more appropriate if the functions involved are sufficiently smooth, which even for ReLU networks that are non-differentiable at a finite number of points I think they should be.\n\nThis paper would be stronger if it compared with Rainforth et al.\u2019s proposed approaches. This would demonstrate the real tradeoffs between bias, variance, and computation. Of course, that involves O(K^2) or O(K^3) computation, which is a weakness. But one could use a small value of K (say, K=5).\n\nThat said, I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples. Basically, these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail.\n\n* Section 3.3: Claim 1 is an interesting observation. But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior. I think there\u2019s an easier way to make this argument:\n\nGiven an unbiased estimator \\hat{Z} of Z, by Jensen\u2019s inequality E[log \\hat{Z}] \u2264 log Z, with equality iff the variance of \\hat{Z} = 0. The only way to get an SMC estimator\u2019s variance to 0 is to drive the variance of the weights to 0. That only happens if you perfectly sample each particle from the true posterior, conditioned on all future information.\n\nAll of which is true as far as it goes, but I think it\u2019s a bit of a distraction. The question is not \u201cwhat\u2019s it take to get to 0 variance\u201d but \u201chow quickly can we approach 0 variance\u201d. In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large. (Although [particle] MCMC is probably a better choice if one wants extremely low bias.)\n\n* Section 3.2: The choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work, but there\u2019s only a very cursory discussion in the main text. I would recommend at least summarizing the main findings of Appendix A in the main text.\n\n* A relevant missing citation: Turner and Sahani\u2019s \u201cTwo problems with variational expectation maximisation for time-series models\u201d (http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf). They discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates (though in a quite different context and with a quite different analysis).\n\n* Figure 1: What is the x-axis here? Presumably phi is not actually 1-dimensional?\n\nTypos etc.:\n\n* \u201clearn a particular series intermediate\u201d missing \u201cof\u201d.\n\n* \u201cTo do so, we generate on sequence y1:T\u201d s/on/a/, I think?\n\n* Equation 3: Should there be a (1/K) in Z?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642536170, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer2", "ICLR.cc/2018/Conference/Paper967/AnonReviewer1", "ICLR.cc/2018/Conference/Paper967/AnonReviewer3"], "reply": {"forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642536170}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642536392, "tcdate": 1511308626397, "number": 1, "cdate": 1511308626397, "id": "HkqLp4GeM", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "signatures": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Auto-encoding Sequential Monte Carlo", "rating": "3: Clear rejection", "review": "Overall:\nI had a really hard time reading this paper because I found the writing to be quite confusing. For this reason I cannot recommend publication as I am not sure how to evaluate the paper\u2019s contribution. \n\nSummary\nThe authors study state space models in the unsupervised learning case. We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters. We want to infer a posterior density given some data.\n\nThe authors propose an algorithm which uses sequential Monte Carlo + autoencoders. They use a REINFORCE-like algorithm to differentiate through the Monte Carlo. The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters.\n\nThe authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments. \n\nThe proposed algorithm seems novel, but I do not understand a few points which make it hard to judge the contribution. Note that here I am assuming full technical correctness of the paper (and still cannot recommend acceptance).\n\nIs the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)? This was very unclear to me.\n\nThe introduction/experiments section of the paper is not well motivated. What is the problem the authors are trying to solve with AESMC (over existing methods)? Is it scalability? Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)? \n\nThe experiments feel lacking. There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) method of IWAE. We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge? Does AESMC give a better generative model?). The authors discuss the advantages of SMC and say that is scales better than other methods, it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642536170, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer2", "ICLR.cc/2018/Conference/Paper967/AnonReviewer1", "ICLR.cc/2018/Conference/Paper967/AnonReviewer3"], "reply": {"forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642536170}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642536233, "tcdate": 1511798238027, "number": 2, "cdate": 1511798238027, "id": "BkaCH2KlG", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "signatures": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting extension of IWAEs using SMC", "rating": "7: Good paper, accept", "review": "[After author feedback]\nI think the approach is interesting and warrants publication. However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments. In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al. \n\n[Original review]\nThe authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure. The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.\n\nThe approach is interesting and the paper is well-written, however, I have some comments and questions:\n\n- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. This seems to mean that we should not expect for q -> p when K increases?\n- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made. If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC? Have you estimated the probability for positive vs. negative gradient values for  K=10? To me it looks like the probability of it being larger than zero is something like 2/3. K>10 is difficult to see from this plot alone.\n- Is there a typo in the bound given by eq. (17)? Seems like there are two identical terms. Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?\n- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS. Have you tried this for high-dimensional models as well? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO. For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1? Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles? I suspect this lower number of particles might be model-dependent.\n\nMinor comments:\n- Section 1, first paragraph, last sentence, \"that\" -> \"than\"?\n- Section 3.2, \"... using which...\" formulation in two places in the firsth and second paragraph was a bit confusing\n- Page 7, second line, just \"IS\"?\n- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?\n- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642536170, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer2", "ICLR.cc/2018/Conference/Paper967/AnonReviewer1", "ICLR.cc/2018/Conference/Paper967/AnonReviewer3"], "reply": {"forum": "BJ8c3f-0b", "replyto": "BJ8c3f-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642536170}}}, {"tddate": null, "ddate": null, "tmdate": 1515489369449, "tcdate": 1515489369449, "number": 5, "cdate": 1515489369449, "id": "SybDOZMEz", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "H1Uo_Namf", "signatures": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"], "content": {"title": "Response to author feedback", "comment": "Thank you for the clarifying comments, I will adjust my review accordingly.\n\n%%% It seems clear that the AESMC bound does not, in general, optimize for q(x|y) to be close to p(x|y)... %%%\nYou argue that increased K is detrimental to optimize q(z|x) to be close to p(z|x). But if the method is not designed to optimize the proposal to be close to the posterior, which you seem to agree with above, why should this be an issue? \n\nYou show that for finite time the alternate seems to perform better, but do you have any results on whether this extends to when the high-sample version has actually converged? I'm referring to the LGSSM example, where the SMC-1000 hasn't yet converged. Paraphrased will the alternate still be better in the limit of infinite computation?\n\nThis relates to another whether if it is possible that further gains on the alternate version can be achieved by an increasing schedule of particles for the proposal learning.\n\n%%% The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive ... %%%\nI think it would be very illustrative if there were experiments where the focus is actually on learning only \\phi, instead of both \\phi and \\theta. Now the claim seems to be that learning proposals (\\phi) with the IWAE  objective and a low number of particles is actually a better way of learning proposals for AESMC with a higher number of particles, rather than directly optimizing the AESMC objective. Could this also be related to the bias in the AESMC gradients?\n\nIn Figure 4 you make mention to max(IS, SMC), in the experiments which one has been picked in each case? Does the ALT algorithm tend to pick one over the other?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "tmdate": 1515178473728, "tcdate": 1515178376290, "number": 4, "cdate": 1515178376290, "id": "BkgqKr6Xf", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "Syi1Z34ZG", "signatures": ["ICLR.cc/2018/Conference/Paper967/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/Authors"], "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for taking the time to review our paper and for their helpful feedback.\n\n%%% Relationship with Rainforth et al %%%\n\n> We would like to clarify that Rainforth et al do not propose any algorithmic approach, they only express the IWAE and VAE objectives in a general form and then carry out a theoretical analysis on which we build.  One can of course always use more Monte Carlo samples (i.e. increase M in their notation) to increase the fidelity of estimates.  The interesting approach which you are suggesting of using K^2 estimates here could be achieved by running K SMC sweeps of K samples.  We agree that this could be an interesting further extension on top of our suggested approach, but we did not have the time to actively investigate it for this revision.\n\n\n%%% Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior...  %%%\n\n> Your intuition here is correct - our proof relates to demonstrating this more formally and highlighting the fact that this requires a particular factorization of the model. Proving the \u201cif\u201d part of propositions 1, 2 is trivial.  However, proving the \u201conly if\u201d part of proposition 2 requires somewhat more care to show that the variance of the estimator can indeed only be zero when the variance of the weights is zero and that the variance of the weights can indeed only be zero if the intermediate targets incorporate all future information, implying a particular factorization of the generative model.\n\n\n%%% .... The question is not \u201cwhat\u2019s it take to get to 0 variance\u201d but \u201chow quickly can we approach 0 variance\u201d... %%%\n\n> Though we agree with your sentiment that the speed of with 0 variance is approached is of critical importance and note that we provide empirical investigation of this through the experiments, we would like to reiterate that the key point of the result is to show that for 1<K<inf one will never learn a perfect estimator unless a particular factorization can be achieved.  This is at odds to cases where K=1, for which infinite training iterations should always lead to q becoming the posterior if q is expressive enough to encode it and the problem is convex.  In other words, in the IS case we can achieve exact posterior samples at test time with a finite K and a perfectly trained q for any model that can be represented by the inference network, but it the SMC case this is only possible if we also learn the optimal factorization of the model (as per proposition 2).\n\n\n%%% Section 3.2 %%%\n\nWe believe that in practice, the bias introduced by ignoring this term is only very small.  We have added a short summary of the results from Appendix A as suggested.\n\n\n%%% Relevant missing citation: Turner and Sahani %%%\n\n> Thank you, we have duly updated the paper to include this reference.\n\n\n%%% Figure 1: What is the x-axis here? Presumably phi is not actually 1-dimensional?\n\n> In general, we are considering each dimension of the gradient separately in our assessment and so this should be read as \\nabla_{\\phi_1}.  Note that each dimension of the gradient being equally likely to be positive or negative corresponds to the overall gradient taking a completely random direction.\n\n\n%%% Typos etc. %%%\n\n* \u201clearn a particular series intermediate\u201d missing \u201cof\u201d.\n\n> Thank you, now fixed.\n\n* \u201cTo do so, we generate on sequence y1:T\u201d s/on/a/, I think?\n\n> Thank you, now fixed.\n\n* Equation 3: Should there be a (1/K) in Z?\n\n> Thank you, now fixed.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "tmdate": 1515174635740, "tcdate": 1515174635740, "number": 3, "cdate": 1515174635740, "id": "S1Eei4pQM", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "HkqLp4GeM", "signatures": ["ICLR.cc/2018/Conference/Paper967/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for taking the time to read through our paper and for their helpful feedback.\n\nWe would like to reiterate the main contributions of the paper:\n- Re-introduction of the AESMC algorithm which was first introduced by Anon (2017) alongside the similar approaches of Maddison et al. (2017), and Naesseth et al. (2017).  We reiterate that our previous work Anon (2017) is only a preprint and so this work still constitutes the first introduction of AESMC to the literature.\n\n- Additional theoretical insights about the ELBOs used for AESMC and the IWAE, in particular demonstrating that increasing the number of particles K can be detrimental to proposal learning.\n\n- Introducing the alternating EBLO algorithm to ameliorate the problems about proposal learning that our theoretical insights highlight can occur for the original AESMC and IWAE algorithms.\n\nRegarding the comments about the experiments, we ran three experiments to illustrate our points:\n- The experiment described in section 5.1 provides evidence that the AESMC algorithm works on a time-series model where we know how to evaluate and maximize the log marginal likelihood exactly. Figure 2 demonstrates that AESMC works better than IWAE.\n\n- In section 5.2 we empirically investigate our claims about the adverse effect of increasing number of particles K on learning q(x|y) (Figure 3, left). We then run the ALT algorithm to ameliorate this effect on a time series data for which the experiments are in Figure 3 (middle, right).\n\n- Finally, we run both IWAE, AESMC and ALT on a neural network model where it is impossible to evaluate the log marginal likelihood exactly and we must resort to max(ELBO_IS, ELBO_SMC) as a proxy. This is a common practice in evaluating deep generative models.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "tmdate": 1515174046100, "tcdate": 1515174046100, "number": 2, "cdate": 1515174046100, "id": "H1Uo_Namf", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "BkaCH2KlG", "signatures": ["ICLR.cc/2018/Conference/Paper967/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/Authors"], "content": {"title": "Response for AnonReviewer1", "comment": "We thank the reviewer for taking the time to read through our paper and their insightful questions.\n\nMinor comments have been incorporated in the revision of our submission.\n\nWe address the specific questions in turn:\n\n%%% It seems clear that the AESMC bound does not, in general, optimize for q(x|y) to be close to p(x|y)... %%%\n\n> This is true, but is effectively equivalent to the fact that the perfect importance sampler is better than the perfect SMC sampler, even though the latter is generally much more powerful when not perfectly optimized, as is almost always the case and hence why SMC is typically a superior inference algorithm. As we show in the set of equations (12-13) for IWAE and (14-15) for AESMC, these objectives can be decomposed to a log marginal likelihood term and a KL term on an extended space. In propositions 1 and 2, we prove that while we should expect q(x|y) = p(x|y) when we optimize the IWAE objective perfectly, this is not the case for AESMC.\n\n\n%%% Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made... %%%\n\n> The second paragraph of section 4 (and the associated Figure 1) is supposed to give an intuition for why using more K might result in a worse q(x|y). The case K=1 is regular ELBO (which is a special case of the IWAE) and the other cases are IWAE with the corresponding number of particles. We formalize this intuition by introducing the notion of a signal-to-noise ratio.\n\nThe probabilities of the \\grad_\\phi ELBO estimator are estimate using 10000 Monte Carlo samples to be:\nK=1: 0.8704\nK=10: 0.6072\nK=100: 0.5226\nK=1000: 0.5036\nWe believe that this is quite conclusive for this simple example, but the aim here is not the assumption that this simple example will generalize, just to show that increase K can be harmful.  However, our theoretical result on the signal-to-noise ratio does provide a generalization to general problems and shows that this effect must always manifest for sufficiently large K for any problem.\n\n\n%%% Is there a typo in the bound given by eq. (17)... %%%\n\n> Neither are typos.  The repeated term is because one gets an identical term in the bias and variance components of the bound, one of which goes to the numerator and one the denominator when we calculate the SNR.  It is indeed the case that I^2=0.  \n\n\n%%% The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive ... %%%\n\n> We agree that discussion and results with regards to the ALT algorithm are at first counter-intuitive, but we believe this is because of the counter-intuitive nature of our observation that increasing K actually harms the training of the proposal q(x|y).  Given this novel realization, using fewer particles to train the proposals becomes a natural thing to do, while our empirical results verify that it can lead to improvements in performance.\n\nWe have indeed tried running ALT where we update \\theta (generative parameters) using SMC with 1000 particles and \\phi (proposal parameters) using IS with 10 particles. The results of this are described in the third paragraph of section 5.2. and the results are in Figure 3 (middle, right). We have also run the experiment on a neural network based model described in section 5.3 and Figure 4."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}, {"tddate": null, "ddate": null, "tmdate": 1512462894597, "tcdate": 1512462894597, "number": 1, "cdate": 1512462894597, "id": "ByvVq0m-G", "invitation": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "forum": "BJ8c3f-0b", "replyto": "HkqLp4GeM", "signatures": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"], "content": {"title": "Regarding contribution", "comment": "I have reviewed the paper as including the AESMC, so I would be interested in the answer to whether this is intended as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Sequential Monte Carlo", "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.", "pdf": "/pdf/e6be8485caa952b7ebc2ed76d1ed8bd9405f6dab.pdf", "TL;DR": "We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.", "paperhash": "le|autoencoding_sequential_monte_carlo", "_bibtex": "@inproceedings{\nanh2018autoencoding,\ntitle={Auto-Encoding Sequential Monte Carlo},\nauthor={Tuan Anh Le and Maximilian Igl and Tom Rainforth and Tom Jin and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ8c3f-0b},\n}", "keywords": ["Variational Autoencoders", "Inference amortization", "Model learning", "Sequential Monte Carlo", "ELBOs"], "authors": ["Tuan Anh Le", "Maximilian Igl", "Tom Rainforth", "Tom Jin", "Frank Wood"], "authorids": ["tuananh@robots.ox.ac.uk", "maximilian.igl@gmail.com", "twgr@robots.ox.ac.uk", "tom@jin.me.uk", "fwood@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725146, "id": "ICLR.cc/2018/Conference/-/Paper967/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ8c3f-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper967/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper967/Authors|ICLR.cc/2018/Conference/Paper967/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper967/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper967/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper967/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper967/Reviewers", "ICLR.cc/2018/Conference/Paper967/Authors", "ICLR.cc/2018/Conference/Paper967/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725146}}}], "count": 12}