{"notes": [{"id": "rJxqZkSFDB", "original": "B1gsinsODB", "number": 1554, "cdate": 1569439490401, "ddate": null, "tcdate": 1569439490401, "tmdate": 1577168212587, "tddate": null, "forum": "rJxqZkSFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "t1euIZwFz2", "original": null, "number": 1, "cdate": 1576798726287, "ddate": null, "tcdate": 1576798726287, "tmdate": 1576800910191, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Decision", "content": {"decision": "Reject", "comment": "This paper develops a method for sample selection that exploits the memorization effect. While the paper has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR in terms of presentation of the results and experimental validation. The paper will benefit from a revision and resubmission to another venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707454, "tmdate": 1576800255666, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Decision"}}}, {"id": "HJx3cO08jB", "original": null, "number": 1, "cdate": 1573476500225, "ddate": null, "tcdate": 1573476500225, "tmdate": 1573830567239, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "ryl61yYAtr", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment", "content": {"title": "Reply to Reviewer#1", "comment": "Thanks for your comments. Please note that you are \"ICLR 2020 Conference Paper1554 AnonReviewer1\".\n\nQ1. It introduces too many basic concepts in autoML\n\nThanks for the suggestion. In the revised version, we have changed the outline of Section 2.2 and removed unnecessary concepts, e.g., supernet and one-shot. Briefly,\n1) search space and algorithm are the most two important components in AutoML\n2) derivative-free and gradient-based are two types of the popular optimization algorithm used\n3) add a paragraph to clarify the difference between existing AutoML works and the proposed one.\n\nIn summary, domain-specific search space and efficient search algorithms are keys to a successful AutoML application (Feurer et al., 2015; Zoph & Le, 2017; Xie & Yuille, 2017; Bender et al., 2018, Hutter et al., 2018).\n\nQ2. The learned curvature (in Fig 5) does not follow the curvature. Does this mean this paper is contradicting itself self?  \n\nPlease check the revised PDF. The paper is NOT contradicting itself.\n1) In practice, R(T) correlates with the memorization effect, which heavily depends on many factors (see Figure 1). Thus, \"Target\" in Figure 2 only represents one possible curvature of R(T), and it does not mean every R(T) should look similar to \"Target\".\n2) In the revised version, we have updated Figure 5. The searched R(T) enjoys much diversity, and some look similar to \"Target\" now, i.e., those on CIFAR-100 (the last row in Figure 5).\n\nQ3. The major difference between this paper and (Han et al. 2018) is how R(t) is defined and learned. The technical contribution of this paper is limited. & The curvature of defined R(t) is not needed?\n\nThanks for pointing this out. Please see our reply to the Q2 and Q3 for all reviewers. Briefly,\n1) Identifying that why R(T) is hard to be searched is the first contribution.\n2) After that, indeed, the difference is only on R(T) compared with Co-teaching. However, how to find a proper R(T) is a non-trivial problem. Please see Figure 1 and 5 in the updated version, R(T) depends on many factors and can exhibit a diverse pattern. \n3) It is the proposed approach that can boost Co-teaching and then get consistently better performance on synthetic (Section 4.1), benchmark (Section 4.2), and real (Section 4.3) data sets.  Specifically, Co-teaching with searched R(T) can even beat methods that use better criterions to find clean samples, i.e., Proposed v.s. Co-teaching+ (Yu et al., 2019) in Figure 4 and Proposed v.s. Co-Mining (Wang et al., 2019) in Table 1."}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxqZkSFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1554/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1554/Authors|ICLR.cc/2020/Conference/Paper1554/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154302, "tmdate": 1576860562070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment"}}}, {"id": "HkgpJ90UiS", "original": null, "number": 3, "cdate": 1573476837357, "ddate": null, "tcdate": 1573476837357, "tmdate": 1573830526270, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rkeYaKRIiB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment", "content": {"title": "Reply to Reviewer#3 (part 2) ", "comment": "Q6. Are all of the basic functions in Fig 2 necessary for the performance of the proposed method? How were they selected? \n\nWe do not select, all of them (in Figure 2) are used, and they are all necessary. A comparison is in Appendix B.2, which shows a simple decay function is not good enough (please also see Q9). \n\nQ7. Why is this motivated by the Taylor expansion?\n\nThanks for the suggestion. We have updated our explanation in the revised version. We want to show R(t) can be approximated by a group of basis functions.\n\nQ8. All of the curves look very similar.\n\nThanks for the comments. In the revised version, we have updated Figure 5. The searched R(T) enjoys much diversity now.\n\nQ9. A reasonable baseline motivated by these results is to apply a simple decay function to R(t) with a single hyperparameter controlling the rate of decay.\n\nThanks for the comments. We add such experiments in Appendix B.2 of the revised version.\nWe can see that performance obtained from the proposed method is much better than that from a simple decay function. This again demonstrates the needs of approximating R(T) by a linear combination of some basic functions.\n\nQ10. All of the gains associated with this method could just be due to co-teaching dropping far fewer examples as training progresses, as its decay rule isn't optimal.\n\nThanks for pointing this out. Yes, the decay rule in origin Co-teaching is not optimal, but it is hard to design such R(T):\n1) Searching R(T) is not an easy problem as it correlates with the memorization effect, which is hard to quantize (see Figure 1).\n2) Please see Figure 5 in the updated version. We have updated experiments now, and we can see that the behavior of R(T) diverse and simply dropping more (also see Q9) does not work. For example, in the last row of Figure 5, all curves first decrease and then increase.\n3) Please also check Appendix B.3, label precision is significantly increased by the searched R(T), which means the quality of samples used for training is greatly improved."}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxqZkSFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1554/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1554/Authors|ICLR.cc/2020/Conference/Paper1554/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154302, "tmdate": 1576860562070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment"}}}, {"id": "rJeIh5CUiB", "original": null, "number": 5, "cdate": 1573477038336, "ddate": null, "tcdate": 1573477038336, "tmdate": 1573830364100, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment", "content": {"title": "Common Questions to All Reviewers", "comment": "We thank all reviewers' efforts in this paper, here we summarize three common questions.\n\nQ1. About the presentation of this paper.\n\nWe have significantly revised the paper in the updated version, and please check the uploaded PDF. Changes are all highlighted in blue, briefly,\n1) make the definition of the memorization effect in the introduction more clear and show this in Figure 1.\n2) re-write the first contribution, make it clear why it is hard to design R(T).\n3) discuss the connection with active learning in Section 2.1.\n4) emphasize the main concepts in AutoML and remove unnecessary ones, and clarify connections of the proposed method with AutoML in Section 2.2.\n5) re-write and re-drawn all figures to make legend and axis more clear\n6) remove the explanation on Taylor expansion, and re-write paragraphs around equation (1)\n7) add an explanation of why the held-out curve is used as a measurement in Section 4.2.\n8) explanation the needs of synthetic noise in Section 4.2.\n9) add experiments on real applications on face recognition in Section 4.3.\n\nQ2. The importance of searching R(T).\n\nFirst, see (Han et al., 2018b, Jiang et al.,2018, Yu et al., 2019), the performance of sample-selection methods heavily depends on R(T). In this revised version, we have emphasized this point above Algorithm 1.\n\nNext, please see Figure 1 in the revised version, since the memorization effect and R(T) correlates with the effect, it is difficult to design R(T) by hand. Such difficulties motivate us to solve this problem by AutoML.\n\nBesides, in the revised version, we have fully updated our experiments, see Figure 5. R(t) represents a great diversity. Simply dropping more samples does not work, and the proposed method can efficiently search a proper R(t) for each problem (Figure 6). \n\nWe also add results on label precision to further explain why the seared curve can be better in Appendix B.3. We can see the searched R(T) can significantly improve the number of clean labels used for training. \n\nFinally, in this way, Co-teaching with searched R(T) can even beat methods that use better criterions to find clean samples, i.e., Proposed v.s. Co-teaching+ (Yu et al., 2019) in Figure 4 on benchmark data sets and Proposed in Co-Mining (Wang et al., 2019) in Table 1 on the real data set.\n\nQ3. Technical contributions.\n\nThe first technical contribution is\n1) Show why R(T) is difficult to design (Figure 1): R(T) correlates with the memorization effect, which depends on many factors and hard to quantize (also see Q2). We also clarify this point in the first contribution of the revised version. \n\nBased on the above observations, we\n2) design a domain-specific search space to exploit the memorization effect.\n3) propose a new method for hyperparameter optimization based on the analysis of problems on existing first order and derivative-free algorithms (Section 3.3.1).\n\nFor a summary of 2) and 3), please also see Reviewer 2 comments: \"The proposed method is based upon natural gradient-based updates to the hparams (which was really the only feasible way to tackle this problem given the complex dependence on the hparams and a good choice)\".\n\nBesides, we also make the 2) and 3) clearer in this revised version. Please see the difference with existing AutoML techniques at the end of Section 2.2."}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxqZkSFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1554/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1554/Authors|ICLR.cc/2020/Conference/Paper1554/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154302, "tmdate": 1576860562070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment"}}}, {"id": "HylWH90Uor", "original": null, "number": 4, "cdate": 1573476920952, "ddate": null, "tcdate": 1573476920952, "tmdate": 1573478276261, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "BkxT0H5DcB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment", "content": {"title": "Reply to Reviewer#2", "comment": "Thanks for your comments.\n\nQ1. Why not demonstrate performance on real datasets?\n\nThanks for the suggestion. We are also aware of this potential problem, and we have done this part after the submission. Please check Section 4.3 in the revised PDF.\n\nFollowing (Wang et al., 2019), which is the new state-of-the-art deep face recognition method, we have tested the proposed method on face data sets. We train with VggFace2-R (Cao et al., 2018) data set, which is a noisy data set collected from the Google image search.\n\nTable 1 shows that the proposed method can consistently achieve the best performance on such real data sets. Thus, our method is not only useful with synthetic noise but also works well on real applications.\n\nQ2. I am uncertain as to how much findings on these simulated noise patterns carry over to real datasets and their associated noise patterns. If there is existing evidence indicating a strong correlation, then perhaps my review may have varied.\n\nThanks for the question. Yes, they are strongly correlated. We add the explanation of the synthetic noise in Section 4.2 of the revised version. Specifically,\n1) The controlling variable can justify the effectiveness of the proposed method under specific conditions. In the context of learning with corrupted labels, the noise pattern is regarded as the key controlling variable. There are several common noise patterns (Patrini et al., 2017, Han et al., 2018), such as symmetric-flip and pair-flip. \n\nAll these noise patterns correspond to real-world scenarios. For example, on the macro-level, class cat flipping to the class dog makes sense, while class dog flipping to class cat also makes sense. Such flipping yields a noise pattern called symmetric-flip (Patrini et al., 2017). On the micro-level, for dogs, class Norfolk terrier flipping to class Norwich terrier makes sense, while class Norfolk terrier flipping to class Australian terrier not. This flipping yields a noise pattern called pair-flip (Han et al., 2018a), which depicts the fine-grained classification case.\n\n2) Since the noise pattern of real-world datasets can be the combination of simple noise patterns, we should first verify whether our proposed method works well on several common noise patterns before delving into complex real-world datasets. This is quite common in the area of learning with corrupted labels.\n\n3) Please also see Q1 above, the performance of the proposed method is therefore consistent on both synthetic and real data sets (Section 4.3)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxqZkSFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1554/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1554/Authors|ICLR.cc/2020/Conference/Paper1554/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154302, "tmdate": 1576860562070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment"}}}, {"id": "rkeYaKRIiB", "original": null, "number": 2, "cdate": 1573476801317, "ddate": null, "tcdate": 1573476801317, "tmdate": 1573476801317, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "Skg9z1HZcH", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment", "content": {"title": "Reply to Reviewer#3 (part 1)", "comment": "Thanks for your comments.\n\nQ1. Section 3.1 isn't very compelling to me. Experiments done on just CIFAR with two architectures and optimizers are certainly not sufficient to make any broad claims. I don't think this qualifies as a \"contribution\" of the paper.\n\nPlease check the new Figure 1 in the revised PDF. We have enumerated more perspectives there, i.e.,\n1) three datasets (i.e., CIFAR-10, CIFAR-100, MNIST) with three noisy types (i.e., symmetric 20%, symmetric 50%, pair-flip 45%)\n2) three models in learning from noisy labels\n3) three optimizers (i.e., SGD, RMSProp, Adam)\n4) two more important hyperparameters for optimizers (i.e., batch size, learning rate)\n5) STD for each learning curve (resulting from 5 different runs)\nThese datasets, models, and optimizers are all popularly used in the noisy label literature (Jiang et al., 2018; Han et al., 2018; Chen et al., 2019; Yu et al., 2019).\n\nWe have also clarified the first contribution to the revised version. The point is that: we want to show why R(T) is hard to design, as it correlates with the memorization effects, which is hard to quantize.\n\nQ2. Most practitioners use early stopping to halt training after the performance on the validation set drops. \n\nThanks for the suggestion, \"early stopping\" is indeed a choice for practical usage.\n1) Please see Q3, \"held-out curve\" is a better measurement than \"early stopping\" to evaluate the robustness of a method. \n2) We also reported the performance with \"early stopping\" in Appendix B.1 of the revised version. As can be seen, the proposed method not only has a better \"held-out curve,\" but also a better performance than \"early stopping.\"\n\nQ3. Why should we care about the held-out curve after the maximum is reached?\n\nThanks for the suggestion. This is a standard practice in the noisy label literature. We add an explanation in Section 4.2 of the revised version. The \"held-out curve\" is a better measurement than \"early stop\" to evaluate how a method is robust to noisy labels (Zhang et al., 2016; Arpit et al., 2017). \n1) Ideally, if a method is robust to noisy labels, then its performance will increase with more training epochs (not to memorize noisy labels). Thus, if a method's held-out curve quick falls after reaching the maximum, then it means the method is NOT robust intrinsically.\n2) If a method has a good \"held-out curve,\" it is more likely to have better performance than \"early stopping.\" This is also the case for the proposed approach.\nFinally, we also report results with the early stop in Appendix B.1 of the revised version.\n\nQ4. Shouldn't we care more about the training curve, as at some point during training, the noisy labels will also be memorized? Isn't this the definition of the \"memorization effect\"? \n\nNo, memorization cannot be seen from the training loss.\n1) Please see our introduction, and (Zhang et al., 2016; Arpit et al., 2017). Memorization means: \"learn easy patterns first and then over-fit on (possibly noisy) training data set.\" This means the training loss with always gets smaller with more epochs, no matter there are noisy labels or not. Thus, we cannot see memorization from the training curve. \n2) Memorization must be seen from the \"held-out curve,\" which will increase first and then significantly decrease resulting from the memorization of noisy labels. This is also why the \"held-out curve\" is a good measurement (please see Q3).\nWe have also shown what is the memorization effect in the revised version, i.e., top of page 2 (Section 1 Introduction) and Figure 1(a-b).\n\nQ5. What about standard baseline methods, e.g., active learning to help with this problem? Active learning seems highly relevant, yet it is not mentioned anywhere in this paper. \n\nThanks for the suggestion. We have added such a discussion in the revised version in Section 2.1. Active learning is not applicable here (see Active Learning Literature Survey, Burr Settles):\n1) To do active learning, we need to obtain a classifier of which the performance is good enough to generate confidence predictions.\n2) Active learning is sensitive to noisy labels and outliers. \nThus, active learning is a choice to get more labeled data when there are only a few high-quality ones, not applicable for directly learning from noisy labels here. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxqZkSFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1554/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1554/Authors|ICLR.cc/2020/Conference/Paper1554/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154302, "tmdate": 1576860562070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Authors", "ICLR.cc/2020/Conference/Paper1554/Reviewers", "ICLR.cc/2020/Conference/Paper1554/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Comment"}}}, {"id": "ryl61yYAtr", "original": null, "number": 1, "cdate": 1571880677379, "ddate": null, "tcdate": 1571880677379, "tmdate": 1572972453326, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of learning from corrupted labels via picking up clean instances from training dataset. The sample selection mainly based on function R(t), which controls how many instances are kept. This paper proposes a unique curvature of R(t) based on intuition and presents how R(t) can be learned via combination of some existing functions. Natural gradient is presented to optimize the parameters in the autoML framework. Experimental results on both synthetic data and real-world data demonstrate the effectiveness of the proposed method. \n\nA few comments on this paper:\n1. The paper is very verbose and hard to follow. It introduces too many basic concepts in autoML.\n2. A key part of the paper is the curvature of R(t), which is based on intuition. Meanwhile, the learned curvature (in Fig 5) doesn't follow the curvature. Does this mean this paper is contradicting its self? The curvature of defined R(t) is not needed?\n3. The major difference between this paper and (Han et al. 2018) is how R(t) is defined and learned. The technical contribution of this paper is limited. \n\nMinor comments:\n1. For all the figures, it is difficult to view the y-axis (or the y-axis is missing). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834752908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Reviewers"], "noninvitees": [], "tcdate": 1570237735689, "tmdate": 1575834752920, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review"}}}, {"id": "Skg9z1HZcH", "original": null, "number": 2, "cdate": 1572060946405, "ddate": null, "tcdate": 1572060946405, "tmdate": 1572972453282, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops a method for sample selection that exploits the memorization effect. In essence, the authors adopt the co-teaching  (Han et al. NeurIPS 2018) and MentorNet (Jiang et al., ICML 2018) framework, which selects some fraction of examples per minibatch that are hopefully \"cleaner\" than noisier examples to compute updates from. While in Han et al. the number of instances R selected depends on the number of epochs that have been completed, this paper instead seeks to learn R by approximating it as a linear combination of different types of basis functions  and using natural gradient as the search algorithm. The search space proposed by the authors seem comprehensive: it encompasses the search space of co-teaching, the prior state of the art. Results on synthetic tasks as well as MNIST/CIFAR appear to show the superiority of the proposed method over random search, co-teaching, and other baselines, although the results don't seem conclusive. Overall, I have concerns with some of the contributions, experiments, and presentation, which leaves me at a weak reject.\n\ncomments:\n- Section 3.1 isn't very compelling to me. Experiments done on just CIFAR with two architectures and optimizers are certainly not sufficient to make any broad claims. I don't think this qualifies as a \"contribution\" of the paper. \n- The paper is difficult to understand, and much of this difficulty stems from poor writing / presentation. The plots depicting experimental results are especially hard to follow. \n- I'm a little confused with the setup here. Most practitioners use early stopping to halt training after performance on the validation set drops. As such, why should we care about the held-out curve after the maximum is reached? Shouldn't we care more about the training curve, as at some point during training the noisy labels will also be memorized? Isn't this the definition of the \"memorization effect\"? \n- What about standard baseline methods e.g., active learning to help with this problem? Active learning seems highly relevant yet is not mentioned anywhere in this paper. \n- Are all of the basis functions in Fig 2 necessary for the performance of the proposed method? How were they selected? Why is this motivated by the Taylor expansion?\n- Figure 5 shows a bunch of R(t) curves learned by the proposed approach across a variety of datasets / noise levels. All of the curves look very similar! A reasonable baseline motivated by these results is to just apply a simple decay function to R(t) with a single hyperparameter controlling the rate of decay. I suspect this would also work better than the co-teaching approach, and perhaps render the more complex method here unnecessary. In fact, all of the gains associated with this method could just be due to co-teaching dropping far less examples as training progresses, as its decay rule isn't optimal. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834752908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Reviewers"], "noninvitees": [], "tcdate": 1570237735689, "tmdate": 1575834752920, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review"}}}, {"id": "BkxT0H5DcB", "original": null, "number": 3, "cdate": 1572476372988, "ddate": null, "tcdate": 1572476372988, "tmdate": 1572972453238, "tddate": null, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on the topic of learning from noisy -- or as they call it \"corrupted\" -- labels. Specifically this focuses on an approach where data selection -- ideally of cleaner/less noisy examples --  can help the learn model overcome data noise, akin to the approaches this builds upon (i.e, the Co-Teaching and MentorNet approaches). The specific idea here is to take an AutoML style approach to the problem  in particular to determine how many examples are selected in each mini-batch. The proposed method is based upon natural gradient based updates to the hparams (which was really the only feasible way to tackle this problem given the complex dependence on the hparams and a good choice). The experimental results using synthetic noise corruption are indicative of improved performance compared to the baseline techniques.\n\nOverall while I thought the paper made for a very interesting read and showed some great promise I had some significant concerns as well.\n\nOn the plus side:\n\n+ The empirical results on the simulated noisy data are quite positive/\n+ The proposed method makes sense as does the search algorithm in the hparam space.\n\nMy main concerns with the work stem from the empirical study and choices made there. While I understand that other existing techniques like the Co-teaching and MentorNet approaches have used simulated noise to study the impact of performance of these robustness techniques, at some point I question their validity on real datasets. Noise patterns in real datasets hardly follow some set pattern and thus I hesitate to read much into results derived solely on synthetic datasets. Given that the goal of these techniques is to improve performance when training with real, noisy labeled data why not actually demonstrate performance on such benchmarks? For example, there are numerous datasets from domains like crowdsourcing that allow you to get \"noisy\" ratings for datapoints. Wouldn't a more compelling argument be derived by showing improved performance on such datasets?\n\nThus to summarize: I worry that the results derived solely on simulated noise may not be very indicative of performance in more realistic settings and would request the authors to consider providing evidence on more realistic datasets.\n\nI also wanted to note that the paper exposition is lacking in some aspects and I needed to reread certain sections to make sure I understood them correctly. I think the paper would benefit from a good proofread not just from the grammar/spelling perspective (which there are multiple instances which could be improved) but also from the overall presentation and legibility perspective.\n\nAll this said: I want to clarify that this topic is not my research focus and hence I am uncertain as to how much findings on these simulated noise patterns carry over to real datasets and their associated noise patterns. If there is existing evidence indicating strong correlation, then perhaps my review may have varied."}, "signatures": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1554/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yhs17@mails.tsinghua.edu.cn", "qyaoaa@connect.ust.hk", "bo.han@riken.jp", "gang.niu@riken.jp"], "title": "Searching to Exploit Memorization Effect in Learning from Corrupted Labels", "authors": ["Hansi Yang", "Quanming Yao", "Bo Han", "Gang Niu"], "pdf": "/pdf/6a55f52606e09c61b8e3148eac69bcf8a3cb384a.pdf", "TL;DR": "Using automated machine learning techniques to exploit memorization effect in learning from corrupted labels", "abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.", "keywords": ["Noisy Label", "Deep Learning", "Automated Machine Learning"], "paperhash": "yang|searching_to_exploit_memorization_effect_in_learning_from_corrupted_labels", "original_pdf": "/attachment/5732d9b95e6d094d1eac640b9bafcbee0cc54973.pdf", "_bibtex": "@misc{\nyang2020searching,\ntitle={Searching to Exploit Memorization Effect in Learning from Corrupted Labels},\nauthor={Hansi Yang and Quanming Yao and Bo Han and Gang Niu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxqZkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxqZkSFDB", "replyto": "rJxqZkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1554/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834752908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1554/Reviewers"], "noninvitees": [], "tcdate": 1570237735689, "tmdate": 1575834752920, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1554/-/Official_Review"}}}], "count": 10}