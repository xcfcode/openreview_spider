{"notes": [{"id": "OpO-ASrrsz", "original": null, "number": 1, "cdate": 1577817600138, "ddate": null, "tcdate": 1577817600138, "tmdate": 1577817600138, "tddate": null, "forum": "HkxWXkStDB", "replyto": "H1xPl92pKB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Public_Comment", "content": {"title": "RE: Why not submerge the image in Gaussian noise?", "comment": "> But, considering CIFAR image size is only 32x32, a patch of size 25 is quite large, how much is the method different from plain whole image Gaussian then?\n\nIf the patch were 32x32, or that the whole image was noisy, then the convnet would never see an image with usual local image statistics. Consequently, clean data becomes out-of-distribution or unforeseen during test time, which is not desirable. With a patch size strictly smaller than the whole image, the network can learn how to respond to noisy images and also usual images."}, "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dan_Hendrycks1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192373, "tmdate": 1576860571409, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Public_Comment"}}}, {"id": "HkxWXkStDB", "original": "HyeuHBn_DB", "number": 1606, "cdate": 1569439512996, "ddate": null, "tcdate": 1569439512996, "tmdate": 1577168274716, "tddate": null, "forum": "HkxWXkStDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "N5TpEAKcyI", "original": null, "number": 1, "cdate": 1576798727708, "ddate": null, "tcdate": 1576798727708, "tmdate": 1576800908797, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Decision", "content": {"decision": "Reject", "comment": "The paper in its current form was just not well enough received by the reviewers to warrant an acceptance rating. It seems this work may have promise and the authors are encouraged to continue with this line of work.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716530, "tmdate": 1576800266697, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Decision"}}}, {"id": "BJxQWxB5sr", "original": null, "number": 7, "cdate": 1573699579037, "ddate": null, "tcdate": 1573699579037, "tmdate": 1573699579037, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HyeLRDxYor", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment", "content": {"title": "Re: Regarding experiments", "comment": "> these experiments only shows the empirical behavior of Patch Gaussian over baselines on some sample datasets\n\nWe stress that ImageNet and CIFAR are the most studied vision datasets, and for robustness they are the only datasets with standardized benchmarks (ImageNet-C and CIFAR-10-C). \n\n> I wish we can design an experiments such that we can see the sensitivity with high frequency directly affected by the patch.\n> When increasing noise, moving from 0 to total cut out. How the behavior progress?\n\nWhile we do not have time to complete this full set of experiments before the rebuttal deadline, we have analyzed the impact of noise and patch size on fourier sensitivity and have added these results to Figure 13 in the Appendix. They demonstrate that the intuition conveyed in the paper around the analysis of the fourier sensitivity plots is accurate and depends on these hyperparameters.\n\nSpecifically, for patch size 16, stdev=1.0 (Fig 13, center), the Patch Gaussian model demonstrates sensitivity very similar to that reported in Figure 4. The main difference being that the smaller patch size (16 here vs 25 in Figure 4) makes the model slightly more sensitive to high frequencies. This makes sense since smaller patch size moves the model further away from a Gaussian-trained one.\n\nWhen we make the scale smaller (patch size 16,  stdev 0.3, left), less information is corrupted in the patch, which moves the model farther from the one trained with Cutout (and therefore closer to a Gaussian-trained one). This can be seen in the increased invariance to high frequencies at the first layer, which is reflected in invariance at test error as well.\n\nIf we, instead, make the scale larger (patch size 16, stdev 2.0, right), we move the model closer to the one trained with Cutout. Notice the higher intensity red in the first layer plot, indicating higher sensitivity to high-frequency features. We also see this sensitivity reflected in the test error, which matches the behavior for Cutout-trained models.\n\nThis confirms that the frequency-based analysis of the models is accurate and reflects changes in hyper-parameters of Patch Gaussian. We also note that this methodology for studying model robustness is not novel to our paper, and has previously been validated and published in NeurIPS 2019. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1606/Authors|ICLR.cc/2020/Conference/Paper1606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153526, "tmdate": 1576860537673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment"}}}, {"id": "HyeLRDxYor", "original": null, "number": 6, "cdate": 1573615566022, "ddate": null, "tcdate": 1573615566022, "tmdate": 1573681210504, "tddate": null, "forum": "HkxWXkStDB", "replyto": "BJgEIn_vjS", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment", "content": {"title": "Regarding experiments", "comment": "I appreciate the experiments that the authors presented in the paper and summarize in the response. However, these experiments only shows the empirical performance of Patch Gaussian over baselines on some sample datasets. I wish we can design more systematic experiments to know how Patch Gaussian works, such as if we can see the Fourier analysis (Fig 4)  directly affected by the patch params. \nFor example:\n1. When increasing patch size from 1 to image size, how does the sensitivity to high frequency changes. Is it monotonous or have a particular shape? \n2. When increasing noise intensity, moving from 0 to total cut out. How does the behavior progress?\nIf we have these, then at least empirically, we can claim that Gaussian or Cutout are specific case of Patch Gaussian and have a better understanding of the problem and solutions. Hence it would be more assured and easy for the practitioners to find a good operating point in a new setting.\nTable 5 and graphs in the supplemental list the choices, but  the affect of them only shows on performance (which can be biased to dataset), not behaviour which is more insightful.\n\nOverall, I buy that this may be a good practice that is useful practically. However, I am not convinced that the authors have fulfil the due diligence on proving the correctness and general behaviour of such technique."}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1606/Authors|ICLR.cc/2020/Conference/Paper1606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153526, "tmdate": 1576860537673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment"}}}, {"id": "BJgEIn_vjS", "original": null, "number": 4, "cdate": 1573518412373, "ddate": null, "tcdate": 1573518412373, "tmdate": 1573518445486, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HylUyxwTKH", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment", "content": {"title": "Response", "comment": "> Although possibly useful practically\n\nWe thank the reviewer for pointing out the practical applications of our method. Indeed, because it is so simple, \u201cthe approach could become one of the standard mechanisms for data augmentation in the toolset of a practical ML engineer,\u201d as R1 puts it.\n\n> this proposal lacks theoretical base on how and why it would be better\n\nWe grant that our work started from an empirical observation. However, we provided an experimental analysis to gain a better understanding of why it works. In particular, Section 5.1 shows that Patch Gaussian seems to allow high-frequency information through at lower layers, but still encourages relatively lower test error sensitivity at high frequencies. Indeed, when we measure accuracy on images filtered with a high-pass filter, we see that Patch Gaussian models can maintain accuracy in a similar way to the baseline and to Cutout, where Gaussian fails to. See Figure 5 for full results.\n\nR1 and R2 agree that our Fourier-theoretic analysis is intuitive. In addition, many practically useful techniques, such as Cutout, do not have completely rigorous mathematical analysis.\n\n> The experiments are rather limited to support the claim\n\nWe show extensive experiments highlighting how Patch Gaussian is the only method that retains the benefits of Cutout and Gaussian:\n* We characterize a trade-off between robustness and accuracy among two standard data augmentations - Cutout and Gaussian (Section 2.1). Specifically, Cutout improves accuracy on clean test data. Despite this, we find it does not lead to increased robustness. Conversely, training with higher sigma of Gaussian can lead to increased robustness to Gaussian noise, but it also leads to decreased accuracy on clean data. Therefore, any robustness gains are offset by poor overall performance.\n* We show that our method (Patch Gaussian) allows us to interpolate between the two augmentations above (Section 3.1), and to overcome the observed trade-off, yielding models that are robust to unseen corruptions, while also maintaining clean accuracy (Figure 1, Section 4.1). In doing so, it achieves a new state of the art in the Common Corruptions benchmark on CIFAR-C and ImageNet-C. (Section 4.2), which highlights that simple methods such as ours are competitive with complex training schemes designed for robustness.\n* We demonstrate that Patch Gaussian can be combined with other regularization strategies (Section 4.3) and data augmentation policies (Section 4.4), and can improve COCO object detection performance as well (Section 4.5).\n* We perform a frequency-based analysis of models trained with Patch Gaussian and find that they can better leverage high-frequency information in lower layers, while not being too sensitive to them at later ones (Section 5.1)\n\nWe are open to suggestions of further experiment proposals that could convince the reviewer of this."}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1606/Authors|ICLR.cc/2020/Conference/Paper1606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153526, "tmdate": 1576860537673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment"}}}, {"id": "HJlhgh_wjH", "original": null, "number": 3, "cdate": 1573518324446, "ddate": null, "tcdate": 1573518324446, "tmdate": 1573518324446, "tddate": null, "forum": "HkxWXkStDB", "replyto": "H1xPl92pKB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the thoughtful comments. We provide some answers to the concerns raised below:\n\n> I\u2019m a bit concerned about the significance of the work though. The method is a straight-forward combination of existing methods, so methodologically the novelty is kind of limited.\n\nWe agree that the method presented is very simple. However, we\u2019d like to emphasize that this was done by design. In showing that such a simple method can be competitive with state-of-the-art methods in the robustness literature, we show that complex training schemes may not be necessary for training models robust to unseen distributions. This is, we believe, where the significance of the work stems. Indeed, R1 mentioned that our method \u201ccould become one of the standard mechanisms for data augmentation in the toolset of a practical ML engineer,\u201d especially since it\u2019s so easy to try.\n\n> I\u2019m expecting more insights from the analysis of the results, to gain more understanding of why it works so well.\n\nIn Section 5.1, we provide an extensive frequency-based analysis and discussion of why Patch Gaussian works well: Patch Gaussian seems to allow high-frequency information through at lower layers, but still encourages relatively lower test error sensitivity at high frequencies. Indeed, when we measure accuracy on images filtered with a high-pass filter, we see that Patch Gaussian models can maintain accuracy in a similar way to the baseline and to Cutout, where Gaussian fails to. See Figure 5 for full results.\n\nWe will re-word this section to clarify these insights to future readers. \n\n> A few examples/pictures of success cases (when the method works) and failure cases (when the method doesn\u2019t work), may help readers (I\u2019m not an expert) to better understand the approach and get more intuitions?\n\nWe thank the reviewer for the suggestion. We have not examined this but we hope to include it in camera-ready. In particular, we expect that images with higher Brightness will be among the most common errors, since Patch Gaussian slightly increases error (mCE 0.592) in these corruptions with respect to the Baseline (mCE 0.582). (see Table 7 in Appendix).\n\n> It\u2019s obvious that Gaussian filter blocks high-frequency components, and Cutout keeps some original parts of the image which allow high-freq details to be captured\n\nWe agree with the reviewer that these insights make intuitive sense. Our work provides a quantitative evaluation of this phenomenon to confirm this intuition. Further, through rigorous frequency-based sensitivity analysis we show that Patch Gaussian is able to retain both the high frequency sensitivity of Cutout and robustness gains of Gaussian augmentation.\n\n> a patch of size 25 is quite large, how much is the method different from plain whole image Gaussian then?\n\nWe remind the reviewer that, while the center of the patch needs to be inside the image, the edges can be outside. This means that, with a patch of size 25, 39.55% of the space is covered in expectation for an image of size 32. Depending on the location of the patch, 16.50% the space is covered (minimum) and other 61.04% is covered (maximum). \n\nIn addition, our experimental results clearly show that patch Gaussian performs significantly differently from adding Gaussian noise to the whole image. For example, as shown in Table 1 in our paper, for a Resnet-50 model on ImageNet(-C), Patch Gaussian gets a clean test accuracy of 76% and mCE of 0.714, whereas Gaussian data augmentation gets a clean test accuracy of 75.6% and mCE of 0.739."}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1606/Authors|ICLR.cc/2020/Conference/Paper1606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153526, "tmdate": 1576860537673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment"}}}, {"id": "rylqOtOvoB", "original": null, "number": 2, "cdate": 1573517681945, "ddate": null, "tcdate": 1573517681945, "tmdate": 1573517681945, "tddate": null, "forum": "HkxWXkStDB", "replyto": "BkeUdm9QcB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the positive comments and helpful summary of our contributions. In particular, we appreciate the summary of the insights demonstrated with the frequency-based analysis (Section 5.1). We hope to incorporate a version of this summary in the camera-ready version as we believe it will be valuable to future readers."}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxWXkStDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1606/Authors|ICLR.cc/2020/Conference/Paper1606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153526, "tmdate": 1576860537673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Authors", "ICLR.cc/2020/Conference/Paper1606/Reviewers", "ICLR.cc/2020/Conference/Paper1606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Comment"}}}, {"id": "HylUyxwTKH", "original": null, "number": 1, "cdate": 1571807198390, "ddate": null, "tcdate": 1571807198390, "tmdate": 1572972446866, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a hybrid approach for adding noise to training images of an image classification model. Instead of either cutting out a patch or adding gaussian noise, the authors propose to adding a patch of gaussian noise to the images. Although possibly useful practically, this proposal lacks theoretical base on how and why it would be better, besides the claim that hopefully the combination will combine the benefit and subtract the weakness. The experiments are rather limitted to support the claim."}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216832823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Reviewers"], "noninvitees": [], "tcdate": 1570237734935, "tmdate": 1575216832834, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review"}}}, {"id": "H1xPl92pKB", "original": null, "number": 2, "cdate": 1571830254628, "ddate": null, "tcdate": 1571830254628, "tmdate": 1572972446823, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a data augmentation method that interpolates between two existing methods (Cutout and Gaussian), for training robust models towards Gaussian and naturally occurring corruptions. The method is shown to improve robustness without sacrificing accuracy on clean data.\nPros:\nThe proposed method, despite being simple, seems to empirically work well in terms of the mCE criterion evaluated in the experiments. This does support the authors\u2019 claim that current methods haven\u2019t reached the robustness/accuracy tradeoff boundary yet.\nCons:\nI\u2019m a bit concerned about the significance of the work though. The method is a straight-forward combination of existing methods, so methodologically the novelty is kind of limited. Hence, I\u2019m expecting more insights from the analysis of the results, to gain more understanding of why it works so well. However, the presentation of the experiments just seems to aim for the best numbers one can get (I\u2019m not certain how significant the numbers are to this field though). A few examples/pictures of success cases (when the method works) and failure cases (when the method doesn\u2019t work), may help readers (I\u2019m not an expert) to better understand the approach and get more intuitions? The frequency analysis seems quite intuitive. It\u2019s obvious that Gaussian filter blocks high-frequency components, and Cutout keeps some original parts of the image which allow high-freq details to be captured. But, considering CIFAR image size is only 32x32, a patch of size 25 is quite large, how much is the method different from plain whole image Gaussian then?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216832823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Reviewers"], "noninvitees": [], "tcdate": 1570237734935, "tmdate": 1575216832834, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review"}}}, {"id": "BkeUdm9QcB", "original": null, "number": 3, "cdate": 1572213614263, "ddate": null, "tcdate": 1572213614263, "tmdate": 1572972446781, "tddate": null, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "invitation": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a novel data augmentations approach that improves the robustness of a model on the CIFAR-10 and ImageNet Common Corruptions benchmarks while maintaining training accuracy on clean data. To achieve this, the paper proposes a rather simple augmentation mechanism that is inspired by CutOut (DeVries & Taylor 2017) and Gaussian (Grandvalet & Kanu, 1997): adding Gaussian noise to random patches in the image. This simple approach is shown to work surprisingly well on the corruption benchmarks. It seems reasonable that while adding Gaussian noise makes the model robust to high frequency noise, since Gaussian noise is not added everywhere, the model is able to exploit high frequency signal when available in the input. The paper is reasonably well written and the experimental validation is convincing. \n\nOverall, the approach could become one of the standard mechanisms for data augmentation in the toolset of a practical ML engineer.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1606/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation", "authors": ["Raphael Gontijo Lopes", "Dong Yin", "Ben Poole", "Justin Gilmer", "Ekin D. Cubuk"], "authorids": ["iraphael@google.com", "dongyin@berkeley.edu", "pooleb@google.com", "gilmer@google.com", "cubuk@google.com"], "keywords": ["Data Augmentation", "Out-of-distribution", "Robustness", "Generalization", "Computer Vision", "Corruption"], "TL;DR": "Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.", "abstract": "Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging, involving major changes in training procedure and datasets.  Prior work has argued that there is an inherent trade-off between robustness and accuracy, as exemplified by standard data augmentation techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. We introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image.  Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNet Common Corruptions benchmarks while also maintaining accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise (similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). We show it can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment.  Finally, we find that the idea of restricting perturbations to patches can also be useful in the context of adversarial learning, yielding models without the loss in accuracy that is found with unconstrained adversarial training.", "pdf": "/pdf/26fc9b8228b82566d2cd3f3b879c34e3dd52792c.pdf", "paperhash": "lopes|improving_robustness_without_sacrificing_accuracy_with_patch_gaussian_augmentation", "original_pdf": "/attachment/9b041cc2c289b57dbd042bea1cad44b3a39e31eb.pdf", "_bibtex": "@misc{\nlopes2020improving,\ntitle={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},\nauthor={Raphael Gontijo Lopes and Dong Yin and Ben Poole and Justin Gilmer and Ekin D. Cubuk},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxWXkStDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxWXkStDB", "replyto": "HkxWXkStDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216832823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1606/Reviewers"], "noninvitees": [], "tcdate": 1570237734935, "tmdate": 1575216832834, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1606/-/Official_Review"}}}], "count": 11}