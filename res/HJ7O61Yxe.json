{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396339533, "tcdate": 1486396339533, "number": 1, "id": "ByhhoGLdg", "invitation": "ICLR.cc/2017/conference/-/paper73/acceptance", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.\n \n I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396340087, "id": "ICLR.cc/2017/conference/-/paper73/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396340087}}}, {"tddate": null, "tmdate": 1485040953736, "tcdate": 1481955020950, "number": 3, "id": "rkHCIUMVg", "invitation": "ICLR.cc/2017/conference/-/paper73/official/review", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer2"], "content": {"title": "Important line of research, muddled presentation and unconvincing empirical results", "rating": "4: Ok but not good enough - rejection", "review": "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512706719, "id": "ICLR.cc/2017/conference/-/paper73/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512706719}}}, {"tddate": null, "tmdate": 1484951829821, "tcdate": 1481911894066, "number": 1, "id": "ryC8AjbVx", "invitation": "ICLR.cc/2017/conference/-/paper73/official/review", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer3"], "content": {"title": "Interesting model, further experiments required", "rating": "4: Ok but not good enough - rejection", "review": "In absence of authors' response, the rating is maintained.\n\n---\n\nThis paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.\n\nThe model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.\n\nThe model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.\n\nFirst of all, the paper should refer to variational auto-encoders / deep gaussian models, which also predict the mean and the variance during inference.\n\nSecondly, the datasets are extremely small. For example, the WHO contains only 91 times series of 52*10 = 520 time points. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that further experiments, with longer time series, are required. The paper could also easily be extended with more information about the model (what is the architecture of the MLP) as well as time complexity comparison between the models (especially between DFGs and this model).\n\nMinor remark:\nThe footnote 2 on page 5 seems to refer to the structural regularization term, not to the dynamical term.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512706719, "id": "ICLR.cc/2017/conference/-/paper73/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512706719}}}, {"tddate": null, "tmdate": 1481951572110, "tcdate": 1481951558527, "number": 3, "id": "rkAHtBMNe", "invitation": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer2"], "content": {"title": "Loosening the linear decoder constraint", "question": "It seems like one of the main limitations of this framework is the use of a linear decoder. This is a bit of a shot in the dark, but could the authors comment on whether any of the ideas in\n\nJohnson, et al., Composing graphical models with neural networks for structured representations and fast inference. NIPS 2016 (arXiv: https://arxiv.org/abs/1603.06277v3).\n\nmight be applicable to your setting?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481951559044, "id": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481951559044}}}, {"tddate": null, "tmdate": 1481933992118, "tcdate": 1481933992118, "number": 2, "id": "SyxnNWM4e", "invitation": "ICLR.cc/2017/conference/-/paper73/official/review", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer1"], "content": {"title": "Interesting idea but formulation and experiments not convincing", "rating": "4: Ok but not good enough - rejection", "review": "This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).\n\nModeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.\n\nThe formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].\n\nMoreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.\n\n\nReferences:\n[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114\n[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.\n[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.\n[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. \n[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014\n\n\nOne innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes \"similar\" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more \"similar\" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. \n\nThe experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). \n\nIt would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.\n\nTimeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512706719, "id": "ICLR.cc/2017/conference/-/paper73/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512706719}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481877714980, "tcdate": 1478192490675, "number": 73, "id": "HJ7O61Yxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJ7O61Yxe", "signatures": ["~Ludovic_Dos_Santos1"], "readers": ["everyone"], "content": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481826077877, "tcdate": 1481826077869, "number": 2, "id": "rkL7kDxNl", "invitation": "ICLR.cc/2017/conference/-/paper73/public/comment", "forum": "HJ7O61Yxe", "replyto": "HyJo-Pj7x", "signatures": ["~Ludovic_Dos_Santos1"], "readers": ["everyone"], "writers": ["~Ludovic_Dos_Santos1"], "content": {"title": "KL-divergence and interdependency", "comment": "- \\Delta_R \"encourages the model to learn similar representations for timeseries that are interdependent\":\n* \"interdepedent time series\": In the paper we assume a given graph with edges between time series, where an edge is indicating a positive correlation (dealing with negatives would require a different cost function), e.g. It could be a spatial distance when forecasting weather or trafic. The strength of the correlation is reflected in the associated weight.\n* \"similar representations\": The structural regularization term consists in a Kullback\u2013Leibler divergence between Z_i and Z_j. Minimizing this term also minimize the total variation norm of the charge in the observation space (mostly due to Pinsker's inequality) - we updated the paper to report on this. Since our prediction model is linear, this gives us a bound on the difference between the predicted values.\n* \"how do you make sure that this regularizer is actually not making the prediction worse?\": This can only be demonstrated experimentally: we have added the results of the model without regularization (i.e. \\lambda_R = 0) to quantify the importance of the graph regularization.\n\n- better exploration of your model in order to better describe the effect of the different terms in the objective\n* with/without the regularizer \\Delta_R: See above\n* with/without uncertainty: We have added the results of Ziat et al. 2016 on the datasets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287740482, "id": "ICLR.cc/2017/conference/-/paper73/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ7O61Yxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper73/reviewers", "ICLR.cc/2017/conference/paper73/areachairs"], "cdate": 1485287740482}}}, {"tddate": null, "tmdate": 1481499031198, "tcdate": 1481499031190, "number": 2, "id": "HyJo-Pj7x", "invitation": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer1"], "content": {"title": "Structural regularization term & experiments", "question": "In 3.3.3 it is stated that \\Delta_R \"encourages the model to learn similar representations for timeseries that are interdependent\". I was wondering, what does \"similar\" and \"interdependent\" mean here in mathematical terms? I think it should be defined what counts are a valid \"interdependency\" for a set of time series (since this interdependency is given by the user as contextual information). In other words, how do you make sure that this regularizer is actually not making the prediction worse?\n\nLinking to the above, I have a second question. Did you try to do some better exploration of your model in order to better describe the effect of the different terms in the objective? E.g. it'd be great to see a series of experiments showing results with/without the regularizer \\Delta_R, or with/without uncertainty (thus comparing with Ziat et al. 2016 to which your paper is an extension). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481951559044, "id": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481951559044}}}, {"tddate": null, "tmdate": 1481103133821, "tcdate": 1481103133814, "number": 1, "id": "BkUmwIHml", "invitation": "ICLR.cc/2017/conference/-/paper73/public/comment", "forum": "HJ7O61Yxe", "replyto": "rkD363kQg", "signatures": ["~Ludovic_Dos_Santos1"], "readers": ["everyone"], "writers": ["~Ludovic_Dos_Santos1"], "content": {"title": "Time complexity", "comment": "Thank you for your question.\nInferring the value of the i-th time series at time (T+N) require to compute f(h^N(Z_i^T)), f being linear if h is a FFNN the time complexity of the inference procedure is then in O(N*d*d') with d' the size of the hidden layer (with O(n*T*d+d*d') parameters to learn) otherwise if h is linear the complexity of the inference is in O(N*d^2)  (with O(n*T*d+d^2) parameters to learn). For the second question, since this is not a bayesian formulation there is no possibility of extra conditioning: our model optimization formula is somehow similar to computing the maximum likelihood of the joint distribution."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287740482, "id": "ICLR.cc/2017/conference/-/paper73/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ7O61Yxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper73/reviewers", "ICLR.cc/2017/conference/paper73/areachairs"], "cdate": 1485287740482}}}, {"tddate": null, "tmdate": 1480736175211, "tcdate": 1480736175205, "number": 1, "id": "rkD363kQg", "invitation": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "signatures": ["ICLR.cc/2017/conference/paper73/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper73/AnonReviewer3"], "content": {"title": "Inference", "question": "What is the time complexity of the inference procedure, and what would change if there were extra conditioning of the hidden states on the observed variables?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modelling Relational Time Series using Gaussian Embeddings", "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.", "pdf": "/pdf/393639b108094123bb479f130715a48dede5fe30.pdf", "TL;DR": "We learn latent gaussian distributions for modelling correlated series.", "paperhash": "santos|modelling_relational_time_series_using_gaussian_embeddings", "keywords": ["Applications", "Deep learning"], "conflicts": ["lip6.fr", "vedecom.fr"], "authors": ["Ludovic Dos Santos", "Ali Ziat", "Ludovic Denoyer", "Benjamin Piwowarski", "Patrick Gallinari"], "authorids": ["ludovic.dossantos@lip6.fr", "ali.ziat@vedecom.fr", "ludovic.denoyer@lip6.fr", "benjamin.piwowarski@lip6.fr", "patrick.gallinari@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481951559044, "id": "ICLR.cc/2017/conference/-/paper73/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper73/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper73/AnonReviewer3", "ICLR.cc/2017/conference/paper73/AnonReviewer1", "ICLR.cc/2017/conference/paper73/AnonReviewer2"], "reply": {"forum": "HJ7O61Yxe", "replyto": "HJ7O61Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper73/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481951559044}}}], "count": 10}