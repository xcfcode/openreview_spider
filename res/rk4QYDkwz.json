{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124462467, "tcdate": 1518463260059, "number": 222, "cdate": 1518463260059, "id": "rk4QYDkwz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rk4QYDkwz", "signatures": ["~Francesco_Locatello1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Clustering Meets Implicit Generative Models", "abstract": "Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.", "paperhash": "locatello|clustering_meets_implicit_generative_models", "_bibtex": "@misc{\n  locatello2018clustering,\n  title={Clustering Meets Implicit Generative Models},\n  author={Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4QYDkwz}\n}", "authorids": ["flocatello@tuebingen.mpg.de", "damienv@google.com", "ilya@tuebingen.mpg.de", "raetsch@inf.ethz.ch", "sylvaingelly@google.com", "bs@tuebingen.mpg.de"], "authors": ["Francesco Locatello", "Damien Vincent", "Ilya Tolstikhin", "Gunnar Ratsch", "Sylvain Gelly", "Bernhard Scholkopf"], "keywords": [], "pdf": "/pdf/c5e0d8053f8c77e1f2b5ba8c00d2daed8cdaaa26.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582865324, "tcdate": 1520556926824, "number": 1, "cdate": 1520556926824, "id": "SJPFjU1Kz", "invitation": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "signatures": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer2"], "content": {"title": "Learn a variety generative models to specialize on different subspaces of the data distribution", "rating": "7: Good paper, accept", "review": "This paper aims to give a model composed of submodels (each of which is a VAE whose role is to focus on different parts of the training distribution) and a learning algorithm for it. The learning algorithm is motivated by the k-means algorithm. For each assignments of datapoints to submodels, maximize the likelihood of datapoints under each submodel (in parallel), then fix the generative submodels and train a discriminator to distinguish between the data distribution and the samples from each submodel. Reassign points to submodels that generate the most similar points (using an argmax over the discriminator's probability distribution) and repeat.\n\nPros:\n* Interesting (and to my knowledge novel) idea\n* The experiments are preliminary and synthetic though they appear to show the model working\n\nCons:\n* Missing a discussion of (Boosted Generative Models) https://arxiv.org/pdf/1702.08484.pdf and a discussion of sensitivity of the learning algorithm to initialization \n* A more natural baseline to compare against than a VAE might be a generative model that assumes a mixture distribution in the prior (see for example - https://arxiv.org/pdf/1603.06277.pdf). \n\nQuestions for the authors: How is c_j initialized? How sensitive is the learning algorithm to this quantity? Are the VAEs reset after each assignment of the c_js?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Clustering Meets Implicit Generative Models", "abstract": "Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.", "paperhash": "locatello|clustering_meets_implicit_generative_models", "_bibtex": "@misc{\n  locatello2018clustering,\n  title={Clustering Meets Implicit Generative Models},\n  author={Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4QYDkwz}\n}", "authorids": ["flocatello@tuebingen.mpg.de", "damienv@google.com", "ilya@tuebingen.mpg.de", "raetsch@inf.ethz.ch", "sylvaingelly@google.com", "bs@tuebingen.mpg.de"], "authors": ["Francesco Locatello", "Damien Vincent", "Ilya Tolstikhin", "Gunnar Ratsch", "Sylvain Gelly", "Bernhard Scholkopf"], "keywords": [], "pdf": "/pdf/c5e0d8053f8c77e1f2b5ba8c00d2daed8cdaaa26.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865136, "id": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper222/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer3"], "reply": {"forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865136}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582740681, "tcdate": 1520664391436, "number": 2, "cdate": 1520664391436, "id": "r1kUkZ-Yf", "invitation": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "signatures": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer1"], "content": {"title": "Interesting work", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes an algorithmic framework to train mixtures of implicit generative models. The framework mimics the k-means algorithm, and the training can be decoupled into independent training each generative model separately through minimizing an upper bound.\n\nThe proposed method is very similar to that of [1], where a mixture of generative models are trained using a similar EM-style algorithm. The major difference is if one data point is \"hard\" or \"soft\"-assigned to one of the generative models. My guess is that \"hard\" version would train faster and generate more vivid images than the \"soft\" counterpart. But it would be interesting to see the quantitative comparisons.\n\n- Please be consistent with the symbols: Line 3 in Algorithm 1 uses \"D(||)\", but equation (2) uses \"D(|)\".\n- My computation shows that the number of parameters in VAE-150 is smaller than that of 9VAEs. Another issue is that if the number of parameters in the discriminator should also be counted. It would be interesting to see the performance of a single VAE with one more layer but similar number of parameters.\n- It would be interesting to see the sensitivity of training with respect to the optimality/epochs of the discriminator.\n\n[1] E. Banijamali, A. Ghodsi, and P. Poupart. Generative Mixture of Networks. https://arxiv.org/abs/1702.03307\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Clustering Meets Implicit Generative Models", "abstract": "Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.", "paperhash": "locatello|clustering_meets_implicit_generative_models", "_bibtex": "@misc{\n  locatello2018clustering,\n  title={Clustering Meets Implicit Generative Models},\n  author={Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4QYDkwz}\n}", "authorids": ["flocatello@tuebingen.mpg.de", "damienv@google.com", "ilya@tuebingen.mpg.de", "raetsch@inf.ethz.ch", "sylvaingelly@google.com", "bs@tuebingen.mpg.de"], "authors": ["Francesco Locatello", "Damien Vincent", "Ilya Tolstikhin", "Gunnar Ratsch", "Sylvain Gelly", "Bernhard Scholkopf"], "keywords": [], "pdf": "/pdf/c5e0d8053f8c77e1f2b5ba8c00d2daed8cdaaa26.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865136, "id": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper222/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer3"], "reply": {"forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865136}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582675558, "tcdate": 1520735529616, "number": 3, "cdate": 1520735529616, "id": "SkMNSzztf", "invitation": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "signatures": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer3"], "content": {"title": "Interesting idea, poorly explained", "rating": "4: Ok but not good enough - rejection", "review": "As I understand it, this paper presents a mixture model p(x) = \\sum_{k} p(x | class_k) p(class_k), where each p(x | class_k) is trained adversarially. The model is trained in an iterative way, much like k-means, where we alternate between computing class/cluster assignments and updating the individual mixture models p(x | class_k). The step calculating the cluster assignments is not probabilistic -- one seems to use a hard argmax to make hard cluster assignments, which produces a hard partition of the data, from which one can adversarially retrain k VAE-based models. I did like the idea of having an adversarial way to train the decoder part of the VAE, although I got confused when the authors said there was a \"cross-entropy\" reconstruction error (?). This leads me nicely to my main criticism of the paper -- it is unnecessarily confusing and written in a highly non-rigorous full of inconsistent notation and confusing statements. \n\nThe introduction talks about causality/structural equations (why?) and introduces X1...XK which are never really used after. All this wasted space could have been used to make a clearer/much more rigorous description of the main algorithm. For example, what is the difference between dP_{d_{j}} and P_{d_{j}}. What is D_{g_{j}}? and so on.\n\nEmpirical results are also weak. MNIST alone won't suffice in my opinion, you need at least SVHN. The FID Score is never defined -- if one is to use this frequently in a paper it would be nice to have a one-liner. Also, for the synthetic data experiment why not numerically compute the f-divergence, instead of the loglikelihood of the true data? ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Clustering Meets Implicit Generative Models", "abstract": "Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.", "paperhash": "locatello|clustering_meets_implicit_generative_models", "_bibtex": "@misc{\n  locatello2018clustering,\n  title={Clustering Meets Implicit Generative Models},\n  author={Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4QYDkwz}\n}", "authorids": ["flocatello@tuebingen.mpg.de", "damienv@google.com", "ilya@tuebingen.mpg.de", "raetsch@inf.ethz.ch", "sylvaingelly@google.com", "bs@tuebingen.mpg.de"], "authors": ["Francesco Locatello", "Damien Vincent", "Ilya Tolstikhin", "Gunnar Ratsch", "Sylvain Gelly", "Bernhard Scholkopf"], "keywords": [], "pdf": "/pdf/c5e0d8053f8c77e1f2b5ba8c00d2daed8cdaaa26.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865136, "id": "ICLR.cc/2018/Workshop/-/Paper222/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper222/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper222/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper222/AnonReviewer3"], "reply": {"forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper222/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865136}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573574165, "tcdate": 1521573574165, "number": 137, "cdate": 1521573573818, "id": "S1060CRtz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rk4QYDkwz", "replyto": "rk4QYDkwz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Clustering Meets Implicit Generative Models", "abstract": "Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.", "paperhash": "locatello|clustering_meets_implicit_generative_models", "_bibtex": "@misc{\n  locatello2018clustering,\n  title={Clustering Meets Implicit Generative Models},\n  author={Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4QYDkwz}\n}", "authorids": ["flocatello@tuebingen.mpg.de", "damienv@google.com", "ilya@tuebingen.mpg.de", "raetsch@inf.ethz.ch", "sylvaingelly@google.com", "bs@tuebingen.mpg.de"], "authors": ["Francesco Locatello", "Damien Vincent", "Ilya Tolstikhin", "Gunnar Ratsch", "Sylvain Gelly", "Bernhard Scholkopf"], "keywords": [], "pdf": "/pdf/c5e0d8053f8c77e1f2b5ba8c00d2daed8cdaaa26.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}