{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396553849, "tcdate": 1486396553849, "number": 1, "id": "S1M52GLul", "invitation": "ICLR.cc/2017/conference/-/paper382/acceptance", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": " This paper provides some interesting ideas for integrating music theoretic structure into an LSTM based music generation model. The work also provides a human study of results (albeit one of limited size).\n \n The posted revision goes part of the way necessary to improve the paper, but a larger evaluation and some improvements to the model architecture might allow this work to have even higher impact in an eventual peer reviewed venue/format.\n We therefore recommend this paper for presentation at the workshop track.\n \n The most negative reviewer notes the following pros and cons:\n \n Pro: \n + Incorporating general musical knowledge into the learned network is a good idea and non-trivial. \n + Idea to introduce a behavioral measure for the quality of the generated samples is useful, as music is very subjective. \n + The multitrack idea seems useful and a clear step beyond Magenta as far as I understand. \n \n Con: \n - multitrack part of the architecture does not seem to work properly, rhythm does not seem to behave differently from melodic movement. \n - The music excerpts sound very simplistic and similar. \n - The pair-wise evaluation metric with 27 supposedly \"random\" subjects is not very meaningful and very likely, not significant. \n - Evaluation of generative models is difficult, but the authors could have done better. \n - The 4 bins of the random scale variable seem ad-hoc.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396554342, "id": "ICLR.cc/2017/conference/-/paper382/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396554342}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484964562515, "tcdate": 1478288733523, "number": 382, "id": "ByBwSPcex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByBwSPcex", "signatures": ["~Hang_Chu1"], "readers": ["everyone"], "content": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": ["BJ2p6a_dg"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484776784598, "tcdate": 1484776784598, "number": 4, "id": "SJFLSPT8x", "invitation": "ICLR.cc/2017/conference/-/paper382/public/comment", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["~Hang_Chu1"], "readers": ["everyone"], "writers": ["~Hang_Chu1"], "content": {"title": "Response to All Reviewers", "comment": "We thank all reviewers for their time, comments and constructive suggestions.\n\nWe conducted a series of additional experiments and evaluation as per reviewers\u2019 comments/suggestions:\n\n- Exp 1. The distribution of musical scales (suggested by Reviewer 1).\nAs per reviewer\u2019s great suggestion, we took a complete list of musical scales from Wikipedia https://en.wikipedia.org/wiki/List_of_musical_scales_and_modes\nThis list includes scales from different cultures, and across different historical periods. Using the scales for which precise notes can be defined, we ended up with 35 different types of scales, each type with a choice of 12 starting notes.\nWe categorized all songs in our 100-hour training data into its most likely scale type, where we used the same note-frequency-based method as described in our paper. \nThrough this experiment, we found that: \n  a. The 4 scales we used in the paper (Major/Minor, Harmonic Minor, Melodic Minor, and Blues) account for 37.8% of the data, and the rest 31 scales account for the remaining 62.2%.\n  b. The 62.2% further breaks into 5 dominant scales that account for 47.7%, which are Acoustic, Algerion, Lydian, Adonai Malakh, and Ukrainian. The rest 26 scales account for the remaining 14.5%. These 26 scales include Insen, Iwato, Yo, Enigmatic, etc.\n  c. With a closer examination of the additional 5 dominant scales, we found out that Lydian and Adonai Malakh is only 1 degree from Major/Minor, Acoustic is 2 degree from Major/Minor, Algerion is only 1 degree from Harmonic Minor, and Ukrainian is only 2 degree from Melodic Minor. Here, 1 degree variation means that in the seven main notes, six are exactly the same, and the last one only differs by one.\nThis experiment shows that even in the most rigorous musical setting that considers all possible scales, at least 85.5% of online pop/game songs are very close to the 4 scales that we used in the paper.\n\n- Exp 2. Complete ablation study of the proposed musical priors (suggested by Reviewer 1).\nWe added an experiment on effects of removing the scale to complete the ablation study in the paper.\nFor this experiment, we trained the model on all songs, without conditioning on scale:\nhttp://www.cs.toronto.edu/songfrompi/supp/additional_qualitative.html\nOne can hear that the generated music changes notes too quickly, and the overall music sounds worse than that produced with the conditional model. This is because without scale identification and scale starting note \u201cnormalization\u201d, all notes appear frequently in the training data, which makes learning more difficult. It should be noted that the Magenta baseline does not show this quick note change as they introduced holding and repeating as new notes. However, this makes their method suffer from producing occasional long silence that we have discussed in the paper. Conditioning on scale improves our generation, and at the same time avoids undesired long silences.\n\n- Exp 3. Evaluating repeat time and longest sub-sequence evaluations for Magenta baseline and human composed music (suggested by Reviewer 2).\nWe added new evaluation for Magenta and human composed music.\nTo evaluate the repeat time, we used the same evaluation metric and the same 2-bar criterion, and evaluated with the same amount of data, i.e., 100 generated 50 second music generated by our and Magenta approach, and 100 randomly 50 second clips from the human composed training data.\nWe found that the average repeat time for Magenta, Ours, and Human are 17.080, 4.036, and 2.328, respectively. This shows that our method is less likely to generate recurring melodies across different generated music pieces compared to Magenta, and is closer to the statistics of human-produced songs.\nFor longest sub-sequence matching, we matched 100 generated melodies with the training data for ours and Magenta. For the human data, we randomly selected 100 clips from training data with the same length,  and matched them with the rest of training data (excluding entire songs which contain the 100 selected clips).\nWe found that the average length of longest sub-sequences for Magenta, Ours, and Human are 4.39, 4.65, and 7.06, respectively. This shows that our method performs similarly as Magenta in terms of copying. It is somewhat surprising that human composers in fact tend to copy more from other songs, which indicates that both generation approaches can be further relaxed in terms copying. Some of these copying effects for humans may also be due to the nature of our data: it contains lots of music from games, for which copying may be less of an issue.\n\n- Exp 4. Applying similar post-processing to Magenta\u2019s generation (suggested by Reviewer 3).\nWe provide results of the Magenta system in which we incorporated our post-processing approach:\nhttp://www.cs.toronto.edu/songfrompi/supp/additional_qualitative.html\nPost-processing here makes only a minor difference. Notes become more likely to be \u201con the beat\u201d within a bar, but for methods that only has melody, the effect is barely noticeable. However, the problem of the baseline, such as the undesired long silence, still remains.\n\nIn the following, we address other comments by the reviewers.\n\n\n***To Reviewer 1:***\n\n- Wrt scale distribution\nWe added more experiments and evaluation on scale distribution and removing music prior, as shown in Exp 1 and Exp 2, as well as two other experiments. We will add the results and discussions to the paper in order to make the evaluation deep and thorough.  \n\n- Wrt rhythm, simplicity, and interestingness\nWe agree with the reviewer that the rhythm and the complexity of the generated music still have room for improvement. Generating appealing and interesting music that captures structure, rhythm, and mood is very hard, and we do not claim to have solved this problem. In our paper, we aimed to show that incorporating knowledge from the music theory into the model, as well as capturing multiple aspects of music (melody, chord and drum) results in better sounding songs. There is an exciting road ahead to improve on this in the future. We will revise the paper and add discussions about the limitations of the current model.\n\n- Wrt human experiments\nAll subjects in our human evaluation experiments are university students at either undergraduate or graduate level, who did not have any prior knowledge about the content of our paper. We also performed significance tests on the evaluation results in Table 1. With alpha value 5%, all comparisons passed the significance test. We have also found out that the lowest alpha values for the significance test to pass are 1e-19, 1e-14, and 1e-19 for the three comparisons respectively. This further backs up our conclusions. \n\n- Wrt \"4 bins of the random scale variable seem ad-hoc\"\nWe have shown in Exp 1 that our setting of 4 scale bins is suitable for our dataset. In Exp 2 we also summarized the effect of our scale prior. \nIn our original paper we only considered the most commonly known 4 scale types. Thanks to the reviewer\u2019s suggestion, we extended our study to incorporate all existing scales as shown in Exp 1. \nAs part of our method, we first find statistically relevant scales, and then condition our model on those. As such, our method is easily adaptable to any kind of music dataset.\n\n- Wrt \u201chow the weighted probability distribution is constructed for the scales and how strong the prior it effectively incorporates is\u201d\nIt should be noted that scale is not a post-processing step. We identify scale types of our training songs and apply scale starting note \u201cnormalization\u201d. Then we condition our generation on the scale by training multiple sets of weights, one set for each scale type, and using the corresponding weights in generation without any additional prior imposed. The effect of removing scale is discussed in Exp 2.\n\n\n***To Reviewer 2:***\n\n- Wrt additional experiments\nWe have conducted further analysis of the repetitions of the baseline, our approach and human written music, as shown in Exp 3. We will add this to the paper. We will also revise the paper to provide more details of the Magenta baseline, which has been shown in an earlier response.\n\n- Wrt interestingness\nPlease see response to Reviewer 1 \u201cWrt rhythm, simplicity, and interestingness\u201d.\n\n- Wrt judging the applications\nWe have provided quantitative evaluation of our pop-song generation, which is the main goal of the paper. Our applications are exploratory and qualitative, and thus it is hard to provide quantitative evaluation. We urge the reviewers to look at them as such -- interesting possibilities one can do with our approach. We also note that our neural singing and dancing results have received more than 650k views on Vimeo, and even inspired fans to sing our songs in person (we provided links in our project page). This indicates that there is merit to our proposed applications, at least in some way.\n\n\n***To Reviewer 3:***\n\n- Wrt minor paper revisions\nWe will revise the paper to provide more training details in earlier sections. We will also tone down claims of \u201cour model is able to generating new music\u201d claim, since there hasn\u2019t been a widely-accepted evaluation criterion on novelty of music generation yet. We will shorten the section with LSTM and refer to the original reference for the formulas. If the reviewer agrees, we would still like to keep the anecdotal results on dancing, since we believe the quality of results is mainly affected by the lack of training data rather than the model. At least the authors, and some of the viewers enjoyed the dancing stickman. We can push this experiment to the appendix section and clearly mark it as a preliminary result.\n\n- Wrt end-to-end vs. incorporating musical knowledge\nMusic theory has a long history, and most musicians follow musical rules when composing new songs. The main intuition of our work is that incorporating music theory into a computational model should help in producing better sounding songs, particularly when the amount of training data is small. Our experiments show that our model performs qualitatively better than the baseline (Magenta) which does not incorporate such knowledge. Lastly, due to its structure, our model is also more interpretable and potentially allows real musicians to interact with it to create new music."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287598120, "id": "ICLR.cc/2017/conference/-/paper382/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByBwSPcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper382/reviewers", "ICLR.cc/2017/conference/paper382/areachairs"], "cdate": 1485287598120}}}, {"tddate": null, "tmdate": 1482072017893, "tcdate": 1482072017893, "number": 3, "id": "SJ9RkQV4g", "invitation": "ICLR.cc/2017/conference/-/paper382/official/review", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer1"], "content": {"title": "Nice idea, lacking thorough evaluation.", "rating": "4: Ok but not good enough - rejection", "review": "In this paper, the authors build music-theoretical structure directly into a music generating LSTM. Even though such simple rules should be learnable from data, this surely is a neat idea for the limited-data regime. Further, it seems like a desirable model when thinking about musicians, for instance, wanting to train on own (and thus limited) source material. \n\nThey consider a dataset of 100 hours of midi and add multiple priors, drawn from basic music theory. The priors as such are OK, but some of them seem rather heuristic and should, in my opinion, be learned from data and it should be discussed how the performance changes if you remove them. \nFurther, the authors evaluate their study on an artificial benchmark, consisting of a behavioral experiment where 27 subjects judge songs generated by Magenta and their approach in a questionable side-by-side evaluation. Using this as a performance criterion is problematic, as no details about the subjects are given away and no attempt is made to assess the statistical significance of such results, let alone discussing the difficulty of pairing the songs. Further, I assume there are standard behavioral batteries, concerned with assessing music preferences that should have been used or at least addressed. Introducing the neural Karaoke and dancing is fun, but does not have much scientific value at this point, as it does not seem to work in a meaningful way. I would recommend to either improve results of the latter drastically or add it as an extra to a blog post and remove it from the paper.\n\nIt is good that the authors make an attempt to encode general prior knowledge into their architecture, but I am not convinced by the results and the heuristic choices being made. Further, it is still not 100% clear to me how the weighted probability distribution is constructed for the scales and how strong the prior it effectively incorporates is. If it is very strong, it is not surprising to me, that the songs sound relatively coherent, as in scale playing with rejected outliers have to sound somewhat coherent. I am not familiar enough with the Magenta baseline system and it is problematic that the baseline is not explained well. If the baseline does not take explicit scale priors into account, it does make sense that it sounds less coherent by definition. This has to be discussed and the effect of the introduced priors has to be evaluated. Finally, the question remains if this will generalize to datasets with more than 4 dominant scales and why the authors chose their thresholds the way they did. Did the model perform worse if one chooses to include more scales? How do you know, that the heavy tail of such a distribution is not desirable and important for natural sounding music, did you investigate this?\n\nThe multitrack idea is great. However, I am not convinced it works in this case. The results sound more like assigning a couple of notes in the melodies to rhythmic sounds, but they do not interact with the melody, they just move along, as if they were part of the melody. This is not how rhythm works in music in most cases.\n\nPro:\n+ Incorporating general musical knowledge into the learned network is a good idea and non-trivial.\n+ Idea to introduce a behavioral measure for the quality of the generated samples is useful, as music is very subjective.\n+ The multitrack idea seems useful and a clear step beyond Magenta as far as I understand.\nCon:\n- However, the multitrack part of the architecture does not seem to work properly, rhythm does not seem to behave differently from melodic movement.\n- The music excerpts sound very simplistic and similar.\n- The pair-wise evaluation metric with 27 supposedly \"random\" subjects is not very meaningful and very likely, not significant.\n- Evaluation of generative models is difficult, but the authors could have done better.\n- The 4 bins of the random scale variable seem ad-hoc.\n\nI have to emphasize that I like the ideas introduced in this paper, but I am not convinced by the way they are presented and evaluated. I would like to suggest this paper for workshop publication.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603380, "id": "ICLR.cc/2017/conference/-/paper382/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2", "ICLR.cc/2017/conference/paper382/AnonReviewer1"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603380}}}, {"tddate": null, "tmdate": 1481942892486, "tcdate": 1481942892486, "number": 2, "id": "HJVuvmzNl", "invitation": "ICLR.cc/2017/conference/-/paper382/official/review", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer2"], "content": {"title": "The paper is well written, clear and proposes a reasonable model for generating melody accompanied with chords and drums.  The evaluation of the model requires clarification or improvements.", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a recurrent neural network (RNN) for generating pop music.  The model is trained on 100 hours of user composed pop songs and video game music and the resulting music is evaluated in user studies against songs produced by the Magenta framework.\n\nOverall, I find the paper to be well written and clear.  I appreciate the review early on of music theory concepts.  I think the paper provides a reasonable support for the connection between how pop music is composed and the hierarchical model for generating melody accompanied with chords and drums.  With some post-processing, the model appears to generate pleasant sounding music as judged by users and from a personal perspective of listening to the examples available on the web.\n\nWhile the generated examples on the web sound pleasant, they also sound quite similar and make it hard to judge what the model has learned.  There are some open questions regarding evaluation of the model.  The paper would benefit from improvements in both user and metric evaluations.\n\n*  The Magenta system serves as a lower baseline for evaluation.  The study would benefit from an upper baseline by also evaluating against human composed songs.  This would help contextualize the findings for both this and future work.\n\n*  The user study could be improved by examining other dimensions of appeal, perhaps to gauge diversity through \"interestingness\" over a collection of samples. \n\n*  I think a paired/side-by-side design for the user study seems limited (examples on http://www.cs.toronto.edu/songfrompi/eval/eval.html).  A simpler design with rating one sample at a time may have been more appropriate because there is no natural way to pair the songs.  The examples from each system used in the experiment should be provided with labels or an answer key so that readers can compare the merits of each of the systems' compositions themselves.\n\n*  The authors propose specific metrics for insight into the diversity (longest subsequence and number of repeats).  These would be more meaningful with some context, e.g. by comparison with baseline Magenta samples and the training data (as an upper baseline).\n\n*  Details of the baseline Magenta system would also benefit the paper.\n\n*  No guidance is provided on how to judge the applications of neural singing, dancing and karaoke.  \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603380, "id": "ICLR.cc/2017/conference/-/paper382/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2", "ICLR.cc/2017/conference/paper382/AnonReviewer1"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603380}}}, {"tddate": null, "tmdate": 1481923004009, "tcdate": 1481923004009, "number": 1, "id": "S1NaYRb4x", "invitation": "ICLR.cc/2017/conference/-/paper382/official/review", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer3"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper describes a recurrent neural network model for generating pop music in the symbolic domain (i.e. MIDI). The layers of the model each generate part of the output, with the first few layers responsible for generating the melody, and further layers generating drums and chords conditioned on the generated melody. The authors argue that this matches how pop music is usually composed. The model is trained on 100+ hours of pop music in MIDI format. The resulting generated music is compared against that produced by another system using human evaluation, which is probably the only way in which such a system can be fairly evaluated. I appreciate that the authors went through the trouble of setting up these experiments.\n\nThe RNNs generating the different outputs (i.e. key, duration, chord, melody) are trained in sequence, conditioned on the output of the previous step(s).I found the text a bit confusing at times as it initially seems to describe a single end-to-end trained model (even if this is never stated explicitly). It only becomes clear later on that the layers are trained in sequence, with additional supervision provided at each stage. This may simply be a personal bias because recent work on hierarchical RNNs that I've read has focused on end-to-end training, but nevertheless it might be useful to mention this more clearly from the get-go.\n\nThe post-processing of model samples described in the 2nd paragraph of Section 4.5 seems to affect results quite dramatically (based on the results in Table 1). It seems equally applicable to the outputs of the Magenta system, so it might be interesting to compare this version to Magenta as well, to get an idea of how much it contributes to the improvement over the Magenta system. It would be somewhat disappointing if it ends up accounting for most of the gain.\n\nI am still unconvinced by the experiment described in the last paragraph of Section 5, where subsequences of generated music fragments are searched for in the training data. While I agree with the authors that minor differences in note choice can have profound effects on how the melody is perceived, I still think this is not particularly convincing, and I think drawing the unambiguous conclusion that \"our model is able to generate new music\" from this experiment is a bit premature.\n\nThe additional applications described in Section 6 feel a bit like an afterthought and the datasets used are probably too small for the results to be meaningful. Instead I would have preferred to read about how to reduce the importance of prior knowledge in the design of the model. Considering the venue this work was submitted to, moving towards a more \"end-to-end learning\" approach (rather than incorporating even more prior knowledge, as the conclusion seems to imply) seems like an interesting direction for future research.\n\nMinor remark: giving the formulas for LSTM is probably a bit of a waste of space, especially if you're not explaining the semantics. A reference is sufficient, and in fact adding a reference to the original LSTM paper is probably a good idea regardless.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603380, "id": "ICLR.cc/2017/conference/-/paper382/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2", "ICLR.cc/2017/conference/paper382/AnonReviewer1"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603380}}}, {"tddate": null, "tmdate": 1481740676499, "tcdate": 1481740676491, "number": 3, "id": "Sk3Fbf1Ex", "invitation": "ICLR.cc/2017/conference/-/paper382/public/comment", "forum": "ByBwSPcex", "replyto": "ryNIkARGx", "signatures": ["~Hang_Chu1"], "readers": ["everyone"], "writers": ["~Hang_Chu1"], "content": {"title": "Response", "comment": "Thank you for the valuable comments and questions.\n\n1) We used melody as the base layer for two reasons. Firstly, as we described in section 2, many previous approaches generate only the melody. We therefore naturally extended this line of work to also create accompanying instruments. We further consulted professional pop music composers, which seemed to agree that music composition can be done in many different ways, and typically depends on the musician. For example, some prefer rhythm-first as per reviewer\u2019s suggestion, others prefer the chord-first approach, and others would start with melody. Exploring the effects of different composing methodologies (and layer order) is a very interesting future research direction.\n\n2) We agree with the reviewer that the dancing moves are not quite right. This is mainly due to the fact that we use very little training data (1 hour of dancing). We believe that using more data would improve dancing. We want to emphasize that the main focus of our paper was music generation, the dancing experiment was, in our view, a nice idea but perhaps preliminary in its results. We can remove this application if the reviewer believes it distracts the reader from the main point.\n\nWe note that the keys that can be generated was not fixed by the scale, and the network learned to follow the correct keys. Music progression/evolution is another great point made by the reviewer. Adding structure to the generating music is a very exciting avenue of work, and has rarely been addressed in the literature. Mainly, approaches like Magenta allow the network to copy the generated music from a few bars ago, which mimics the repetition of chorus. We use the same idea in our work. Capturing richer structure of pop-songs is however out of the scope of this paper, but definitely a direction we want to explore in the future."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287598120, "id": "ICLR.cc/2017/conference/-/paper382/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByBwSPcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper382/reviewers", "ICLR.cc/2017/conference/paper382/areachairs"], "cdate": 1485287598120}}}, {"tddate": null, "tmdate": 1481740534629, "tcdate": 1481740534623, "number": 2, "id": "HkJbbGkVg", "invitation": "ICLR.cc/2017/conference/-/paper382/public/comment", "forum": "ByBwSPcex", "replyto": "BkwxtQ1Qe", "signatures": ["~Hang_Chu1"], "readers": ["everyone"], "writers": ["~Hang_Chu1"], "content": {"title": "Response", "comment": "Thank you for the valuable comments and questions.\n\nFor evaluating overfitting, we argue that images and music are quite different in nature. Music is a temporal signal, and altering any single note can lead to a change in the musical flavour. Pixels are more correlated, and thus changing a few will leave the image less affected. While there may be better measures to evaluate overfitting than the longest common subsequence, we believe that it is an informative measure of similarity.\n\nWe thank the reviewer for a great suggestion to evaluate the diversity of the generated music. We performed an additional experiment, where we break each generated melody into segments of 2-bars in length (the 2-bar length was inspired by common definition of music plagiarism). We then compare each segment to all segments in the rest of the 100 generated songs. The average number of repeats across 2500 segments is 4.036. This gives insight into the diversity of our song generation. We also note that in this evaluation all of the generated songs are conditioned on the same scale and the same starting note, thus the diversity could be further increased by incorporating scale variation.\n\nWe do however agree that the songs have somewhat similar general feel (are more happy). Generating more diverse music with different emotions is part of our future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287598120, "id": "ICLR.cc/2017/conference/-/paper382/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByBwSPcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper382/reviewers", "ICLR.cc/2017/conference/paper382/areachairs"], "cdate": 1485287598120}}}, {"tddate": null, "tmdate": 1481740427051, "tcdate": 1481740427045, "number": 1, "id": "HymqxfkNe", "invitation": "ICLR.cc/2017/conference/-/paper382/public/comment", "forum": "ByBwSPcex", "replyto": "Skyfuh1Ql", "signatures": ["~Hang_Chu1"], "readers": ["everyone"], "writers": ["~Hang_Chu1"], "content": {"title": "Response", "comment": "Thank you for the valuable comments and questions.\n\nWe kept track of Magenta through their official web blog. We have noticed Magenta has been improved very recently with reinforcement learning, which is a submission to this ICLR. Since this was only available after the ICLR deadline, we were not able to compare to this version.\n\nIn our comparison, we used the Lookback RNN version of Magenta, which was the last version at the time of our submission. We trained the Magenta as well as our method on the melody tracks of 100-hour music from midi_man. We also used the same optimization parameters in both training procedures.\n\nIn this link (http://www.cs.toronto.edu/songfrompi/eval/eval.html), we provide the examples we used in our comparison experiment. The samples were generated with 10 different random seeds. We did not try to match up or re-order the samples; the pairs were matched randomly. Our experiment had three parts. Part 1 corresponds to Ours vs. Magenta evaluation, Part 2 corresponds to Ours-MelodyOnly vs. Magenta, and Part 3 corresponds to Ours vs. Ours-NoAlignment. The two samples in each pair have random order (we did not use a consistent order to avoid bias).\n\nWe believe that the real human-composed music is generally better than the generated one (both ours and Magenta). While we are happy to perform this experiment we believe that our method is not ready to pass the \u201cmusic turing test\u201d yet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287598120, "id": "ICLR.cc/2017/conference/-/paper382/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByBwSPcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper382/reviewers", "ICLR.cc/2017/conference/paper382/areachairs"], "cdate": 1485287598120}}}, {"tddate": null, "tmdate": 1480734727317, "tcdate": 1480734727311, "number": 3, "id": "Skyfuh1Ql", "invitation": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer2"], "content": {"title": "Comparison system(s)", "question": "Could you provide more detail about the Magenta system that you compared against?  I know that it is being improved fairly regularly, and you provide a citation for the website in general.  Is it possible to reference the specific version you were using?  Did you use it to generate the data yourself, or did you use pre-generated music that they distributed?  What parameters did you use?  Was it trained on their data or yours?  etc.  Even providing the comparison Magenta signals in the example webpage would be helpful.  \n\nAlso, how did you decide which Magenta outputs to pair with which outputs of your system?  Are both systems completely unguided in their synthesis?  If you had a way of matching up Magenta outputs and your outputs, could you also use it to match up your outputs with real midi examples from the training data for a more direct evaluation of realism?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959310221, "id": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer1", "ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959310221}}}, {"tddate": null, "tmdate": 1480698094748, "tcdate": 1480698094743, "number": 2, "id": "BkwxtQ1Qe", "invitation": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer3"], "content": {"title": "diversity", "question": "The last paragraph of section 5 discusses an experiment to assess to which extent the model is able to generate new music. This is done by generating a bunch of sequences and checking for the longest subsequences that also occur in the training data.\n\nIn the context of generative models of images, it has been established that searching for generated examples in the training data is not a great way to determine whether or not the model is overfitting. To what extent do the authors think these concerns also apply here?\n\nAdditionally, such an experiment does not demonstrate diversity of the samples, so the model might still be overfitting to a particular subset of modes of the distribution. For all we know, it could have generated the same sample 100 times. Could you comment on the diversity of the samples generated by the model and perhaps also how this could be demonstrated (or even measured quantitatively)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959310221, "id": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer1", "ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959310221}}}, {"tddate": null, "tmdate": 1480675148438, "tcdate": 1480675148432, "number": 1, "id": "ryNIkARGx", "invitation": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "forum": "ByBwSPcex", "replyto": "ByBwSPcex", "signatures": ["ICLR.cc/2017/conference/paper382/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper382/AnonReviewer1"], "content": {"title": "How meaningful is the learned part?", "question": "A nice paper and nice idea!\n\nI have two clarification questions:\n\n1) Why is the rhythm generation not the lowest layer? Most musicians have an explicit or implicit idea of the metre their composition is based upon.\nFurther, rhythm is the foundation of melody in many ways. So what made you think, melody should be the base layer?\n\n2) How do you make sure that your outputs are not mere artifacts of the models and priors you use?\nI don't see any meaningful relationship between dancing, text, and music in the karaoke video. The music changes, but the same sentence is repeated and the stick person moves in ways that are not clearly related to the music, other than in trivial ways.\nAnd similarly, the rhythm patterns the network learns do not add anything meaningful to the chord/melody progressions.\nIt seems even though the musical results sound coherent in terms of the scale (but that one you fix manually anyways), the evolution in the pieces doesn't seem to be meaningful.\n\nIt would be great if you can elaborate on the two points, so it is easier for me to interpret the results in the proper way."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "pdf": "/pdf/f3a2607fd70152913005f63894f3e14c8052d7a1.pdf", "TL;DR": "We present a novel hierarchical RNN for generating pop music, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed.", "paperhash": "chu|song_from_pi_a_musically_plausible_network_for_pop_music_generation", "keywords": ["Applications"], "conflicts": ["cs.toronto.edu"], "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "authorids": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959310221, "id": "ICLR.cc/2017/conference/-/paper382/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper382/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper382/AnonReviewer1", "ICLR.cc/2017/conference/paper382/AnonReviewer3", "ICLR.cc/2017/conference/paper382/AnonReviewer2"], "reply": {"forum": "ByBwSPcex", "replyto": "ByBwSPcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper382/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959310221}}}], "count": 12}