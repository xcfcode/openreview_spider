{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1496368548928, "tcdate": 1480732393960, "number": 2, "id": "BkfxJnkQx", "invitation": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "content": {"title": "Why CPUs, reward choices, generalization, BPTT length, dataset choice", "question": "This paper is intriguing.\n\nFor a question more out of curiousity than anything else, why did you use CPUs for training the language modeling task rather than GPUs? Just an excess number of CPUs at your disposal?\n\nRegarding the recurrent cell search, you force some of the cells, such as c_t and c_t-1 / h_t and h_t-1. Did you think of or experiment with allowing the model to add these itself?\n\nThis may be my lack of knowledge, but what's the reasoning for some of the reward choices? Last five epochs _cubed_ or c / validation_ppl**2? Obviously these are all generally ad hoc in RL but I'd just not seen these as standard choices in the past.\n\nWhat was the BPTT length for the word level language modeling? Was it 35 as in Zaremba et al.?\n\nFinally, you tested generalization on the character level Penn Treebank task, though that is quite small and odd in some ways (being a letter for letter version of the word level PTB, it also has a vocabulary of 10,000 words, no numbers / punctuation, and <unk> being a character by character \"word\"). Testing the generalization of the architecture to essentially the same dataset but at a character level doesn't seem an extreme generalization. Were you interested in running experiments over larger datasets such as text8 (character level), One Billion LM (word level, sentential context), or WikiText-103 (word level with context)?\n\nOverall I'm excited for the potential that policy gradient based architecture search may bring!"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481188409415, "id": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2", "ICLR.cc/2017/conference/paper251/AnonReviewer3"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481188409415}}}, {"tddate": null, "nonreaders": null, "tmdate": 1496368421140, "tcdate": 1495171474256, "number": 1, "id": "Byq9b-nxb", "invitation": "ICLR.cc/2017/conference/-/paper251/official/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "content": {"title": "Hyperparameters for SotA PTB word level LM", "comment": "The paper reports that \"[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\"\n\nIs it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with \"Recurrent Neural Network Regularization\" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.\n\nThis will likely also be desired for the other experiments, such as character LM."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665395, "id": "ICLR.cc/2017/conference/-/paper251/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665395}}}, {"tddate": null, "tmdate": 1494007234343, "tcdate": 1494007234343, "number": 20, "id": "By96T49y-", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "rJ8xa16Ax", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "We have fixed this in our updated version."}, "nonreaders": ["barretzoph@google.com", "qvl@google.com"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1494007199802, "tcdate": 1478280685622, "number": 251, "id": "r1Ue8Hcxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1Ue8Hcxg", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 28, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1493134574436, "tcdate": 1493134574436, "number": 19, "id": "rJ8xa16Ax", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Inconsistent performance numbers between abstract and section 1 for CIFAR-10 network ? ", "comment": "The abstract says: \"Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than\nthe previous state-of-the-art model\" while the last paragraph of section one has: \"Our CIFAR-10 model achieves a 3.84 test set error, while being 1.2x faster than the current best model.\". Should these be the same numbers or what is the reason for the difference (or is there a newer revision of the paper) ?"}, "nonreaders": ["barretzoph@google.com", "qvl@google.com"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1493121420134, "tcdate": 1493121420134, "number": 18, "id": "By45Fnn0x", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "S1SGw63Mg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Request for details on running time and number of machines", "comment": "Can you also give details on how many of these GPUs/CPUs were used (e.g. was it a single dual processor / dual GPU machine or rather a cluster) -- I think this is interesting information for readers to get an idea how feasible the task is with the computational resources they have access to."}, "nonreaders": ["barretzoph@google.com", "qvl@google.com"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1486611137772, "tcdate": 1486611137772, "number": 17, "id": "H1qazwYOg", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Auto-regressive model", "comment": "In the paper, you wrote that the controller is auto-regressive and that \"each prediction is fed into the next time step as input\". What is the input to the controller for the prediction of the first token - a random seed or constant?\n\nAdditionally, is the controller an extension of the sequence decoder used in sequence-to-sequence learning? What are the important changes between the NAS controller and the S2S decoder?\n\nThanks for the excellent paper and congratulations on the acceptance!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396460362, "tcdate": 1486396460362, "number": 1, "id": "BJV42fUul", "invitation": "ICLR.cc/2017/conference/-/paper251/acceptance", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This was one of the most highly rated papers submitted to the conference. The reviewers all enjoyed the idea and found the experiments rigorous, interesting and compelling. Especially interesting was the empirical result that the model found architectures that outperform those that are used extensively in the literature (i.e. LSTMs).", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396460868, "id": "ICLR.cc/2017/conference/-/paper251/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396460868}}}, {"tddate": null, "tmdate": 1486036686467, "tcdate": 1486036686467, "number": 16, "id": "B1URCqgOx", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Code available for the design space (not just the final models)?", "comment": "Thanks for a very interesting paper!\n\nIt is nice to see that the authors will make the code for running the models found by their controller available.\nI am wondering whether the code for the design space will also be available. I.e., the spaces from which the policy samples, for CIFAR and Penn Treebank -- e.g. as code with free choices. That would make it much more likely that someone else can reproduce these results. Thanks!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1484526892900, "tcdate": 1484526892900, "number": 15, "id": "rJS4B5FLl", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "HJoC77Tre", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "Yes this is what we do."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1484526815211, "tcdate": 1484526815211, "number": 14, "id": "rJvkBcKUg", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "rJ0FDEw4x", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "Thank you for pointing out these two related papers. We will include a comparison in our related work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1484526754189, "tcdate": 1484526754189, "number": 13, "id": "Hk9jVqYIx", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "BJlICsTXe", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "Thank you for mentioning this related work to us. We will include a comparison in the related work section of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1483713367499, "tcdate": 1483713367499, "number": 12, "id": "rJJPimaSx", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Control experiments", "comment": "Thank you for the really interesting paper. I have a few questions about the control experiments.\n\nIn control experiment 1, how well did the model with the max/sin functions do in terms of perplexity? That seems perhaps more important than how similar the architecture itself is.\n\nWhat was the distribution over architectures used by random search in control experiment 2? Did you also do grid finetuning of hyperparameters with random search?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1483711443213, "tcdate": 1483711443213, "number": 11, "id": "HJoC77Tre", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "On validation dataset", "comment": "Dear Authors,\n\nThank you for sharing such an impressive work. I just have a question, in your experimental part, \"We then run a small grid search over learning rate, weight decay,\nbatchnorm epsilon and what epoch to decay the learning rate.\" May I know that in such a grid search, is the validation accuracy also computed on your 5,000 validation samples from which the reward (used in REINFORCE) is computed?\n\nThanks a lot!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1482274694383, "tcdate": 1482274694383, "number": 10, "id": "rJ0FDEw4x", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Some relevant related work", "comment": "Overall, I find it very exciting that the actor-critic framework can produce such good results. \n\nThere is some related work in Bayesian optimization for architecture search that the authors may not know about. The authors are correct in stating that standard Bayesian optimization methods are limited to a fixed-length space, but there are also some Bayesian optimization methods that support so-called conditional parameters that allow them to effectively search over variable-depth architectures. Two particularly relevant papers are: \n\n1. http://jmlr.org/proceedings/papers/v28/bergstra13.pdf\nBergstra, Yamins, and Cox: Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures (ICML 2013). This allowed for different numbers of layers and used the Bayesian optimization algorithm TPE to search for the best model in a space of up to 238 hyperparameters.\n\n2. http://www.jmlr.org/proceedings/papers/v64/mendoza_towards_2016.pdf\nMendoza, Klein, Feurer, Springenberg, and Hutter: Towards Automatically-Tuned Neural Networks (AutoML@ICML 2016). \nThis did joint architecture search and hyperparameter optimization in the special case of feed-forward neural networks using the Bayesian optimization algorithm SMAC and generated custom per-dataset architectures that won two datasets in the AutoML competition against human experts.\n\nThe authors' approach has clear advantages over these methods (I believe especially in terms of generality) but also seems to have some disadvantages (I believe especially in terms of data efficiency). It would be nice if the authors could briefly contrast these approaches in their paper. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481949859009, "tcdate": 1481949859009, "number": 3, "id": "HksjMBGNe", "invitation": "ICLR.cc/2017/conference/-/paper251/official/review", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer2"], "content": {"title": "Review", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.\n\nThe paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.\n\nThis is a well written paper on an interesting topic with strong results. I recommend it be accepted.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512648606, "id": "ICLR.cc/2017/conference/-/paper251/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer3", "ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512648606}}}, {"tddate": null, "tmdate": 1481903880247, "tcdate": 1481903880247, "number": 2, "id": "rkeMJc-Ee", "invitation": "ICLR.cc/2017/conference/-/paper251/official/review", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer4"], "content": {"title": "Review", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.\n\nThis is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.\n\nIt would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.\n\nOverall, an excellent and interesting paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512648606, "id": "ICLR.cc/2017/conference/-/paper251/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer3", "ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512648606}}}, {"tddate": null, "tmdate": 1481772942904, "tcdate": 1481772942897, "number": 9, "id": "H1v5kck4g", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1T0ms0mg", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "Hello thank you for the nice review. We tried to address the issue of testing the generality of our architectures on different datasets. Our most recent revision contains more experiments where we ran the cell our neural architecture search algorithm found on a very benchmarked machine translation task, in addition to new experiments on character level language modeling."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481712596863, "tcdate": 1481712596852, "number": 1, "id": "r1T0ms0mg", "invitation": "ICLR.cc/2017/conference/-/paper251/official/review", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer3"], "content": {"title": "A nice paper", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512648606, "id": "ICLR.cc/2017/conference/-/paper251/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer3", "ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512648606}}}, {"tddate": null, "tmdate": 1481649736536, "tcdate": 1481649736527, "number": 8, "id": "BJlICsTXe", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "How do you position yourself to: https://arxiv.org/abs/1606.02492 \"Convolutional Neural Fabrics\"", "comment": "They obtain a performance which is worse that reported in your work, however they train their model on a single GPU in contrast to 800 used in the proposed work.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481336852464, "tcdate": 1481336852460, "number": 7, "id": "Bk2GuyFQg", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "SJ-SVi8Xx", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "1. You are correct that 20 shards are probably more than we needed since the model is small, this choice was somewhat arbitrary.\n\n\n2. We used a base of 8 to be sure the search space was interesting enough for our search algorithm. A LSTM actually has a base of 4, but we believe our comparisons to be fair because the cells all have the same number of parameters and virtually the same amount of computation being done."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481336774827, "tcdate": 1481336774821, "number": 6, "id": "rJkCwyY7x", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "B1qPmqBXl", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "The wording for that section was confusing and we revised the paper to make this more clear. What we meant was that we trained a total of 12,800 different architectures."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481336723107, "tcdate": 1481336723100, "number": 5, "id": "ryj9wJYQg", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "BkfxJnkQx", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "\"For a question more out of curiousity than anything else, why did you use CPUs for training the language modeling task rather than GPUs? Just an excess number of CPUs at your disposal?\"\nYes we used CPUs because they were more at our disposal and the models were small enough that the speed degradation was minimal from switching from a GPU to CPU.\n\n\"Regarding the recurrent cell search, you force some of the cells, such as c_t and c_t-1 / h_t and h_t-1. Did you think of or experiment with allowing the model to add these itself?\"\nYes, we have thought and designed a model to learn these, but we did not experiment with it yet.\n\n\"This may be my lack of knowledge, but what's the reasoning for some of the reward choices? Last five epochs _cubed_ or c / validation_ppl**2? Obviously these are all generally ad hoc in RL but I'd just not seen these as standard choices in the past.\"\nYes these choices are just ways to try and get the reward for models to lie in the 0-1 range.\n\n\"What was the BPTT length for the word level language modeling? Was it 35 as in Zaremba et al.?\"\nIt was 35, the same in Zaremba et al.\n\n\"Finally, you tested generalization on the character level Penn Treebank task, though that is quite small and odd in some ways (being a letter for letter version of the word level PTB, it also has a vocabulary of 10,000 words, no numbers / punctuation, and <unk> being a character by character \"word\"). Testing the generalization of the architecture to essentially the same dataset but at a character level doesn't seem an extreme generalization. Were you interested in running experiments over larger datasets such as text8 (character level), One Billion LM (word level, sentential context), or WikiText-103 (word level with context)?\"\nYes we are interested in trying this out in other datasets. We currently have experiments going on text8 and WMT translation tasks.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481336526905, "tcdate": 1481336526897, "number": 4, "id": "HkPR8Jtme", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "SkcgSiJmx", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Response", "comment": "1. We believe this method to be very generalizable and only choose these two tasks because we believed they were sufficiently different and challenging. They are also two of the most benchmarked datasets in deep learning. Though it can be extended to, our method may struggle for problems where model types are not defined yet (e.g., graphs, multimodal data).\n\n\n2. Yes we believe this would definitely help performance, but decided against doing this in this paper to try and show we could start with less and still achieve good results.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1481188408874, "tcdate": 1481188408867, "number": 3, "id": "SJ-SVi8Xx", "invitation": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer3"], "content": {"title": "several questions", "question": "1. Why does the distributed systems use S shards? It seems that the model does not have a huge number of parameters.\n\n2. In section 4.2, why does the model use base number of 8, not 2? It seems that it is more fair to compare with LSTM with base number 2.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481188409415, "id": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2", "ICLR.cc/2017/conference/paper251/AnonReviewer3"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481188409415}}}, {"tddate": null, "tmdate": 1481118562489, "tcdate": 1481118562484, "number": 3, "id": "B1qPmqBXl", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "How many networks were trained in total? 102.400 for CIFAR-10?", "comment": "This is very interesting work. \nI am wondering how many networks you trained in total. For CIFAR-10, you say you trained the controller for 12,800 iterations. Since every controller used m=8 child replicas in every iteration, does this mean you trained 12.800 * 8 = 102.400 networks in total for CIFAR-10? Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1480729842337, "tcdate": 1480729842331, "number": 1, "id": "SkcgSiJmx", "invitation": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["ICLR.cc/2017/conference/paper251/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper251/AnonReviewer4"], "content": {"title": "Generalization", "question": "Quite interesting paper!\n\n1. Can you please comment on how generalizable the method is? Is there anything inherent to the two selected problems that make it work? What types of problems would be difficult for this method?\n2. The method is clearly computationally very intensive. Would it help to bootstrap it with an initial human-architected DNN? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481188409415, "id": "ICLR.cc/2017/conference/-/paper251/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper251/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper251/AnonReviewer4", "ICLR.cc/2017/conference/paper251/AnonReviewer2", "ICLR.cc/2017/conference/paper251/AnonReviewer3"], "reply": {"forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper251/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481188409415}}}, {"tddate": null, "tmdate": 1480541965411, "tcdate": 1480541965406, "number": 2, "id": "S1SGw63Mg", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "SJMOWnjMx", "signatures": ["~Barret_Zoph1"], "readers": ["everyone"], "writers": ["~Barret_Zoph1"], "content": {"title": "Request for details on running time and number of machines", "comment": "For training the Cifar10 models we used K80 GPUs and for training the Penn TreeBank models we used Intel Haswell CPUs. The Cifar jobs took 3 - 4 weeks and the Penn TreeBank jobs took 2 - 3 weeks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}, {"tddate": null, "tmdate": 1480470889695, "tcdate": 1480470889692, "number": 1, "id": "SJMOWnjMx", "invitation": "ICLR.cc/2017/conference/-/paper251/public/comment", "forum": "r1Ue8Hcxg", "replyto": "r1Ue8Hcxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Request for details on running time and number of machines", "comment": "This is a very interesting paper! Can you please provide some information on the amount of time that was required in order to train your model and the hardware that you used?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "pdf": "/pdf/a737e298ef4f25808b2a4b464c913678234b1a5d.pdf", "paperhash": "zoph|neural_architecture_search_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665535, "id": "ICLR.cc/2017/conference/-/paper251/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Ue8Hcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper251/reviewers", "ICLR.cc/2017/conference/paper251/areachairs"], "cdate": 1485287665535}}}], "count": 29}