{"notes": [{"id": "u8X280hw1Mt", "original": "n6Sdtbnji8s", "number": 253, "cdate": 1601308036605, "ddate": null, "tcdate": 1601308036605, "tmdate": 1614985736966, "tddate": null, "forum": "u8X280hw1Mt", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gRjhlhAjdwZ", "original": null, "number": 1, "cdate": 1610040401293, "ddate": null, "tcdate": 1610040401293, "tmdate": 1610473997278, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The equivalence rules for the margin are quite interesting, but I have two main concerns with the current paper (1) the theory does not seem to justify why increasing the number of negative examples helps in contrastive learning -- in fact Table 9 shows that the bound gets smaller as K increases. (2) The experiments use a large batch size (N) but use a smaller number of negative examples (K), which does not reduce the computation cost by much. The theoretical issue (1) can be a matter of improving the writing to put less emphasis on the theory. I invite the authors to address these issues and resubmit to other ML venues. \n\nDetailed feedback: I believe you are missing a negative sign in your definition of optimal \"loss\" $\\mathcal{L}_{opt}$ in Eq. (3), which resulted in AnonReviewer4's final comment."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040401279, "tmdate": 1610473997260, "id": "ICLR.cc/2021/Conference/Paper253/-/Decision"}}}, {"id": "g6Hdn0vdAss", "original": null, "number": 4, "cdate": 1604644297229, "ddate": null, "tcdate": 1604644297229, "tmdate": 1606804449424, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Review", "content": {"title": "Good observation and experiments, but a bit over-claim", "review": "This paper introduces a margin term for positive pair in contrastive loss, and discovered an equivalent rule between margin m, temperature \\tau, and number of negatives K. The authors demonstrate that once they set the three values following the equivalent rule, then relatively smaller number of negatives can also work well for contrastive learning.\n\nPros:\n- It is interesting to introduce a margin m, and properly set a relation between it and temperature and number of negatives.\n- Theoretically, the authors show that once you set the term contains K in the lower bound of MI to be constant, then the lower bound will be irrelevant to K. The equivalent rule is very interesting, but the theorem and Figure 1 is a bit trivial to me(see cons).\n- The experimental results with MoCo, MoCo v2, and SimCLR shows that the equivalent rule improves the stability over different number of negatives for contrastive learning.\n\nCons:\n- The reason why the lower bound now doesn't depends on K, is because you set the term that contains K as constant now. Now K is not an variable for the lower bound anymore, and the only variable in the lower bound is the density ratio, which is almost stable once the training converges. Also, theoretically the converged density ratio should be invariant to K. Therefore, you don't even need to plot Figure 1(b), but you will guess it's constant. To extreme, you can set I >= 0 = f_bound, but it tells you nothing. Thus,\n(1) The real question is that, how this bound can sensitively and faithfully reflect the change of ground truth mutual information? To this end, I would recommend authors to follow the experiments in [A1] to generate variables that share different amount of mutual information, and check how your bounds track that. E.g., Figure 2 in A1.\n(2) you can do such bound estimation on held-out data to check.\n- I found myself not convinced by some statements in this paper:\n(1) In abstract, \"for the first time, we can perform self-supervised contrastive training using only a few negative pairs\", some prior work [A2] has already reduce the number of negatives. So the tone here is a bit arrogant to me. Besides, BYOL paper has already shown that you can completely remove the denominator term in contrastive loss, and only do alignment.\n(2) The first bullet of contribution section that says \"large size of negative samples is critical\" is a wrong interpretation. I kind of agree that small negative also works well, at the ImageNet scale. But what will happen for uncurated dataset whose size is far beyond ImageNet, is still unknown. So the claim here should be more restricted.\n- The SIMO design choices you proposed here is exactly the same as SimCLR v2 [A3].\n(1) No memory bank, key and negative from the same batch of momentum encoder. Yes, check SimCLR v2.\n(2) SyncBN. Yes, it's in SimCLR and SimCLR v2.\n- Missing related work:\n(1) Big Self-Supervised Models are Strong Semi-Supervised Learners. NeurIPS 2020.\n(2) Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. NeurIPS 2020.\n(3) What makes for good views for contrastive learning. NeurIPS. NeurIPS 2020.\n(4) Whitening for Self-Supervised Representation Learning. arXiv\n\n[A1] On Variational Bounds of Mutual Information. ICML 2019\n[A2] Whitening for Self-Supervised Representation Learning.\n[A3] Big Self-Supervised Models are Strong Semi-Supervised Learners. NeurIPS 2020.\n\nOverall, I like the equivalent rule for stabilizing the training across different number of negatives. Meanwhile, I hope the authors could reconsider some statements. Also, it would be of great interest to see the experiments on estimating mutual information. I would consider changing my rating if my concerns get solved properly.\n\n\n==== update ====\n\nI found the bound the author provides is problematic (see my response for details). In short, the bound is problematic in that the bound will be more accurate when only one sample from the product of marginal (or negative) is used, i.e., $K=1$, while more negatives leads to less accurate bound. This definitely counters the intuition in theory where more samples should let you estimate the KL divergence between $p(x,y)$ and $p(x)p(y)$ better.\n\nI believe there is a mathematical issue of directly setting up $m=\\tau \\log \\frac{\\alpha}{K}$ as in Eq(5). Though the empirical results look good, I recommend **NOT** accept papers with theory issues.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper253/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147138, "tmdate": 1606915772975, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper253/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Review"}}}, {"id": "VqaVeBKG1AV", "original": null, "number": 1, "cdate": 1603423680178, "ddate": null, "tcdate": 1603423680178, "tmdate": 1606802350243, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Review", "content": {"title": "Excellent and Insightful Discussion on Negative Samples in Self-supervised Contrastive Learning", "review": "*****************************\nSummary: \n\nThis paper focuses on self-supervised contrastive learning. Previous contrastive learning methods heavily rely on a large number of negative samples. This paper proposed a novel method with an additional margin term, and mathematically investigate the relationship among the margin term, the temperature, and the number of negative samples. The number of negative samples can be significantly reduced by tuning the margin term, while the performance remains more stable compared to previous contrastive learning methods. Furthermore, this paper proposed a MoCo-based strong baseline that can achieve comparable results with an extremely small number of negative samples.\n\n*****************************\nReasons for Score: \n\nOverall, I vote for acceptance. This paper proposed equivalent rules which successfully reduce the number of negative samples for contrastive representation learning while maintaining the performance. This motivation is very novel and interesting, and I believe the findings in this paper significantly contributes to the community of contrastive representation learning. My main concern is the assumption that the prior probabilities $P^+$ and $P^-$ satisfy $P^+/P^-=e^{-m/\\tau}$ (see Sec. 2.1). \n\n*****************************\n \nStrengths: \n\n\\+ This paper focused on the problem of using a large number of negative samples in self-supervised contrastive learning, which is very important but not fully investigated before.\n\n\\+ This paper proposed a simple but strong baseline that can achieve comparable results with an extremely small number of negative samples. To the best of my knowledge, this is the first time to succeed with a small number of negative samples in contrastive learning. It may also inspire the applications in other self-supervised learning problems where a large number of negative samples are hard to be maintained.\n\n\\+ This paper provides detailed mathematical analysis and theorems, which makes the paper theoretically strong.\n\n\\+ The proposed strategy achieves significant improvement compared to the most popular self-supervised contrastive learning methods, MoCo and SimCLR, with a small number of negative samples. \n\n*****************************\n \nConcerns & Questions: \n\n\\- The Eq. (2) includes $P^+$ and $P^-$ as prior probabilities. It is not clear to me how the priors are taken into account and whether it is theoretically correct. In addition, it is not clear the assumption $P^+/P^\u2212=e^{\u2212m/\\tau}$ is correct. Could more mathematical details be introduced? Why is including $P^+$ and $P^-$ called a generalized form?\n\n\\* In this paper, the temperature $\\tau$ is fixed when tuning $K$ and selecting $m$. It would be an interesting issue if Eq. (5) is rewritten as $\\tau=\\frac{m}{\\log\\alpha-\\log K}$. I wonder whether it is possible to select $\\tau$ by fixing $m$ and tuning $K$. If not, is there any insight behind this?\n\n\\* In the proposed SiMo, all the negative examples are obtained from the current batch via the momentum encoder rather than the dictionary. Table 3 in the appendix shows that momentum update is important. Note that the momentum encoder in MoCo is used for the consistent dictionary of negative samples. Since the dictionary is not used in SiMo, the momentum update should have a different role in contrastive learning, perhaps like Mean Teacher. It would be great if more analysis or explanation on this can be provided.\n\n\\* SiMo uses Sync BN rather than Shuffling BN. I wonder whether Sync BN causes extra computational cost or slowers the training speed.\n\n\n\n\n\n============================================\n\n\nAfter rebuttal:\n\nAccording to the reviewers' feedback, I would keep my score to 8 and still vote for acceptance. However, there are still two details that are expected to be fixed in the future version. First, the reason why momentum update in SiMo is important is not convincing to me. It is not clear why letting $\\theta_k=\\theta_q$ cannot ensure the loss becoming smaller. More theoretical and experimental analyses are expected to address this issue. I still encourage the authors to rethink this detail. Second, the computational cost is provided in the feedback. I encourage the authors to include the numbers in the paper. 35\\% additional cost cannot be ignored.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper253/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147138, "tmdate": 1606915772975, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper253/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Review"}}}, {"id": "40GIBKn1DYm", "original": null, "number": 2, "cdate": 1603732153385, "ddate": null, "tcdate": 1603732153385, "tmdate": 1606763565063, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Review", "content": {"title": "Interesting topic and observation, but there are some issues with the theory and result", "review": "This paper studies the performance of contrastive learning under a varying number of negative examples. The authors propose a small modification to the InfoNCE loss and show tuning some hyper-parameter could significantly improve the results with a small number of negatives. I found this topic very interesting, and the proposed solution to be quite simple. It challenges the conventional belief that a large number of negatives are required for contrastive learning (though this has been challenged before, recently by BYOL). The results show that when the number of negatives (K) is small, by using the proposed method, it could greatly improve the performance.\n\nHowever, there are some questions/issues that I came across:\n\nThe theoretical analysis shows by setting m using eq 5, the mutual information lower bound could be made completely irrelevant to K; instead it\u2019s controlled by a constant alpha. This is clearly problematic: 1) it suggests one could simply adjust alpha to improve the lower bound to an arbitrarily large number, which is impossible given that mutual information is a fixed number given a dataset. 2) experimental results show there is still a significant gap when using small K (Figure 2). This suggests alpha is not some number one could arbitrary set to improve the bound, and it may actually be related to K, which makes the current theoretical analysis potentially flawed or incomplete.\n\nThe experimental results are pretty nice when K is small (compared to baselines), however, these are somewhat inconsequent since a small K is obtained by dropping out some negative examples in the current or past mini-batch, not actual small batch size (one still cannot use a small batch size due to batch norm and actual training time). The improvement on the best of MoCov2 result looks somewhat marginal and may not be related to the number of negatives (which contradicts the claim on why SiMo helps), as the performance of MoCov2 has saturated with increased K (Table 6). If the author\u2019s claim were true, it should be possible to improve SimCLR's performance on small batch size (to reach the same performance of large batch size training), but this was not shown in the paper (Figure 2b). What's more, Figure 2b looks worse than expected. According to SimCLR paper (Table B.1), the gap between small and big batch sizes can be significantly reduced by using a proper learning rate, so it would be more convincing to use a better learning rate (or compare performances with different learning rates).\n\nOther minor issues: 1) for Table 7, the number of training epochs is not specified. 3) the methods in Table 2 looks a bit arbitrary, several existing similarly/better performing methods (e.g. BYOL, SWAV) are not included.\n\nGiven these issues, I will give an initial rating of 5, which is contingent on the responses from the authors.\n\n--after rebuttal--\n\nI appreciate the response, and it addressed my initial concern on the mutual information bound. However, the major concern remains which is the toy-ish setup and its practical value. The main result authors showed is that dropping some negatives doesn't hurt the performance if one adjusts m using EqCo rule. While this is interesting, the current setting of few negatives (by throwing out available negatives) is toy-ish, and may not reflect the real situation when a mini-batch is small thus only few negatives are available. I'd really like to see how this could be used to deal with the situation where you indeed only has few negatives. In MoCo, one could always easily buffer negatives with EMA the network, so there's no need to use a smaller set of negatives. In SimCLR, there seems to be a bigger potential of improvement from EqCo with actual smaller batch size, but the authors should also tune the learning rate (for smaller batch sizes) to make the baseline convincing, and compare to the results where large batch size is used. While the authors added a new point of alpha=4096, the current results of the paper are still incomplete, so I would keep my score. I'd encourage the authors to update/complete these results regardless the paper is immediately accepted by ICLR.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper253/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147138, "tmdate": 1606915772975, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper253/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Review"}}}, {"id": "FYCjq8ibpdA", "original": null, "number": 8, "cdate": 1606233724625, "ddate": null, "tcdate": 1606233724625, "tmdate": 1606242049118, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "VqaVeBKG1AV", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for the review comments. \n\nQ1: It is not clear to me how the priors $P^+$ and $P^-$ are taken into account and whether it is theoretically correct. In addition, it is not clear the assumption $P^+/P^-=e^{-m/\\tau}$ is correct. Could more mathematical details be introduced? Why is including $P^+$ and $P^-$ called a generalized form?\n\nA1: We try to explain the priors as the following viewpoints:\n\nFirst, our derivation is mainly inspired by CPC (Oord et al. (2018)). CPC does not introduce the priors explicitly, but from the Bayesian view, there must exists a prior probability $\\text{P}(H_i)$ for each candidate distribution $H_i$. CPC implicitly set those priors to be equal, however, which is not demonstrated to be correct. So, in the paper, we explicitly formulate $P^+$ and $P^-$ to take them into account, which is called the generalized form of CPC. \n\nSecond, we point that $P^+/P^-=e^{-m/\\tau}$ is not an assumption but a definition, which is parameterized by a variable $m$; therefore, the formulation is always \"correct\". However, the value of $m$ is still difficult to be calculated. We further assume that $m$ is a function of $K$, and the \"correct value\" of the $(m, K)$ pair corresponds to the best performance. So, our equivalent rule in Eq.(6) can be interpreted as follows: first, we fix the value of $m$ (e.g. $m=0$) and explore the $K$ with best performance (noted as $K=\\alpha$), in which we assume the pair $(m=0, K=\\alpha)$ is \"correct\"; then, based on the equivalent condition  that MI lower bound should be unchanged, we can predict the \"correct\" value of $m$ under other number of negative samples $K'$.\n\n\nQ2: In this paper, the temperature $\\tau$ is fixed when tuning $K$ and selecting $m$. It would be an interesting issue if Eq. (5) is rewritten as $\\tau=\\frac{m}{\\text{log}\\ \\alpha - \\text{log}\\ K}$. I wonder whether it is possible to select $\\tau$  by fixing $m$ and tuning $K$. If not, is there any insight behind this?\n\nA2: As we explained in the paragraph above Sec 2.2, $\\tau$ affects not only the MI lower bound but also the gradient (Eq. 7). So, to get equivalent at least learning rate also needs to be tuned. Our preliminary study shows that it seems to work under some configurations of $(m, K)$, but not in general.  \n\n\nQ3: In the proposed SiMo, all the negative examples are obtained from the current batch via the momentum encoder rather than the dictionary. Table 3 in the appendix shows that momentum update is important. Note that the momentum encoder in MoCo is used for the consistent dictionary of negative samples. Since the dictionary is not used in SiMo, the momentum update should have a different role in contrastive learning, perhaps like Mean Teacher. It would be great if more analysis or explanation on this can be provided.\n\nA3: The reason why momentum update in SiMo still matters is that we only update the network with the partial gradient subject to the parameters in the query encoder, which follows the convention in MoCo. Specially, the loss in SiMo can be formulated as follows:\n\n$$\nL=L(q(\\theta_q), k(\\theta_k))\n$$\n\nAnd we update the parameters using the partial gradient:\n\n$$\n\\nabla_{\\theta_q} L = \\frac{\\partial L}{\\partial q} \\frac{\\partial q}{\\partial \\theta_q}\n$$\n\nIf we do not use momentum update, i.e. simply letting $\\theta_k = \\theta_q$, we cannot ensure that for each iteration the loss becomes smaller only using $\\nabla_{\\theta_q} L$, because another term $\\nabla_{\\theta_k} L$ is not taken into account. However, if we introduce momentum update, the update of $\\theta_k$ thus becomes very slow, so for each iteration we can roughly regard $\\theta_k$ as a constant. So, the full gradient is only determined by $\\nabla_{\\theta_q} L$, thus our optimization becomes correct. Furthermore, if $\\theta_k$ is set to be learnable, it is equivalent to SimCLR, which already shows competitive performance.\n\nQ4: Whether Sync BN causes extra computational cost or slower the training speed.\n\nA4: It just depends on the implementation and the computing devices. By using the Sync BN implemented in Detectron, the additional cost is around 45\\%, while built-in SyncBatchNorm of PyTorch only requires about 35\\% additional cost. It can be further accelerated with GPUs that support P2P access, e.g. V100. "}, "signatures": ["ICLR.cc/2021/Conference/Paper253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u8X280hw1Mt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper253/Authors|ICLR.cc/2021/Conference/Paper253/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873016, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment"}}}, {"id": "otkwvdlo1h", "original": null, "number": 7, "cdate": 1606231361791, "ddate": null, "tcdate": 1606231361791, "tmdate": 1606231361791, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "7LdQ9PcWV5d", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thanks for the review comments. \n\nQ1. In figure 2, even though the accuracy is less sensitive to K, but increasing K still helps the performance. This result is against the theorem 1 and 2. Is there any other explanation?\n\nA1. Please refer to our response for AnonReviewer3 (Q2) and Sec.4 (first paragraph) for details. In short, it is because both Theorem 1 and Theorem 2 indicate the equivalence property in the sense of expectation, however, the variance indeed becomes large if $K$ is small while $\\alpha$ is large.\n\n\nQ2. Gradient and lower bound may not be the only thing that affect the performance. What about generalization? Will large negative sample size help generalization? This needs a discussion. \n\nA2. We agree it is an interesting topic to theoretically analyze the generation bound under EqCo, which we will investigate in the future. But empirically, Table 8 shows that SiMo ($K=256, \\alpha=65536$) achieves similar performance as MoCo v2 ($K=65536$) on COCO detection task, which may indicate that generation may not be a severe problem even though the physical negative batch size is small under EqCo. \n \nQ3. Does representation learning through minimizing the mutual information help improve the performance of the downstream classifier (linear or MLP)? Will large negative sample size help because of the downstream classifier is linear/MLP? This needs a discussion.\n \nA3. Most of the experiments in the paper are performed with linear classification protocol. The results have shown that under EqCo the number of negative samples does not affect much of the performance. The review may concern whether EqCo over fits the downstream linear classifier. We are pleased to provide the experiments with MLP instead of linear downstream classifier afterwards, but we think it is not likely. \n "}, "signatures": ["ICLR.cc/2021/Conference/Paper253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u8X280hw1Mt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper253/Authors|ICLR.cc/2021/Conference/Paper253/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873016, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment"}}}, {"id": "Ivq7ncajK1q", "original": null, "number": 6, "cdate": 1606229408435, "ddate": null, "tcdate": 1606229408435, "tmdate": 1606229693722, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "g6Hdn0vdAss", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thanks for the review comments. \n\nQ1. About the overclaiming issue. \n\nA1. We have revised those statements. For example, we remove the claim that \"for the first time, we can perform self-supervised contrastive training using only a few negative pairs\" in the abstract. We summarize our first contribution as \"we interpret it from a\ndifferent view: it may be because the hyper-parameters are not set to the optimum\". \n\nQ2. The theorem and Figure 1 is a bit trivial to me. You don't even need to plot Figure 1(b), but you will guess it's constant. To extreme, you can set $\\text{I} >= 0 = \\text{f}_\\text{bound}$, but it tells you nothing.\nThe real question is that, how this bound can sensitively and faithfully reflect the change of ground truth mutual information.\n\nA2. First, we do not agree that the bound and Fig.1 is as trivial as $\\text{I} >= 0 = \\text{f}_\\text{bound}$ . \nNotice that \n\n$ \\text{log} (1 + K e^{m/\\tau}) - \\mathcal{L}_{NCE} \\triangleq \\hat{\\text{f}}_\\text{bound}$\n\nwhile $\\mathcal{L}_{NCE}$ is still a function of $K$ and $m$ and needs to be optimized. When $m$ and $K$ changes, the landscape of the loss thus changes, we think it is still necessary to check whether $\\hat{\\text{f}}_\\text{bound}$ converges to the similar values as expected, as shown in Fig.1. It is very different from the (trivial) concept of $\\text{I} >= 0$ which even does not contain terms to be optimized. \n\nSecond, the reviewer may concern about the tightness of the bound. It is known that the conventional InfoNCE estimator is of low variance but high bias. Our proposed EqCo just aims to simulate the behavior of InfoNCE loss using smaller $K$s, which does not help to reduce the bias. In Appendix C we introduce a verification following the settings of [A1] as suggested by the reviewer. Table 9 shows that with EqCo the estimation of MI keeps steady with different $K$s, which is able to simulate the conventional InfoNCE estimator under $K=512$ as we expected. In addition, both estimators reflect the change of ground truth MI. But, we also notice that when the ground truth MI is large (MI=10), the gap between the estimator and the ground truth also increases. \n\n\nQ3. The SIMO design choices you proposed here is exactly the same as SimCLR v2 [A3]. (1) No memory bank, key and negative from the same batch of momentum encoder. Yes, check SimCLR v2. (2) SyncBN. Yes, it's in SimCLR and SimCLR v2.\n\nA3. We add the discussion in Sec.3 and Footnote 6. We argue that the key difference is, the best configuration suggested by SimCLR v2 still uses memory bank. Though their experiments compare the one without memory bank, the score is worse. While in SiMo, thanks to EqCo we can remove the memory bank without accuracy drop. Despite all this, we have revised the statements to avoid misleading. \n\nQ4. Missing related work.\n\nA4. We have added the references. However, we have to point out [A2] is also a submission in ICLR 2021.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u8X280hw1Mt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper253/Authors|ICLR.cc/2021/Conference/Paper253/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873016, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment"}}}, {"id": "Al-heLK0Vbl", "original": null, "number": 4, "cdate": 1606223908526, "ddate": null, "tcdate": 1606223908526, "tmdate": 1606223908526, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "40GIBKn1DYm", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thanks for the review comments. \n\nQ1. The theoretical analysis is clearly problematic: it suggests one could simply adjust alpha to improve the lower bound to an arbitrarily large number, which is impossible given that mutual information (MI) is a fixed number given a dataset.\n\nA1. Not really. In Eq.(6), for simplicity, we assume \n$\\frac{\\text{P}(\\mathbf{k}_0)}{\\text{P}(\\mathbf{k}_0|\\mathbf{q})} = c \\quad (0<c<1)$ is a constant. Thus the bound becomes:\n\n$$\nf_{\\text{bound}}(0, \\alpha) \\approx \\text{log} (1 + \\alpha) - \\text{log} (1 + \\alpha c)\n= \\text{log} \\left(\\frac{1/\\alpha + 1}{1/\\alpha + c}\\right),\n$$\n\nwhich monotonously increases with $\\alpha$; when $\\alpha \\to +\\infty$, the limit is $-\\text{log} (c)$, which cannot be \"arbitrarily large number\". Experiments in Appendix C further validates that our modified loss can estimate MI as conventional InfoNCE loss. \n\nQ2. Experimental results show there is still a significant gap when using small K (Figure 2). This suggests alpha is not some number one could arbitrary set to improve the bound. The improvement on the best of MoCo v2 result looks somewhat marginal and may not be related to the number of negatives (which contradicts the claim on why SiMo helps). The improvement on the best of MoCov2 result looks somewhat marginal.\n\nA2. We acknowledge it correlates to a limitation of our method, as discussed in Sec.4 (first paragraph). The problem is, both Theorem 1 and Theorem 2 indicate the equivalence property in the sense of expectation, however, the variance indeed becomes large if $K$ is small while $\\alpha$ is large. For example, in Eq.(13) the variance of the approximation clearly increases with large $\\alpha$ and small $K$. So if $K$ is too small, the gradient noise could be large, which may lead to inferior results. However, it dose not mean our theoretical analysis is \"clearly problematic\". Our experiments in Fig. 2, Fig. 3 also show that the equivalent rule can still work within a wide range of $\\alpha$ and $K$s. Furthermore, in SiMo and SimCLR (using SyncBN), equipped with EqCo we find the degradation of small $K$ is much smaller than that in MoCo series (using Shuffling BN), which may be because SyncBN helps to reduce the gradient noise. \n\nQ3. It should be possible to improve SimCLR's performance on small batch size (to reach the same performance of large batch size training), but this was not shown in the paper (Figure 2b). What's more, Figure 2b looks worse than expected. \n\nA3. In the revised version we introduce a new experiment using $\\alpha = 4096$ and $K=256$ in SimCLR (see Fig.2(b)), which brings significant improvements (from 62.0 to 65.3). Though the result is slightly worse than that reported in SimCLR paper (66.6, with a physical batch size of 4096), we argue that the implementation may be not fully aligned (see our comments in Footnote 4 and 5). We are pleased to provide the fair comparison after our distributed SimCLR implementation is ready. In addition, we point that our reproduction of SimCLR under small batch size ($K=256$) is consistent with the counterpart in SimCLR paper (62.0 vs 61.9), not \"worse than expected\". \n\nQ4. According to SimCLR paper (Table B.1), the gap between small and big batch sizes can be significantly reduced by using a proper learning rate, so it would be more convincing to use a better learning rate. \n\nA4. We argue that EqCo is applicable but not the only way to bridge the gap between small and large batches. We agree that carefully tuning the learning rate may also help; but from the experiments in SimCLR paper, the gap seems still significant, and results under smaller batch sizes ($K<256$) is are not provided. We comment it in Footnote 3. We are pleased to add the related experiments afterwards. \n\nQ5. Other minor issues.\n\nA5. We have fixed them in the revised version. See Table 2, Footnote 7 and Table 7. "}, "signatures": ["ICLR.cc/2021/Conference/Paper253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u8X280hw1Mt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper253/Authors|ICLR.cc/2021/Conference/Paper253/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873016, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment"}}}, {"id": "OTpMmDFFsOW", "original": null, "number": 3, "cdate": 1606216785888, "ddate": null, "tcdate": 1606216785888, "tmdate": 1606216785888, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "Dear reviewers:\n\nWe have uploaded a new revision of our paper. Compared with the initial version, the change mainly includes:\n\nA. Presentation\n\n1) We revised some statements which may be misleading or overclaiming. \n\n2) We add some descriptions and footnotes to justify the relations or differences to existing methods.\n\n3) Some related works are added as suggested by R4.\n\nB. Experiment\n\n1) We add a new experiment on SimCLR, adjusting large $\\alpha=4096$ to simulate large batch training while using small physical batch size $K=256$. Please refer to Fig.2(b) and the related text for details. \n\n2) We add a new experiment to evaluate the InfoNCE lower bound with/without EqCo under different $K$s. See Appendix C for details. "}, "signatures": ["ICLR.cc/2021/Conference/Paper253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u8X280hw1Mt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper253/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper253/Authors|ICLR.cc/2021/Conference/Paper253/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873016, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Comment"}}}, {"id": "7LdQ9PcWV5d", "original": null, "number": 3, "cdate": 1603873856605, "ddate": null, "tcdate": 1603873856605, "tmdate": 1605024730564, "tddate": null, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "invitation": "ICLR.cc/2021/Conference/Paper253/-/Official_Review", "content": {"title": "Good paper with some concerns", "review": "The authors propose Eqco which can get rid of the effect of the large negative sample size in contrastive learning. \n\nPros:\n1. The authors tackle an important question whether the large negative sample size is important in contrastive learning\n2. The authors propose Eqco which introduces an margin term to the InfoNCE loss. \n3. The authors empirically shows that the result algorithm Simo is less sensitive to negative sample size.\n4. The authors prove that large negative sample size does not affect the lower bound and the bound of the gradient.\n5. The authors improve Moco-v2's result using Eqco.\n\nCons:\n1. In figure 2, even though the accuracy is less sensitive to K. But increasing K still helps the performance. This result is against the theorem 1 and 2. Is there any other explanation?\n2. Gradient and lower bound may not be the only thing that affect the performance. What about generalization? Will large negative sample size help generalization? This needs a discussion\n3. Does representation learning through minimizing the mutual information help improve the performance of the downstream classifier (linear or MLP)? Will large negative sample size help because of the downstream classifier is linear/MLP? This needs a discussion.\n\nSignificance:\nThe problem is significant and the authors made some progress on it.\n\nClarity:\nThe paper is clearly written in general.\n\nOriginality:\nThe paper is novel.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper253/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper253/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EqCo:   Equivalent Rules for Self-supervised Contrastive Learning", "authorids": ["~Benjin_Zhu1", "~Junqiang_Huang1", "~Zeming_Li2", "~Xiangyu_Zhang1", "~Jian_Sun4"], "authors": ["Benjin Zhu", "Junqiang Huang", "Zeming Li", "Xiangyu Zhang", "Jian Sun"], "keywords": [], "abstract": "In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.", "pdf": "/pdf/13d95e0522001abb6f4e81e7f18c20a3cf4b51a8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|eqco_equivalent_rules_for_selfsupervised_contrastive_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQj2libhJO", "_bibtex": "@misc{\nzhu2021eqco,\ntitle={EqCo:   Equivalent Rules for Self-supervised Contrastive Learning},\nauthor={Benjin Zhu and Junqiang Huang and Zeming Li and Xiangyu Zhang and Jian Sun},\nyear={2021},\nurl={https://openreview.net/forum?id=u8X280hw1Mt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u8X280hw1Mt", "replyto": "u8X280hw1Mt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147138, "tmdate": 1606915772975, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper253/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper253/-/Official_Review"}}}], "count": 11}