{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488557626472, "tcdate": 1478310113140, "number": 541, "id": "HJKkY35le", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJKkY35le", "signatures": ["~Tong_Che1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 27, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396666318, "tcdate": 1486396666318, "number": 1, "id": "H1ezZ6zIue", "invitation": "ICLR.cc/2017/conference/-/paper541/acceptance", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper presents some intuitions about why GAN training is difficult, and proposes a somewhat remedy that seems to help. \n \n Pros\n  - a detailed explanation of the motivation\n  - side experiments to demonstrate the properties of the new method\n \n Cons\n  - There are already several closely-related approaches, namely VAEGAN, with similar motivations and procedures. This paper argues that their method is conceptually distinct from the VAEGAN, but I and other reviewers found the arguments underdeveloped unconvincing.\n  - Most of the evaluations are qualitative.\n \n The search space of reasonable modifications to the GAN objective is so large that either a more principled or systematic (as in, do more comparison experiments) exploration of the space is necessary. \n\nGiven the importance of the topic, the PCs still believe this paper should be accepted, but we encourage the authors to further revise their paper to address the remaining outstanding issues.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396666818, "id": "ICLR.cc/2017/conference/-/paper541/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396666818}}}, {"tddate": null, "tmdate": 1485235351112, "tcdate": 1485235351112, "number": 1, "id": "ByysEwEvl", "invitation": "ICLR.cc/2017/conference/-/paper541/public/review", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Good intuition while lacking analysis", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution.  The presented approaches are entirely based on some intuitive arguments. As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point.  The paper will benefit from more rigorous theoretical justification of the presented approaches. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485235351811, "id": "ICLR.cc/2017/conference/-/paper541/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk", "ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs", "(anonymous)"], "cdate": 1485235351811}}}, {"tddate": null, "tmdate": 1484437364793, "tcdate": 1484437364793, "number": 24, "id": "BJa_DNdIl", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["~Yanran_Li2"], "readers": ["everyone"], "writers": ["~Yanran_Li2"], "content": {"title": "Update the submission paper", "comment": "Dear reviewers and readers,\n\nThanks for your constructive and thoughtful reviews and comments. We've updated our submission paper and added experiments in the appendix sections. Please kindly refer to our newest version. \n\nThanks again."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482170585671, "tcdate": 1482170585671, "number": 3, "id": "SkMk-sH4g", "invitation": "ICLR.cc/2017/conference/-/paper541/official/review", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/conference/paper541/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper541/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\n\nThis paper proposes several regularization objective such as \"geometric regularizer\" and \"mode regularizer\" to stabilize the training of GAN models. Specifically, these regularizes are proposed to alleviate the mode-missing behaviors of GANs.\n\nReview:\n\nI think this is an interesting paper that discusses the mode-missing behavior of GANs and proposes new evaluation metric to evaluate this behavior. However, the core ideas of this paper are not very innovative to me. Specifically, there has been a lot of papers that combine GAN with an autoencoder and the settings of this paper is very similar to the other papers such as Larsen et al. As I pointed out in my pre-review comments, in the Larsen et al. both the geometric regularizer and model regularizer has been proposed in the context of VAEs and the way they are used is essentially the same as this paper. I understand the argument of the authors that the VAEGAN is a VAE that is regularized by GAN and in this paper the main generative model is a GAN that is regularized by an autoencoder, but at the end of the day, both the models are combining the autoencoder and GAN in a pretty much same way, and to me the resulting model is not very different. I also understand the other argument of the authors that Larsen et al is using VAE while this paper is using an autoencoder, but I am still not convinced how this paper outperforms the VAEGAN by just removing the KL term of the VAE. I do like that this paper looks at the autoencoder objective as a way to alleviate the missing mode problem of GANs, but I think that alone does not have enough originality to carry the paper.\n\nAs pointed out in the public comments by other people, I also suggest that the authors do an extensive comparison of this work and Larsen et al. in terms of missing mode, sample quality and quantitative performances such as inception score.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512548106, "id": "ICLR.cc/2017/conference/-/paper541/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper541/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper541/AnonReviewer3", "ICLR.cc/2017/conference/paper541/AnonReviewer2", "ICLR.cc/2017/conference/paper541/AnonReviewer1"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512548106}}}, {"tddate": null, "tmdate": 1482043561751, "tcdate": 1482043561751, "number": 20, "id": "SJfnlnQNx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "rJtxviX4e", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comment", "comment": "From what I understand, you are using plain autoencoder meaning that p(z|x) is just a delta function (degenerate gaussian). To be honest, I don't see how this gives a regularization property which is missing (for some unknown reason) for a KL-penalized gaussian p(z|x). Probabilistic formalism aside, the KL-penalty encourages certain smoothness of the latent space and I personally feel that this is not an undesirable effect.\n\nI would like to stress that I'm not claiming that AE and VAE is the same thing. Instead, I'm saying that changing VAE to AE in the existing work without theoretically and empirically justifying this single modification is not sufficient to call the resulting model a superior invention. I would also want to clarify that I'm not stating that the aforementioned modification is useless but rather I'm trying to encourage the authors to fill the missing gap and properly prove the significance of the contribution.\n\nIf the evaluation reveals that the both approaches perform similarly, I would suggest reframing the paper as an analysis of the family of related methods."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482041073408, "tcdate": 1482041073408, "number": 19, "id": "rJtxviX4e", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HkgiRw7Ve", "signatures": ["~Tong_Che1"], "readers": ["everyone"], "writers": ["~Tong_Che1"], "content": {"title": "reply", "comment": "Thanks again for comments\n\nHere VAE targets means the sum of the prior loss and auto-encoder loss, which is the optimization target for VAEs as in the original VAE paper (Kingma and Welling, 2013). The reason why this VAE target cannot be used as regularizer is that it imposes probabilistic assumptions  to posterior distribution p(z|x) as pointed above. However, in our model, if G(z), z~p_prior is a good generative model, one can always expect that there can be an encoder E, such that x and G(E(x)) is close. Mathematically our model does not require p(z|x) has a Gaussian shape. This is why our additional loss can be used as a regularizer. \n\nAs I said, the autoencoder used in our model is a plain auto-encoder. It is different from a variational autoencoder. We do not need any variational approximations to make the model work. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482026648186, "tcdate": 1482026648186, "number": 18, "id": "HkgiRw7Ve", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "rygJ8vX4g", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Clarification", "comment": "Thanks for your answer! The following statement is not clear to me: \"What\u2019s more, the VAE targets in the VAEGAN are not used as regularizers. By regularizers, we mean that the term will control model variance but probably introduce biases\". What do you mean by \"VAE targets\"? In which sense could one you use those targets as regularizers?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482024407933, "tcdate": 1482024407933, "number": 17, "id": "rygJ8vX4g", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "r1-E8UX4x", "signatures": ["~Tong_Che1"], "readers": ["everyone"], "writers": ["~Tong_Che1"], "content": {"title": "Differences.", "comment": "Thanks for your thoughtful comments!\n\nIn fact, this model is different from VAEGAN, just like VAEs are different from plain autoencoders. VAE used in VAEGAN has to satisfy some probabilistic assumptions. For example, VAEs need also to train a network to approximate the variance of the posterior distribution q(z|x). Also, the approximate posterior distribution q(z|x) has to be assumed as a factorized Gaussian distribution in their case. In an arbitrary GAN architecture, where these assumptions are not assumed, you cannot combine VAEs to that setting. What\u2019s more, the VAE targets in the VAEGAN are not used as regularizers. By regularizers, we mean that the term will control model variance but probably introduce biases. \n\nSo we believe that these two models are different. But we also admit that there is important similarity in terms of target for optimization. We plan to add more analysis of these differences and similarities to our next version of paper. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482022725301, "tcdate": 1482022725301, "number": 16, "id": "rkTrkvXNe", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HyEXhrQVx", "signatures": ["~Tong_Che1"], "readers": ["everyone"], "writers": ["~Tong_Che1"], "content": {"title": "Reply for constructive comment.", "comment": "Thanks for your immediate and always constructive comments!\n\nWe made this claim based on the samples they\u2019ve provided in their papers. Unfortunately they didn\u2019t provide many samples. So we agree that this claim alone might be questionable in terms of number of evidences. In hope to link the differences between these two models and the differences in the performance, we will carry out more quantitative study and update the paper as soon as possible.\n\nWe agree that we should compare our model and theirs in terms of missing modes. We\u2019re trying to run their official code and integrate our evaluation in. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482020479071, "tcdate": 1482020392734, "number": 15, "id": "r1-E8UX4x", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "BJMXcOs7l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Striking similarity of proposed model to VAEGAN (Larsen et al.)", "comment": "Please read around equation 9 of VAEGAN (https://arxiv.org/pdf/1512.09300v2.pdf). They indeed weight the loss from VAE and the loss from GAN as well. VAEGANs can also be combined with different GAN variants.\n\nYour argument that VAEGAN is a VAE with GAN loss, while MRGAN is a GAN with an autoencoder is nothing more than nomenclature. Both VAEGAN and proposed model are training decoders. It's wrong to say that in your case GANs are the generative model, while in VAEGAN's case, VAEs are the generative model. In both case, the neural network which represents p(x | z) (i.e. the decoder) is the generative model.\n\nMoreover, if the difference between VAEGAN and proposed model is just conceptual, perhaps the paper should be written along the lines of explaining and analyzing VAEGAN, rather than claiming to have discovered a new model/architecture.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482018027045, "tcdate": 1482017820334, "number": 14, "id": "HyEXhrQVx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "r15NBE7Eg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "More comments", "comment": "Thanks for your answers! \n\nIn my opinion, the most crucial part of the comparison (VAEGAN vs the proposed method) is not visual inspection of samples but rather extensive missing mode analysis as it is the main selling point the present paper.\n\nI guess, what I'm trying to say in the second comment is that I don't see why VAEGAN would produce blurry samples (something that the authors claim in the sentence: \"The samples from VAEGAN and DCGAN are unsatisfactory due to insufficient sharpness\"). The architectural difference between VAEGAN and the proposed method seems minor (both approaches have GAN as a part of the pipeline) and I'm not convinced that this difference makes the proposed method superior to the work of Larsen et al. (especially given the fact that the authors did not conduct any direct comparison at all).\n\nTo summarize my concerns: I would suggest the authors conduct a direct empirical comparison between their work and Larsen et al to prove that the introduced differences are crucial for obtaining superior performance. In my opinion, in its current form the paper downplays the existing work without supporting the claims (about VAEGAN) with experimental results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1482011953638, "tcdate": 1482011953638, "number": 13, "id": "r15NBE7Eg", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "Bybg2pf4x", "signatures": ["~Yanran_Li1"], "readers": ["everyone"], "writers": ["~Yanran_Li1"], "content": {"title": "Replies for comments", "comment": "Thanks for the comment. We didn\u2019t find the repository you\u2019ve provided (we\u2019ve did a Google search \u2018VAEGAN\u2019\u2019 and we missed the code link in the original paper). Thank you. And the official code page didn\u2019t provide more samples than they reported. So, we will run it as fast as possible and add more VAEGAN samples in our paper soon.\n\nWe\u2019re not sure what did you mean by \u201cMoreover, the authors explain lower quality of the VAEGAN samples by the fact that Larsen et al. use only hidden space of D (and no other pre-trained model) to obtain the reconstruction loss...\u201d We guessed that you\u2019re saying in theory our L2-reconstruction should lead to blurrer samples and it seems contradict our conclusion in the paper. It is well known that VAE by L2 loss tend to generate blurry images, and using hidden space of D tend to generate sharper images. But for our case, L2 loss is enough because the generative model is GAN, the weight on the L2 loss is very small, so we can generate sharp images even without using any advanced distances. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481989158661, "tcdate": 1481985000887, "number": 12, "id": "Bybg2pf4x", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "r1TuK3GVx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comments", "comment": "Thanks for the answers!\n\n1. There is an official open-sourced implementation of VAEGAN (http://github.com/andersbll/autoencoding_beyond_pixels) and the link is provided in the original paper. The authors could use either this one or the unofficial repo mentioned above to conduct the comparison. Given that VAEGAN has a striking similarity to the proposed method, leaving it out of the comparison seems suspicious.\n\n2. The authors admit they did not run the code for VAEGAN. In this case, I don't think, few samples from the original paper are enough to perform a reasonable evaluation of the prior work (e.g. \"The samples from VAEGAN and DCGAN are unsatisfactory due to insufficient sharpness\"). Moreover, the authors explain lower quality of the VAEGAN samples by the fact that Larsen et al. use only hidden space of D (and no other pre-trained model) to obtain the reconstruction loss, but at the same time, in the present paper, only L2-reconstruction in the pixel space is performed (Figure 8). Several existing papers (including Dosovitskiy and Brox [1]) show that L2 leads to blurry images."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481980276663, "tcdate": 1481980276663, "number": 11, "id": "r1TuK3GVx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "rkeSiZRQx", "signatures": ["~Yanran_Li1"], "readers": ["everyone"], "writers": ["~Yanran_Li1"], "content": {"title": "perceptual similarity measure alone is not sufficient", "comment": "Thanks for your comment.\n\n1. As there is no official open-sourced implementation of Larsen et al., and the unofficial implementation (https://github.com/anitan0925/vaegan) seems produce poorer results in terms of sample quality, we can not build up fair comparison on ours and theirs. But it's a good idea to try your suggestion if reliable implementation once available.\n\n2. We sincerely apologize for our carelessness. Since there is no satisfying implementation of VAEGAN, the samples were directly extracted from their paper via a screenshot tool, and there might be some quality loss due to zooming. After your suggestions, we have found out the way to directly extract the image from their compiled PDF file. Now we have updated our paper. Also, we notified our reader to reference to the original papers if they have questions about sample quality. \n\nThe perceptual similarity measure is a good choice for models to generate samples that are more natural looking. However, we think it is still insufficient, which is consistent with the work of Dosivitskiy and Brox [1]. They additionally use a loss from image data space, and suggest a real image prior is important to generate sharper images. Similar ideas and extra model improvements are done in PPGN. For example, they use Activation Maximization to synthesize image-related priors and features into generated images to encourage the quality of images. Furthermore, their samplers are also contributing to the generation quality. \n\nThanks again for your valuable comments. \n\n[1] Generating Images with Perceptual Similarity Metrics based on Deep Networks. https://arxiv.org/pdf/1602.02644.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481928266405, "tcdate": 1481928266405, "number": 2, "id": "rkGUAyfNl", "invitation": "ICLR.cc/2017/conference/-/paper541/official/review", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/conference/paper541/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper541/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "The authors identify two very valid problem of mode-missing in Generative Adversarial Networks, explain their intuitions as to why these problems occur and propose ways to remedy it. The first problem is about the discriminator becoming too good (close to 0 on fake, and 1 on real data) and providing 0 gradients to the generator. The second problem is that GANs are prone to missing modes of the data generating distribution entirely. The authors propose two regularization techniques to address these problems: Geometric Metrics Regularizer and Mode Regularizer\n\nOverall, I felt that this is a good paper, providing a good analysis of the problems and proposing sensible solutions - if lacking solid from-first-principles motivation for the particular choices made. My other critique is the focus on manifolds, almost completely disregarding the probability density on the manifold - see my detailed comment below.\n\nDetailed comments on the Geometric Metrics Regularizer: The motivation for this is to provide a way to measure and penalize distance between two degenerate probability distributions concentrated on non-overlapping manifolds, those of the generator and of the real data. There are different ways one could go about measuring difference between two manifolds or probability distributions concentrated on manifolds, for example:\n\n- projection heuristic: measure the average distance between each point x on manifold A and the corresponding nearest point on manifold B (let\u2019s call it the projection of x onto B).\n- earth mover\u2019s distance: establish a smooth mapping between the two manifolds that maps denser areas on manifold A to nearby denser areas of manifold B, and measure the average distance between corresponding pairs.\n\nThe two heuristics are similar but while the earth mover distance is a divergence measure for distributions, the projection heuristic only measures the divergence of the manifolds, disregarding the distributions in question.\nThe authors propose measuring the average distance between a point on the real data manifold and a point it gets mapped to by the composition of the encoder and the generator. While E\u25cbG will map to the generative manifold, it is unclear to me if they would map to a high-probability region on that manifold, so this probably doesn\u2019t implement anything like Earth Mover\u2019s Distance. On this note, I have just remembered seeing this before: https://github.com/danielvarga/earth-moving-generative-net As the encoder is trained so that E\u25cbG(x) is close to x on average, it feels like a variant of the projection heuristic above. Would the authors agree with this assessment?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512548106, "id": "ICLR.cc/2017/conference/-/paper541/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper541/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper541/AnonReviewer3", "ICLR.cc/2017/conference/paper541/AnonReviewer2", "ICLR.cc/2017/conference/paper541/AnonReviewer1"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512548106}}}, {"tddate": null, "tmdate": 1481903527186, "tcdate": 1481903527186, "number": 1, "id": "Skkn6YbNx", "invitation": "ICLR.cc/2017/conference/-/paper541/official/review", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/conference/paper541/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper541/AnonReviewer3"], "content": {"title": "Clearly identifies and attacks a key problem in GANs", "rating": "7: Good paper, accept", "review": "This paper does a good job of clearly articulating a problem in contemporary training of GANs, coming up with an intuitive solution via regularizers in addition to optimizing only the discriminator score, and conducting clever experiments to show that the regularizers have the intended effect. \n\nThere are recent related and improved GAN variants (ALI, VAEGAN, potentially others), which are included in qualitative comparisons, but not quantitative. It would be interesting to see whether these other types of modified GANs already make some progress in addressing the missing modes problem. If code is available for those methods, the paper could be strengthened a lot by running the mode-missing benchmarks on them (even if it turns out that a \"competing\" method can get a better result in some cases).\n\nThe experiments on digits and faces are good for validating the proposed regularizers. However, if the authors can show better results on CIFAR-10, ImageNet, MS-COCO or some other more diverse and challenging dataset, I would be more convinced of the value of the proposed method. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512548106, "id": "ICLR.cc/2017/conference/-/paper541/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper541/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper541/AnonReviewer3", "ICLR.cc/2017/conference/paper541/AnonReviewer2", "ICLR.cc/2017/conference/paper541/AnonReviewer1"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512548106}}}, {"tddate": null, "tmdate": 1481673527742, "tcdate": 1481673527736, "number": 10, "id": "rkeSiZRQx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "BJMXcOs7l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "More questions", "comment": "1. Did you conduct a comparison between your work and Larsen et al. in terms of missing modes? Is there any difference except for conceptual?\n\n2. On page 8, you mention that samples from VAEGAN are insufficiently sharp. Larsen et al. are using perceptual similarity measure known to produce high-quality images (e.g. http://www.evolvingai.org/ppgn). Also, the samples from the original VAEGAN paper look significantly sharper than the ones you report in your paper. How would you explain this difference?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481505306446, "tcdate": 1481505306436, "number": 9, "id": "BJMXcOs7l", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "H1oV0u_Xx", "signatures": ["~Tong_Che1"], "readers": ["everyone"], "writers": ["~Tong_Che1"], "content": {"title": "Differences with VAEGAN.", "comment": "Thanks for your comment.\n\nThere is indeed a similarity on the objective we are optimizing comparing with theirs. \nHowever, there are several important differences:\n\n1. The main difference is due to the motivations which are different: They are trying to merge the VAE and GAN frameworks, so for example the noise in h-space is regulated by the VAE KL(q(z|x)||p(z)) whereas we don't have this penalty, we are just trying to visit the neighborhood of missed modes so as to rebalance the set of examples on which the generator gets feedback, to account for the fact that the examples from missing modes will come rarely but count a lot in the mismatch to the data distribution.\n\n2. From a theoretical point of view, VAE objectives (including VAEGAN) assumes that the conditional distribution q(z|x) is factorized distribution. This assumption usually does not hold in the context of GANs. So it may happen that the VAE objective and GAN objective they are trying to optimize conflict with each other. \n\n3. The optimization target we are using is conceptually different. In VAEGAN, the generative model is in fact the VAE, the GAN discriminator is only employed to provide more accurate measure than pixel-wise similarity. So the VAE objective is not reweighted in their work. However, in our model, the GAN model is the only generative model. The encoder is only employed to penalize missing modes. The encoder losses we use are often multiplied by a very small coefficient. This makes our technique possible to combine with other GAN training techniques, such as DCGAN, improved GAN, energy-based GAN and Unrolled GAN. \n\nThanks again for your careful reading."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481309746762, "tcdate": 1481309746755, "number": 2, "id": "H1oV0u_Xx", "invitation": "ICLR.cc/2017/conference/-/paper541/pre-review/question", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/conference/paper541/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper541/AnonReviewer1"], "content": {"title": "pre-review question", "question": "What do the authors think is the main difference between this work and Larsen et al? Specifically, both the geometric regularizer (autoencoding cost in Larsen et al) and the mode regularizer (Discriminating based on samples from p(z) and q(z|x) in Larsen et al) have been proposed in the context of VAEs in Larsen et al and I feel the way they are used is essentially the same as this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481309747280, "id": "ICLR.cc/2017/conference/-/paper541/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper541/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper541/AnonReviewer3", "ICLR.cc/2017/conference/paper541/AnonReviewer1"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481309747280}}}, {"tddate": null, "tmdate": 1481095279785, "tcdate": 1481095279780, "number": 8, "id": "HJ_OO4SXg", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "Sy4lRrWXx", "signatures": ["~Yanran_Li2"], "readers": ["everyone"], "writers": ["~Yanran_Li2"], "content": {"title": "Details Added", "comment": "Thanks for your comment. We've added more details in our latest version, especially, the architecture details are added to Appendix. For the second question, the maximization target in Eqn.(1) is chosen to fight the gradient vanishing problem. Hence, we're maximizing log(D(G(z)) instead of minimizing log(1-D(G(z))). Please kindly refer to the original DCGAN paper (Radford et al., 2015) for more detailed discussion on this problem. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481094418260, "tcdate": 1481094418255, "number": 7, "id": "B1cMSErQe", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "H1M6pdCMl", "signatures": ["~Yanran_Li2"], "readers": ["everyone"], "writers": ["~Yanran_Li2"], "content": {"title": "Pseudo Code added in Appendix", "comment": "Thanks for your comment! In our latest version, we have added in the Appendix the pseudo code for MDGAN that you've suggested. We hope it's clearer now for better understanding of our proposed techniques. Please check it in the Appendix. Thanks a lot."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481094205419, "tcdate": 1481094205412, "number": 6, "id": "S1SHE4Sme", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "SJyXUQ8bx", "signatures": ["~Yanran_Li2"], "readers": ["everyone"], "writers": ["~Yanran_Li2"], "content": {"title": "Experiments added in Appendix", "comment": "Thanks for your comment! In our latest version, we have added an synthetic experiment similar as that you've suggested, which also qualitatively demonstrated how the proposed techniques alleviate the missing mode problem. Please check it in the Appendix. Thanks a lot."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1481093935814, "tcdate": 1481093935807, "number": 5, "id": "ryd4mVBXe", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "Hy3dyvVWe", "signatures": ["~Yanran_Li2"], "readers": ["everyone"], "writers": ["~Yanran_Li2"], "content": {"title": "fixed.", "comment": "Thanks for your proof-reading. We're sorry for this careless mistake and have fixed it in the latest version. Thanks again."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1480838635854, "tcdate": 1480838635849, "number": 4, "id": "Sy4lRrWXx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "More experiments details", "comment": "1. Could there be more details on each experiment (could be in Appendix), so that the results can be reproduced? In particular, it is not mentioned what exactly the \"encoder\" is.\n\n2. I think the generator should minimize log(1-D(G(z))) instead of log(D(G(z))). Or, if Eqn.(1) is meant to be the maximization target, then you may need to redefine the distance measure d()."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1480654265919, "tcdate": 1480654265914, "number": 1, "id": "H1M6pdCMl", "invitation": "ICLR.cc/2017/conference/-/paper541/pre-review/question", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["ICLR.cc/2017/conference/paper541/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper541/AnonReviewer3"], "content": {"title": "Training pseudocode", "question": "How is the model trained? Section 3.3 refers to a \"manifold step\" and a \"diffusion step\" but only describes them informally. Do the steps consist of one step of gradient descent, or perhaps something else?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481309747280, "id": "ICLR.cc/2017/conference/-/paper541/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper541/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper541/AnonReviewer3", "ICLR.cc/2017/conference/paper541/AnonReviewer1"], "reply": {"forum": "HJKkY35le", "replyto": "HJKkY35le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481309747280}}}, {"tddate": null, "tmdate": 1479059608442, "tcdate": 1479058966952, "number": 2, "id": "SJyXUQ8bx", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Synthetic Experiments on missing mode", "comment": "Nice paper. It would be interesting to conduct some synthetic experiments to qualitatively demonstrate how the proposed techniques alleviate the missing mode problem. For example, Fig 3, 4 in \"Generative Adversarial Parallelization\" look nice to me."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}, {"tddate": null, "tmdate": 1478944359994, "tcdate": 1478942579554, "number": 1, "id": "Hy3dyvVWe", "invitation": "ICLR.cc/2017/conference/-/paper541/public/comment", "forum": "HJKkY35le", "replyto": "HJKkY35le", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is this a typo?", "comment": "Very interesting work.\n\nIn equation (3), p(y) refers to label distribution of the training data. However, in the original paper (Salimans et al. 2016), p(y) means label distribution of the generated images, which contradicts with p*(y) in equation (4). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a\nvariety of generative tasks, they are regarded as highly unstable and prone to miss\nmodes. We argue that these bad behaviors of GANs are due to the very particular\nfunctional shape of the trained discriminators in high dimensional spaces, which\ncan easily make training stuck or push probability mass in the wrong direction,\ntowards that of higher concentration than that of the data generating distribution.\nWe introduce several ways of regularizing the objective, which can dramatically\nstabilize the training of GAN models. We also show that our regularizers can help\nthe fair distribution of probability mass across the modes of the data generating\ndistribution during the early phases of training, thus providing a unified solution\nto the missing modes problem.", "pdf": "/pdf/dbc3a1a8eb32546aa4ea026dc4503018939cfc34.pdf", "paperhash": "che|mode_regularized_generative_adversarial_networks", "conflicts": ["umontreal.ca"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Tong Che", "Yanran Li", "Athul Jacob", "Yoshua Bengio", "Wenjie Li"], "authorids": ["tong.che@umontreal.ca", "csyli@comp.polyu.edu.hk", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "cswjli@comp.polyu.edu.hk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287530092, "id": "ICLR.cc/2017/conference/-/paper541/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJKkY35le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper541/reviewers", "ICLR.cc/2017/conference/paper541/areachairs"], "cdate": 1485287530092}}}], "count": 28}