{"notes": [{"id": "biH_IISPxYA", "original": "oJZRwakpGxO", "number": 2925, "cdate": 1601308324472, "ddate": null, "tcdate": 1601308324472, "tmdate": 1614985644106, "tddate": null, "forum": "biH_IISPxYA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise", "authorids": ["~Yan_Yan10", "~Yuhong_Guo1"], "authors": ["Yan Yan", "Yuhong Guo"], "keywords": [], "abstract": "Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the PL problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. We conduct extensive experiments on both synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the- art performance for partial label learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|multilevel_generative_models_for_partial_label_learning_with_nonrandom_label_noise", "one-sentence_summary": "This is the first partial label learning method that handles non-random label noise with a consistent multi-level generative model.", "pdf": "/pdf/12cc456e89e02e1812f0ddd75004cb3c37ca7dac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UBW-RL5jol", "_bibtex": "@misc{\nyan2021multilevel,\ntitle={Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise},\nauthor={Yan Yan and Yuhong Guo},\nyear={2021},\nurl={https://openreview.net/forum?id=biH_IISPxYA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Oru8ui-L1WT", "original": null, "number": 1, "cdate": 1610040517933, "ddate": null, "tcdate": 1610040517933, "tmdate": 1610474126311, "tddate": null, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "invitation": "ICLR.cc/2021/Conference/Paper2925/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Dear Authors,\n\nThank you very much for your detailed feedback to the reviewers in the rebuttal phase. This certainly clarified some of the concerns raised by the reviewers and contributed highly to deepen their understanding of your work.\n\nWe positively evaluated the novelty and the superior empirical performance of the proposed method. However, we still have concern about the justification since the proposed model is so complex that it is not clear what was the key for the good performance. \n\nFor this reason, I suggest rejection of this submission, in comparison with many other strong submissions. I hope that the reviewers' feedback is useful for improving your work for future publication.\n\nBest,\nAC"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise", "authorids": ["~Yan_Yan10", "~Yuhong_Guo1"], "authors": ["Yan Yan", "Yuhong Guo"], "keywords": [], "abstract": "Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the PL problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. We conduct extensive experiments on both synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the- art performance for partial label learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|multilevel_generative_models_for_partial_label_learning_with_nonrandom_label_noise", "one-sentence_summary": "This is the first partial label learning method that handles non-random label noise with a consistent multi-level generative model.", "pdf": "/pdf/12cc456e89e02e1812f0ddd75004cb3c37ca7dac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UBW-RL5jol", "_bibtex": "@misc{\nyan2021multilevel,\ntitle={Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise},\nauthor={Yan Yan and Yuhong Guo},\nyear={2021},\nurl={https://openreview.net/forum?id=biH_IISPxYA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040517920, "tmdate": 1610474126295, "id": "ICLR.cc/2021/Conference/Paper2925/-/Decision"}}}, {"id": "qCVdGfq2TIJ", "original": null, "number": 1, "cdate": 1603526257545, "ddate": null, "tcdate": 1603526257545, "tmdate": 1605024103661, "tddate": null, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "invitation": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review", "content": {"title": "Novel idea with good performance", "review": "Overall, I like the idea of this paper and it is well-written. In this paper, the authors propose multi-level generative models for partial label learning with non-random label noise. It consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features. With the proposed minmax adversarial loss, the proposed framework achieved state-of-the-art performance for partial label learning.\\\n\\\nI think this paper has the following advantages:\n1. It is novel to exploit multi-level generative models to model non-random noise labels for the partial label learning problem.\n2. The experiments in this paper are complete and thorough. The authors have tested the model in many datasets and designed the ablation study to verify the effect of each loss.\n3. The proposed model achieved the state-of-art results.\n\nDespite the above advantages, I still have the following questions:\n1. Is the true label vector $\\mathbf{z}$ given for each training sample as the ground truth or it is sampled from the multinomial distribution?\n2. In Algorithm 1, why the parameter $\\Theta$ needs to be limited in $[-c, c]$?\n3. In equation 3, it is not clear what the \u201c$n$\u201d in $\\mathbf{y}_n$ stands for? In the previous context, $n$ is the size of the training set.\n4. In Fig. 1, the input for $\\mathcal{L}_c$ is $\\mathbf{z}$ and $F(\\mathbf{x})$, while in equation 4 it is $F(\\mathbf{x})$ and $\\mathbf{y}\\ominus G_n(F(\\mathbf{x}),\\epsilon))$\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2925/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2925/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise", "authorids": ["~Yan_Yan10", "~Yuhong_Guo1"], "authors": ["Yan Yan", "Yuhong Guo"], "keywords": [], "abstract": "Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the PL problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. We conduct extensive experiments on both synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the- art performance for partial label learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|multilevel_generative_models_for_partial_label_learning_with_nonrandom_label_noise", "one-sentence_summary": "This is the first partial label learning method that handles non-random label noise with a consistent multi-level generative model.", "pdf": "/pdf/12cc456e89e02e1812f0ddd75004cb3c37ca7dac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UBW-RL5jol", "_bibtex": "@misc{\nyan2021multilevel,\ntitle={Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise},\nauthor={Yan Yan and Yuhong Guo},\nyear={2021},\nurl={https://openreview.net/forum?id=biH_IISPxYA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2925/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085935, "tmdate": 1606915805147, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2925/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review"}}}, {"id": "uXskmJ-G1LL", "original": null, "number": 2, "cdate": 1603967118840, "ddate": null, "tcdate": 1603967118840, "tmdate": 1605024103602, "tddate": null, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "invitation": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review", "content": {"title": "containing some interesting ideas while not so solid", "review": "This paper presents a multi-level generative model for partial label learning. The basic idea is to use a conditional noise label generation network to model the label noise. The noise label generator and the data feature generator are learned in an adversarial manner. Experiments on synthesized and real-world data sets show competitive performance. \n\nOverall the idea modeling conditional noise label is interesting. The paper is easy to follow. My concerns are mainly as following. \n\n1.\tIt is not unclear for me for the motivation that previous methods are based on the separate label distribution estimation steps or the error-prone label confidence estimation process. Such kinds of approaches do not mean that they are not good methods. Actually using the separate label distribution estimation steps or the error-prone label confidence estimation process may be benefit to be more accurate and more efficient. Therefore, such motivation is not so convincing, and leads to an important new contribution. \n2.\tThe second motivation is based on non-random noise. This is an interesting observation. The proposed method involves a conditional probability to model the correlation. However, this might be sensitive and restrict. In practical, such kind of information may be not correct for infrequent patterns. For frequent patterns, this could be trivial to hold. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2925/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2925/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise", "authorids": ["~Yan_Yan10", "~Yuhong_Guo1"], "authors": ["Yan Yan", "Yuhong Guo"], "keywords": [], "abstract": "Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the PL problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. We conduct extensive experiments on both synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the- art performance for partial label learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|multilevel_generative_models_for_partial_label_learning_with_nonrandom_label_noise", "one-sentence_summary": "This is the first partial label learning method that handles non-random label noise with a consistent multi-level generative model.", "pdf": "/pdf/12cc456e89e02e1812f0ddd75004cb3c37ca7dac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UBW-RL5jol", "_bibtex": "@misc{\nyan2021multilevel,\ntitle={Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise},\nauthor={Yan Yan and Yuhong Guo},\nyear={2021},\nurl={https://openreview.net/forum?id=biH_IISPxYA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2925/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085935, "tmdate": 1606915805147, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2925/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review"}}}, {"id": "2JXf5FnvDWg", "original": null, "number": 3, "cdate": 1604029263332, "ddate": null, "tcdate": 1604029263332, "tmdate": 1605024103542, "tddate": null, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "invitation": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review", "content": {"title": "Strong experimental results but weak motivation in model design ", "review": "This submission proposes a new method of learning from data with partially observed labels. In this problem, every instance has a label candidate set, which contains the true label. This submission introduces adversarial learning to improve the disambiguation of inexact labels. Particularly, there are two adversarial learning component. In the first component, a generator tries to match the distribution of label candidate sets given the \"true\" label of an instance. In the second component, a generator tries to learn the distribution of instances give their \"true\" labels. Since the the \"true\" label is not accessible, the \"true\" label is actually from a predictive model. \n\nThe submission has done extensive experiments and shows that the proposed method outperforms several baseline methods. \n\nIn a summary, the experiment results of this submission is strong, but I feel the motivation of the model design is not clear. \n\nMy biggest question is the motivation behind adversarial training. To make it simple, adversarial training aims to match a generative distribution to the data distribution. \n\n1. Loss (4) is somewhat reasonable to me. Can I interpret it as: F(x) predict z, and z generate a label vector y'; if y' does not match the partial label y, then there is a loss? Then the model needs to learn both the predictive distribution p(z | x) through F, and the label distribution p(y | z).  \n\nIf my interpretation is correct, have you considered to use alternative distributions for p(y | z) instead of a generator? There are many choices such as Restricted Boltzmann machine. Since you treat y as continuous variables, there are even more choices such as a decoder in a conditional variational autoencoder.  I don't mean to say that GAN distribution is not a good choice, but if you could make this task clear so people may consider alternative choices. \n\n2.The generative distribution p(x | z) is more confusing. The structure in (5) seems like an encoder-decoder: decoding x, y gives z, and z should recover x. With this loss, the model has some flavor of generative modeling because it models the feature vector x. As a principle in machine learning, generative modeling is less powerful than discriminative models in classification tasks, so I don't see why we want to model input features. \n\n3. In the ablation study, have you tried to use only the classification model and losses from adversarial labels? \n\n4. Since the model has better performance than baseline methods, there might be several explanations. \n\n1) differences in base models. What are base predictive models of F in your algorithm and equivalent in competing methods? Is the difference significant? In the results from synthetic datasets, p(y | z) does not contain much information since noise labels are randomly sampled. The proposed method still outperform baseline methods by a good margin. Is this an evidence that the model is still better without much contribution from the label generator? In another word, if you replace G_n(z, \\epsilon) by your groundtruth distribution, the model should perform even better? \n\n2) The combination of the generative model Gx(Z, epsilon), which works like a generative model. When it works with F(x) and other loss terms, the entire model is like a combination of two neural networks. This might provide extra classification power? \n\n5. The loss in (4) is unnatural to me: the loss is computed from probabilities, and square loss may not be the best choice. \n\nI am asking a lot of questions, trying to explain why the experiment results. In another work, I wish the submission could answer some of these questions. \n\nThe proposed model also have many parameters and neural network components. I don't how easy it is for others to tune the model. \n\nThe writing of the submission also has many issues. A lot of symbols are not well defined. A few examples. \n1. n has two meaning: number of instances and \"noisy\" labels. \n2. what is p_z? \n3. The minimization problem in 2 is invalid since the true label z is unknown. \n\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2925/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2925/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise", "authorids": ["~Yan_Yan10", "~Yuhong_Guo1"], "authors": ["Yan Yan", "Yuhong Guo"], "keywords": [], "abstract": "Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the PL problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. We conduct extensive experiments on both synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the- art performance for partial label learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|multilevel_generative_models_for_partial_label_learning_with_nonrandom_label_noise", "one-sentence_summary": "This is the first partial label learning method that handles non-random label noise with a consistent multi-level generative model.", "pdf": "/pdf/12cc456e89e02e1812f0ddd75004cb3c37ca7dac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UBW-RL5jol", "_bibtex": "@misc{\nyan2021multilevel,\ntitle={Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise},\nauthor={Yan Yan and Yuhong Guo},\nyear={2021},\nurl={https://openreview.net/forum?id=biH_IISPxYA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "biH_IISPxYA", "replyto": "biH_IISPxYA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2925/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085935, "tmdate": 1606915805147, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2925/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2925/-/Official_Review"}}}], "count": 5}