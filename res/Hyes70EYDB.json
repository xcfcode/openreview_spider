{"notes": [{"id": "Hyes70EYDB", "original": "Skg8k0SODS", "number": 1051, "cdate": 1569439267249, "ddate": null, "tcdate": 1569439267249, "tmdate": 1577168226744, "tddate": null, "forum": "Hyes70EYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "x4Kyjmgyef", "original": null, "number": 1, "cdate": 1576798713263, "ddate": null, "tcdate": 1576798713263, "tmdate": 1576800923180, "tddate": null, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Decision", "content": {"decision": "Reject", "comment": "This work focuses on how one can design models with robustness of interpretations. While this is an interesting direction, the paper would benefit from a more careful treatment of its technical claims.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721305, "tmdate": 1576800272299, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Decision"}}}, {"id": "BylQGnF_jr", "original": null, "number": 12, "cdate": 1573587978776, "ddate": null, "tcdate": 1573587978776, "tmdate": 1573761982651, "tddate": null, "forum": "Hyes70EYDB", "replyto": "BJlD8OODoH", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Clarification on motivation", "comment": "Thanks for your question. \n\nThe attack (fooling an interpretability method, not just the classifier) could be a threat model for a neural network whose usage relies jointly on interpretation maps and classification results. \n\nOne can use an interpretability method as a beneficial post-hoc supplement to visualize/interpret the prediction result, e.g., localizing the most discriminative image region of a medical image (with respect to the predicted label) might help doctors in disease diagnosis and medical recommendation (Ghorbani et al.; Subramanya et al.). In this case, if the interpreter can be arbitrarily controlled and fooled by the adversary, then it creates inconsistency between an interpreter and a classifier by making the interpretation not reflect the network's classification.  In general, the ISA attack could provide a way to evaluate the consistency between a classifier and an interpreter (different classification vs. similar interpretation). If the inconsistency holds, it can undermine the trustworthiness and transparency of the DL system.\n\nIn addition to specific examples, exploring the relationship between robustness in interpretation and robustness in classification is an interesting scientific question by itself. Previous work has not provided a complete answer on the question of the interpretability-classification relationship in the context of adversarial robustness. Our work, we believe, made an effort and showed that with a proper discrepancy metric, interpretability can not easily be fooled arbitrarily by the adversary when facing input perturbation.\n\nAmirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681\u20133688, 2019.\nAkshayvarun Subramanya, et al., Towards hiding adversarial examples from network interpretation. arXiv preprint arXiv:1812.02843, 2018. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "BJlD8OODoH", "original": null, "number": 11, "cdate": 1573517390579, "ddate": null, "tcdate": 1573517390579, "tmdate": 1573517390579, "tddate": null, "forum": "Hyes70EYDB", "replyto": "BkxsmlNwsH", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Still confused on motivation", "comment": "Hello,\n\nThank you for your response. However, I am still confused about the motivation:\n\n> Why is it important for adversarial attacks to yield similar interpretations?\n\nIn your response you cite other papers that consider this threat model, but the fact that other people have considered it does not make it a well-motivated model. I would be most convinced by a self-contained explanation of a scenario where an attacker would need to fool an interpretability method (and not just the classifier) in order for it to be successful."}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "r1lgak4PsB", "original": null, "number": 5, "cdate": 1573498808345, "ddate": null, "tcdate": 1573498808345, "tmdate": 1573503915411, "tddate": null, "forum": "Hyes70EYDB", "replyto": "r1lqKyVwjH", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (Part 2)", "comment": "#Question: 3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170  are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitations of the proposed method.#\n\nResponse: \nWe will make our statement as accurate as possible. We would like to convey that our proposal achieved  robust classification by promoting robustness of interpretability alone (without using the adversarial loss). Compared to Adv and TRADES, our approach Int (2-class without using adversarial loss) achieves reasonable but worse adversarial test accuracy (ATA) as $\\epsilon < 0.3$ on MNIST and $\\epsilon \\leq 8/255$ on CIFAR. As $\\epsilon$ used in PGD attack achieves the value (0.3 on MNIST and 8/255 on CIFAR) used for robust training, Int yields ATA $0.790$ against ATA $0.890$ from Adv on MNIST, but outperforms the conventional adversarial training (Adv) on CIFAR ($0.270$ vs $0.170$). Interestingly, the advantage of Int becomes more evident as the adversary becomes stronger, i.e., $\\epsilon > 0.3$ on MNIST and $\\epsilon > 8/255$ on CIFAR. For example, the newly conducted experiments in Table 3 show that Int outperforms Adv in terms of ATA: 21% vs. 8.5% on CIFAR at eps=10/255, and 14% vs 0% on MNIST at eps=0.4.  We have further provided the details of our experiment results in the revision to make our statement as clear as possible.\n\nOn the other hand, when comparing to Int-1-class and IG-Norm that use 1-class based interpretability discrepancy measure, our approach generally outperforms them in ATA by large margins (Int: 0.790, Int-1-class: 0.125, IG-Norm: 0.005 on MNIST at $\\epsilon = 0.3$, Int: 0.270, Int-1-class: 0.065 on CIFAR at $\\epsilon = 8/255$; see Table 3). In this sense, the proposed interpretability-aware robust training methods yields promising results due the appropriate interpretability regularization. \n\nFollowing the reviewer\u2019s suggestion, when the perturbation radius $\\epsilon$ becomes much higher, we see more evident benefits of our proposed Int (2-class) approach. On both MNIST and CIFAR models, our approach outperforms adversarial training on $\\epsilon$ beyond the one used for training (Int: 0.175, Adv: 0.000 on MNIST at $\\epsilon = 0.4$, Int: 0.210, Adv: 0.085 on CIFAR at $\\epsilon = 10/255$; see Table 3).\n\n#Question: 4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition# \n\nResponse: Thanks for pointing out this typo. We have tried our best to make clear and accurate presentation. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "HkxDzAXPsB", "original": null, "number": 3, "cdate": 1573498382925, "ddate": null, "tcdate": 1573498382925, "tmdate": 1573503511118, "tddate": null, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "General response to all reviewers and summary of revisions", "comment": "We thank all reviewers for their insightful and valuable comments. Our paper has been greatly improved based on these comments. The major modifications are summarized below.\n\nWe have updated Sec. 1$\\&$2 for a clearer motivation on our research to explore the relationship between network interpretability and adversarial robustness in classification.  \nWe have updated Sec. 3 for elaborating on why we care about interpretability sneaking attack (ISA), how to evaluate the hidden effect, and why interpretability discrepancy matters when generating and evaluating ISA.\nWe have updated Sec. 4 to reveal a better connection between robustness in interpretation and robustness in classification. \nWe have updated Sec. 5 and Appendix to clarify our experiment setup, and conducted new experiments on the regularization parameter $\\gamma$ as well as the large perturbation radius $\\epsilon$; see updated Figure 3, new Figure A1 and updated Tables 2,3 for larger perturbation radius.\n\nWe marked our major modifications in BLUE.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "r1lqKyVwjH", "original": null, "number": 4, "cdate": 1573498754479, "ddate": null, "tcdate": 1573498754479, "tmdate": 1573499935552, "tddate": null, "forum": "Hyes70EYDB", "replyto": "rJlQeu8GiB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (Part 1)", "comment": "#Question: 1. This paper states several times that \"adversarial examples can be hidden from neural network interpretability\". It is not clear on the definition of \"hidden\" in terms of  \"interpretability\". Therefore, how this \"hidden\" is related and why this \"hidden\" is important are unclear too.#\n\nResponse: The definition of \u201chidden\u201d was borrowed from (Zhang et al., 2018; Subramanya et al., 2018), which refers to the interpretation map of an adversarial example generated from  Interpretability Sneaking Attack (ISA) being visually similar to the original interpretation map of the benign example. We quantify this \u201chidden\u201d effect with the Kendall\u2019s Tau order rank correlation between interpretation maps before and after performing ISA. As a visual example, \nFigure 2c shows that the conventional ISA generated under 1-class interpretability discrepancy (namely, $\\ell_1$ 1-class ISA) minimizes the interpretability discrepancy  w.r.t. the true label $t$ only, supported by a high correlation value 0.7107.\n\nIf ISA is a real threat, then it could create confusion between the model interpreter and the classifier, and it could further confuse AI systems which use network interpretations in down-stream actions, e.g., medical recommendation (G Quellec, et al., 2017) and transfer learning (Shafahi et al., 2019). Thus, we think that studying the \u201chidden\u201d effect of ISA is an important aspect to explore the relationship between network interpretability and robust classification. \n\nHowever,  the main theme of this paper is not to make a claim that adversarial examples are able to be hidden from network interpretability checkers. Instead, we would like to have a deeper understanding on the plausibility of ISA. We found that ISA can fail when the 2-class interpretability discrepancy was examined; e.g., Figure 2c column 3 versus column 1 (compared to column 4 versus column 1). Our main point for ISA is that hiding adversarial attack from network interpretation is actually challenging. Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations.\n\n\n\nXinyang Zhang, et al., Interpretable deep learning under fire. \nAkshayvarun Subramanya, et al., Towards hiding adversarial examples from network interpretation. arXiv preprint arXiv:1812.02843, 2018. \nG Quellec, et al. Deep image mining for diabetic retinopathy screening. Medical image analysis, 39:178, 2017.\nShafahi, Ali, et al. \"Adversarially robust transfer learning.\" arXiv preprint arXiv:1905.08232 (2019).\n\n#Question: 2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and effects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt.#\n\nResponse: \n\nWe apologize for the missing details such that you felt our proposal 'suspicious\u2019. \nIn Sec. 5 and Appendix B-D, we have tried our best to present a clear experiment setup. The parameter $\\lambda$ used during finding ISA controls the tradeoff between an attack changing the network classification and minimizing the interpretability discrepancy. As explained in section 3.1, $\\lambda$ is chosen using the bisection method to find a successful attack with the minimum interpretability discrepancy.\n\nAnd we have conducted additional experiments on the training regularization parameter $\\gamma$. As we can see in Appendix C, Figure A1, $\\gamma$ controls the tradeoff between clean accuracy and adversarial test accuracy. The value of $\\gamma$ chosen for our experiments ($\\gamma=0.01$) is the value yielding the highest robust accuracy on the model tested. Our results indicate that $\\gamma$ can be chosen to smoothly interpolate between normal training and maximally robust interpretability-aware training.\n \nWe have also released our code for reviewer\u2019s reproducibility and applicability check. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "S1gJybVvsB", "original": null, "number": 8, "cdate": 1573499095261, "ddate": null, "tcdate": 1573499095261, "tmdate": 1573499756148, "tddate": null, "forum": "Hyes70EYDB", "replyto": "S1lhseVDjB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 3)", "comment": "#Questions: Comments regarding the surprisingness of interpretation robustness implying classification robustness.#\n\nResponse: We thank the reviewer very much on this constructive comment. \n\nWe are sorry to learn that the current title is misleading, and we thank the reviewer\u2019s summary on \"robustness of interpretability implies robustness of classification\". However, this does not fully align the research theme of the entire paper.  Our detailed response is provided as below.\n\nIn a general setting, robustness of interpretability does NOT alway IMPLY robustness of classification. If the proper interpretability discrepancy metric is not used, we may get negative results. For example, the Int-1-class method and IG-Norm that uses IG-based robust attribution regularization (Chen et al., 2019), which imposes robust interpretation only for the true class of training data, does not result in strong robustness in classification (Int-1-class: 12.5% vs Int(-2-class): 79% on MNIST at $\\epsilon=0.3$, Int-1-class: 6.5% vs Int(-2-class): 27% on CIFAR at $\\epsilon=8/255$; see Table 3). This also implies that we need careful analysis and studies to achieve this seemingly non-surprising result. \n\nWe also do not think that our study is trivial. To support the result that robustness of interpretability implies robustness of classification, we find that it requires 1) interpretation methods to satisfy the completeness axiom, and 2) the proper choice of $\\ell_p$ norm and number of classes in measuring interpretability discrepancy. The point 1) enables us to consider the more computationally-light interpretation method e.g., CAM instead of IG used in (Chen et al., 2019), given the fact  that IG satisfies completeness axiom but is computationally intensive. The point 2) provides a guideline on how to achieve robustness in classification through the lens of interpretability. We have showed that even in the absence of adversarial classification loss, analyzing a proper interpretability discrepancy alone helps adversarial robustness in classification.\n\nBased on the aforementioned analysis and the concern raised by the reviewer, we would like to highlight the role of the proper interpretability discrepancy in the title, e.g., the new one \u201cProper Network Interpretability Helps Adversarial Robustness in Classification\u201d.\n\n\n#Question: Comments on paper clarity.#\n\nResponse: We have improved the clarity of our motivations in the revised version. Hopefully, our detailed response and the revised manuscript could address most of the reviewers\u2019 concerns about writing. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "S1lhseVDjB", "original": null, "number": 7, "cdate": 1573499044035, "ddate": null, "tcdate": 1573499044035, "tmdate": 1573499719356, "tddate": null, "forum": "Hyes70EYDB", "replyto": "BkxsmlNwsH", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 2)", "comment": "# Question: Is a human necessary for interpretability-driven defenses to adversarial examples? #\n\n\nResponse: In our work, including others on adversarial examples and network interpretation (Chen et al., 2019), a human is not required to inspect the interpretation to make a defensive decision. First, the information on the human assigned label is usually not accessed to detect the adversarial example, since the adversarial example is a testing-phase threat model (Metzen et al., 2017), and the human is not assumed to be in the loop when DL/ML models make inferences. Second, we would like to highlight that our work is not based on detection of adversarial attacks. Even for interpretability-based detectors of adversarial attacks (Tao et al., 2018), a human is not required.  Indeed, if a human inspection was available, then a human could assign a label herself even in the absence of ML/DL classification. Third, in our proposed interpretability-aware robust training method, a human is not needed to inspect the interpretation. Instead of relying on interpretations to be viewed from a human perspective, our defense uses a 2-class interpretability metric itself as a robustness measure.  By regularizing with the proposed interpretability discrepancy penalty in the standard training objective, the learnt model yields robust classification against testing-phase adversarial examples; see its comparable performance to state-of-the-art adversarial training method (using adversarial loss) in Figure 3. \n\nJan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff. On Detecting Adversarial Perturbations. ICLR 2017.\nGuanhong Tao, Shiqing Ma, Yingqi Liu, Xiangyu Zhang. Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples. NeurIPS 2018.\nJiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. Robust attribution regularization, NeurIPS, 2019.\n\n\n\n\n\n#Question: How is robustness in interpretation related to the observation that regularizing based on interpretability yields robustness? These seem like two fairly separate results.#\n\nResponse: The observation that regularizing based on proper interpretability metrics yields robustness is used to enforce robustness in classification through the lens of interpretation map discrepancy. The rationale behind that is that constraining the $\\ell_1$-norm based two-class interpretability discrepancy helps to prevent misclassification. This fact is both supported theoretically (see Proposition 1) and in our experiments: Our method achieves comparable robust accuracy to adversarial training to standard PGD attacks (27% vs. 17% on CIFAR at $\\epsilon=8/255$, 79% vs 89% on MNIST at $\\epsilon=0.3$, and 21% vs. 8.5% on CIFAR at $\\epsilon=10/255$, 14% vs 0% on MNIST at $\\epsilon=0.4$; see Table 3) and unforeseen attacks (44% vs 46% against JPEG $\\ell_\\infty$, 26% vs 23% against JPEG $\\ell_1$ on CIFAR; see Table 5). Here we use $\\epsilon = 0.3$ on MNIST and $8/255$ on CIFAR for robust training. The above results show that our approach is able to provide a better robustness even as facing a strong adversary. \n\n\nAs another benefit, regularizing based on interpretability yields robustness in interpretation since the interpretability discrepancy is minimized against input perturbations during training. This is supported by the defensive results against attack against interpretability (AAI). Note that AAI is not a threat model belonging to adversarial attacks, since it only manipulates network interpretation maps but keeps classifier\u2019s decision intact. Thus compared to standard defenses such as adversarial training, our defense achieves higher robustness to AAI than adversarial training in terms of rank-correlation of the original and attacked interpretations (0.913 vs 0.857 on MNIST at $\\epsilon=0.3$, 0.682 vs 0.629 on CIFAR at $\\epsilon=8/255$; see Table 2). In our revision, we have added additional experiments on larger eps values beyond the trained one to additionally illustrate the higher robustness on more powerful AAI (0.349 vs 0.136 on MNIST at $\\epsilon=0.4$, 0.661 vs 0.552 on CIFAR at $\\epsilon=10/255$; see Table 2).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "H1eezZEDjr", "original": null, "number": 9, "cdate": 1573499143861, "ddate": null, "tcdate": 1573499143861, "tmdate": 1573499143861, "tddate": null, "forum": "Hyes70EYDB", "replyto": "rkeeRAvCtB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank reviewer #2 very much for their positive comments and for accurately summarizing our key contributions. We are very glad to learn that the reviewer found the topic interesting and our study meaningful. Thanks for carefully checking our presentation. We have fixed the typo in Figure 2: Yes, the first $\\mathbf x^\\prime$ should be $\\mathbf x$. We have also carefully  revised our paper for a better version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "BkxsmlNwsH", "original": null, "number": 6, "cdate": 1573498915433, "ddate": null, "tcdate": 1573498915433, "tmdate": 1573498951009, "tddate": null, "forum": "Hyes70EYDB", "replyto": "S1eSBR_CtH", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 1)", "comment": "We thank reviewer #3 for the valuable comments. \n\n#General response on the motivation of our work#\n\nIn what follows, we would like to further clarify the motivation of our work. The manuscript is updated to make our points clearer.\n\nThe primary motivation of our work is to investigate the relationship between network interpretability and adversarial robustness. Based on previous literature, there are seemingly two possible hypotheses for how robust interpretability affects adversarial robustness of classification: (a) From an attack perspective (Zhang et al., 2018; Subramanya et al., 2018), robust interpretability does not significantly help robust classification since empirically adversarial examples with a small interpretation discrepancy may have a large classification error. (b) From a defense perspective (Chen et al., 2019), robust interpretability helps robust classification since robust interpretations make aspects of the network less sensitive to small perturbations.\n\nWe aim to make a unified answer to the debate between (a) and (b). We found that the choice of interpretability discrepancy matters when drawing conclusions. We showed that with an appropriate choice of interpretability discrepancy (namely, the proposed $\\ell_1$ norm based 2-class measure), the claim (a) may NOT be correct since hiding adversarial examples from network interpretation could be difficult (see Prop. 1 and the corresponding detailed examples and analysis in Sec. 3.1). Moreover, our interpretability-aware robust training results provide a positive answer to (b): robust interpretability does help robust classification by solely penalizing the proposed interpretability discrepancy (see Sec. 4) during training but the choice of interpretability discrepancy again matters. \n\nReferences: \n\nXinyang Zhang, et al., Interpretable deep learning under fire. \nAkshayvarun Subramanya, et al., Towards hiding adversarial examples from network interpretation. arXiv preprint arXiv:1812.02843, 2018. \nJ. Chen, et al.,Robust attribution regularization, NeurIPS, 2019.\n\n\n\n# Question: \u201cWhy is it important for adversarial attacks to yield similar interpretations?\u201d #\n\n\nResponse: Adversarial attacks that yield similar interpretations is a recent threat model (we call interpretability sneaking attack (ISA)) proposed by (Zhang et al., 2018; Subramanya et al., 2018), which showed that there exist adversarially chosen perturbations that fool an image classifier but minimize discrepancy of corresponding interpretation maps. If these perturbations are possible, then it could create confusion between the model interpreter and the classifier, and it could further confuse AI systems which use network interpretations in down-stream actions, e.g., transfer learning (Shafahi et al. 2019) and medical recommendation (G Quellec, et al.). Thus, we believe that ISA is a practical attacking scenario to study the relationship between network interpretation and classification.\n\nHowever, the goal of this paper is not to make a claim that ISA is a strong attack. Instead, we would like to have a deeper understanding on the plausibility of ISA. We found that ISA can fail when the 2-class interpretability discrepancy was examined; see Figure 2 and more results in experiments. This showed that hiding adversarial attack from network interpretation is challenging. Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations.\n\nG Quellec, et al. Deep image mining for diabetic retinopathy screening. Medical image analysis, 39:178, 2017.\nShafahi, Ali, et al. \"Adversarially robust transfer learning.\" arXiv preprint arXiv:1905.08232 (2019).\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyes70EYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1051/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1051/Authors|ICLR.cc/2020/Conference/Paper1051/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162014, "tmdate": 1576860556560, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Authors", "ICLR.cc/2020/Conference/Paper1051/Reviewers", "ICLR.cc/2020/Conference/Paper1051/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Comment"}}}, {"id": "rJlQeu8GiB", "original": null, "number": 3, "cdate": 1573181418556, "ddate": null, "tcdate": 1573181418556, "tmdate": 1573181418556, "tddate": null, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "In summary, this paper studies if interpretation robustness (i.e., similar examples should have similar interpretation) can help enhance the robustness of the model, especially in terms of adversarial attacks. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. Moreover, some promising results can be observed in part of the empirical study. However, this paper can be improved a lot as follows.\n\n1. This paper states several times that \"adversarial examples can be hidden from neural network interpretability\". It is not clear on the definition of \"hidden\" in terms of  \"interpretability\". Therefore, how this \"hidden\" is related and why this \"hidden\" is important are unclear too.\n\n2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and affects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt.\n\n3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170  are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitation of the proposed method.\n\n4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition 1.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500964416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Reviewers"], "noninvitees": [], "tcdate": 1570237743116, "tmdate": 1575500964430, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review"}}}, {"id": "rkeeRAvCtB", "original": null, "number": 1, "cdate": 1571876551985, "ddate": null, "tcdate": 1571876551985, "tmdate": 1572972518740, "tddate": null, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Interesting work and good contribution\n#Summary:\nThe paper demonstrated that by having an l1-norm based 2-class interpretability discrepancy measure, it can be shown both empirically and theoretically that it is actually difficult to hide adversarial examples. Furthermore, the authors propose an interpretability-aware robust training method and show it can be used to successfully defend adversarially attacks and can result in comparable performance compared to adversarial training.\n\n#Strength\nThe paper is well written and structured, with a clear demonstration of technical details. Compared with other works that tried to use model interpretation to help improve the model\u2019s robustness, the authors not only consider the saliency map computed for the actual target label but also the label that corresponds to the adversarial example. The proposed interpretability discrepancy measure is novel and has been proven effective to defend interpretability sneaking attacks that aiming to fool both classifiers and detectors and against interpretability-only attacks. Furthermore, extensive experiments have been done to prove the effectiveness of interpretability-aware training, which strengthens the claims of the entire paper.\n\n#Presentation\nGood coverage of the literature in both adversarial robustness and model interpretation.\nSome minor typos need to be fixed. For example, in the second last line of the caption of Figure. 2, one L(x\u2019,i) should be L(x,i) if I understand correctly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500964416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Reviewers"], "noninvitees": [], "tcdate": 1570237743116, "tmdate": 1575500964430, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review"}}}, {"id": "S1eSBR_CtH", "original": null, "number": 2, "cdate": 1571880508938, "ddate": null, "tcdate": 1571880508938, "tmdate": 1572972518697, "tddate": null, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "invitation": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The present work considers adversarial attacks that also yield similar outputs for \"interpretability methods\", which are methods that output some vector corresponding to a given classification (usually the vector is e.g. an image or a similar object). It also shows that by regularizing nearby inputs to have similar interpetations (instead of similar classifications), robustness can be achieved similar to adversarial training. \n\nI did not understand the motivation of the paper. Why is it important for adversarial attacks to yield similar interpretations? A human would need to assess the interpretations to detect the attack, but it would already be trivial for an attack to be detected given human oversight (just check whether the classification of the image matches the human-assigned label). It also wasn't clear how this was related to the other observation that regularizing based on interpretability yields robustness; these seem like two fairly separate results.\n\nFinally, I found the claim that \"interpretability alone helps robustness\" to be misleading and not substantiated by the paper. The purported justification is that regularizing nearby inputs to have the same interpretation yields robustness. But a better summary of this observation is that \"robustness of interpretability implies robustness of classification\", which is not surprising, and is in fact a trivial corollary of the fact that the metric on interpretations dominates the classification error metric (an observation which is made in the paper).\n\nMore minor, but I found it hard to follow the writing in the paper (this is related to the motivation being unclear). This is exacerbated by the paper being longer than unusual (10 pages instead of 8)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1051/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilan@mit.edu", "sijia.liu@ibm.com", "gaoyuan.zhang@ibm.com", "pin-yu.chen@ibm.com", "shiyu.chang@ibm.com", "dluca@mit.edu"], "title": "Visual Interpretability Alone Helps Adversarial Robustness", "authors": ["Akhilan Boopathy", "Sijia Liu", "Gaoyuan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Luca Daniel"], "pdf": "/pdf/acccff40479a296fa1bda080c1bf7a27bb58fd24.pdf", "TL;DR": "Exploring the connection between robustness in interpretability and robustness in classification", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.", "keywords": ["adversarial robustness", "visual explanation", "CNN", "image classification"], "paperhash": "boopathy|visual_interpretability_alone_helps_adversarial_robustness", "original_pdf": "/attachment/53e7861faa24a789020453ee5ace18bb85596146.pdf", "_bibtex": "@misc{\nboopathy2020visual,\ntitle={Visual Interpretability Alone Helps Adversarial Robustness},\nauthor={Akhilan Boopathy and Sijia Liu and Gaoyuan Zhang and Pin-Yu Chen and Shiyu Chang and Luca Daniel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyes70EYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyes70EYDB", "replyto": "Hyes70EYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500964416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1051/Reviewers"], "noninvitees": [], "tcdate": 1570237743116, "tmdate": 1575500964430, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1051/-/Official_Review"}}}], "count": 14}