{"notes": [{"id": "HJxrVA4FDS", "original": "rkerAmLuvS", "number": 1073, "cdate": 1569439276682, "ddate": null, "tcdate": 1569439276682, "tmdate": 1583912047242, "tddate": null, "forum": "HJxrVA4FDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "xoa23UcmPK", "original": null, "number": 1, "cdate": 1576798713864, "ddate": null, "tcdate": 1576798713864, "tmdate": 1576800922614, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "All the reviewers recommend acceptance. The reviews found the paper to be interesting with substantial insights. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714168, "tmdate": 1576800263959, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Decision"}}}, {"id": "H1xpB_j6KB", "original": null, "number": 2, "cdate": 1571825733320, "ddate": null, "tcdate": 1571825733320, "tmdate": 1574721765617, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposed a dataset and designed a relevant network structure to analyze the function of horizontal and top-down connections for perceptual grouping. The used two datasets smartly isolate the requirements for exploiting Gestalt cues and object-based strategies. Appendix A detailed describes the cABC dataset, and the control experiments in Appendix B further validate the designing of the cABC. The proposed network flexibly integrates three types of connections and successfully solves both two challenges. The visualization results in Figure 4, S8 and S9 are insightful and also validate the intuitions. Overall the paper is clearly written and easy to follow.\n\n\n\nAlthough the use of different types of connections in the proposed network is clear, I think the author should conduct more analysis for the standard networks and datasets.\n\nIn section 3, the author claims that the two challenges will cause a high computational burden for feedforward models like ResNets. However, in Figure 5, the results show that ResNet-152 and ResNet-50 could easily solve the Pathfinder. For cABC, ResNet-18 could solve and ResNet-50 and ResNet-152 will fail while the author illustrates this is due to ResNet-50 and ResNet-152 overfit to cABC. Those phenomena are quite misaligning with the arguments. \n\n- Can we find a suitable ResNet structure that could solve both two challenges? \n- Can we increase the depth of U-Net and make it solve both two challenges? \n- Can we add more data or increase the difficulty for cABC and prevent the overfitting for ResNet-50 and ResNet-152? \n- Can we keep increasing the difficulties of cABC and straining ResNet-18 and UNet? Does that difficulty level would also strain TD+H-CNN?  \n- We could also perform similar experiments on Pathfinder that increasing its difficulty for ResNet-50 and ResNet-152. \n\nAlso, if we train the TD+H-CNN on standard image datasets such as cifar-10 or ImageNet, what about the recurrent activities and the final acc?\n\n=========================================================\nAfter Rebuttal:\n\nI thank the author for the response. \n\nI hope the comments are useful for preparing a future version of this work when you have enough time.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910239794, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Reviewers"], "noninvitees": [], "tcdate": 1570237742754, "tmdate": 1575910239810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review"}}}, {"id": "H1eArVEioH", "original": null, "number": 6, "cdate": 1573762118280, "ddate": null, "tcdate": 1573762118280, "tmdate": 1573762118280, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "BJg5myXoiS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment", "content": {"title": "Clarification", "comment": "The two different versions of the manuscript can be found by clicking on the \"Show revisions\" link. The latex diff is the most recent upload, and the revised manuscript is the second most recent upload. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxrVA4FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1073/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1073/Authors|ICLR.cc/2020/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161675, "tmdate": 1576860535931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment"}}}, {"id": "SygWsyQjsH", "original": null, "number": 5, "cdate": 1573756824700, "ddate": null, "tcdate": 1573756824700, "tmdate": 1573756824700, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "SkeA5ZK6FH", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review! We have tried to address your comments:\n\n<<In the second paragraph of the introduction, the authors state that the two feedback mechanisms should be iterative. Can the authors provide elaborate as to why these strategies should be inherently iterative and simply applying the same model a small/finite amount of times is not enough?>>\nOur claims of iterative processing were based on the qualitative results of Fig 4, S10, and S11. Based on this referee\u2019s question, we have also included a new experiment to show that performance of our recurrent models depends on iterative processing through time. In Fig. S7, we record performance of TD+H-CNN, TD-CNN, and H-CNN models trained on Pathfinder and cABC challenges with 8-timesteps of recurrence vs. 1-timestep of recurrence. These models are not able to solve the \u201chard\u201d versions of these tasks when they have only 1-timestep of processing.\n\n<<The authors claim that the relatively low performance of ResNet-18 and U-Net on Pathfinder is due to a higher computational burden, yet the reason for the poor performance of ResNet-50 and ResNet-152  on cABC is the result of overfitting. Is there any evidence to support this distinction or are the authors simply arguing this because it is the most plausible explanation?>>\nThank you for this comment. We have included new experiments to test this claim of computational burden vs. overfitting for shallow vs. deep ResNets. First, we found that the ResNet-18 performs better on the Pathfinder challenge when it is widened and given approximately the same capacity as the ResNet-50 (although it still does not perform as well as ResNet-50; see Fig. S8). Second, we found that ResNet-50 performs better on cABC when it is trained with 4x as many unique samples as usual. The failures of ResNets on these challenges is somewhat of a diversion from the main thrust of our paper, which is to show when different forms of feedback are useful for segmentation. That said, it is notable that ResNet architectures are not as generally effective as our recurrent architectures in solving these challenges. These additional experiments, which you motivated, have helped bolster this finding.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxrVA4FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1073/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1073/Authors|ICLR.cc/2020/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161675, "tmdate": 1576860535931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment"}}}, {"id": "S1xq_1Qosr", "original": null, "number": 4, "cdate": 1573756786065, "ddate": null, "tcdate": 1573756786065, "tmdate": 1573756786065, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "H1xpB_j6KB", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the comments!\n\n<<In section 3, the author claims that the two challenges will cause a high computational burden for feedforward models like ResNets. However, in Figure 5, the results show that ResNet-152 and ResNet-50 could easily solve the Pathfinder. For cABC, ResNet-18 could solve and ResNet-50 and ResNet-152 will fail while the author illustrates this is due to ResNet-50 and ResNet-152 overfit to cABC. Those phenomena are quite misaligning with the arguments. >>\nWe have included new experiments that help clarify the failures of ResNets on Pathfinder and cABC tasks. First, we tested if the failure of ResNet-18 on Pathfinder was due to limitations in its capacity. Indeed, we found that a \u201cwide\u201d ResNet-18, with approximately the same number of weights as a ResNet-50 performed far better than the original ResNet-18 on pathfinder (not that it still does not totally solve the hard Pathfinder; Fig. S8). Second, we tested if the failure of deeper ResNets on cABC was due to overfitting. Verifying this hypothesis, we found that a ResNet-50 trained on 4x the amount of training data was able to solve the cABC challenge. \n\n<<Can we find a suitable ResNet structure that could solve both two challenges?>>\nOur new experiments make it clear that it is extremely difficult if not impossible to find a single ResNet architecture that solves both Pathfinder and cABC challenges efficiently (i.e., with limited sample training sets). While the main purpose of our work is to examine the computational role of different forms of feedback for visual segmentation, we believe we also demonstrate that purely feedforward models like ResNets are not as robust as recurrent models with horizontal and top-down feedback in solving Pathfinder or cABC challenges.\n\n<<Can we increase the depth of U-Net and make it solve both two challenges?>>\nWe believe that U-Nets will experience a similar tradeoff as ResNets between performance on Pathfinder vs. cABC as the architecture is made deeper/shallower. We agree that this is an important question and could be a promising future research direction, but we did not have time to run these experiments during this rebuttal period.\n\n<<Can we add more data or increase the difficulty for cABC and prevent the overfitting for ResNet-50 and ResNet-152?>>\nThis comment is addressed by our new experiments where we find ResNet-50 can learn cABC when trained on 4x as much data.\n\n<<Can we keep increasing the difficulties of cABC and straining ResNet-18 and UNet? Does that difficulty level would also strain TD+H-CNN? [Same question for Pathfinder and deep ResNets.]>>\nWe believe that increasing the difficulty of either challenge will eventually make networks that rely on some type of template matching fail. That said, we also tried to constrain the parameters of our datasets so that the tasks are still trivial for humans (as shown in our psychophysical evaluation).\n\n<<Also, if we train the TD+H-CNN on standard image datasets such as cifar-10 or ImageNet, what about the recurrent activities and the final acc?>>\nThe TD+H-CNN uses fGRU models, which were first introduced by [1]. They build deep hierarchical recurrent networks with fGRUs, which were trained to solve contour detection tasks in natural images and electron microscopy imaging. They found that these models are more sample efficient than state-of-the-art feedforward solutions. They also visualize the recurrent activities and find qualitatively interesting changes in processing over the model\u2019s timecourse.\n[1] Linsley D, Kim J, & Serre T. 2019. Sample-efficient image segmentation through recurrence. ArXiv.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxrVA4FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1073/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1073/Authors|ICLR.cc/2020/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161675, "tmdate": 1576860535931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment"}}}, {"id": "BklhB17siB", "original": null, "number": 3, "cdate": 1573756739824, "ddate": null, "tcdate": 1573756739824, "tmdate": 1573756739824, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "ryesUNO59H", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your time and feedback! To address your questions:\n\n<<How do variants of the architecture (deeper/shallower) perform under the same settings? Do the conclusions change with network depth>>\nThe TD+H-CNN network is relatively shallow (see Fig. 3b), so we developed a deeper version with more convolutional layers between it\u2019s low- and high-level fGRU modules. As shown in Fig. S8, this model performed similarly to the original TD+H-CNN on the Pathfinder and cABC challenges.\n\n<<How do these results generalize to real-world segmentation datasets? Are both top-down and horizontal connections needed for e.g., PASCAL/COCO?>>\nThe fGRU modules that we use in our models were introduced in [1]. In that paper it was shown that hierarchical models with fGRUs are far more sample efficient than state-of-the-art models for object contour detection in the BSDS500 dataset and cell segmentation in electron microscopy imaging, and that the combination of top-down and horizontal connections improves performance over models with only one or the other.\n\n[1] Linsley D, Kim J, & Serre T. 2019. Sample-efficient image segmentation through recurrence. ArXiv."}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxrVA4FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1073/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1073/Authors|ICLR.cc/2020/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161675, "tmdate": 1576860535931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment"}}}, {"id": "BJg5myXoiS", "original": null, "number": 2, "cdate": 1573756706429, "ddate": null, "tcdate": 1573756706429, "tmdate": 1573756706429, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment", "content": {"title": "Response to reviewers", "comment": "We thank the reviewers for their thoughtful and constructive reviews. We have revised our manuscript to address the critiques that were raised. We have uploaded two versions of the new manuscript: (1) our revision, and (2) a diff between the revision and original submission. Our revision includes the following changes:\n\n- (R1/R2/R3) Figure S8, which shows performance of a deeper version of the TD+H-CNN on cABC and Pathfinder challenges.\n\n- (R1/R2/R3) Our new Figure S8 also demonstrates that our fGRU models are more robustly able to solve the cABC and Pathfinder challenges than ResNet models. We originally found that ResNet-50/152 solved Pathfinder but not cABC, and ResNet-18 solved cABC but not Pathfinder. We found that we can rescue ResNet-50 on cABC by training it on 4x as much data. We also found that we can improve ResNet-18 performance on Pathfinder by widening it to have approximately the same number of parameters as a ResNet-50 (its performance however still lags well behind ResNet-50). In other words, these feedforward architectures are relatively brittle, w.r.t. the success of any particular configuration, in solving cABC and Pathfinder.\n\n- (R1/R3) We carried out an experiment (described at the end of the \u201cScreening feedback computations\u201d portion of Section 5) to rescue very deep ResNet (50/152-layer) performance on cABC. We find that when we quadruple the training set size, ResNet-50 is able to learn the task. We take this result as evidence ResNets are very sensitive to overfitting on cABC.\n\n- (R1) Figure S7, which compares the performance of fGRU model variants trained with 8 timesteps vs. 1 timestep of processing. We believe that this result considered alongside the qualitative evidence of fGRU model strategies for solving cABC and Pathfinder (Figs. 4, S10, and S11), stands as strong evidence for these models utilizing iterative strategies to solve the tasks.\n\n-(R1/R2/R3) We have included other edits to improve clarity.\n\nWe will address remaining reviewer comments directly. Please let us know if you have any other questions or concerns that we can address before the response deadline."}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxrVA4FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1073/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1073/Authors|ICLR.cc/2020/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161675, "tmdate": 1576860535931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Authors", "ICLR.cc/2020/Conference/Paper1073/Reviewers", "ICLR.cc/2020/Conference/Paper1073/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Comment"}}}, {"id": "ryesUNO59H", "original": null, "number": 3, "cdate": 1572664403251, "ddate": null, "tcdate": 1572664403251, "tmdate": 1572972516132, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This is an interesting paper. It seeks to disentangle the need for top-down and horizontal connections for grouping tasks using (a) a new synthetic dataset that seeks to evaluate one over the other, and (b) a new recurrent neural network model.\n\n\nI think this is an interesting scientific question that is worth answering. It is especially useful given the context of what we know about the visual cortex in the brain, which is the presence of a large number of horizontal and top-down connections.\nI think this paper takes a good first step in understanding this. I liked the problem setup and the approach of looking at accuracy at a given sample complexity rather than accuracy alone. I also liked the fact that the authors used both deeper networks and state-of-the-art baselines and corrected for the parameter count. In these respects the paper is novel and thought-provoking.\n\nI would like the evaluation to be stronger, however. I would like to see the following experiments: \n(1) How do variants of the architecture (deeper/shallower) perform under the same settings? Do the conclusions change with network depth?\n(2) How do these results generalize to real-world segmentation datasets? Are both top-down and horizontal connections needed for e.g., PASCAL/COCO?\n\nSince this is not a vision conference, I am giving this a weak accept, but I would really like to see at least an evaluation of (2)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910239794, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Reviewers"], "noninvitees": [], "tcdate": 1570237742754, "tmdate": 1575910239810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review"}}}, {"id": "SkeA5ZK6FH", "original": null, "number": 1, "cdate": 1571815830342, "ddate": null, "tcdate": 1571815830342, "tmdate": 1572972516088, "tddate": null, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "invitation": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The article tries to examine existing hypotheses from the neuroscience and perception literature by using neural networks as a computational model of the brain. Namely, the authors assess the efficiency of different strategies for solving two visual challenges, one of which is novel. The authors also evaluate the level of consistency between the performance of humans and different types of neural architectures.\n\nI believe that the quality of this work is above the acceptance threshold. The results seem to clearly support the claims. The experiments are well-designed and an adequate number of baselines are provided. However, it should be mentioned that the conclusions are by no means surprising.\n\nThe following sentence should probably be fixed:\n> \".. models that not learn overfit the training set.\"\n\nSome questions (answering is optional):\n -  In the second paragraph of the introduction, the authors state that the two feedback mechanisms should be iterative. Can the authors provide elaborate as to why these strategies should be inherently iterative and simply applying the same model a small/finite amount of times is not enough?\n - The authors claim that the relatively low performance of ResNet-18 and U-Net on Pathfinder is due to a higher computational burden, yet the reason for the poor performance of ResNet-50 and ResNet-152  on cABC is the result of overfitting. Is there any evidence to support this distinction or are the authors simply arguing this because it is the most plausible explanation?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1073/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling neural mechanisms for perceptual grouping", "authors": ["Junkyung Kim*", "Drew Linsley*", "Kalpit Thakkar", "Thomas Serre"], "authorids": ["junkyung_kim@brown.edu", "drew_linsley@brown.edu", "kalpit_thakkar@brown.edu", "thomas_serre@brown.edu"], "keywords": ["Perceptual grouping", "visual cortex", "recurrent feedback", "horizontal connections", "top-down connections"], "TL;DR": "Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.", "code": "https://bit.ly/2wdQYGd", "pdf": "/pdf/867434cbc79e43a38c672dfaa275569e7d763529.pdf", "paperhash": "kim|disentangling_neural_mechanisms_for_perceptual_grouping", "_bibtex": "@inproceedings{\nKim*2020Disentangling,\ntitle={Disentangling neural mechanisms for perceptual grouping},\nauthor={Junkyung Kim* and Drew Linsley* and Kalpit Thakkar and Thomas Serre},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxrVA4FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/79fddf176ef862de482f17f8fbcb51237668fa5a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxrVA4FDS", "replyto": "HJxrVA4FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910239794, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1073/Reviewers"], "noninvitees": [], "tcdate": 1570237742754, "tmdate": 1575910239810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1073/-/Official_Review"}}}], "count": 10}