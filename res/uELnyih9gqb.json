{"notes": [{"id": "uELnyih9gqb", "original": "bRUha-Xq2Yu", "number": 2296, "cdate": 1601308253081, "ddate": null, "tcdate": 1601308253081, "tmdate": 1614985730107, "tddate": null, "forum": "uELnyih9gqb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "T6LZ0aKPygT", "original": null, "number": 1, "cdate": 1610040409436, "ddate": null, "tcdate": 1610040409436, "tmdate": 1610474006628, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposed a regularization term to control the bit-width and encourage the DNN weights moving to the quantization intervals. The paper is well-written and the idea of using the sinusoidal period as a continuous representation is novel. However, the theoretical analysis provided are not consistent with the proposed method.\n\nAs for the experimental results, the proposed method incurs significant degradation as compared to the baseline, and comparison with recent quantization methods is lacking."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040409423, "tmdate": 1610474006611, "id": "ICLR.cc/2021/Conference/Paper2296/-/Decision"}}}, {"id": "-AW9SIzdoDD", "original": null, "number": 7, "cdate": 1606214420791, "ddate": null, "tcdate": 1606214420791, "tmdate": 1606277592127, "tddate": null, "forum": "uELnyih9gqb", "replyto": "DnXIqb4p1S2", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment", "content": {"title": "Author Response", "comment": "\nThank you for the insightful and stimulating comments.\n\n===== (1) The theoretical analysis ===========\n\nIt seems that there is a misunderstanding in interpreting our theoretical and experimental results, accordingly we provide more details to clarify this.\n\nFirst, we would like to clarify that while the theorem is stated in terms of a limit as the regularization parameter vanishes, the proof in fact gives a corresponding stability result. Namely, if the regularization parameter is sufficiently small relative to the main loss then the minimizers will be \"almost\" quantized.\n\nSecond, we provide results to the updated paper comparing two profiles of the regularization strength ($\\lambda_w$). \n\n-- Profile 1: $\\lambda_w$ gradually increases as training proceeds then gradually decays towards the end of training. (Figure 7(a)).  \n\n-- Profile 2: $\\lambda_w$ gradually increases as training proceeds and remains high (Figure 7 (c)). \n\nFigure 7 (a,c) depicts different loss components and Figure 7 (b,d) visualizes weights trajectories.\nBoth profiles show that accuracy loss is unimpededly minimized along with WaveQ loss. \n\nOur theoretical results (as originally stated) align with Profile 1 (Figure 7(b)) . With Profile 2, although $\\lambda_w$ decays back towards the end of training, the weights mostly remain tied to their quantization levels except for a few deflections that cause slight increase of the regularization loss towards end of training. In terms of test accuracy, both profiles yield similar results (Profile 1, $74.95$%) vs (Profile 2, $74.45$%). \n\n===== (2) Baseline improvement and comparison to HAQ ========\n\nBaseline improvements: \n\nOur results (as reported in Table 1) show that the incorporation of WaveQ consistently enhances the performance of existing techniques and outperforms multiple state-of-the-art methods considered as our baseline.\n\nWaveQ in comparison to HAQ: \n\nHAQ only provides a method for bitwidth selection using reinforcement learning. Such methods have a high computational complexity as the bitwidth policy must be learned, which involves many iterations of finetuning/evaluation for different bitwidth combinations for the DNN under consideration. \n\nIn terms of runtime comparison, reinforcement learning based methods, such as HAQ, typically take up to multiple days to find optimum combination of quantization bitwidths for a single network, while gradient based methods such as WaveQ can finish in hours as it involves a single training pass while biggybagging on stochastic gradient descent without extra computation cost.\n\nIn terms of scalability, such high computational cost, in addition to being sample inefficient, prohibits reinforcement learning based methods, like HAQ, from being scalable to very deep networks. In contrast, WaveQ does not incur scalability overhead, so it can seamlessly extend to very deep networks.\n\nLastly, we would like to emphasize that WaveQ is a quantization-aware regularization method rather than being an explicit quantized training method. In this sense, WaveQ is a complementary approach as it can be applied on top of other existing quantization methods, such as HAQ, DoReFa, etc. Incorporating WaveQ during conventional training is orthogonal to other methods and provides a better starting point for any post-training based quantization methods. \n\n==== (3) Comparison to Achterhold et al., 2018 =====\n\nThe following highlight some of the differences.\n\n(1) WaveQ also offers to learn the bitwidth per-layer granularity; while VNQ does not, instead it chooses the numbers of bits of precision manually.\n\n(2) VNQ takes a variational approach and requires a careful choice prior distribution of the weights, which is not straightforward, and the model is often intractable. In contrast, WaveQ is directly applicable without introducing extra hyperparameters to optimize.\n\n(3) VNQ involves approximations, for example a closed-form expression for the KL divergence cannot be obtained, so they use a differentiable approximation. WaveQ involves no approximations by employing unconditionally continuous and differentiable objectives.\n\n(4) VNQ takes on a probabilistic approach, while WaveQ is a deterministic approach towards soft quantization.\n\n(5) VNQ provides results for ternary quantization only (bitwidth = 2). In contrast, WaveQ directly supports arbitrary bitwidth quantization and provides results for a variety of bitwidths.\n\n\n\n\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uELnyih9gqb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2296/Authors|ICLR.cc/2021/Conference/Paper2296/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850078, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment"}}}, {"id": "NlyQOxMxXI", "original": null, "number": 5, "cdate": 1606210305064, "ddate": null, "tcdate": 1606210305064, "tmdate": 1606277149950, "tddate": null, "forum": "uELnyih9gqb", "replyto": "gnKffpHu_rY", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the insightful and stimulating comments.\n\n=== (1) Activation quantization ===\n\nThis paper focuses on weight quantization and simultaneously learning the bitwidths, which is unprecedented. Due to this unique and novel gradient-based approach in quantization, it can be applied on top of other activation quantization techniques and due to its gradient-based nature even complement them better since WaveQ adjusts the weights while considering the effects of activation quantization in its updates. WaveQ actually has been used in consort with activation quantization for results in Table 1.\n\nMoreover, weight quantization has its own significant merit and benefits. For instance, weight quantization alone provides orders of magnitude energy reduction as energy consumption is dominated by memory access (for example, DRAM memory access energy is around 3 orders of magnitude of an add operation).\n\n=== (2) Comparison to other related works ===\n\nThanks for the pointer to these related work, which is included in the updated version.  The following highlights some fundamental differences.\n\n(1)  WaveQ not only regulates weights towards quantized values, but also learns the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. While none of these works learn the bitwidth.\n\n(2) All the related methods define a new optimization problem and use a special method for solving it. For example, [1] uses a proximal gradient method (adds a prox step after each stochastic gradient step), [2] uses ADMM, and [3] takes a Bayesian approach. This only makes the training more difficult, slower and increases the computational complexity. In contrast, WaveQ exploits the conventional stochastic gradient descent method while jointly optimizing for the original training loss while softly constraining it to simultaneously learn the quantized parameters, and more importantly bitwidths.\n\n(3) WaveQ incorporates an unconditionally continuous and differentiable objective with arbitrary bitwidth and that is how it seamlessly blends with the conventional training loss in a unified optimization setting. Other works get around the inherent discontinuities in the quantization process by incorporating various approximations. For example, the resulting regularizer in [1] is a W-shaped non-smooth regularizer for binary quantization. \n\n\n=== (3) Regularization strengths ===\n\n(1) Hyperparameters setting (such as learning rates and regularization strengths) is an imperative part of the general development of deep neural networks more than being specific to the proposed method. And each quantization technique has its own hyperparamters.\n\n(2) The particular procedure followed and illustrated in the paper is intuitive and its complexity is comparable to other procedures of similar studies. Further, several experiments, as reported in Tables I, II, and III, established the applicability of the used procedure across 8 different neural networks (one of them is Transformer model), with 4 different datasets.\n\n(3) The bitwidth regularization strength $\\lambda_\\beta$ is multiplied by the continuous variable $\\beta$ which is proportional to the compression rate; that is the sinusoidal period. That entire product forms the second objective which is what goes into the optimizer. The sole purpose of $\\lambda_\\beta$ is to define an intermediate window for learning the bitwidth and consequently the compression rate while balancing that relative to other objectives.\n\n=== (4) min-max ranges  ===\n\n(1) Determining the min-max range of the weights is an issue associated with quantization-aware training methods other than specific to this proposed method, and WaveQ is no different than other techniques\n\n(2) For finetuning purposes, the common approach is statically quantizing the weights to a single min-max range, since weights have already occupied a stable range which does not tend to change much during finetuning. \n\n(3) For training from scratch, some adaptive techniques could be incorporated. In our case, however, during phase one (Figure 2 (e) ), we let the network learn the range of the weights considering the original task loss only, while both $\\lambda_\\beta$ and $\\lambda_w$ are set to zeros. By the time we transition to phase 2, the larger $\\lambda$\u2019s gradually engage the regularization, we adopt the min-max range as learnt during phase 1.\n\n=== (5) Comparison to other bit-width allocation methods  ===\n\nWe added one more comparison against [1] to the updated paper. Table 7 shows this comparison results. In summary, for MobileNet-V2, WaveQ quantizes the network to an average bitwidth of 3.95 (that is 20.66 Giga Bits per OPerations  (GBOPs)) compared to 5.90 (that is 29.16 GBOPs) reported by [1]. Additionally, for ResNet-18, WaveQ achieves an average bitwidth of 3.57 (62.56 GBOPs) compared to 5.47 (65.90 GBOPs) by [1].\n\n[1] Uhlich, et al., Mixed precision DNNs: All you need is a good parametrization. ICLR20\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uELnyih9gqb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2296/Authors|ICLR.cc/2021/Conference/Paper2296/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850078, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment"}}}, {"id": "-01hKevJKc", "original": null, "number": 8, "cdate": 1606217103842, "ddate": null, "tcdate": 1606217103842, "tmdate": 1606238419733, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment", "content": {"title": "To Reviewers and the Area Chair ", "comment": "We thank all the reviewers for their encouraging and stimulating comments. We have addressed all the comments and feedback from the reviewers in our revision and provided a detailed answer in the comments section.\n\nIn particular, we added the following revisions to the paper.\n\n(1) We updated our theoretical results section (based on comments from AnonReviewer2 regarding the regularization strength profile) clarifying that while the theorem is stated in terms of a limit as the regularization parameter vanishes, the proof in fact gives a corresponding stability result. Namely, if the regularization parameter is sufficiently small relative to the main loss then the minimizers will be \"almost\" quantized. To illustrate and demonstrate this further, we provided results to the updated paper (Figure 7) comparing two profiles of the regularization strength. \n\n(2) We added Table 7 to the updated version of the paper (in the supplementary material) to provide a comparison against [1] and further demonstrate the effectiveness of the proposed method as a bit-width allocation method following suggestions from AnonReviewer3,2.\n\n(3) We updated the method section by moving the \u201cQuantizer\u201d paragraph into the main paper in Section 2.1, as suggested by AnonReviewer4.\n\n(4) We updated the related work section following suggestions from reviewers AnonReviewer1,2,4.\n\n\n\n[1] Uhlich, et al., Mixed precision DNNs: All you need is a good parametrization. ICLR (2020).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uELnyih9gqb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2296/Authors|ICLR.cc/2021/Conference/Paper2296/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850078, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment"}}}, {"id": "oGWZlFs2Pn7", "original": null, "number": 6, "cdate": 1606211604036, "ddate": null, "tcdate": 1606211604036, "tmdate": 1606211636573, "tddate": null, "forum": "uELnyih9gqb", "replyto": "P5FD1Wdg5e", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the insightful and encouraging comments.\n\n======= (1) Clipping and the use of tanh (major.1 and minor.2 ) ==========\n\nThanks for the detailed review and the observation. In fact, tanh is used to limit the value range to [-1,1], so smooth clipping is implied in Equation B.1. Accordingly, we clarified that further in the updated text.\n\n\n======= (2) Updating missing related work (major.2) ==========\n\nThanks for pointing that out, we have updated our related work to include it.\n\n\n======= (3) Activation quantization (major.3) ==========\n\nIn the reported experiments, the proposed regularization method only applied to weights. For activation quantization, we utilized an existing quantization method (DoReFa), and compared the end-to-end results with vs without the proposed regularization.\n\n\n======= (4)  Moving equation B.1, B.2 to the front (minor.1) ==========\n\nThank you for the insightful suggestion, accordingly we moved the Quantizer subsection (including equations B.1, B.2) to the main paper in Section 2.1.\n\n\n======= (5) Typos (minor.3) ==========\n\nThanks, fixed in the updated version.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uELnyih9gqb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2296/Authors|ICLR.cc/2021/Conference/Paper2296/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850078, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment"}}}, {"id": "hJkbL-ihhRI", "original": null, "number": 4, "cdate": 1606209094350, "ddate": null, "tcdate": 1606209094350, "tmdate": 1606209112347, "tddate": null, "forum": "uELnyih9gqb", "replyto": "5j07Sf3fUeB", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the insightful and encouraging comments.\n\n===== Comparison to other heuristic methods ======\n\nRegarding comparing to another heuristic variable-bitwidth baseline. We added one more comparison against [1] to the updated paper. Table 7 shows the comparison results. In summary, for MobileNet-V2, WaveQ quantizes the network to an average bitwidth of 3.95 (that is 20.66 GBOPs [2]) compared to 5.90 (that is 29.16 GBOPs) reported by [1]. Additionally, for ResNet-18, WaveQ achieves an average bitwidth of 3.57 (62.56 GBOPs) compared to 5.47 (65.90 GBOPs) also reported by [1].\n\n===== Rule based bitwidth selection ======\n\nThe results show that the bitwidth patterns are complex and very dependent on the specific DNN under-quantization. The following points provide reasoning why the bitwidths do not follow certain patterns. \n(1) Different layers in different DNNs capture different representations with varying degrees of influence on the classification results due to the architectural variations. (2) Because of different degrees of overparameterization, different layers exhibit different levels of tolerance to imprecision. (3) each layer assume a distribution of weights (typically bell-shaped) each of which has a different dynamic range leading to different degrees of robustness to quantization bitwidths; (4) recent experimental work [2] also shows that layers can be categorized as either \"ambient\" or \"critical\" towards post-training re-initialization and re-randomization. In other words, no straightforward set of rules is observed to decide bitwidth assignment a priori. \n\nFurthermore, our results (also commensurate with other works in literature) show that, given a particular accuracy-performance tradeoff, there may not be a unique optimum heterogeneous solution, but rather a set of solutions exhibiting different heterogeneities, which further suggests that choosing the optimal bitwidths is not straightforward. Therefore, gradient-based methods that can learn the optimal bitwidth assignment provide a necessary effective step in heterogenous bitwidth quantization.\n\n[1] Uhlich, et al., Mixed precision DNNs: All you need is a good parametrization. ICLR (2020).\n\n[2] Zhang, C., Bengio, S., and Singer, Y. Are all layers created equal? CoRR abs/1902.01996 (2019).\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uELnyih9gqb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2296/Authors|ICLR.cc/2021/Conference/Paper2296/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850078, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Comment"}}}, {"id": "DnXIqb4p1S2", "original": null, "number": 1, "cdate": 1603603266918, "ddate": null, "tcdate": 1603603266918, "tmdate": 1605024245023, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review", "content": {"title": "Interesting Motivation but Unfortunately  Unsatisfactory Empirical and Theoretical Results", "review": "The paper proposes WaveQ which proposes a sinusoidal regularization approach for quantizing Neural Networks. The motivation is to enable mixed-precision quantization, since homogeneously quantizing the model to low precision can lead to accuracy degradation.\n\nThe main problem with heterogeneous/mixed-precision bit-width is that its search space is exponentially large. To address this the authors propose to automatically learn this bit-precision by adding a sinusoidal regularization where the period learn able.\n\nWhile the approach is interesting but the theoretical and empirical results are not satisfactory and as such it is not clear how the proposed method is superior to other methods proposed in the literature. In particular:\n\n\n- The theoretical analysis provided does not apply to the proposed method. This is because the theory requires that the regularization function vanish after many iterations, whereas in the experiments the opposite approach is used.\n\n- The accuracy results provided in the empirical section incur significant degradation as compared to the baseline. Furthermore, the comparison is performed with old quantization methods. Newer mixed-precision quantization results using for example HAQ leads to much better quantization. As such, it is not clear what is the advantage of the proposed method?\n\n- How is this approach different than Achterhold et al., 2018?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099646, "tmdate": 1606915775255, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2296/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review"}}}, {"id": "P5FD1Wdg5e", "original": null, "number": 2, "cdate": 1603731967441, "ddate": null, "tcdate": 1603731967441, "tmdate": 1605024244964, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review", "content": {"title": "Good paper, suggests a useful method for quantization-aware training", "review": "The paper proposes using a sinusoidal regularizer for neural network quantization. The regularizer \u201cWaveQ\u201d (sin^2) pushes floating-point parameters towards quantized values. Because the period of the function is highly related to the required bit-width, it can be used to determine the bit-width while keeping good characteristics - continuous and trainable. The authors provide experiments on both CNN and Transformers. The proposed method is widely adaptable and easy-to-use with quite promising results.\n\nMajor questions/suggestions:\n\n1.\tIn equation 2.1, the period (=quantization step) is s= 1/ (2^beta \u2013 1), so the regularizer tries to move \u2018w\u2019 towards the multiple of \u2018s\u2019. As I understand, the desired range of \u2018w\u2019 is (-1, 1), to properly match the actual quantized value. But there is no clipping nor normalization applied to \u2018w\u2019. Is there something I missed?\n\n2.\tThere is a missing work [Nyuyen, 2020] which suggests a similar regularizer (|cos|). However, this paper is still valuable and does not hurt novelty.\n[Ngugen, 2020] Quantization Aware Training with Absolute-Cosine Regularization for Automatic Speech Recognition\n\n3.\tIt seems that WaveQ is also used for activation quantization (Section 4.1.), but there are not enough details about applying WaveQ for activations.\n\nMinor issues:\n\n1.\tEquation B.2 is frequently used in Section 2. Also, the \u2018alpha\u2019 term needs more explanation when it first appears. (\u2018scaling factor\u2019 is not enough) Consider moving the equation B.1, B.2. to the front.\n\n2.\tMany recent quantization papers use w / max(|w|) to normalize weight. Is there any advantage to use tanh(w) / max(tanh(w)) in Equation B.1?\n\n3.\tTypo: \u2018bitwidh\u2019 in Sec.4.1, \u2018citep\u2019 in Sec.5, \u2018ofcitepp\u2019 in Sec.6, \n\n4.\tI believe the theoretic part is OK but am not sure about the exactness and its importance.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099646, "tmdate": 1606915775255, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2296/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review"}}}, {"id": "gnKffpHu_rY", "original": null, "number": 3, "cdate": 1603866136858, "ddate": null, "tcdate": 1603866136858, "tmdate": 1605024244898, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review", "content": {"title": "Borderline", "review": "This paper proposed a regularization term to control the bit-width and encourage the DNN weights moving to the quantization intervals. The key in such regularization is the Sinusoidal function, where the penalty is maximized in the middle of quantization levels and minimized at the quantization points. The sinusoidal period is regarded as the continuous representation of the bit-width.\n\nPros:\n1. The paper is well written and easy to understand. Illustration figures help a lot to present the proposed method. E.g., Figure 2 clearly shows the key idea of this paper.\n\n2. The idea of using the sinusoidal period as a continuous representation is novel and it makes it natural to use gradient descent methods to optimize the bit-width.\n\n3. Experiments on different datasets and tasks show that the proposed method can bring improvement on accuracy by using better regularization and / or better bit-width allocation.\n\nCons:\n1. The quantization regularization can only be used on weight quantization, while activation quantization is very important and sometimes more sensitive to the accuracy drop. On this point, the proposed method cannot be used to allocate the bit-width of activation, while activations can actually have different bit-width to the weights.\n\n2. Some related works are missing.\nFor quantization regularization, it is used in ProxQuant[1] as well, where the weights are regularized with a norm-based quantization regularization. It also discussed the similar observation that the regularization can help quantization-aware training.\nFor bit-width allocation, [2] formulated the bit-width allocation as a knapsack problem and used ADMM to iteratively optimize the NN compression. [3] used Bayesian optimization to allocate the bit-width for different layers.\n\n3. The procedure to tune the regularization strengths is a little complicated. Although the authors proposed a detailed procedure for tuning these hyper-parameters, it's questionable that the same procedure can be widely applied in different networks/optimizers/datasets. For $\\lambda_\\beta$, it actually determines the number of total bits for the entire network, so it should depends on the expected compression rate of the quantized DNN.\n\n4. Using the sinusoidal period to decide the bit-width has an assumption that the range of the weights is fixed: otherwise one can reduce both the period and value range to keep the bit-width unchanged. However, the range of weights is usually not fixed in the training.\n\n5. The experiments on bit-width allocation may be not enough to show the effectiveness of the proposed method as a bit-width allocation method. It's better to compare with some other bit-width allocation methods.\n\nIn general, my rating is borderline. I hope the authors can give some response to the cons listed above.\n\nReference:\n\n[1] Bai, Y., Wang, Y.X. and Liberty, E., 2018. Proxquant: Quantized neural networks via proximal operators. arXiv preprint arXiv:1810.00861.\n\n[2] Yang, H., Gui, S., Zhu, Y. and Liu, J., 2020. Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach. In CVPR 2020.\n\n[3] Tung, F. and Mori, G., 2018. Clip-q: Deep network compression learning by in-parallel pruning-quantization. In CVPR 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099646, "tmdate": 1606915775255, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2296/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review"}}}, {"id": "5j07Sf3fUeB", "original": null, "number": 4, "cdate": 1603963592805, "ddate": null, "tcdate": 1603963592805, "tmdate": 1605024244831, "tddate": null, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "invitation": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review", "content": {"title": "How to include bitwidths in your training objective", "review": "When training quantized neural networks, one typically first fixes the desired bitwidth $b$ (of weights and activations). While training, one maintains and updates full-bitwidth weights during backprop and \"cheats\" by using $b$-quantized versions of these full-precision weights during forward propagation. The quantization scheme used may vary.\n\nWhat if we wished instead to *learn* $b$? This is especially attractive, for instance, if we wish a distinct $b_i$ for each layer $i$. We could introduce a variable $\\beta_i$ to represent $b_i$ during training. We would probably add a regularizer to minimize $\\Sigma \\beta_i$ to encourage training toward small bitwidths. However, this still leaves the problem that the $\\beta_i$ would be real, not integer, as required for quantization. We could cheat here by using $\\lceil \\beta_i \\rceil$, but note that quantization error $\\beta_i - \\lceil \\beta_i \\rceil$ may be quite large relative to the small integer values $\\beta$ is expected to take. Is there a way to ensure that training automatically produces $\\beta$s that are close to integer values?\n\nThis is where the current paper introduces a neat trick. By embedding $\\beta$ inside a (suitably scaled) sinusoidal loss function, they ensure that the $\\beta$s tend to have integer values (that correspond to the minima of the sinusoid).\n\nI haven't seen this trick before, and it seems like an elegant approach to the problem of favoring integer values within the global optimization process. The goal of producing distinct bitwidths for each layer is definitely a valid one, so this technique does also address a practical problem.\n\nThe technique's practical impact seems limited. In particular, it seems that the main effect is to reduce the average bitwidth of SOTA quantized models from 4 to >= 3.6 at roughly the same accuracy. This should correspond roughly to a 10% gain in performance (runtime), which is quite good but not great. I have no idea how to to interpret the power numbers, so I will ignore those for now.\n\nOne question I have though is how the technique compares to a decent heuristic variable-bitwidth baseline. The authors do compare to a  \"decrement the bitiwidth of a single layer\" baseline, which is a good start. However, what if you do something more informed than that as baseline? Can you tell us what bitwidths your techniques chose for Resnet, for instance? Is there a simple pattern there (e.g. high bitwidth for early layers, and lower later on)? Based on this, what is the \"best\" non-learned bitwidth selection you can come up with?\n\nIn any case, the sinusoid trick for learning integer-like values is a good one for people to know, and it seems that the authors have shown that it can be implemented to have practical impact. Even if bitwidth reduction is currently modest, I can imagine follow-up work to increase it. So I support acceptance of the paper.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2296/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2296/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "authorids": ["~Ahmed_T._Elthakeb1", "~Prannoy_Pilligundla1", "telgindi@ucsd.edu", "~Fatemehsadat_Mireshghallah1", "~Charles-Alban_Deledalle2", "~Hadi_Esmaeilzadeh1"], "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "Tarek Elgindi", "Fatemehsadat Mireshghallah", "Charles-Alban Deledalle", "Hadi Esmaeilzadeh"], "keywords": [], "abstract": "Deep quantization of neural networks below eight bits can lead to superlinear benefits in storage and compute efficiency. However, homogeneously quantizing all the layers to the same level does not account for the distinction of the layers and their individual properties. Heterogenous assignment of bitwidths to individual layers is attractive but opens an exponentially large non-contiguous hyperparameter space (${Available Bitwidths}^{\\# Layers}$). As such finding the bitwidth while also quantizing the network to those levels becomes a major challenge. This paper addresses this challenge through a sinusoidal regularization mechanism, dubbed WaveQ. Adding our parametrized sinusoidal regularizer enables us to not only find the quantized weights but also learn the bitwidth of the layers by making the period of the sinusoidal regularizer a trainable parameter. In addition, the sinusoidal regularizer itself is designed to align its minima on the quantization levels. With these two innovations, during training, stochastic gradient descent uses the form of the sinusoidal regularizer and its minima to push the weights to the quantization levels while it is also learning the period which will determine the bitwidth of each layer separately. As such WaveQ is a gradient-based mechanism that jointly learns the quantized weights as well as the heterogeneous bitwidths. We show how WaveQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is versatile and can also be used with predetermined bitwidths by fixing the period of the sinusoidal regularizer. In this case. WaveQ enhances quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and outperforms multiple state-of-the-art techniques. Finally, WaveQ applied to quantizing transformers\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "elthakeb|waveq_gradientbased_deep_quantization_of_neural_networks_through_sinusoidal_regularization", "one-sentence_summary": "GRADIENT-BASED DEEP QUANTIZATION OF NEURAL NETWORKS THROUGH SINUSOIDAL REGULARIZATION", "supplementary_material": "/attachment/097e753ca406796a6064e388c2f611d4e388cfd4.zip", "pdf": "/pdf/62fefa4b31c228d87a063089c889906a51cea6e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JFqGKfhZA-", "_bibtex": "@misc{\nelthakeb2021waveq,\ntitle={{\\{}WAVEQ{\\}}: {\\{}GRADIENT{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}QUANTIZATION{\\}} {\\{}OF{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}} {\\{}THROUGH{\\}} {\\{}SINUSOIDAL{\\}} {\\{}REGULARIZATION{\\}}},\nauthor={Ahmed T. Elthakeb and Prannoy Pilligundla and Tarek Elgindi and Fatemehsadat Mireshghallah and Charles-Alban Deledalle and Hadi Esmaeilzadeh},\nyear={2021},\nurl={https://openreview.net/forum?id=uELnyih9gqb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uELnyih9gqb", "replyto": "uELnyih9gqb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2296/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099646, "tmdate": 1606915775255, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2296/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2296/-/Official_Review"}}}], "count": 11}