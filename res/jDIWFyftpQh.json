{"notes": [{"id": "jDIWFyftpQh", "original": "dM8ucSt-9fh", "number": 1187, "cdate": 1601308133106, "ddate": null, "tcdate": 1601308133106, "tmdate": 1614985629710, "tddate": null, "forum": "jDIWFyftpQh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_AuWS9EN-q9", "original": null, "number": 1, "cdate": 1610040530700, "ddate": null, "tcdate": 1610040530700, "tmdate": 1610474140216, "tddate": null, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "invitation": "ICLR.cc/2021/Conference/Paper1187/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a method for data augmentation by cross-modal data generation. While the reviewers agree that the paper addresses a relevant and important problem in medical imaging, they also agree on that the paper has limited novelty over the state of the art. Also the setup of experimental validation to comparison methods is questioned."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040530687, "tmdate": 1610474140200, "id": "ICLR.cc/2021/Conference/Paper1187/-/Decision"}}}, {"id": "z0okLeUou9m", "original": null, "number": 1, "cdate": 1603589474675, "ddate": null, "tcdate": 1603589474675, "tmdate": 1606761870286, "tddate": null, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "invitation": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review", "content": {"title": "Nice results, somewhat limited novelty", "review": "The authors propose an algorithm to enlarge the training set for image classification problems in certain medical applications where training data of the target modality is scarce. They do so by training an unpaired image-to-image translator network and an image classifier end-to-end in order to utilize labeled images acquired through various imaging modalities. Moreover, they demonstrate the effectiveness of the proposed algorithm via extensive numerical experiments on a prediction problem and compare their method to several different approaches ranging from transfer learning to data augmentation.\n\nPositives:\n+ The problem is well-motivated. Obtaining large medical image datasets is difficult, therefore it is very important to find ways to use the available data as efficiently as possible.\n+ In most parts, the paper is well-written, clear and easy to follow.\n+ Authors compared their method to a wide range of other possible techniques.\n+ The experimental results are very promising for the given prediction problem.\n\nNegatives:\n- The novelty of the paper seems to be somewhat limited. Using unpaired image-to-image translation networks for cross-modal medical image synthesis has been studied in [1, 2] (as the authors pointed out in the paper). The only novelty compared to [2] seems to be that authors apply the idea to an image classification problem instead of segmentation. Authors claim that part of the novelty is that in their method translation is guided by the predictive task, however in [2] the translator is also jointly trained with the segmentation network.\n- The consistency of anatomical  structures is not enforced in any way in the translated images (see shape consistency in [2]). For instance, CT images generated from PET are very dissimilar to original PET images (Fig. 2). This might lead to loss of features crucial for diagnosing certain diseases, or even to false diagnosis via 'hallucinated' features. Medical diagnosis is often made based on very specific, fine features on the image. The experiments performed in the paper pertain to physiological age prediciton where specific fine-grained features might not be necessary for correct prediction. Therefore further experiments might be needed to judge the wider applicability of the method.\n- There is some confusion around the experimental results. First, in Table 3 some results (CycleGAN for MRI-CT and DA for PET-CT) of competing methods are worse than simply training on the CT images (PURE-CT in Table 2). Second, based on the Appendix, in some experiments the backbone model is ResNet18 and in other experiments it is ResNet50. The data augmentation experiments from Table 4 were performed using ResNet-18, whereas for the proposed method ResNet50 has been used. It is not clear how the results are comparable given the difference in network capacity.\n\nEven though the paper shows promise, given the limited novelty and somewhat questionable applicability of the presented results as detailed above, in this form I would recommend rejection.\n\nCould the authors clarify what the main contribution of the paper is compared to other techniques exploiting cross-modal image synthesis for medical applications. Furthermore, could the authors comment on my concerns about geometric distortions originating in image translation and how it may effect medical diagnosis? Lastly, please address the issues raised about the experimental results above.\n\nAdditionally, I have some minor comments and suggestions:\n- The algorithm could be better differentiated from other results in the related work section.\n- It is not clear how the discussion on text-to-image generation methods in Section 2.2 is relevant to the paper.\n- Using the same colormap for all figures for different modalities would be helpful for better comparison.\n- It is difficult to match the experimental details in the appendix to the experiments in the paper.\n- There is a typo in the title of Section 3.3\n\n[1] Chartsias et al, Adversarial image synthesis for unpaired multi-modal cardiac data, International workshop on simulation and synthesis in medical imaging, 2017\n\n[2] Zhang et al, Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network, CVPR, 2018\n\nPost-rebuttal: The authors did not provide feedback and therefore I keep my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1187/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1187/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1187/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124674, "tmdate": 1606915808643, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1187/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review"}}}, {"id": "gaFPxfHMdzJ", "original": null, "number": 2, "cdate": 1603856728993, "ddate": null, "tcdate": 1603856728993, "tmdate": 1605024508291, "tddate": null, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "invitation": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review", "content": {"title": "Good direction but lacks technical novelty", "review": "Summary:\nThis paper studies the problem of learning a predictor from a specified modality using a dataset where each example has images from only one modality. The proposed approach is to set up a CycleGAN to translate between the modalities and a predictor from the required modality to the prediction target. The authors propose to learn the CycleGAN and the predictor parameters jointly. Experimental results show that this performs better than either not doing the translation at all (i.e. only learning the predictor from examples with the required modality) or learning the translation followed by learning the prediction.\n\n\nStrengths:\n+ Interesting problem setting. I find the medical imaging use case convincing.\n+ The proposed idea is simple and clean\n+ Empirical evaluation uses a comprehensive set of baselines\n\n\nConcerns / weaknesses:\n- The technical novelty in this paper is unclear. It has been generally known that an end-to-end approach typically works better than learning the two parts (CycleGAN-translator and predictor) separately.\n- Authors state that they randomly split images into training, validation and test sets. However, since there are ~30 images per patient, this could result in images from the same patient ending up in training as well as test sets. I imagine nearby slices of the 3D volume represented by a single CT-scan to be very similar to each other, and so this raises a serious concern about train/test overlap.\n- Related to above (and even after using per-patient splits), the std-dev of the prediction errors needs to be estimated using independent samples (i.e. from different patients), since samples from the same patients would not be independent. It is unclear how these correlations were handled.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1187/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1187/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1187/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124674, "tmdate": 1606915808643, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1187/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review"}}}, {"id": "8PxXXzrE58r", "original": null, "number": 3, "cdate": 1603938972838, "ddate": null, "tcdate": 1603938972838, "tmdate": 1605024508228, "tddate": null, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "invitation": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This paper combines the Cycle-GAN with the predictor for downstream task to augment the training data in the target modality. On two applications, the experimental results illustrate the effectiveness of their method.\n\nStrengths:\n(1) The paper is very well-written and easy to be understood.\n(2) Compared with the baselines, the proposed method significantly improves the performance of downstream regression task.\n  \nWeakness:\n(1) The comparison between the baselines and proposed method is not fair. For example, Data augmentations methods only use the target dataset, but the proposed method use both the source and target datasets.\n(2) The combination between the Cycle-GAN and downstream predictor is not used at first. [1] has introduced the segmentor to constrain the generated images. The difference is that the proposed method is applied to regression task and [1] solved the segmentation task.\n \n[1] Zhang, Zizhao, Lin Yang, and Yefeng Zheng. \"Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018..\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1187/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1187/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1187/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124674, "tmdate": 1606915808643, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1187/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review"}}}, {"id": "mkW1xWf2UJ", "original": null, "number": 4, "cdate": 1604177728250, "ddate": null, "tcdate": 1604177728250, "tmdate": 1605024508165, "tddate": null, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "invitation": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review", "content": {"title": "interesting approach", "review": "Summary:\nThis paper discusses an approach to augment a medical imaging dataset using images from another modality. The images in the other (i.e. source) modality should have been originally collected & labeled to perform the same discrimination or regression task as the target modality. A network consisting of a prediction network for the mixture of augmented and target images & another network based on CycleGAN for the image translation network are jointly optimized using end to end training.  \n \nReasons for score:\nThe objective of the paper (i.e. addressing small training set size in medical imaging) is quite important, and the approach is interesting. However, the evaluation, discussion, and generalizability of the approach to other tasks require additional clarification, simulations, and extra information.\n\nDetailed review:\n1) Is physiological age in the clinic estimated from a single slice in a CT volume, or an entire volume? If latter, the entire evaluation strategy should change to include the set of images from the volume rather than single images. Otherwise, the task selected will not have any bearing on the actual clinical task of estimation of physiological age. This would also impact the approach which uses a 2D image to image translation, as it will likely mean that a 3d to 3d image translation should be used.\n2) The authors have not provided sufficient information about the distribution of attributes of the different datasets. This information is critical in assessing the generalizability of results, as well as whether the experiments were set up in a meaningful way. \na) What is the distribution of actual age in the different datasets from the different modalities?\nb) What are the image acquisition / reconstruction attributes of source/target datasets? Images from the same patient will look very different and contain varying degrees of anatomical detail depending on slice thickness, dose in CT & imaging sequence in MRI, reconstruction algorithm, etc. When doing image to image translation, does e.g. slice thickness across the modalities need to match each other?\n3) In medical imaging we often have severe class imbalance, where disease positive samples are much more rare compared with disease negative samples. Given that datasets from different modalities will likely have different ratios of positive to negative samples, how will this affect the overall training strategy? E.g. would the authors only augment the positives in this way , or both positives & negatives? Otherwise, if images from source modality have different class balance than the one in target?\n4) The proposed approach has been described for a medical imaging task that involves macro anatomical features only. It is therefore not clear whether this approach would generalize to a task that pertains to more micro features (e.g. tumor classification/detection, or tumor segmentation). Based on Fig2, it appears that the MRI images converted to CT contain obvious anatomical inaccuracies (e.g. Fig 4 shows consistently thick skulls in the generated CT images, which would affect a brain volume estimation task). Have the authors used this approach for other tasks that require more micro level anatomical precision? If not, this should be explicitly stated as a shortcoming of the current approach.\n5) The selection of hyper-parameters, experimental setups, and split of data into train/test using different approaches requires more explanation:\na) It is not clear how the lambda values were selected. Also, for eq 3 the authors state that a lambda of 0.001 was used, which seems to severely favor the GAN loss rather than the prediction loss. Given that the prediction task is the more important among the two, no justification has been provided on why the weight of the corresponding loss value would be so small compared to the GAN loss.\nb) What was the stopping criteria for the different scenarios in Table 2? In the Appendix, the authors have the number of training epochs for each approach (which is different for different experiments), but it is not clear what determined the end of training. This is important, since all approaches should be trained using the same rule to ensure a fair comparison.\nc) On p5, the authors state that they randomly select images from the CT cases to split the data into train/val/test buckets; Does this mean that images from same patient are mixed into both train/test? This would not be appropriate, since the different slices of CT data from the same patient share anatomical similarity, which means that the train/test data are likely cross-contaminated.\nd) In Table 2, what error is being shown? Is it l1 norm, l2 norm, or absolute error of predicted age relative to annotated age?\ne) What is the actual loss function in eq 2? The loss for L_adv has been described on p6 but not the loss in eq2. Also, predicting age is a regression task, and not a discrimination task. Why is eq2 described as showing a discrimination task?\n6) In Table 3, why is the cyclegan result  significantly worse than other methods for MRI (it actually deteriorates the performance compared to baseline)? On the other hand for PET, cyclegan is better than other methods; The opposite trend is happening for domain adaptation (i.e. worse for PET and better for MRI). Is this related to the amount of training data? Otherwise, this would seem to indicate that perhaps cyclegan did not train properly due to implementation issues, etc.\n7) In Figs2-4, visual comparison of the translated images to real CT images have been provided. However, since unpaired data was used, it is not clear how anatomically correct the translated images are. It would be best if for a small number of samples that have paired data (i.e. patient was imaged using both CT and MR, or CT and PET), the authors show a comparison of the translated images from MR or PET compared to the actual patient images in CT. Such a comparison would show beyond doubt that the translated images are anatomically correct or not.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1187/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1187/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications", "authorids": ["~Yue_Yang2", "~Pengtao_Xie3"], "authors": ["Yue Yang", "Pengtao Xie"], "keywords": ["Deep learning", "Medical imaging", "Cross-Modal Learning"], "abstract": "While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|discriminative_crossmodal_data_augmentation_for_medical_imaging_applications", "supplementary_material": "/attachment/d88efeb97b88d4a20c5a44211a0d0246e9bb3fcd.zip", "pdf": "/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZpzVZpU0_2", "_bibtex": "@misc{\nyang2021discriminative,\ntitle={Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications},\nauthor={Yue Yang and Pengtao Xie},\nyear={2021},\nurl={https://openreview.net/forum?id=jDIWFyftpQh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jDIWFyftpQh", "replyto": "jDIWFyftpQh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1187/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124674, "tmdate": 1606915808643, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1187/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1187/-/Official_Review"}}}], "count": 6}