{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396579553, "tcdate": 1486396579553, "number": 1, "id": "B1hohfUug", "invitation": "ICLR.cc/2017/conference/-/paper429/acceptance", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance $\\eta/(1+t)^\\gamma$. There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.\n \n We encourage the authors to address these outstanding issues and to resubmit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396580109, "id": "ICLR.cc/2017/conference/-/paper429/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396580109}}}, {"tddate": null, "tmdate": 1485004296740, "tcdate": 1485004296740, "number": 2, "id": "r1-fRAgvg", "invitation": "ICLR.cc/2017/conference/-/paper429/official/comment", "forum": "rkjZ2Pcxe", "replyto": "S15rQgL4x", "signatures": ["ICLR.cc/2017/conference/paper429/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper429/AnonReviewer1"], "content": {"title": "I'm sympathetic to some of your argument, but...", "comment": "I am sympathetic to your point that you shouldn't be held to a higher standard (w.r.t. comparing against competitive baselines) than the other papers that also profess to improve the training of neural networks. Ideally, all of these paper would include comparisons against the other training strategies. That said, the paper remains, in my opinion, only an incremental contribution to the literature. \n\nI have increase my score by one point to take into account the updated text regarding the current state-of-the-art. \n\nOne point I would like to refute is your assertion that papers published on arXiv should somehow not be counted as prior art. I think this is a problematic perspective. Members of the research community benefit enormously by others making their work available and your argument would appear to undermine this resource. It also seems that you are being inconsistent with this perspective when you present citations of the arXiv version of this paper as evidence of its importance to the research community.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580383, "id": "ICLR.cc/2017/conference/-/paper429/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper429/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper429/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580383}}}, {"tddate": null, "tmdate": 1485001683257, "tcdate": 1481984112885, "number": 2, "id": "HyYuOpMVe", "invitation": "ICLR.cc/2017/conference/-/paper429/official/review", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["ICLR.cc/2017/conference/paper429/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper429/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. \n\nIn particular, the authors state \"However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work.\" This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.\n\nThe proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.\n\nUnfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512588302, "id": "ICLR.cc/2017/conference/-/paper429/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper429/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper429/AnonReviewer3", "ICLR.cc/2017/conference/paper429/AnonReviewer1", "ICLR.cc/2017/conference/paper429/AnonReviewer2"], "reply": {"forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512588302}}}, {"tddate": null, "tmdate": 1482342940910, "tcdate": 1482342940910, "number": 3, "id": "HJHmMSuVe", "invitation": "ICLR.cc/2017/conference/-/paper429/official/review", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["ICLR.cc/2017/conference/paper429/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper429/AnonReviewer2"], "content": {"title": "better than no noise, but lack of comparison with results in the literature", "rating": "4: Ok but not good enough - rejection", "review": "The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines).\n\nOn these networks, the question of optimization has so far not been studied as extensively as for more standard networks. A study specific to this class of models is therefore welcome. Results consistently show better optimization properties from adding noise in the training procedure.\n\nOne issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise. A comparison to results obtained in the literature would be desirable.\n\nFor example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy). I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512588302, "id": "ICLR.cc/2017/conference/-/paper429/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper429/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper429/AnonReviewer3", "ICLR.cc/2017/conference/paper429/AnonReviewer1", "ICLR.cc/2017/conference/paper429/AnonReviewer2"], "reply": {"forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512588302}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482266144357, "tcdate": 1478290434870, "number": 429, "id": "rkjZ2Pcxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkjZ2Pcxe", "signatures": ["~Luke_Vilnis1"], "readers": ["everyone"], "content": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482193069184, "tcdate": 1482193069184, "number": 6, "id": "ryBhueUVe", "invitation": "ICLR.cc/2017/conference/-/paper429/public/comment", "forum": "rkjZ2Pcxe", "replyto": "BJB-ZBxNx", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Author Response", "comment": "We thank the reviewer for their thorough review and comments. With respect to the comment that \u201cmany people may also add it to their repertoire of optimization tricks,\u201d we have indeed found that this is already the case. Since our original preprint came out, many practitioners have used this technique for optimizing their models, including at least 2 submissions to ICLR 2017 which we note in our response to Reviewer 1.\n\nWe too are interested in theoretical examinations and arguments for usefulness of this approach, and we cite some recent work in this area in our Related Work section, including Mobahi\u2019s \u201cTraining Recurrent Networks by Diffusion\u201d which analogizes the noise process to a low-pass filter applied to the objective function.\n\nWe agree that the work is quite similar to Langevin dynamics, but note that the combination with adaptive learning algorithms (which we find to be empirically very important in some experiments), makes it not quite equivalent to classic SGLD.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580506, "id": "ICLR.cc/2017/conference/-/paper429/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580506}}}, {"tddate": null, "tmdate": 1482192770385, "tcdate": 1482191682316, "number": 3, "id": "S15rQgL4x", "invitation": "ICLR.cc/2017/conference/-/paper429/public/comment", "forum": "rkjZ2Pcxe", "replyto": "HyYuOpMVe", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Author Response", "comment": "\nSummary of our Response:\n1. We did accidentally mischaracterize some recent recurrence-compatible normalization methods in our Related Work, and are uploading a fixed version.\n2. We agree that comparing the effects of normalization- and noise-based approaches to optimizing deep networks is a worthy research goal but those experiments are outside of the scope of this work for several reasons:\n           a. We perform many experiments investigating the effectiveness of annealed adaptive gradient noise in concert with other common optimization strategies, and show improvements on difficult tasks. We think this is in and of itself a valuable contribution.\n           b. The paper is already quite long, and as noted by Reviewer3 many of our experiments are large-scale, conducted on many thousands of computers.\n           c. Most of the relevant work on normalization is not published or peer-reviewed, and some is actually in submission to this same conference (without comparing to our method). \n           d. We do not make the argument that this is the only method to improve optimization for complex neural architectures, but that it is a simple and valuable tool to try out.\n3. With regard to whether the method is useful to the ICLR 2017 community: a preprint of this paper has been available for some time, and the method is already being cited and used by practitioners to improve their own models, some of which are quite contemporary and in submission to this same conference.\n\nFull rebuttal:\n\nWe thank the reviewer for pointing out the error in our Related Work section, which stated that batch-norm type techniques have never been shown to be effective for recurrent networks. While much of the paper including related work and fine-grained experiments are new compared to the previous version of our work, this was an oversight. We are updating the draft\u2019s Related Work section to better summarize the most recent (mostly non-peer reviewed) batch-norm style approaches.\n\nAdding experiments with batch normalization is outside of the scope of this work for several reasons. Firstly, we do not claim to make any arguments about whether our method is better or worse than a recurrence-compatible batch normalization, and view them as orthogonal. The purpose of our paper is to examine gradient noise and its interaction with several common (but by no means all) approaches for training deep architectures. In addition to demonstrating raw improvements on several complex architectures, we also perform a set of experiments to contrast the effect of an annealed, adaptive Gaussian noise from the batch-size dependent SGD noise, dropout, weight (non-adaptive) noise, and non-annealed noise. Because many of our experiments involve thousands of computers, it is not possible to easily conduct every experiment we might want. The paper is already quite long and includes many experiments, and we believe it performs a solid examination of the benefits of these various types of noise and the sort of models that can be best improved by them.\n\nWe also must note that although Weight Normalization and Normalization Propagation were published recently in NIPS 2016 and ICML 2016, the other mentioned algorithms such as Layer Normalization and Recurrent Batch Normalization, are not yet published and peer-reviewed parts of the literature. In fact, Recurrent Batch Normalization is in submission to this same conference. It does not contain any experiments comparing to our gradient noise approach, nor do we believe it necessarily should, as those authors demonstrate that their method is broadly useful compared to some reasonable baselines. Further, Normalization Propagation is proposed for non-recurrent architectures, and the only recurrent architecture considered in the Weight Normalization work is the DRAW model, which is quite different from the recurrent models we experiment with in our paper. \n\nConcerns about peer-review aside, we address the question of the method being out-of-date and otherwise not useful. Because, as the reviewer notes, preprint versions of this work have been available for some time, many other researchers have had the opportunity to cite it and use it in their own work, where it has helped them optimize similar architectures to those we explore in this work. A few recent examples include \u201cProgramming with a Differentiable Forth Interpreter,\u201d by Bo\u0161njak et al., and \u201cNeural Functional Programming\u201d by Feser et al., which are both submitted to this same conference.\n\nWe agree that comparison of the effectiveness of gradient noise when combined with recent applicable batch-norm style methods is a very interesting avenue for future work, but we do not feel that such a comparison is necessary to defend the contribution of this work. Our claim is not that this single method is the only way to get state-of-the-art results for every model, nor that it renders all other methods redundant, but that it constitutes a simple and often valuable tool for optimizing complex architectures.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580506, "id": "ICLR.cc/2017/conference/-/paper429/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580506}}}, {"tddate": null, "tmdate": 1482192297296, "tcdate": 1482192297296, "number": 4, "id": "H1-3Bx84x", "invitation": "ICLR.cc/2017/conference/-/paper429/public/comment", "forum": "rkjZ2Pcxe", "replyto": "rJLknDeEe", "signatures": ["~Luke_Vilnis1"], "readers": ["everyone"], "writers": ["~Luke_Vilnis1"], "content": {"title": "recurrent batch-norm", "comment": "Hi Tim,\n\nActually, there is a bunch of new content in this paper compared to the previously available draft. However, we agree that we left out some of the recent work on recurrent-compatible normalization methods in our Related Work section, and are updating it. \n\nExperiments combining normalization with noise are a good idea, and we would be interested to see those results. In our current work, we note a negative result combining gradient noise with standard LSTM language modeling, so these experiments would be most instructive if done on more complex architectures for e.g. program induction and algorithm learning. We discuss why those experiments are beyond the scope of this current paper in our response to Reviewer 1.\n\nBest,\nLuke\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580506, "id": "ICLR.cc/2017/conference/-/paper429/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580506}}}, {"tddate": null, "tmdate": 1481829342033, "tcdate": 1481829342024, "number": 2, "id": "rJLknDeEe", "invitation": "ICLR.cc/2017/conference/-/paper429/public/comment", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["~Tim_Cooijmans1"], "readers": ["everyone"], "writers": ["~Tim_Cooijmans1"], "content": {"title": "Use with batch-normalized recurrent networks", "comment": "This work was done quite a while ago, and in the meantime optimization of recurrent neural networks has been improved by various batch normalization-like schemes. Have you tried combining the technique with recurrent batch normalization[1], layer normalization[2] or weight normalization[3]? I'd be very curious to see whether the technique still brings improvements for normalized networks.\n\n[1] https://arxiv.org/abs/1603.09025\n[2] https://arxiv.org/abs/1607.06450\n[3] https://arxiv.org/abs/1602.07868"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580506, "id": "ICLR.cc/2017/conference/-/paper429/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580506}}}, {"tddate": null, "tmdate": 1481818365524, "tcdate": 1481818365516, "number": 1, "id": "BJB-ZBxNx", "invitation": "ICLR.cc/2017/conference/-/paper429/official/review", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["ICLR.cc/2017/conference/paper429/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper429/AnonReviewer3"], "content": {"title": "Review: Adding Gradient Noise Improves Learning for Very Deep Networks", "rating": "7: Good paper, accept", "review": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512588302, "id": "ICLR.cc/2017/conference/-/paper429/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper429/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper429/AnonReviewer3", "ICLR.cc/2017/conference/paper429/AnonReviewer1", "ICLR.cc/2017/conference/paper429/AnonReviewer2"], "reply": {"forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512588302}}}, {"tddate": null, "tmdate": 1480805046958, "tcdate": 1480805022119, "number": 1, "id": "Sk8i9axQe", "invitation": "ICLR.cc/2017/conference/-/paper429/public/comment", "forum": "rkjZ2Pcxe", "replyto": "rkRV90hMg", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "The negative results are likely because the learning problem for a well initialized net on MNIST is not difficult enough for the noise based exploration to help", "comment": "In general, it is likely that the performance could increase by careful hyperparameter tuning. With respect to the MNIST experiment, the gradient noise technique is helpful if we do not use the analytically-derived ReLU initialization techniques.  When we use good initialization, we did try tuning for the MNIST model and didn't find any benefit over the settings we ended up recommending, so we didn't report that. The negative results are likely because the learning problem for a well initialized net on MNIST is not difficult enough for the noise based exploration to help."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287580506, "id": "ICLR.cc/2017/conference/-/paper429/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkjZ2Pcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper429/reviewers", "ICLR.cc/2017/conference/paper429/areachairs"], "cdate": 1485287580506}}}, {"tddate": null, "tmdate": 1480546869900, "tcdate": 1480546869894, "number": 1, "id": "rkRV90hMg", "invitation": "ICLR.cc/2017/conference/-/paper429/pre-review/question", "forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "signatures": ["ICLR.cc/2017/conference/paper429/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper429/AnonReviewer3"], "content": {"title": "Couldn't the lack of hyperparameter tuning for the annealing scheme explain some of the negative results?", "question": "Couldn't the lack of careful hyperparameter tuning for the annealing scheme explain some of the negative results? This includes the MNIST experiments where gradient noise didn't seem to help."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "pdf": "/pdf/cf488a38dc0af2d1c4205d69a0b2c8b28a418c7c.pdf", "TL;DR": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "paperhash": "neelakantan|adding_gradient_noise_improves_learning_for_very_deep_networks", "keywords": [], "conflicts": ["cs.umass.edu", "google.com", "openai.com", "cs.toronto.edu"], "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959285671, "id": "ICLR.cc/2017/conference/-/paper429/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper429/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper429/AnonReviewer3"], "reply": {"forum": "rkjZ2Pcxe", "replyto": "rkjZ2Pcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper429/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959285671}}}], "count": 12}