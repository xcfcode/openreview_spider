{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028557407, "tcdate": 1490028557407, "number": 1, "id": "HylBfOtajg", "invitation": "ICLR.cc/2017/workshop/-/paper26/acceptance", "forum": "S1SED1MYe", "replyto": "S1SED1MYe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples for Semantic Image Segmentation", "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations\nthat lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "pdf": "/pdf/62219c5ec51b513cf8b47ee020a7b6d509d87db4.pdf", "TL;DR": "This work analyses the phenomenon of adversarial examples for the task of semantic image segmentation.", "paperhash": "fischer|adversarial_examples_for_semantic_image_segmentation", "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "conflicts": ["bosch.com", "uni-freiburg.de", "dfki.de", "uni-bremen.de"], "authors": ["Volker Fischer", "Mummadi Chaithanya Kumar", "Jan Hendrik Metzen", "Thomas Brox"], "authorids": ["volker.fischer@de.bosch.com", "chaithu0536@gmail.com", "janhendrik.metzen@de.bosch.com", "brox@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028558052, "id": "ICLR.cc/2017/workshop/-/paper26/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1SED1MYe", "replyto": "S1SED1MYe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028558052}}}, {"tddate": null, "tmdate": 1489053007502, "tcdate": 1489053007502, "number": 1, "id": "ryDLHjA9l", "invitation": "ICLR.cc/2017/workshop/-/paper26/public/comment", "forum": "S1SED1MYe", "replyto": "Hk2DDpace", "signatures": ["~Volker_Fischer1"], "readers": ["everyone"], "writers": ["~Volker_Fischer1"], "content": {"title": "Re: Interesting addition to adversarial examples", "comment": "Thank you very much for your review and feedback.\n\nWe would like to address the point of meaningful quantitative comparisons:\n* One main reason we used no other adversarial attacks in comparison was that we were restricted to targeted attacks (in order to achieve a specific target segmentation) and related work is typically non-targeted.\n* We validated our findings statistically on a subset of the cityscapes validation dataset (images containing enough pixels of person class, which were over half the validation images) also comparing the influence of different noise-sizes \\epsison (please also compare Fig. 2).\n* We plan to give a more sophisticated analysis / comparison in future work which we were not able due to the 3 page restriction for workshop submissions.\n\nWe want to thank the review again for their comments and hope this response clarifies the main point of concern.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples for Semantic Image Segmentation", "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations\nthat lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "pdf": "/pdf/62219c5ec51b513cf8b47ee020a7b6d509d87db4.pdf", "TL;DR": "This work analyses the phenomenon of adversarial examples for the task of semantic image segmentation.", "paperhash": "fischer|adversarial_examples_for_semantic_image_segmentation", "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "conflicts": ["bosch.com", "uni-freiburg.de", "dfki.de", "uni-bremen.de"], "authors": ["Volker Fischer", "Mummadi Chaithanya Kumar", "Jan Hendrik Metzen", "Thomas Brox"], "authorids": ["volker.fischer@de.bosch.com", "chaithu0536@gmail.com", "janhendrik.metzen@de.bosch.com", "brox@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487169325644, "tcdate": 1487169325644, "id": "ICLR.cc/2017/workshop/-/paper26/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper26/reviewers"], "reply": {"forum": "S1SED1MYe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487169325644}}}, {"tddate": null, "tmdate": 1488996195763, "tcdate": 1488996195763, "number": 2, "id": "Hk2DDpace", "invitation": "ICLR.cc/2017/workshop/-/paper26/official/review", "forum": "S1SED1MYe", "replyto": "S1SED1MYe", "signatures": ["ICLR.cc/2017/workshop/paper26/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper26/AnonReviewer2"], "content": {"title": "Interesting addition to adversarial examples", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents and interesting twist to adversarial examples: given a trained neural segmentation model, with slight changes to the input image that given a segmentation model, it is possible to change the segmentation prediction of certain classes or even individual objects without significantly changing the predictions to anything else. This is useful direction to explore and understand better.\n\nWeaknesses: the methods utilized are relatively similar to earlier works on adversarial examples, so the novelty of the approach is not very high. However The main weakness of the work is that the demonstration comes without any meaningful quantitative comparisons.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples for Semantic Image Segmentation", "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations\nthat lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "pdf": "/pdf/62219c5ec51b513cf8b47ee020a7b6d509d87db4.pdf", "TL;DR": "This work analyses the phenomenon of adversarial examples for the task of semantic image segmentation.", "paperhash": "fischer|adversarial_examples_for_semantic_image_segmentation", "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "conflicts": ["bosch.com", "uni-freiburg.de", "dfki.de", "uni-bremen.de"], "authors": ["Volker Fischer", "Mummadi Chaithanya Kumar", "Jan Hendrik Metzen", "Thomas Brox"], "authorids": ["volker.fischer@de.bosch.com", "chaithu0536@gmail.com", "janhendrik.metzen@de.bosch.com", "brox@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1488996196634, "id": "ICLR.cc/2017/workshop/-/paper26/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper26/AnonReviewer1", "ICLR.cc/2017/workshop/paper26/AnonReviewer2"], "reply": {"forum": "S1SED1MYe", "replyto": "S1SED1MYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper26/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1488996196634}}}, {"tddate": null, "tmdate": 1488234724958, "tcdate": 1488234724958, "number": 1, "id": "SJakYXGql", "invitation": "ICLR.cc/2017/workshop/-/paper26/official/review", "forum": "S1SED1MYe", "replyto": "S1SED1MYe", "signatures": ["ICLR.cc/2017/workshop/paper26/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper26/AnonReviewer1"], "content": {"title": "Review For Adversarial Examples For Semantic Image Segmentation", "rating": "7: Good paper, accept", "review": "The authors demonstrate how to build adversarial examples for pixel segmentations.\n\nPros:\n- Authors convincingly show how they can minimally distort an image so that it is perceptually identical yet the segmentation for person is entirely obliterated.\n- Authors show a single example that clearly demonstrates the effect where the person is entirely removed from the prediction. Additionally, the authors show some nice summary statistics where the person can be selectively removed quite reliably from the segmentation.\n- First paper to approach adversarial example generation for pixel segmentation.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples for Semantic Image Segmentation", "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations\nthat lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "pdf": "/pdf/62219c5ec51b513cf8b47ee020a7b6d509d87db4.pdf", "TL;DR": "This work analyses the phenomenon of adversarial examples for the task of semantic image segmentation.", "paperhash": "fischer|adversarial_examples_for_semantic_image_segmentation", "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "conflicts": ["bosch.com", "uni-freiburg.de", "dfki.de", "uni-bremen.de"], "authors": ["Volker Fischer", "Mummadi Chaithanya Kumar", "Jan Hendrik Metzen", "Thomas Brox"], "authorids": ["volker.fischer@de.bosch.com", "chaithu0536@gmail.com", "janhendrik.metzen@de.bosch.com", "brox@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1488996196634, "id": "ICLR.cc/2017/workshop/-/paper26/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper26/AnonReviewer1", "ICLR.cc/2017/workshop/paper26/AnonReviewer2"], "reply": {"forum": "S1SED1MYe", "replyto": "S1SED1MYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper26/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1488996196634}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487585163909, "tcdate": 1487169325001, "number": 26, "id": "S1SED1MYe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1SED1MYe", "signatures": ["~Volker_Fischer1"], "readers": ["everyone"], "content": {"title": "Adversarial Examples for Semantic Image Segmentation", "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations\nthat lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "pdf": "/pdf/62219c5ec51b513cf8b47ee020a7b6d509d87db4.pdf", "TL;DR": "This work analyses the phenomenon of adversarial examples for the task of semantic image segmentation.", "paperhash": "fischer|adversarial_examples_for_semantic_image_segmentation", "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "conflicts": ["bosch.com", "uni-freiburg.de", "dfki.de", "uni-bremen.de"], "authors": ["Volker Fischer", "Mummadi Chaithanya Kumar", "Jan Hendrik Metzen", "Thomas Brox"], "authorids": ["volker.fischer@de.bosch.com", "chaithu0536@gmail.com", "janhendrik.metzen@de.bosch.com", "brox@cs.uni-freiburg.de"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}