{"notes": [{"id": "rygoURNYvS", "original": "Sylt6Qw_wr", "number": 1151, "cdate": 1569439315078, "ddate": null, "tcdate": 1569439315078, "tmdate": 1577168257554, "tddate": null, "forum": "rygoURNYvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QR6ufLDQHj", "original": null, "number": 18, "cdate": 1576881096283, "ddate": null, "tcdate": 1576881096283, "tmdate": 1576881096283, "tddate": null, "forum": "rygoURNYvS", "replyto": "I-dzn5yAVm", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Regarding strong baselines", "comment": "The focus of our paper has been to study the effectiveness of the BERT-style pre-training for source code and to bring the latest advances in the NLP\u00a0field around pre-training to the ML-for-programming field. We therefore compared fine-tuning of CuBERT against end-to-end training (of both BiLSTM and Transformer models) and training with learned Word2Vec embeddings. These were the baselines we compared against.\n\nAs argued in the paper and subsequently in the rebuttal phase, even though our fine-tuning tasks are motivated by similar ones in the literature, those papers have used different languages or different formulations of the tasks, making it infeasible to directly compare against them. During the discussion, this was also acknowledged by the reviewer(s) as orthogonal to our contributions. Nevertheless, as suggested by two reviewers during rebuttal, we added an additional task, that of variable misuse localization and repair, and compared against the SOTA model from prior work.\n\nWhile we respect the decision of the reviewers/AC/PC on the paper, we find the rationale for the rejection (lack of strong baselines) to be incongruous with the reviews and the entire discussion."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "I-dzn5yAVm", "original": null, "number": 1, "cdate": 1576798715848, "ddate": null, "tcdate": 1576798715848, "tmdate": 1576800920683, "tddate": null, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents  CuBERT (Code Understanding BERT), which is BERT-inspired pretraining/finetuning setup, for source code contextual embedding. The embedding results are tested on classification tasks to demonstrate the effectiveness of CuBERT. \n\nThis is an interesting application paper that extends existing models to source code analysis. The authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. The authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers. \n\nOne major concern is the lack of strong baselines. All reviewers are concerned about this issue. The paper could lead to a good publication in the future if the issues can be addressed. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717082, "tmdate": 1576800267297, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Decision"}}}, {"id": "S1lNE5poor", "original": null, "number": 14, "cdate": 1573800492440, "ddate": null, "tcdate": 1573800492440, "tmdate": 1573800492440, "tddate": null, "forum": "rygoURNYvS", "replyto": "SJgw3J7osS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Re: Dataset Availability", "comment": "It would be great if the datasets and code can be shared so that readers can reproduce the results in the paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "HyeWwisosr", "original": null, "number": 13, "cdate": 1573792601291, "ddate": null, "tcdate": 1573792601291, "tmdate": 1573792601291, "tddate": null, "forum": "rygoURNYvS", "replyto": "r1gz76diir", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for the answers and clarifications. I read the new draft, and it addresses my concerns much better \u2014 I particularly appreciated addition of a more complex version of the VarMisuse task.\n\nMy recommendation will stay at \"Weak Accept\" as (a) the paper's modeling contributions are relatively weak, as opposed to pretraining/empirical contributions, and (b) as an application of BERT to the program analysis domain, it does not yet make sufficient use of the information available in the domain. That said, I agree with the authors that it could be left out of the scope of this paper, and will argue for acceptance nonetheless."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "r1gz76diir", "original": null, "number": 12, "cdate": 1573780761584, "ddate": null, "tcdate": 1573780761584, "tmdate": 1573780761584, "tddate": null, "forum": "rygoURNYvS", "replyto": "H1lNfO-9sB", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Follow-up", "comment": "Dear reviewer #3,\n\nWe hope that our response addresses your comments. Please let us know if you need any additional clarifications.\n\nThank you for your review."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "BygpADHoiB", "original": null, "number": 11, "cdate": 1573767125498, "ddate": null, "tcdate": 1573767125498, "tmdate": 1573767125498, "tddate": null, "forum": "rygoURNYvS", "replyto": "rkxNHZXoir", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "yes", "comment": "yes"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "rkxNHZXoir", "original": null, "number": 10, "cdate": 1573757243889, "ddate": null, "tcdate": 1573757243889, "tmdate": 1573757459862, "tddate": null, "forum": "rygoURNYvS", "replyto": "S1eV3hAqoS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "About the LSTM+pointer line in Table 5.", "comment": "We would like to clarify that the LSTM+pointer line in Table 5 represents exactly the model from Vasic et al. It's a unidirectional LSTM, with the pointer-prediction layers described in that work. Similarly to Vasic et al., it is trained end-to-end (without pre-training) on the task dataset.\n\nCan you please confirm if this addresses your comment about this table?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "SJgw3J7osS", "original": null, "number": 9, "cdate": 1573756847253, "ddate": null, "tcdate": 1573756847253, "tmdate": 1573756847253, "tddate": null, "forum": "rygoURNYvS", "replyto": "rylnnhfsir", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Dataset Availability", "comment": "We agree that public benchmarks are important for this community. It is true that we had to expend considerable effort to prepare datasets for this work and wish to contribute the outcome to the community. Our plan is to publish 1) the list of files (and versions) in our deduplicated pre-training dataset -- the actual files are already publicly available in GitHub or BigQuery's public GitHub datastore; 2) our entire finetuning datasets, 3) our pre-trained/finetuned models, 4) associated utilities, and 5) relevant vocabularies."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "rylnnhfsir", "original": null, "number": 8, "cdate": 1573756084245, "ddate": null, "tcdate": 1573756084245, "tmdate": 1573756084245, "tddate": null, "forum": "rygoURNYvS", "replyto": "HkxPK6RqsH", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Dataset", "comment": "Given that the modeling contributions of this work are relatively weak (CuBERT is a BERT architecture with relatively small changes) I am arguing for a \"weak accept\". A public dataset augments the existing contributions. So, to clarify my original response: Should the authors wish to keep the data private, my recommendation is still a \"weak accept\", but I won't fight for this work to get accepted and I can see the arguments for a \"weak reject\". If the data will be made public, then this makes this work more valuable and I am willing to argue for it to be accepted to the AC and the other reviewers.\n\nDatasets are valuable contributions. Papers need to be rewarded for the data collection/generation efforts independently of other methodological contributions to the field. I believe that ICLR should welcome dataset-based papers on important problems.\n\nAs R1 suggests, baselines on problems are important. But reimplementing them every time on new datasets (for different programming languages) isn't a realistic way for this field to progress. I suspect that the authors of this work have stumbled upon this problem (hence the lack of baselines/comparisons with prior work, e.g. Pradel et al. 2018, Allamanis et al. 2015, Allamanis et al. 2018, Cambronero et al. 2019, Raychev et al. 2015, etc). I don't think that the authors of this work should be penalized for the lack of good public datasets/code on the problems they study for a single programming language (Python in their case)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "HkxPK6RqsH", "original": null, "number": 7, "cdate": 1573739903498, "ddate": null, "tcdate": 1573739903498, "tmdate": 1573739903498, "tddate": null, "forum": "rygoURNYvS", "replyto": "HJxaoap9iH", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "On datasets", "comment": "While I also like the idea that the data should be public to help further work on top of the paper,  I do not believe it is right to demand it from the authors as a condition for accepting the paper.\n\nWhat is important is that not all their results are independent from the rest of the world, because this would limit the ability of the reader to compare it with future or past techniques (also will make it worse for everyone after them to publish without ignoring their work)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "S1eV3hAqoS", "original": null, "number": 6, "cdate": 1573739691573, "ddate": null, "tcdate": 1573739691573, "tmdate": 1573739691573, "tddate": null, "forum": "rygoURNYvS", "replyto": "B1l63cWcjH", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thank you for adding the additional results and discussing the way the dataset was collected. With this, at least one task can now be related to previous work. I know there is limited time, but at least at a later revision, it would be much more clear if the baselines for this task of Vasic et al are included in Table 5 (RNN w/o pretraining as in Vasic et al.)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "HJxaoap9iH", "original": null, "number": 5, "cdate": 1573735845366, "ddate": null, "tcdate": 1573735845366, "tmdate": 1573735845366, "tddate": null, "forum": "rygoURNYvS", "replyto": "rke73iWciB", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thank for your responses. I find the comments above satisfactory and the additional experiments helpful.\n\nOn the concerns of Reviewer #1, I understand that comparison with past work is not possible at this time (which is annoying) but due to the varied nature of the data that previous work has used. I'd argue that publishing the data used in this paper _could_ be a step towards achieving this in the community.  Because of this, I would like to argue for the acceptance of this work on the condition of publishing all the relevant (real and synthetic) data. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "ryeldyfcjS", "original": null, "number": 4, "cdate": 1573687144257, "ddate": null, "tcdate": 1573687144257, "tmdate": 1573687144257, "tddate": null, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Paper updated", "comment": "We thank the reviewers for their detailed comments and suggestions. We are responding to the individual comments below.\n\nWe have updated our writeup as follows:\n1) We have added a more complex finetuning task for joint classification, localization and repair of variable-misuse errors (based on Vasic et al. 2019), which we compare to the multi-headed pointer model from Vasic et al. (2019), trained and evaluated on our datasets (Section 4.7). The results show that CuBERT performs well on this task also.\n2) We have added an appendix (Appendix A) giving details about data generation for the finetuning tasks and include a discussion on the careful use of pseudorandomness towards dataset reproducibility.\n3) We have added clarifications and discussed possible extensions in the paper based on the reviewers\u2019 suggestions.\n\nWe would be happy to answer any additional queries.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "rke73iWciB", "original": null, "number": 3, "cdate": 1573686186643, "ddate": null, "tcdate": 1573686186643, "tmdate": 1573686186643, "tddate": null, "forum": "rygoURNYvS", "replyto": "BygAMHy5_H", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for the helpful comments and suggestions.\n\n>> Addition of more complex task\n\nWe have now added a more complex task (Section 4.7), that of joint classification, localization and repair of variable misuse errors as proposed in Vasic et al. (2019). This requires learning two pointers for localization and repair of variable misuse bugs, and is an extension of the variable misuse classification task we have already considered.\n\nWe had considered the variable and method naming tasks as candidate finetuning tasks. The masked language modeling (MLM) pre-training task works by masking/replacing tokens (Section 3.5) and training the network to predict them. Variable and method naming tasks would be very similar to MLM (wherein we mask the names and ask the network to predict the masked tokens) and hence, we decided not to include them in this submission.\n\nSince CuBERT produces only a pre-trained encoder, the docstring prediction and language modeling/autocomplete tasks would require learning a decoder from scratch. A recent work on transfer learning (\u201cExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u201d, https://arxiv.org/abs/1910.10683) recasts the BERT pre-training objective into a text-to-text setting, wherein both Transformer encoder and decoder are pre-trained. We consider this reformulation of BERT to be a great future avenue to enable direct finetuning for generative tasks like docstring prediction and autocompletion.\n\n>> Renaming the Variable Misuse task\n\nThe task we had described was a classification task where the model needs to identify if any of the variables in a function body is misused. To avoid any misunderstanding, we have now renamed it to \u201cVariable Misuse Classification\u201d. To also match the full task from Vasic et al. (2019), we now also have the \u201cVariable Misuse Localization and Repair\u201d task (Section 4.7).\n\n>> Dataset for the Function-Docstring Mismatch task and related references\n\nThe Py150 dataset is used in this task (and all other fine-tuning tasks). We have updated the writeup to make this clear. Thank you for the references, we have discussed them in the writeup now.\n\n>> Details on reproducibility\n\nWe have added an appendix (Appendix A) with the details of the dataset generation for the finetuning tasks, including a discussion of the careful use of pseudorandomness to ensure reproducible dataset generation. In addition, we plan to release the datasets for public use.\n\n>> Details on data generation and proportion of positive/negative examples\n\nWe have included these details in Appendix A in the revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "B1l63cWcjH", "original": null, "number": 2, "cdate": 1573685940853, "ddate": null, "tcdate": 1573685940853, "tmdate": 1573685940853, "tddate": null, "forum": "rygoURNYvS", "replyto": "SklbqFhatB", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for the helpful comments and suggestions.\n\n>> Lack of good baselines\n\nThough we consider a variety of tasks from the literature, the associated datasets came from different languages and varied sources. Due to the importance of keeping the pre-training corpus distinct from the finetuning corpus, we decided to set up all our finetuning tasks on the ETH Py150 corpus and constructed strong and suitable baselines. As baselines, we use multi-layered BiLSTMs and Transformers, both of which are widely-used architectures for sequential data. Since we study the transferability of pre-trained contextual embeddings, we have also trained and used 4 variants of (non-contextual) Word2Vec word embeddings. To our knowledge, these Word2Vec embeddings also constitute the only word embeddings trained for source code at a massive scale. As a result, we compare with not only strong architectures but with also pre-trained word embeddings of source code.\n\n>> Justification for high accuracy of BiLSTM models\n\nThere is certainly work in the literature that shows that results can be improved by incorporating additional annotations in the program representations. However, there is also a large body of work that shows impressive results with only tokenized source code representation. In the absence of systematic benchmarking and comparisons, we shy away from claiming either of them to be superior in general. For this work, we have put in extensive efforts to ensure that the baseline models are not at a disadvantage when compared to CuBERT. In particular, we have purposefully chosen bidirectional LSTMs since the Transformer encoder underlying CuBERT uses bidirectional context. Another potential reason for our BiLSTM models doing well is that many existing works in the literature use token-level vocabularies whose size is capped. This can result in out-of-vocabulary words and hampers the learning. We use a subword vocabulary (the same one that we use for CuBERT) which avoids this. Further, we use well-trained Word2Vec embeddings in our BiLSTM models.\n\n>> Variation in the performance of the BiLSTM models\n\nNote that compared to the other classification tasks, the Exception Type classification task is a multi-class problem with 20 classes and has a much smaller dataset (21K training examples). So the BiLSTMs do not perform that well on this task. In general, the model performance is subject to idiosyncrasies of the problem and dataset.\n\n>> Comparison to previous work\n\nFor reasons discussed above (under \u201cLack of good baselines\u201d), we had to design baselines ourselves. Nevertheless, we now present an additional pointer prediction task (Variable Misuse Localization and Repair), in which we compare CuBERT against the model proposed in Vasic et al. (2019) on our dataset.\n\n>> Appendix explaining details of design of finetuning tasks\n\nThank you for the suggestion to add more details about the design of the finetuning tasks in the appendix. We have added an appendix (Appendix A) with the requisite details and additional discussion.\n\n>> Comparison with Vasic et al. (2019)\n\nTo facilitate direct comparison between Vasic et al. (2019) and our work, we have now added the joint classification, localization and repair task proposed in Vasic et al. (2019). The results are presented in Section 4.7. The results are consistent with the results of our other finetuning tasks, in the sense that CuBERT outperforms the model from Vasic et al. (2019). For fair comparison, the results are obtained on the same dataset for both CuBERT and Vasic et al.\u2019s model (a unidirectional LSTM with two pointers). In their paper, they had done the evaluation only on a random sample of 12,218 test examples (Section 4, Benchmarks, from their paper), whereas we provide results on all of 430K test examples (Table 1, from our paper). We also use the same subword vocabulary for both models, whereas they had used a word-level fixed-size vocabulary.\n\n>> Are the results from the paper SOTA?\n\nOur finetuning tasks are motivated by the tasks in the literature. The reason we do not have one-to-one comparison with existing works is because the existing works use different languages, datasets, tokenizations, etc. (Section 3.4). For example, Pradel & Sen (2018) use limited surrounding context for classification, whereas we do classification at the level of function bodies. Therefore, even though our results are consistently high (>90% accuracy except for the Exception Type task), there are no references that we can appeal to for comparison. However, we now also have results on Variable Misuse Localization and Repair task (Section 4.7) where we evaluate against the model from Vasic et al. (2019) on our dataset, where CuBERT attains the highest accuracies.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "H1lNfO-9sB", "original": null, "number": 1, "cdate": 1573685259978, "ddate": null, "tcdate": 1573685259978, "tmdate": 1573685259978, "tddate": null, "forum": "rygoURNYvS", "replyto": "Hkgs8EAk5r", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for the helpful comments and suggestions.\n\n>> Choice to ignore program structure (e.g. abstract syntax trees) or features (e.g. types)\n\nNatural languages are also endowed with structure (e.g., different types of parse trees, phrase structures, etc.). However, the prevailing pre-training methods in NLP such as BERT do not make explicit use of such structure, and still attain state-of-the-art results. The task of learning useful (structural) features is left to the self-attention mechanism of the Transformer model. In this work, we apply the same approach to program-understanding tasks. We recognize that it may be possible to extend CuBERT with explicitly provided structural information using approaches like relation-aware Transformers (see \"Self-attention with relative position representations\", https://www.aclweb.org/anthology/N18-2074.pdf) in place of the usual Transformers based on sinusoidal positional encodings, and hope to try this in future work; this submission will provide a strong baseline to evaluate such future work. We thank the reviewer for raising this point. We now include this possibility as a future extension in Section 5, which we rename from \u201cConclusions\u201d to \u201cConclusions and Future Work\u201d.\n\nWith regard to types, we do not assume that the source code is written in a statically typed language and hence, do not use types as features. We train CuBERT for Python code, which is dynamically typed. We leave exploring type information for statically-typed languages for future work.\n\n>> Burden of program analysis\n\nThe reviewer\u2019s point is well taken. We have reworded our relevant text in the paper. To recap, our goal is to understand and evaluate a BERT-like pre-training approach (i.e., purely on lexical information) for program-understanding tasks, without exposing to the pre-training model additional information gleaned through program analysis.\n\n>> Simplicity of the Function-Docstring Mismatch task\n\nWe agree with the reviewer\u2019s observation. Nevertheless, the inherent ability of Transformer to relate every pair of tokens through self-attention plays a crucial role in CuBERT getting +7.5% improvement over the baseline model even on this relatively simple task.\n\n>> Utility of Next Sentence Prediction task\n\nA recent work has argued that using only the Masked Language Model objective (coupled with more training on larger datasets) can improve the performance of BERT (see \"RoBERTa: An optimized method for pretraining self-supervised NLP systems\", https://arxiv.org/abs/1907.11692). It will take more experimentation to check how inclusion/exclusion of next-sentence-prediction affects CuBERT.\n\n>> Explanation of similarity metric\n\nTwo files are considered similar to each other if the Jaccard similarity between the sets of tokens (identifiers and string literals) is above 0.8 and in addition, it is above 0.7 for multi-sets of tokens. This is based on the criteria used in Allamanis (2018). We have added this explanation in the revised version.\n\n>> Fraction of positive/negative examples in the finetuning tasks\n\nWe have provided these details in Appendix A now. To summarize, all classification tasks except Exception Type classification, and the new pointer task (Section 4.7), have a 50-50 split of buggy/bug-free examples. The per-class counts for the Exception Type classification task are shown in the (new) Table 6.\n\n>> Motivation for making Variable Misuse and Wrong Operator/Operand as classification tasks instead of correction tasks\n\nWe have now included experimentation for the joint task of classification, localization and repair of variable misuse errors from Vasic et al. (2019). Please see Section 4.7 for the results. The original Wrong Operator and Swapped Operand tasks from (Pradel & Sen 2018) are binary classification tasks similar to ours. They were not error correction tasks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygoURNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1151/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1151/Authors|ICLR.cc/2020/Conference/Paper1151/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160455, "tmdate": 1576860544319, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Authors", "ICLR.cc/2020/Conference/Paper1151/Reviewers", "ICLR.cc/2020/Conference/Paper1151/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Comment"}}}, {"id": "BygAMHy5_H", "original": null, "number": 1, "cdate": 1570530582307, "ddate": null, "tcdate": 1570530582307, "tmdate": 1572972506153, "tddate": null, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes a BERT-based pre-training for source code related task. By pre-training on BERT-like models on source code and finetuning on a set of 5 tasks, the authors show good performance improvements over non-pretrained models. The authors make a series of ablation studies showing that pre-training is indeed useful.\n\nOverall, I find this work relevant and interesting, albeit somewhat unsurprising. Nevertheless, I see no reason to reject this paper. To make my \"weak accept\" to a \"strong accept\" I would like to see experiments on more tasks, preferably more complex tasks. For example, such tasks could include (a) variable naming (b) method naming (c) docstring prediction/summarization (d) language modeling/autocompletion. I believe it's unclear to the reader if pre-training is also helpful for any of those tasks too and adding such experiments would significantly strengthen the paper.\n\nSome clarifications comments/questions to the authors:\n\n* I would insist that the authors rename the \"Variable Misuse\" task to \"Variable Misuse Localization\". To my understanding the current model points to the misused variable (if any), but does not attempt to suggest a fix. This tackles only a part of the task discussed in Vasic et al. (2019), Allamanis et al (2018) and this might confuse readers who want to compare with those works.\n\n* For the Function-Docstring Mismatch task (Section 3.4):\n    * It's unclear to me which dataset is used. Is it the Py150 dataset or the Barone & Sennrich (2017)?\n    * I believe that the citations [a], [b] would be appropriate here.\n\n* Overall, for the all the tasks except from \"Exception Type\", there is a replicability issue: Since the authors manually mutate the code (e.g. introduce a variable misuse, swap an operand), for anyone to compare directly, they would need access to the mutated samples. I would strongly encourage the authors to provide more details on how they create mutated samples and (eventually) the source code that achieves that.\n\n* For the Variable Misuse, Wrong Binary Operator, Swapped Operand tasks. There are a few things that need to be clarified:\n   * How long is each code snippet? One would expect that the longer the code snippet the harder the task. Do the authors pass a whole function?\n   * What is the proportion of positive/negative examples in each task?\n\n\n\n[a] Cambronero, Jose, et al. \"When Deep Learning Met Code Search.\" arXiv preprint arXiv:1905.03813 (2019).\n[b] Louis, Annie, et al. \"Deep learning to detect redundant method comments.\" arXiv preprint arXiv:1806.04616 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845737116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Reviewers"], "noninvitees": [], "tcdate": 1570237741609, "tmdate": 1575845737132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review"}}}, {"id": "SklbqFhatB", "original": null, "number": 2, "cdate": 1571830153314, "ddate": null, "tcdate": 1571830153314, "tmdate": 1572972506109, "tddate": null, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a transformer based approach to address a number of problems for machine learning from code. Then, the paper adds BERT-based pretraining to significantly improve on the results. The paper is nicely written and easy to follow, but with relatively thin contributions besides the large amount of experiments.\n\nInitially, I was quite positive on the entire paper, but as I read it in more details, I got less convinced that this is something I want to see at ICLR. First, there are no good baselines. BiLSTM gets quite high accuracy on most of the tasks, which is unexpected because most prior works show that the tasks benefit from some semantic understanding of code. I cannot relate any of the numbers with a previous work. Right now, I even have reduced confidence that the authors do not have bugs in the tasks or the reported numbers. Then, for the Operator and Operand tasks, BiLSTM is also doing impressively well (these tasks were done differently in prior works). Interestingly, things get reversed on the last two tasks. Given that most of the experiments are not the same as in any previous paper, I would strongly appreciate if much more details are given in the appendix. In fact, the appendix right now does not have much useful information besides praising how good job the attention is doing. What would be needed is information on how many samples were generated, how large were they, was any data thrown out? Table 1 is a good start, but it actually raises questions. You split the Python dataset into separate functions like Vasic et al and the number of functions 2x higher (counting the artificial samples, I guess), did you put a limit on how large functions you consider? 250 tokens was the limit of Vasic et al. To which of the tasks in the Vasic et al paper can I relate? Is the BiLSTM on the level of that work or it is substantially worse or better? Also, are the results coming from the paper SOTA or uncomparable to other works?\n\nFive tasks are evaluated, which is impressive. This is one reason I want to see the results published. The problem is also quite important. The experiments that show the effect on reduced labelled data are quite important and interesting - in fact, for many tasks, we can start curating datasets and having model working on small data will be crucial. However, I think the paper needs more work before it is something to present, cite or build upon.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845737116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Reviewers"], "noninvitees": [], "tcdate": 1570237741609, "tmdate": 1575845737132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review"}}}, {"id": "Hkgs8EAk5r", "original": null, "number": 3, "cdate": 1571968083375, "ddate": null, "tcdate": 1571968083375, "tmdate": 1572972506046, "tddate": null, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Summary\n\nThis paper presents a BERT-inspired pretraining/finetuning setup for source code tasks. It collects a corpus of\nunlabeled Python files for BERT pretraining, designs or adopts 5 tasks on established smaller-scale Python corpora, and\nadjusts the BERT model to tokenize and encode source code snippets appropriately.\n\n# Strengths\n\n* The idea of applying the pretraining/finetuning paradigm to program analysis tasks makes sense, and has been\n  informally attempted by multiple groups in the community in 2019. This is the first high-quality submission to a\n  top-tier ML conference I've seen on the subject, though.\n* The authors exercised commendable care and diligence in preparing the training data, adopting BERT to source code\n  inputs, and ensuring correctness of the experimental setup. I appreciated all the provided details on tokenization\n  (Section 3.3), deduplication (Sections 3.1-3.2), and task setup (Section 3.5). This should become a technical standard\n  in the community.\n* The paper is written clearly and concisely, and is generally a pleasure to read.\n\n# Weaknesses\n\nI have a gripe with the authors' choice to ignore program structure (e.g. abstract syntax trees) or features (e.g.\ntypes) in their program representation. Without this extra information (easily available from a compiler/interpreter\nAPI) the pipeline is not substantially different from the original NLP pipeline of BERT et al. The main program-related\nrepresentation insight comes in tokenization (Section 3.3) and the definition of \"sentences\". To repeat, I appreciate\nthe effort the authors put in making tokenization appropriate for BERT processing of source code, but this is a drop in\nthe bucket compared to the all the other program-related features the work is leaving off the table. Programs are not\nnatural language.\nThe argument that source code analysis would \"pass on the burden ... to downstream tasks\" (Page 3) is odd. First, most\ndownstream tasks of interest occur in the settings where this analysis is already available: IDEs, code review\nassistants, linters, etc. Second, one often needs program analysis to even define downstream tasks in the first place --\nfor example, determining whether function arguments are swapped required detecting a function call and boundaries of its\narguments, thus parsing the program!\n\nThis work obtains (and nicely analyzes) impressive results obtained by applying CuBERT. However, it does not put the\nresults in context with prior work based on structured program representations. Without this, it is difficult to say\nwhether the improvement comes from pretraining or from the language model simply learning a better \"parsed\"\nrepresentation of an input program from all the unlabeled corpus. If it's the latter, one might argue that supplying the\nmodel with structured program features explicitly might eliminate much of the need for the unlabeled corpus.\nI personally think that there will still be a gap between pretraining and finetuning even with structured program\nfeatures simply due to the sheer volume of available data, which, as the authors showed, is crucial for good\ngeneralization of Transformer. However, this still needs to be shown empirically.\n\nThe \"Function-Docstring Mismatch\" task, as presented, seems too easy. If the distractors (negative examples) are truly\nchosen at random, most of them are going to use obviously different vocabulary from the original function signature (as\nFigure 4 demonstrates). A well designed task would somehow bias the sampling toward subtle distractors such as `get` vs.\n`set` docstrings, but this seems challenging.\nThis also explains why the task is not influenced as much by reduction of training data (Table 3).\n\nThe Next Sentence Prediction pretraining task, as adapted for CuBERT, seems too difficult, in contrast. If the paired\nsentences (i.e. code lines) are chosen at random, the model would lack most of the context required to make a decision\nabout the logical relationship between them, such as which variables are defined and available in context, which\nfunctionality is being implemented, etc. I wonder, can the authors experiment with pretraining CuBERT only with the\nMasked Language Model task? Will it worsen the results substantially or at all?\n\n# Questions\n\nSection 3.2: \"similar files according to the same similarity metric...\"\nWhat are these metrics?\n\nWhat is the fraction of positive/negative examples in the constructed finetuning datasets?\n\nWhat is the motivation for making Variable Misuse and Wrong Operator/Operand into a simple classification tasks instead\nof the original (more useful) correction task?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1151/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pre-trained Contextual Embedding of Source Code", "authors": ["Aditya Kanade", "Petros Maniatis", "Gogul Balakrishnan", "Kensen Shi"], "authorids": ["akanade@google.com", "maniatis@google.com", "bgogul@google.com", "kshi@google.com"], "keywords": [], "abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "pdf": "/pdf/b73ef41c6dc4943f3ed388d1b3baf8c9f3f129ba.pdf", "paperhash": "kanade|pretrained_contextual_embedding_of_source_code", "original_pdf": "/attachment/c562d0433e23cf3335af18c3d084f4b2f26bb04a.pdf", "_bibtex": "@misc{\nkanade2020pretrained,\ntitle={Pre-trained Contextual Embedding of Source Code},\nauthor={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=rygoURNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygoURNYvS", "replyto": "rygoURNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1151/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845737116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1151/Reviewers"], "noninvitees": [], "tcdate": 1570237741609, "tmdate": 1575845737132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1151/-/Official_Review"}}}], "count": 20}