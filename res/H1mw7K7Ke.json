{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028567861, "tcdate": 1490028567861, "number": 1, "id": "rkgXdYpje", "invitation": "ICLR.cc/2017/workshop/-/paper45/acceptance", "forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028568449, "id": "ICLR.cc/2017/workshop/-/paper45/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028568449}}}, {"tddate": null, "tmdate": 1489611971832, "tcdate": 1489611971832, "number": 4, "id": "By363mwjg", "invitation": "ICLR.cc/2017/workshop/-/paper45/public/comment", "forum": "H1mw7K7Ke", "replyto": "B1IVoa8ox", "signatures": ["~Arash_Mehrjou1"], "readers": ["everyone"], "writers": ["~Arash_Mehrjou1"], "content": {"title": "Response to AnonReviewer2", "comment": "We appreciate your comments and acknowledgments of the differences of our method with \u201cinstance noise\u201d in that we only annealed the data distribution. We agree that in the current formulation, there is a clear correspondence between inverse beta (temperature) and the noise level. But we strongly believe that having \\emph{additive} noise is not crucial here, in that any quasi-static process can be used in this framework. This is in contrast to the theory developed by Arjovsky and Bottou where noise must be additive. We did not have enough space in the extended abstract to elaborate on the theory and provide more experiments. This would be treated in the extended version. We like to emphasise again that we developed this algorithm independently of \u201cinstance noise\u201d. Finally, we did not think of AIS reference as unnecessary -- the inspiration for annealing inverse temperature geometrically (instead of annealing noise linearly) was from that paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487274844037, "tcdate": 1487274844037, "id": "ICLR.cc/2017/workshop/-/paper45/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "reply": {"forum": "H1mw7K7Ke", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487274844037}}}, {"tddate": null, "tmdate": 1489586990327, "tcdate": 1489586990327, "number": 1, "id": "B1IVoa8ox", "invitation": "ICLR.cc/2017/workshop/-/paper45/official/comment", "forum": "H1mw7K7Ke", "replyto": "HyO3zXAcl", "signatures": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2"], "content": {"title": "Still seems to be some confusion over noise vs annealing", "comment": "Thanks for adding the requested revisions to the paper. There still seems to be some confusion over terminology however. You say that annealing is different from adding noise - yet the equation where you define the heated data distribution is just that of a Gaussian mixture model with centers at the data, which you would sample by picking elements of the dataset and...adding noise. How are you even able to sample in the infinite temperature limit? You say that temperature is more general than noise variance. In other contexts that may be true, but in your application here the temperature is literally *identical* to the variance of a Gaussian. They are one and the same. And the reference to annealed importance sampling seems wholly superfluous - you aren't computing an estimate of a partition function or running an MCMC chain of any sort.\n\nI sympathize with your desire to differentiate your method from instance noise. \"Adding noise\" sounds like a hack. \"Annealing\" sounds like you are forging a sword of Damascene steel. But in this context, if I understand your paper correctly, they are one and the same. It seems to me that the only distinction here is that you add the noise only to the data, and reduce the variance of the noise as training progresses. That's not nothing. But it is not fair to try to present it as somehow dramatically different from other approaches."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487274844037, "tcdate": 1487274844037, "id": "ICLR.cc/2017/workshop/-/paper45/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "H1mw7K7Ke", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper45/reviewers", "ICLR.cc/2017/workshop/paper45/areachairs"], "cdate": 1487274844037}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489540594267, "tcdate": 1489527395577, "number": 3, "id": "By3DMk8oe", "invitation": "ICLR.cc/2017/workshop/-/paper45/public/comment", "forum": "H1mw7K7Ke", "replyto": "SyioDzo5g", "signatures": ["~Saeed_Saremi1"], "readers": ["everyone"], "writers": ["~Saeed_Saremi1"], "content": {"title": "revisions", "comment": "We added a discussion section with new references, expanding the response below. We removed Figure 1 for space. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487274844037, "tcdate": 1487274844037, "id": "ICLR.cc/2017/workshop/-/paper45/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "reply": {"forum": "H1mw7K7Ke", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487274844037}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489527915138, "tcdate": 1487274843472, "number": 45, "id": "H1mw7K7Ke", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "H1mw7K7Ke", "signatures": ["~Arash_Mehrjou1"], "readers": ["everyone"], "content": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489527497404, "tcdate": 1489526823591, "number": 2, "id": "BygVgyIse", "invitation": "ICLR.cc/2017/workshop/-/paper45/public/comment", "forum": "H1mw7K7Ke", "replyto": "BJZTVFrjg", "signatures": ["~Saeed_Saremi1"], "readers": ["everyone"], "writers": ["~Saeed_Saremi1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your comments. We added the discussion section to the paper clarifying our formulation in relation to other related works that have addressed stabilization of GAN training. \n\nRegarding the question:  Both $\\theta_D$ and $\\theta_G$ depend on beta implicitly, since the generator and the discriminator are trained while the data distribution is being annealed. The notation in Algorithm 1 emphasizes this implicit dependence."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487274844037, "tcdate": 1487274844037, "id": "ICLR.cc/2017/workshop/-/paper45/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "reply": {"forum": "H1mw7K7Ke", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487274844037}}}, {"tddate": null, "tmdate": 1489503417272, "tcdate": 1489503417272, "number": 2, "id": "BJZTVFrjg", "invitation": "ICLR.cc/2017/workshop/-/paper45/official/review", "forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "signatures": ["ICLR.cc/2017/workshop/paper45/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper45/AnonReviewer1"], "content": {"title": "relation and comparisons to other work missing ", "rating": "5: Marginally below acceptance threshold", "review": "This work proposes a new method of stabilizing GAN training and encouraging the generator to cover the data distribution better (i.e. less mode dropping).  As I understand it they do so by gradually annealing the data distribution so that it initially has high entropy and gradually move towards the true distribution. \n\nWhile this approach is potentially promising, the paper in its current form lacks and discussion of its relation to other approaches to stabilizing GANs. I think this method needs to be placed in context to be better understood.\n\nFinally, I have one question: in algorithm 1, the generator network is written as a function of beta, but as I understand it only the data distribution is annealed, not the generator distribution? Please explain.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489503418005, "id": "ICLR.cc/2017/workshop/-/paper45/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2", "ICLR.cc/2017/workshop/paper45/AnonReviewer1"], "reply": {"forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489503418005}}}, {"tddate": null, "tmdate": 1489019568443, "tcdate": 1489019568443, "number": 1, "id": "HyO3zXAcl", "invitation": "ICLR.cc/2017/workshop/-/paper45/public/comment", "forum": "H1mw7K7Ke", "replyto": "SyioDzo5g", "signatures": ["~Arash_Mehrjou1"], "readers": ["everyone"], "writers": ["~Arash_Mehrjou1"], "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for your comments and the pointers to the literature. We approached this problem from a principled perspective, rooted in statistical mechanics and were not aware of the heuristics mentioned in the blog post and in the paper \u201cAmortised MAP Inference for Image Super-resolution\u201d. Regarding the first comment, we would like to emphasize that we did not add any noise to samples from the generator. Unfortunately, there is no space for comprehensive literature review in a three-page abstract, but we will briefly outline some important distinctions below, and expand on them in the long version of the work.\n\n\u2013 We should emphasize that temperature is a more general concept than noise variance. We start the training at beta=0 (infinite temperature), where all data distributions become uniform distribution and GAN stability is achieved in a unified framework for all f-divergences.\n\u2013 In Arjovsky et. al. (2017) noise parameters were treated as hyper-parameters related to the distance between data distribution and generative distribution. This is not needed in our framework, where we start from infinite temperature. \n\u2013 In Sonderby et. al. (2016) the noise was annealed in a linear way. We think it is more physical to anneal the inverse temperature in a geometric fashion. This intuition is rooted in physics and in the classic work \u201cAnnealed Importance Sampling\u201d by Radford Neal (2001). This last minor issue could become important for large datasets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487274844037, "tcdate": 1487274844037, "id": "ICLR.cc/2017/workshop/-/paper45/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "reply": {"forum": "H1mw7K7Ke", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487274844037}}}, {"tddate": null, "tmdate": 1488820130878, "tcdate": 1488820130878, "number": 1, "id": "SyioDzo5g", "invitation": "ICLR.cc/2017/workshop/-/paper45/official/review", "forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "signatures": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2"], "content": {"title": "Fails to address the relevant prior literature", "rating": "4: Ok but not good enough - rejection", "review": "The authors present a heuristic for improving the stability of training GANs by annealing the data distribution. With the exception that they do not add noise to samples from the generator (though I could be mistaken) the method seems identical to the instance noise heuristic proposed by Sonderby et al (http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/) and analyzed formally in Arjovsky and Bottou (2017), section 3. The authors do not discuss this connection anywhere. The paper also seems to be missing a discussion section. If these omissions were fixed and the contribution beyond instance noise were explained then this paper could be considered for acceptance, but not in its current state.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Annealed Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently emerged as powerful generative models. GANs are trained by an adversarial process between a generative network and a discriminative network. It is theoretically guaranteed that, in the nonparametric regime, by arriving at the unique saddle point of a minimax objective function, the generative network generates samples from the data distribution. However, in practice, getting close to this saddle point has proven to be difficult, resulting in the ubiquitous problem of \u201cmode collapse\u201d. The root of the problems in training GANs lies on the unbalanced nature of the game being played. Here, we propose to level the playing field and make the minimax game balanced by heating the data distribution. The empirical distribution is frozen at temperature zero; GANs are instead initialized at infinite temperature, where learning is stable. By annealing the heated data distribution, we initialized the network at each temperature with the learnt parameters of the previous higher temperature. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable, and proposed an algorithm in corollary. In our experiments, the annealed GAN algorithm, dubbed beta-GAN, trained with unmodified objective function was stable and did not suffer from mode collapse.\n", "pdf": "/pdf/248275d608e51c2703b94894c80576da1f7cce1a.pdf", "TL;DR": "We introduce a new algorithm to stabilize the training of generative adversarial networks and address the problem of mode collapse by \u201cheating\u201d the data distribution in an annealing framework.", "paperhash": "mehrjou|annealed_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["tuebingen.mpg.de", "berkeley.edu"], "authors": ["Arash Mehrjou", "Saeed Saremi"], "authorids": ["arash.mehrjou@tuebingen.mpg.de", "saeed@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489503418005, "id": "ICLR.cc/2017/workshop/-/paper45/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper45/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper45/AnonReviewer2", "ICLR.cc/2017/workshop/paper45/AnonReviewer1"], "reply": {"forum": "H1mw7K7Ke", "replyto": "H1mw7K7Ke", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper45/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489503418005}}}], "count": 9}