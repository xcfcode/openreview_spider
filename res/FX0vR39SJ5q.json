{"notes": [{"id": "FX0vR39SJ5q", "original": "0N_5aR3D98P2", "number": 1757, "cdate": 1601308193934, "ddate": null, "tcdate": 1601308193934, "tmdate": 1615358741773, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aaJfW2iGu1W", "original": null, "number": 1, "cdate": 1610040394491, "ddate": null, "tcdate": 1610040394491, "tmdate": 1610473989376, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper makes an innovative change to the adjacency matrix definition in graph convolutional neural networks (GCNs) (Kipf & Welling, 2017).  The change results in computationally-efficient isometric transformation invariance.   There were a number of concerns raised by reviewers, and the author responses and revisions, and the subsequent discussion, resulted in most of these concerns being satisfactorily addressed.  On reviewer continued to feel the paper was entirely theoretical and therefore not appropriate to ICLR, but that opinion was not shared more broadly and is not held by the area chair."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040394477, "tmdate": 1610473989360, "id": "ICLR.cc/2021/Conference/Paper1757/-/Decision"}}}, {"id": "8U1uaT3XD6V", "original": null, "number": 2, "cdate": 1604512405541, "ddate": null, "tcdate": 1604512405541, "tmdate": 1606766534245, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review", "content": {"title": "Strong core idea, but unclear section 3 and missing related work and baselines", "review": "Summary:\nThe paper proposes a network that operates on features of graphs that are embedded in a d-dim Euclidean space. The paper considers equivariance to a group G that is the direct product of permutations of N points and Euclidean transformations. The features they consider are tensor products of the N-dimensional natural representations of permutations and the d-dimensional standard representation of O(d). From the coordinates, an \u201cisometric adjacency matrix\u201d is created, which is such a tensor. This matrix is combined in various G-equivariant ways with the features and then linearly combined with learnable weights to create new features. These operations are interleaved with non-linearities to form the network. The authors compare to several graph network methods and show competitive performance on several tasks.\n\nStrengths:\n-\tThe authors build a bridge between the global permutation equivariant methods on graphs, as they consider matrix-like features and equivariant maps between them, and Euclidean-equivariant methods on point clouds. This combination seems a powerful approach to point-cloud neural networks.\n\nWeaknesses:\n-\tImportant pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Sch\u00fctt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018. \n-\tThe experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate.\n-\tThe core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and \u201cthe transformation invariant rank-2 tensor\u201d T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1? \n-\tIn section 3, the authors speak of \u201ccollections of rank-p tensors\u201d. However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2.  Except for in sec 3.2.2, in which a p=3 tensor has a=1. \n-\tIn Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it\u2019s the former, I don\u2019t see how the network is equivariant. If it\u2019s the latter, I don\u2019t understand the last paragraph of 3.2.2, which says 1H \\in R^{N x f_in}, which looks like a 0-tensor.\n-\tCan the authors clarify \u201cTo achieve translation equivariance, a constant tensor can be added to the output collection of tensors.\u201d? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance.\n-\tAm I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a na\u00efve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations).\n-\tThe authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices? \n\nRecommendation:\nIn its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I\u2019d like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score.\n\nSuggestions for improvement:\n-\tBe clear about what the G object is and what eq 1 means.\n-\tBe explicit about types the objects, be more explicit about the indices that refer to the permutation representation, to the indices that refer to the Euclidean representation and the indices that refer to copies of the same representation. I think there is an opportunity to be more clear, more explicit, while reducing notational clutter.\n-\tExpand the related work section\n-\tCompare to the strong baselines that use the coordinates.\n-\tProvide argumentation for the claim to scale to 1M vertices.\n\nMinor points:\n-\tEq 7, \\times should be \\otimes?\n-\tEq 14, what is j?\n-\tThe authors write: \u201cA, B and C are X, Y and Z respectively\u201d. Perhaps this could be re-written to the easier to read \u201cA=X, B=Y and C=Z\u201d. This happens each time the word \u201crespectively\u201d is used.\n-\tTable 3 typo, gluster -> cluster\n\n\n### Post rebuttal\nThe authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111291, "tmdate": 1606915771045, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review"}}}, {"id": "3DK5_x_eN70", "original": null, "number": 3, "cdate": 1604672279552, "ddate": null, "tcdate": 1604672279552, "tmdate": 1606726278630, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review", "content": {"title": "Equivariant GCN with lacking clarity", "review": "The paper proposes a graph convolutional network that can be in-/equivariant to isometric transformation. The method is applied to a linear toy problem of predicting the result of several diiferential operators and a nonlinear heat diffusion dataset in comparison to finite element analysis (FEA).\n\nA major problem of the paper is its lack of clarity, which is largely due to the unusual definition of established terms, for example:\n- The contraction (Eq. 4) is usually an operator defined for a single tensor, not two.\n- It is not completely clear to me how Eq. 3 is related to the convolution. Summing over index j look similar to a typical graph convolution, however, here you have an additional index l. Written loosely, a convolution usually has the form $\\sum_j f(i, j) g(j) $, but here you have $\\sum_j f(i, j) g(j, l)$.\n- The notion of a \"collection of tensors\" does not work well here, in particular when aiming to define a convolution (over a space or group). Given the particular application, the notion of tensor fields might be more appropriate.\n\nThe authors mention previous work on equivariant neural networks such as steerable convolutions, tensor field networks, and covariant networks, which they claim are inefficient since they use message-passing, while their approach uses GCNs. This contradicts their statement from above, that GCNs are message-passing neural networks. It should be clarified what exactly makes these approaches less efficient. Moreover, since equivariant neural networks are clearly more suitable for the presented experiments then previous GCNs, some of them should be included in the model comparisons with timings. In particular, going to tensors with large rank (as suggested in Eqs. 8-10) should be rather inefficient compared to equivariant networks using spherical harmonics due to the exponentially increasing number of parameters.\n\nRegarding the timings in Table 4, it is not clear to me, whether the timing include the preprocessing of the adjacency matrix for the inference. Also the caption mentions that a single CPU core was used. Does this refer to both the neural network and the FEA? Otherwise it does not seem like fair comparison.\n\nPros\n====\n- incorporating the various differential operators in the network structures in Sec 3.3 is an interesting idea\n\nCons\n====\n- lacking clarity in method description and notation\n- conceptual differences to steerable convolutions and equivariant networks do not become sufficiently clear\n- experiment section not very strong since comparison to equivariant networks is missing and the timings in Table 4 need some clarification\n\nUpdate: I read the comments of the authors and thank them for the clarifications. The additional baselines improved the paper. I raised my score to reflect that.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111291, "tmdate": 1606915771045, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review"}}}, {"id": "RgM5oq03l0w", "original": null, "number": 6, "cdate": 1606242058744, "ddate": null, "tcdate": 1606242058744, "tmdate": 1606242058744, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment", "content": {"title": "General comments", "comment": "We thank all the reviewers for the feedback and comments. We have uploaded the revised manuscript. The major changes are summarized as follows:\n* We updated the introduction section to emphasize the benefit of our results in the application to physical simulations.\n* We refined the related work sections to explain the origins of our key ideas, which are GCN (Kipf et al. 2017) and tensor field network (Thomas et al. 2018). \n* We cleaned up our notation regarding tensors and IsoAMs. That is a big change and we apologize for any inconvenience. However, we believe that the notation is now more consistent, and the discussion is easier to follow. \n* We added tensor field networks (Thomas et al. 2018) and SE(3)-Transformers (Fuchs et al. 2020) as equivariant baseline models in the experiments, showing the clear advantage of IsoGCN in terms of computation time.\n\nWe hope that our revised manuscript and responses could address all the questions and uncertainties."}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "FX0vR39SJ5q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1757/Authors|ICLR.cc/2021/Conference/Paper1757/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856035, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment"}}}, {"id": "56OZu040SI", "original": null, "number": 5, "cdate": 1606241305175, "ddate": null, "tcdate": 1606241305175, "tmdate": 1606241305175, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "cVX5lVY6Z7C", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for the time and effort in reviewing our work. We respond to the points in the review.\n\nQ. More explanations about the superiority of isometric transformation invariance and equivariance need to be addressed, since this purpose seems quite important throughout this paper. Besides, how this purpose helps to achieve the superiority needs to be answered.\n\nA. We added a discussion regarding the superiority of isometric transformation invariance and equivariance (sec 1). In the literature, many benefits have been discussed, such as high interpretability due to the shared weight, stability, and predictability of the model output, and the efficiency of training due to data augmentation not being required. All these points are essential, especially for learning physical simulations because every physical quantity and physical law is either isometric transformation invariant or equivariant.\n\nIn real applications, physical simulations can be performed under various isometric transformations. For instance, if we imagine simulating every window on a big building, each window has the same shape but a different position and thus is transformed isometrically. If we use a model without equivariance, when facing different inference results for windows A and B, we cannot determine why they are different because it may be due to the physical condition or lack of model equivariance. Using the equivariant model, one can determine that the physical condition is different and makes more reliable decisions. Thus, the model's stability and predictability are particularly important in the real application to physical simulations.\n\nQ. How to interpret the adjacency matrix up to m hops, what is the construction of the adjacency matrix mentioned in the paper?\n\nA. We added the definition of the adjacency matrix up to m hops and adjacency. Because we deal with mesh-structured data, we define two nodes that are adjacent when they share the same cell, which is commonly assumed in physical simulation methods (e.g., finite volume method and finite element method).\n\nQ. In the experiment results, though the proposed isoGCN gave the minimum loss when comparing with other GCN variants, the scale of loss is around 10^(-5/-6), which may not give a significant goodness.\n\nA. This is because we evaluated loss in the original scale (although we performed training with scaling). We prefer to evaluate at the original scale because it can be more intuitive, especially when dealing with physical data. Nevertheless, the absolute value has less meaning when comparing model performances. Looking at the standard error of the mean and qualitative comparison, we clearly see that IsoGCN is significantly better than other GCN-like models.\n\nQ. In table 4, though isoGCN gave the least running time, it also gave larger loss compared with the method FrontISTR. How to explain the trade-off between running time and the loss?\n\nA. In fact, the IsoGCN loss is smaller than that of the FrontISTR with $\\Delta t = 1.0$ for $|V| = 21,289$ and $155,019$ cases. Compared to FrontISTR with $\\Delta t = 0.5$, IsoGCN has a larger loss, as the reviewer pointed out. Because FrontISTR calculates the inverse matrix obtained from the partial differential equation, it can calculate interactions between vertices that are distant from each other. In contrast, IsoGCN looks only in a limited area to compute the next state of the considered vertex. Of course, it is possible to increase the number of hops considered by the IsoGCN, but this will lead to extending the computation time. To answer the question, the trade-off between running time and the loss is found here. Considering only a part of the graph (like IsoGCN) tends to lead to shorter computation time and moderate accuracy. Conversely, considering a huge or entire part of the graph (such as FrontISTR) tends to lead to longer computation time and high accuracy (this is also because FrontISTR is the ground truth).\n\nQ. The paper is too theoretical and does not fit to ICLR. Real impact and real application should be emphasized more.\n\nA. As pointed out, our work has a strong background in theory. However, IsoGCN demonstrates a real impact, that is, high accuracy and high stability due to the isometric transformation invariance and equivariance in addition to the significantly shorter computation time compared to a physical simulation, which was impossible with the existing equivariant models. Suppose the computation time of a machine learning model is longer than that of a physical simulation. In this case, the benefit of the machine learning model degrades because one has no reason to prefer the machine learning model to the physical simulation. Therefore, we can say that IsoGCN is the first model to learn physical simulations \"properly,\" in terms of both equivariance (please see the answer to the first question) and fast computation time."}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "FX0vR39SJ5q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1757/Authors|ICLR.cc/2021/Conference/Paper1757/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856035, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment"}}}, {"id": "axtAunp-z-O", "original": null, "number": 3, "cdate": 1606241037910, "ddate": null, "tcdate": 1606241037910, "tmdate": 1606241221172, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "8U1uaT3XD6V", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment", "content": {"title": "Response to AnonReviewer5 (part 1)", "comment": "We thank the reviewer for the time and effort in reviewing our work. We respond to the points in the review.\n\nQ. Important pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Sch\u00fctt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018.\n\nA. We added a reference to Maron et al. 2018 (Section 3.2) and removed the proof regarding permutation equivariance because this is already done, as you pointed out.\n\nQ. The experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate.\n\nA. Thank you for the suggestion. We added tensor field networks (Thomas et al. 2018) and SE(3)-Transformers (Fuchs et al. 2020) to the baseline models in the experiments (Section 4), and IsoGCN showed competitive performance against them and significantly faster computation time, which supports our claim.\n\nQ. The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and \"the transformation invariant rank-2 tensor\" T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1?\n\nA. We added explanations and figures in Section 3.1. T can be determined by users, as long as it meets the requirement (tensor rank and non-trainability). The point is that we would like to keep the assumption as small as possible to demonstrate that the concept of IsoGCN applies to a wide range of graphs. Thus, we have retained the definition of G abstract. In contrast, D is a specific form of G, but D can be obtained validly only for a graph derived from a mesh. Otherwise, we cannot define $M^{-1}$ in a general graph.\n\nQ. In section 3, the authors speak of \"collections of rank-p tensors\". However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2. Except for in sec 3.2.2, in which a p=3 tensor has a=1.\n\nA. Thank you for the suggestion. We have updated our notation to use \"tensor fields\" instead of \"collections of tensors.\" At the same time, we clarified the representation of each index, i.e., a rank-p tensor field is indexed $H_{i; g; k_1 k_2 ... k_p}$, where i corresponds to permutation, g denotes the feature index, and k_1, k_2, ..., k_p correspond to spatial indices. $G$ is indexed $G_{i, j;; k}$, where i and  j correspond to permutation, and k corresponds to spatial index. These are detailed at the beginning of Section 3 and 3.1.\n\nQ. In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it's the former, I don't see how the network is equivariant. If it's the latter, I don't understand the last paragraph of 3.2.2, which says 1H \\in R^{N x f_in}, which looks like a 0-tensor.\n\nA. There were typos in that section; we intended f_in and f_out to be the number of features and a rank-3 tensor field should be in the shape of $R^{|V| \\times f_\\mathrm{in} \\times d^3}$ (in case of input). We corrected them and it now should be clear."}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "FX0vR39SJ5q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1757/Authors|ICLR.cc/2021/Conference/Paper1757/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856035, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment"}}}, {"id": "YOgX9LbppdF", "original": null, "number": 4, "cdate": 1606241122388, "ddate": null, "tcdate": 1606241122388, "tmdate": 1606241122388, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "axtAunp-z-O", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment", "content": {"title": "Response to AnonReviewer5 (part 2)", "comment": "Q. Can the authors clarify \"To achieve translation equivariance, a constant tensor can be added to the output collection of tensors.\"? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance.\n\nA. We added clarification of that point at the end of Section 3.2.2. The point is to define x_ref as a reference point and then added it to an entire graph, which makes the graph transformed.\n\nQ. Am I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a na\u00efve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations).\n\nA. We typically use sparse matrices; thus the complexity does not scale cubically with the number of vertices. Appendix D.4 was updated to mention the sparsity of IsoAMs derived from meshes and complexity analysis. In our implementation (which is uploaded as supplementary material), the order scales cubically with the spatial dimension, which is usually small (e.g., 3).\n\nQ. The authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices?\n\nA. We updated Table 4 to show the results with 1M vertices. The reason for not having them in the first version was that the computation time of the ground truth was too long to meet the deadline.\n\nQ. Recommendation: In its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I'd like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score.\n\nA. Thank you for your positive comment. We refined Section 3 and added equivariant baseline models, which made improved our paper. We hope that our update resolves all the issues. Also, thank you for your suggestions. We incorporated them as much as possible and now think that our paper is much the better for it.\n\nQ. Minor points\n\nA. All are fixed (j in Eq 14 (Eq 16 in the updated version) should be l), while we retained a few \"respectively\"s when it sounds natural. Thank you again for the encouraging feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "FX0vR39SJ5q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1757/Authors|ICLR.cc/2021/Conference/Paper1757/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856035, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment"}}}, {"id": "eMr_Cw3NwAQ", "original": null, "number": 2, "cdate": 1606240878073, "ddate": null, "tcdate": 1606240878073, "tmdate": 1606240878073, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "3DK5_x_eN70", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for the time and effort in reviewing our work. We respond to the points raised in the review.\n\nQ. The contraction (Eq. 4) is usually an operator defined for a single tensor, not two.\n\nA. As pointed out, the contraction can be defined for a single tensor, but at the same time, the contraction between two or more tensors can be defined. For instance, please see p.4 in [1] or Eq. (4.22), (5.7) in [2]. Therefore, we concluded that our usage of \"contraction\" is common.\n\n\nQ. It is not completely clear to me how Eq. 3 is related to the convolution. Summing over index j look similar to a typical graph convolution, however, here you have an additional index l. Written loosely, a convolution usually has the form $\\sum_j f(i,j)g(j)$ , but here you have $\\sum_j f(i,j) g(j,l)$ .\n\nA. Our definition of convolution is based on GCN (Kipf et al. 2017). In GCN, we have $\\sum_j A_{ij} H_{jl}$, which is quite analogous to our definition. In fact, denoting $g(i, 1) = x(i), g(i, 2) = y(i), \\dots,$ we can confirm that $\\sum_j f(i,j) g(j,l)$ is a generalization of $\\sum_j f(i,j)g(j)$ . In this paper, we emphasize the relationship between our convolution and that in the GCN (Section 3.1).\n\nQ. The notion of a \"collection of tensors\" does not work well here, in particular when aiming to define a convolution (over a space or group). Given the particular application, the notion of tensor fields might be more appropriate.\n\nA. Thank you for your suggestion. We replaced the term a \"collection of tensors\" with a \"(discrete) tensor field,\" which also clarifies the permutation representation.\n\nQ. The authors mention previous work on equivariant neural networks such as steerable convolutions, tensor field networks, and covariant networks, which they claim are inefficient since they use message-passing, while their approach uses GCNs. This contradicts their statement from above, that GCNs are message-passing neural networks. It should be clarified what exactly makes these approaches less efficient.\n\nA. Thank you for the correction. We misused these terms. Our point is that message passing with nonlinear (usually deep) networks is more time-consuming than linear message passing, which is done in GCN (Kipf et al. 2017) and our work. We clarified our point in the paper ( Section 2). In addition, experimental results show that IsoGCN has significant computational efficiency compared to tensor field networks and SE(3)-Transformers. We hope that this is now clarified.\n\nQ. Moreover, since equivariant neural networks are clearly more suitable for the presented experiments then previous GCNs, some of them should be included in the model comparisons with timings.\n\nA. We updated our experiments to include tensor field networks (Thomas et al. 2018) and SE(3)-Transformer (Fuchs et al. 2020), and IsoGCN showed competitive performance against these models with significantly shorter computation time (Section 4 and especially Table 4).\n\nQ. In particular, going to tensors with large rank (as suggested in Eqs. 8-10) should be rather inefficient compared to equivariant networks using spherical harmonics due to the exponentially increasing number of parameters.\n\nA. We added a table summarizing the computation time on tasks outputting the rank-1 tensor and rank-2 tensor (Table 7). As pointed out, the computation time is extended when the rank-2 tensor is output to the rank-1 tensor. However, the magnitude of the increase is not as large as that of the SE(3)-Transformer. Moreover, in Appendix D.4, we added a discussion of the computational complexity of IsoGCN, showing that it increases exponentially regarding the spatial dimension d, which is usually very small (typically, it is 3).\n\nQ. Regarding the timings in Table 4, it is not clear to me whether the timing include the preprocessing of the adjacency matrix for the inference.\n\nA. Thank you for pointing this out. We updated Table 4 to put preprocessing time + inference time because we agree that this is a fairer comparison.\n\nQ. Also the caption mentions that a single CPU core was used. Does this refer to both the neural network and the FEA? Otherwise, it does not seem like fair comparison.\n\nA. Yes, we performed all the computations in Table 4 on a CPU. We have added a clarification regarding this point in the caption of Table 4.\n\n[1] Vojinovic, Marko. \"TENSOR CALCULUS.\" (2010).\n[2] Dullemond, K. \"Introduction to Tensor Calculus. Kees Dullemond and Kasper Peeters (1991).\"\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "FX0vR39SJ5q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1757/Authors|ICLR.cc/2021/Conference/Paper1757/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856035, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Comment"}}}, {"id": "cVX5lVY6Z7C", "original": null, "number": 1, "cdate": 1603588025782, "ddate": null, "tcdate": 1603588025782, "tmdate": 1605024364094, "tddate": null, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "invitation": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review", "content": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks. Summary: This paper proposed a transformation invariant and equivariant GCN, which mainly focused on the construction of the newly defined isometric adjacency matrix. Apply this isometric adjacency matrix to change the formulation of the GCN proposed by Kipf & Welling.", "review": "Comments:\nPros:\n1.\tThis paper gives a comprehensive derivation of propositions used in the construction of isoGCN. \n2.\tThe properties provided are clear\n3.\tThe idea to realize the purpose of being isometric transformation invariant and equivariant is good.\n\nCons:\n1.\tMore explanations about the superiority of isometric transformation invariance and equivariance need to be addressed since this purpose seems quite important throughout this paper. Besides, how this purpose helps to achieve the superiority needs to be answered.\n2.\tHow to interpret the adjacency matrix up to m hops, what is the construction of the adjacency matrix mentioned in the paper?\n3.\tIn the experiment results, though the proposed isoGCN gave the minimum loss when comparing with other GCN variants, the scale of loss is around 10^(-5/-6) which may not give a significant goodness.\n4.\tIn table 4, though isoGCN gave the least running time, it also gave larger loss compared with the method FrontISTR. How to explain the trade-off between running time and the loss?\n5.     The paper is too theoretical and does not fit to ICLR.  Real impact and real application should be emphasized more.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1757/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1757/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Isometric Transformation Invariant and Equivariant Graph Convolutional Networks", "authorids": ["~Masanobu_Horie1", "morita@ricos.co.jp", "hishinuma@ricos.co.jp", "ihara@ricos.co.jp", "mitsume@kz.tsukuba.ac.jp"], "authors": ["Masanobu Horie", "Naoki Morita", "Toshiaki Hishinuma", "Yu Ihara", "Naoto Mitsume"], "keywords": ["Machine Learning", "Graph Neural Network", "Invariance and Equivariance", "Physical Simulation"], "abstract": "Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.", "one-sentence_summary": "We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "horie|isometric_transformation_invariant_and_equivariant_graph_convolutional_networks", "supplementary_material": "/attachment/ef3f7b520cfdc434ce7f780bdede1c39abd179b8.zip", "pdf": "/pdf/dc6ec76f7b5bbad9a8a07d1ff870306354097c3c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhorie2021isometric,\ntitle={Isometric Transformation Invariant and Equivariant Graph Convolutional Networks},\nauthor={Masanobu Horie and Naoki Morita and Toshiaki Hishinuma and Yu Ihara and Naoto Mitsume},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=FX0vR39SJ5q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "FX0vR39SJ5q", "replyto": "FX0vR39SJ5q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111291, "tmdate": 1606915771045, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1757/-/Official_Review"}}}], "count": 10}