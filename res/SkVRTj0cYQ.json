{"notes": [{"id": "SkVRTj0cYQ", "original": "rJlLsXTqt7", "number": 853, "cdate": 1538087878309, "ddate": null, "tcdate": 1538087878309, "tmdate": 1545355420686, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkxuYr5fx4", "original": null, "number": 1, "cdate": 1544885632305, "ddate": null, "tcdate": 1544885632305, "tmdate": 1545354494549, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Meta_Review", "content": {"metareview": "Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The greatest concern was that the novelty beyond past work has not been sufficiently demonstrated.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Needs significant justification of novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper853/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper853/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353061651, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper853/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper853/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper853/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353061651}}}, {"id": "r1eXjyItn7", "original": null, "number": 2, "cdate": 1541132187058, "ddate": null, "tcdate": 1541132187058, "tmdate": 1542910809553, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "content": {"title": "Well-motivated problem, but incremental improvement over previous work?", "review": "[Post-rebuttal update] No author response was provided to address the reviewer comments. In particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern. I've left my overall score for the paper unchanged.\n\n[Summary] The authors propose a protocol for training a model over private user data in a federated setting. In contrast with previous approaches which tried to ensure that a model would not reveal too much about any individual data point, this paper aims to prevent leakage of information about any individual client. (There may be many data points associated with a single client.)\n\n[Key Comments] The submission generally seems polished and well-written. However, I have the impression that it's largely an incremental improvement over recent work by McMahan et al. (2018).\n* If the main improvement of this paper over previous work is the dynamic adaptation of weight updates discussed in Section 3, the experimental results in Table 1 should compare the performance of the protocol with vs. without these changes. Otherwise, I think it would be helpful for the authors to update the submission to clarify their contributions.\n* Updating Algorithm 1 / Line 9 (computation of the median weight update norm) to avoid leaking sensitive information to the clients would also strengthen the submission.\n* It would also be helpful if the authors could explicitly list their assumptions about which parties are trusted and which are not (see below).\n\n[Details]\n[Pro 1] The submission is generally well-written and polished. I found the beginning of Section 3 especially helpful, since it breaks down a complex algorithm into simple/understandable parts.\n\n[Pro 2] The proposed algorithm tackles the challenging/well-motivated problem of improving federated machine learning with strong theoretical privacy guarantees.\n\n[Pro 3] Section 6 has an interesting analysis of how the weight updates produced by clients change over the course of training. This section does a good job of setting up the intuition for the training setup used in the paper, where the number of clients used in each round is gradually increased over the course of training.\n \n[Con 1] I had trouble understanding the precise threat model used in the paper, and I think it would be helpful if the authors could update their submission to explicitly list their assumptions in one place. It seems like the server is trusted while the clients are not. However, I was unsure whether the goal was to protect against a single honest-but-curious client or to protect against multiple (possibly colluding) clients.\n\n[Con 2] During each round of communication, the protocol computes the median of a set of values, each one originating from a different client, and the output of this computation is used to perform weight updates which are sent back to the clients. The authors note that \"we do not use a randomized mechanism for computing the median, which, strictly speaking, is a violation of privacy. However, the information leakage through the median is small (future work will contain such privacy measures).\" I appreciate the authors' honesty and thoroughness in pointing out this limitation. However, it does make the submission feel like a work in progress rather than a finished paper, and I think that the submission would be a bit stronger if this issue was addressed.\n\n[Con 3] Given the experimental results reported in Section 4, it's difficult for me to understand how much of an improvement the authors' proposed dynamic weight updates provide in practice. This concern could be addressed with the inclusion of additional details and baselines:\n* Few details are provided about the model training setup, and the reported accuracy of the non-differentially private model is quite low (3% reported error rate on MNIST; it's straightforward to get 1% error or below with a modern convolutional neural network). The authors say they use a setup similar to previous work by McMahan et al. (2017), but it seems like that paper uses a model with a much lower error rate (less than 1% based on a cursory inspection), which makes direct comparisons difficult.\n* The introduction argues that \"dynamically adapting the dp-preserving mechanism during decentralized training\" is a significant difference from previous work. The claim could be strengthened if the authors extended Table 1 (experimental results for differentially private federated learning) in order to demonstrate the effect of dynamic adaptation on model quality.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper853/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "cdate": 1542234362054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper853/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817169, "tmdate": 1552335817169, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper853/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1g9TUss3m", "original": null, "number": 3, "cdate": 1541285569930, "ddate": null, "tcdate": 1541285569930, "tmdate": 1541533635830, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "content": {"title": "Differentially private variant of the federated learning framework", "review": "The paper revisits the federated learning framework from McMahan in the context of differential privacy.  The general concern with the vanilla federated learning framework is that it is susceptible to differencing attacks. To that end, the paper proposes to make the each of the interaction in the server-side component of the gradient descent to be differentially private w.r.t. the client contributions. This is simply done by adding noise (appropriately scaled) to the gradient updates.\n\nMy main concern is that the paper just described differentially private SGD, in the language of federated learning. I could not find any novelty in the approach. Furthermore, just using the vanilla moment's accountant to track privacy depletion in the federated setting is not totally correct. The moment's accountant framework in Abadi et al. uses the \"secrecy of the sample\" property to boost the privacy guarantee in a particular iteration. However, in the federated setting, the boost via secrecy of the sample does not hold immediately. One requirement of the secrecy of the sample theorem is that the sampled client has to be hidden. However, in the federated setting, even if one does not know what information a client sends to the servery, one can always observe if the client is sending *any* information. For a detailed discussion on this issue see https://arxiv.org/abs/1808.06651 .", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper853/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "cdate": 1542234362054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper853/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817169, "tmdate": 1552335817169, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper853/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkenUlAliQ", "original": null, "number": 1, "cdate": 1539526740246, "ddate": null, "tcdate": 1539526740246, "tmdate": 1541533635376, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "content": {"title": "interesting direction but confusing presentation", "review": "The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. \n\nSome of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:\n1.\tThe main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record \u2013 why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork & Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?\n2.\tAnother issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals. Therefore, are the sizes used in the experiments reasonable?\n3.\tIn the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016\n4.\tThe theoretical analysis of the algorithm is only implied and not stated clearly\n5.\tIn reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?\n6.\tIn describing the experiment it says that \u201cFor K\\in\\{1000,10000} data points are repeated.\u201d This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?\n7.\tSince grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds\n8.\tThe results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.\n\nAdditional comments\n9.\tThe introduction is confusing since it uses the term \u201cfederated learning\u201d as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.\n10.\tIn the abstract the term \u201cdifferential attacks\u201d is used \u2013 what does it mean?\n11.\t\u201cAn independent study McMahan et al. (2018), published at the same time\u201d- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.\n12.\tIn the section \u201cChoosing $\\sigma$ and $m$\u201d it is stated that the higher \\sigma and the lower m, the higher the privacy loss. Isn\u2019t the privacy loss reduced when \\sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. \n13.\tAt the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.\n14.\tIn the experiments (Table 1), what does it mean for \\delta^\\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?\n15.\tThe methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?\n16.\tThe citations are used incorrectly, for example \u201csometimes referred to as collaborative Shokri & Shmatikov (2015)\u201d should be \u201csometimes referred to as collaborative (Shokri & Shmatikov, 2015)\u201d. This can be achieved by using \\citep in latex. This problem appears in many places in the paper, including, for example, \u201cwe make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).\u201d Which should be \u201cwe make use of the moments accountant as proposed by Abadi et al. (2016).\u201d In which case you should use only \\cite and not quote the name in the .tex file.\n17.\t\u201cWe use the same de\ufb01nition for differential privacy in randomized mechanisms as Abadi et al. (2016):\u201d \u2013 the definition of differential privacy is due to Dwork, McSherry, Nissim & Smith, 2006\n18.\tNotation is followed loosely which makes it harder to follow at parts. For example, you use \u201cm_t\u201d for the number of participants in time t but in some cases,  you use only m as in \u201cChoosing $\\sigma$ and $m$\u201d.\n19.\tIn algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. \n20.\tFigure 2: I think it would be easier to see the results if you use log-log plot\n21.\tDiscussion: \u201cFor K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.\u201d \u2013 it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.\n22.\tIn the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan\n\n\nIt is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper853/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Official_Review", "cdate": 1542234362054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper853/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817169, "tmdate": 1552335817169, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper853/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygeEICS3X", "original": null, "number": 6, "cdate": 1540904487857, "ddate": null, "tcdate": 1540904487857, "tmdate": 1540904487857, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "rkgvwEsl2m", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "Thank you very much for your question.\nOn page 4, in the section 'Choosing S' we provide information about the scale. As proposed by Abadi et al. (2016) we chose S to be the median of the second norms of the client contributions. We thereby ensure that the noise does not explode when a client provides very large updates but also do not trim too much of the true updates. ", "title": "Choosing S"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}, {"id": "rkgvwEsl2m", "original": null, "number": 5, "cdate": 1540564062694, "ddate": null, "tcdate": 1540564062694, "tmdate": 1540564062694, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "It is unclear about the scale of the clip bound S. Could you please\nadd some details about the scale of the S, due to the S\nis a key factor to the final performance. ", "title": "The scale of the S"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}, {"id": "Bkx4AgGjim", "original": null, "number": 4, "cdate": 1540198603637, "ddate": null, "tcdate": 1540198603637, "tmdate": 1540198603637, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "rJl-Xe68j7", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "Thank you for the comment. Indeed this is a mistake. In line 10 of the algorithm, \\sigma must be replaced with \\sigma_t. \nThe parameter variance is introduced when defining the between clients variance (it is just an in-between step to make that definition easier). \n\nIn the discussion we explain: \nFor a certain noise scale at iteration t: \\frac{sigma^2_t}{m_t}, the privacy loss is smaller for both sigma_t and m_t being small. Now if the clients provided very similar updates we would therefore go for small sigma_t and small m_t. But if the clients provided very distinct updates, a communication round with a small m_t would not improve the model even if the overall noise scale didn't change. (remember: In federated learning client-data might be non-IID).\n\nWe show that over the course of federated learning (for highly non-IID clients) the similarity of updates decreases (between clients variance increases) and it is therefore advantageous to start with a low m_t and keep increasing it during subsequent iteration rounds. If \\sigma_t is held constant for all t that means the noise scale decreases over the course of training. \n\nThe precise choices of \\sigma_t and \\m_t over the course of training highly depend on the  federated learning scenario (the privacy budget, the data, the amount of clients and how data is distributed among them). We therefore cannot give a general iterative rule in the algorithm but just provide a tendency to be followed when these parameters are to be chosen for a new setting.", "title": "Missing _t"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}, {"id": "BklIfi7voQ", "original": null, "number": 3, "cdate": 1539943182200, "ddate": null, "tcdate": 1539943182200, "tmdate": 1539944216581, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "S1x_i3hUsm", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "Thank you very much for your question.\nWe do point out the similarity to LEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS in the introduction. The research was conducted at the same time as ours but published at last year\u2019s ICLR-conference, whereas we presented ours at a workshop.\n\nThe reason why we now decided to aim for a conference-publication is that the two research projects aimed at opposite extreme cases and we want to motivate research in ours. \nLEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS shows that with lots of clients, performance of language models can be maintained high while privacy is ensured. The work is centered around mobile phone users where hundreds of millions of clients are a realistic scenario. \nDIFFERENTIALLY PRIVATE FEDERATED LEARNING: A CLIENT LEVEL PERSPECTIVE aims at the other extreme. We were primarily interested in institutions such as hospitals jointly learning models. In these scenarios, the number of clients (e.g. hospitals) could be as low as a hundred. We want to motivate research of differentially private federated learning in this less commercial area and point out its potential for hospitals, laboratories and universities that have high privacy standards but could greatly benefit from one another (e.g. the authors of [Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation] state that the integration of differential privacy into their research would make it applicable to sensitive data.) In our research we focus on different phases of federated learning and how low numbers of participants influence these phases and the privacy loss during them. \n\nTLDR;\nWe did not want to show: 'We can include privacy into already existing language models learned from millions of clients without drawbacks'\nBut instead: 'Hospitals, labs or universities, that do not cooperate in learning models as of today, could greatly benefit from one another without revealing sensitive information.'", "title": "Opposite extreme cases"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}, {"id": "rJl-Xe68j7", "original": null, "number": 2, "cdate": 1539915801402, "ddate": null, "tcdate": 1539915801402, "tmdate": 1539915857769, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "It remains unclear in the algorithm. Firstly, what's the purpose to introduce the parameter variance V?\nThen, the author used \\sigma_t as the noise scale,  but used \\sigma  in the following updating.\nPlease give some comment on how to change the noise scale at each iterative step. \n", "title": "Please give a more clear desribtion of the algorithm part."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}, {"id": "S1x_i3hUsm", "original": null, "number": 1, "cdate": 1539914911947, "ddate": null, "tcdate": 1539914911947, "tmdate": 1539914911947, "tddate": null, "forum": "SkVRTj0cYQ", "replyto": "SkVRTj0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "content": {"comment": "This work is similar paradigm to LEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS. So, what's the\ndifference to previous work?  ", "title": "What's the contribution of this paper?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper853/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. \nIn this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. \nHowever, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. \nWe tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. \nEmpirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. ", "keywords": ["Machine Learning", "Federated Learning", "Privacy", "Security", "Differential Privacy"], "authorids": ["geyerr@ethz.ch", "tassilo.klein@sap.com", "moin.nabi@sap.com"], "authors": ["Robin C. Geyer", "Tassilo J. Klein", "Moin Nabi"], "TL;DR": "Ensuring that models learned in federated fashion do not reveal a client's participation.", "pdf": "/pdf/b0820366c53400ca4be650c144b551219c0b5fe2.pdf", "paperhash": "geyer|differentially_private_federated_learning_a_client_level_perspective", "_bibtex": "@misc{\ngeyer2019differentially,\ntitle={Differentially Private Federated Learning: A Client Level Perspective},\nauthor={Robin C. Geyer and Tassilo J. Klein and Moin Nabi},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVRTj0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper853/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311737353, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVRTj0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper853/Authors", "ICLR.cc/2019/Conference/Paper853/Reviewers", "ICLR.cc/2019/Conference/Paper853/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311737353}}}], "count": 11}