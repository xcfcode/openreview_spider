{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396376115, "tcdate": 1486396376115, "number": 1, "id": "S1xy2GUdg", "invitation": "ICLR.cc/2017/conference/-/paper136/acceptance", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers present a detailed set of concerns regarding the paper. In particular, the paper lacks comparison to other sketching works. The sketches used in the paper are rudimentary and in practice, there are more sophisticated sketches employed.\n \n Sketching has a different goal from function approximation. Sketching aims to reconstruct (with a certain error). The paper uses sketching in a naive manner, in the sense that the error due to the sketch is incorporated into the overall error. But presumably, networks can be compressed without achieving reconstruction on each instance, while maintaining a guarantee on the overall accuracy. I would find such a framework to be more interesting and practically relevant.\n \n On the other hand, it is important for the ICLR community to be exposed to sketching frameworks. Hence, I would like to invite the paper to be presented in the workshop, in the hope that it can spur further ideas.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396376697, "id": "ICLR.cc/2017/conference/-/paper136/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396376697}}}, {"tddate": null, "tmdate": 1483668259213, "tcdate": 1483668259213, "number": 4, "id": "ByiQiO3Hx", "invitation": "ICLR.cc/2017/conference/-/paper136/official/review", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer4"], "content": {"title": "Interesting paper", "rating": "4: Ok but not good enough - rejection", "review": "Summary: This paper presents a novel sketching method in solving sparse polynomial function of a sparse binary vector within a single-layer neural network. \n\nPros:1. The paper is written clearly. \n     2. I like the idea of using improper learning to reduce the dimensionality.\n     3. It's novel to use neural network to do sketching decoding.\nCons:1. The experiments part is kind of weak in comparison with other sketching/projection methods.\n     2. Any theoretical analysis to show why this approach needs fewer paramerters than feature hash, etc. My other concern is that since t and m are tunable, how to set these parameter for different problems? \n     3. The authors claim they have faster training. However it's not shown in experiments.\nComments: It may be interesting to compare with sparse feed-forward neural networks.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483668259751, "id": "ICLR.cc/2017/conference/-/paper136/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper136/AnonReviewer1", "ICLR.cc/2017/conference/paper136/AnonReviewer3", "ICLR.cc/2017/conference/paper136/AnonReviewer2", "ICLR.cc/2017/conference/paper136/AnonReviewer4"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483668259751}}}, {"tddate": null, "tmdate": 1482258971604, "tcdate": 1482258971604, "number": 9, "id": "HkNX9xPVl", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "HyP8A4VEe", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "writers": ["~Kunal_Talwar1"], "content": {"title": "reply to AnonReviewer2 comment", "comment": "The reviewer seems to be pointing to the improper learning aspect of our work. Recall that improper learning is the idea of learning a function f from a hypothesis class H by using a function g from a larger hypothesis class H\u2019. This approach is often used to make learning more efficient computationally. From a learning point of view, the primary motivation for restricting f to be from H in the first place is to get good generalization, and using an H\u2019 that is not \u201cmuch larger\u201d suffices for this purpose. It is very rarely the case that g not being in H is an issue. Most previous applications of improper learning allow us to get around NP-hardness results and get polynomial time learning. In our setting, we use it to get faster running times.\n\nIn our experiments, we could have added more constraints to make our H\u2019 even smaller as the reviewer suggests. We view the fact that we did not have to do that as a feature: the simplicity allows us to deploy our sketching approach with little changes to modern learning infrastructures. Our theorems hold for exactly the setting that we use in the experiments: the theorems show that H\u2019 contains (good approximations to) every f in H. They do not require that H\u2019 does not contain anything else."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1482079823426, "tcdate": 1482079823426, "number": 1, "id": "HyP8A4VEe", "invitation": "ICLR.cc/2017/conference/-/paper136/official/comment", "forum": "r1br_2Kge", "replyto": "HydN_QfEe", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "content": {"title": "inconsistency btw theory and experiments", "comment": "Thank you for your comment. \n\nThe one layer neural network space may represent functions outside the functions induced by the hashing studied in the paper. Hence if we don't restrict to this class, we don't know if we are really evaluating this class or not. We might be learning a function outside the function class we are studying. This is my main concern with the empirical evaluation. I think the paper will be more coherent if those issues are addressed.  \n\nOtherwise another analysis is needed for the type of learning done in the experiments, using the similarity induced by the hashing between two points, and see how it approximates the original function space. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713485, "id": "ICLR.cc/2017/conference/-/paper136/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713485}}}, {"tddate": null, "tmdate": 1481943295231, "tcdate": 1481943295231, "number": 8, "id": "HkPbtmzNg", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "B1IqQJhzx", "signatures": ["~Nevena_Lazic1"], "readers": ["everyone"], "writers": ["~Nevena_Lazic1"], "content": {"title": "Some experiments comparing Sketches and MinHash", "comment": "We performed additional experiments comparing sketching the input to using as a MinHash as an input layer. We generated synthetic datasets for sparse linear and polynomial regression using settings similar to those described in Section 6. Denote by k be the number of non-zero features per example. We reduced dimensionality using the following settings:\nSketches: 2^3 blocks of size 2^3 * k\nMinHash: 2^3 * k blocks of size 2^3 (3 bits)\nThis resulted in the two representations having equal size (2^6*k) and equal number of nonzero entries (2^3*k).  We used an efficient implementation of MinHash using hash functions, as described in https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/a13-li.pdf. We used 4-universal hash functions for both MinHash and Sketches. Note that MinHash is more expensive to compute, as it scales as k^2, whereas Sketches are linear in k.  We found that Sketches substantially outperform MinHash. \nOn the linear regression task (k=50, s1=50, |I|=50, 25 \u201cnoise\u201d features per example, d=10K), MinHash mean squared error (MSE) was 1.39 (0.35), whereas the MSE with Sketches was 0.23 (0.05), over 50 generated datasets. \nOn the polynomial regression task (k=100, s2=25, s3=25, |I|=50, 50 noise features per example), the MinHash MSE was 0.28 (0.42) and the Sketches MSE was 0.056 (0.014). \nWe plan to report a more exhaustive comparison in the next version of the paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1481943199641, "tcdate": 1481943199641, "number": 7, "id": "rk_iu7GVe", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "BksOoLpXg", "signatures": ["~Nevena_Lazic1"], "readers": ["everyone"], "writers": ["~Nevena_Lazic1"], "content": {"title": "Reply to AnonReviewer3 review", "comment": "The theoretical contribution of our paper is to show that neural networks can learn a sparse linear or polynomial functions from smaller sketches than Lasso can. For this reason we sometimes refer to our approach as improper learning. While the data-generating model may attain the best performance on the original training data, it is not necessarily the best choice for sketched inputs, as we also demonstrate empirically in Fig 2 (center). As the reviewer points out, the existence of a network does not convey anything about how difficult it is to learn such a network. Our empirical results are positive, but we provide no theoretical guarantees beyond the formal reconstruction guarantees. We do not understand the comment on generalization as the results of, for instance Bartlett, are applicable in our setting as well. \n\nOur experiments do include an NLP task with a very large vocabulary (entity type tagging), and we do in fact compare our approach to using the original features (mapped to 32bit integers and hashed). We demonstrate that sketches can achieve comparable accuracy, with significant savings in the number of parameters (see Figure 3 right). Training sketches may take slightly longer than original data, since the number of non-zero features per example is increased by a factor of #blocks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1481943087821, "tcdate": 1481943087821, "number": 6, "id": "HydN_QfEe", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "r1EWX_eNl", "signatures": ["~Nevena_Lazic1"], "readers": ["everyone"], "writers": ["~Nevena_Lazic1"], "content": {"title": "Reply to AnonReviewer2 review", "comment": "We would like to emphasize that our main results pertain to existence of neural networks that can implement a \u201csketch decoding\u201d function. The particular network we describe is just one way of implementing such a function, and there exist other possibilities, including networks with negative weights (and even different non-linearities). Thus we did not consider it necessary to explicitly enforce non-negativity and block-sparsity. This setting is called improper learning as first described by Valiant over 20 years ago in his seminal on PAC learnability. We hope this clarifies our experimental setting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1481832391937, "tcdate": 1481831163829, "number": 3, "id": "r1EWX_eNl", "invitation": "ICLR.cc/2017/conference/-/paper136/official/review", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "content": {"title": "Interesting idea, some mismatch between theory and experiments", "rating": "5: Marginally below acceptance threshold", "review": "Summary of the paper:\n\nThe paper introduces sketches that approximates linear and sparse polynomials on binary data. The paper shows that such sketches can be represented as a one layer neural network. Experiments are conducted on some language processing tasks. \n\nNovelty:\n\nApproximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel. Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).\n\nThe main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.\n\nClarity: \n\nThe paper is  written in a clear way.\n\nComments :\n\n- The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model. \nWhen Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper. When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1]. On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w. \n\n- Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown. Many works have tackled that issue see for instance http://www.di.ens.fr/~fbach/IEEE_TSP_shervashidze_bach_2015.pdf\n\n- The experiments are limited to small sets.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483668259751, "id": "ICLR.cc/2017/conference/-/paper136/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper136/AnonReviewer1", "ICLR.cc/2017/conference/paper136/AnonReviewer3", "ICLR.cc/2017/conference/paper136/AnonReviewer2", "ICLR.cc/2017/conference/paper136/AnonReviewer4"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483668259751}}}, {"tddate": null, "tmdate": 1481628530697, "tcdate": 1481628530687, "number": 2, "id": "BksOoLpXg", "invitation": "ICLR.cc/2017/conference/-/paper136/official/review", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer3"], "content": {"title": "I think the paper presents interesting ideas but some theoretical and experimental settings have room for improvement", "rating": "5: Marginally below acceptance threshold", "review": "Sketching is a powerful tool that has found application in a number of classical machine learning methods like least squares, kernel learning and tensor methods. It is very exciting that such application can be extended to deep neural networks, which represent the state-of-the-art for numerous learning tasks.\n\nHowever, I have doubts on some aspects of the experimental and analytical settings adopted in this paper, which (in my opinion) might limit its impact in both theoretical understanding and practical applicability.\n\n1. On the theoretical side, I found the sparse linear/polynomial classifiers assumption very troublesome. If the classifier that one wishes to learn is indeed sparse linear or sparse polynomial, why should neural networks be a good option compared to, say, Lasso or sparse logistic regression models, which are theoretically sound and even optimal for the particular function class to be learnt?\n\nI think a more relevant setting is where one wants to learn a feedforward neural network, with perhaps sparse weights on each of its layers. It is an interesting question of whether dimensionality reduction preserves the richness of such sparse weight networks, and how aggressively one can reduce the dimension into (i.e., t in the paper), which leads to much smaller parameter space.\n\nAlso, from a learning theoretical perspective, the existence of a small neural network that approximates a sparse linear/polynomial classifier does not mean such a network can be effectively learnt. It would be good that some VC dimension /generalization aspects are analyzed as well. For example, intuitively network that operates on sketched inputs should have smaller model complexity. Does it lead to improved generalization, and how such improvement should be traded off for the richness of the network (which desires *larger* sketch length)?\n\n2. On the practical side, it seems the most relevant application would be in NLP where inputs are naturally sparse high-dimensional vectors. Reducing the input dimension indeed reduces the size of the model and at least save storage cost. However, in practice the high dimensionality is typically dealt with using a LookupTable structure (encoding), which is not difficult to train at all, because the gradient for each data point only involves one entry in the LookupTable. I think the experiments should include comparison with such traditional approaches and comment on both accuracy and training/testing time. Also, if the argument is that the vocabulary size is so large that LookupTable cannot be stored in GPU memory, such an example should be demonstrated and show how sketching helps. ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483668259751, "id": "ICLR.cc/2017/conference/-/paper136/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper136/AnonReviewer1", "ICLR.cc/2017/conference/paper136/AnonReviewer3", "ICLR.cc/2017/conference/paper136/AnonReviewer2", "ICLR.cc/2017/conference/paper136/AnonReviewer4"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483668259751}}}, {"tddate": null, "tmdate": 1480874985620, "tcdate": 1480874985613, "number": 5, "id": "HkMxh0WXl", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "By_kXmRfx", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "writers": ["~Kunal_Talwar1"], "content": {"title": "Response on uniformity", "comment": "We thank the reviewer for the constructive comments and for the references to the papers on fast random projections and spherical random features. We will discuss these works in the updated version, and also clarify the terms proper and improper learning\n\nAll our results, much like all results on projections in learning that we cite, are for any single fixed w. Corollary 3.2 talks about a single x at a time. Thm 4.1 and 4.2 need the \u201cuniformity\u201d in the sense that the same neural network parameters work simultaneously for (almost) all x. The reviewer is correct that uniform guarantees should be possible with k log d sized sketches. For our requirement, we need not just a small sketch, but also a decoding step that can be implemented by a low depth neural network. To our knowledge, the known algorithms in the compressed sensing literature that give for all guarantees with sparse sketching matrices, such as EMP, SMP and l_1 minimization, require larger depth, as they are iterative. In appendix C in our paper, we present a deterministic sketch that gives a for all guarantee. For the simpler sketch, a for-all type guarantee is not true unless we increase the sketch size to about k^2.\n\nIn both the sparse linear functions case and the sparse polynomial functions case, we learn all parameters including the matrix V. This is necessary because we do not a priori know the support of w, and hence do not know which bits need to be decoded. Decoding all bits would negate the advantages of projection, which is why we need to learn V."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1480872504585, "tcdate": 1480872504579, "number": 4, "id": "S1bBf0-Qg", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "B1IqQJhzx", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "writers": ["~Kunal_Talwar1"], "content": {"title": "Thanks for the suggestions.", "comment": "We thank the reviewer for the constructive comments and for the references to the min-hashing papers. We plan to discuss these points and related work in the updated version. \n\nMin-hashing is known to work well for similarity detection when the similarity function of interest is the Jaccard distance. The b-bit min hashing used in citation (1) above shows that they work also for linear regression problems. As pointed out by the authors of (1), it does not work well as a dimension reduction technique since the dimension of the final projection is 2^b k. Since b needs to be about 8 to 10 in order to get good performance, the experiments in (1) end up using, e.g. a projected dimension of 51k for the webspam dataset.\n\nAdditionally, as the reviewer points out, it is not clear whether minwise hashing allows for reconstruction or for polynomial approximation. The limited results in (5) on the polynomial case have an exponential dependence on the degree just like other previous work. We view one of the main contributions of our work to be the drastic provable reduction in sketch size in the polynomial case, that results from the fact that we use improper learning.\n\nIn light of the above, we believe our approach has significant benefits over b-bit min-hash. In the next revision, we will also add an experimental comparison to it."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1480631322051, "tcdate": 1480631007875, "number": 2, "id": "By_kXmRfx", "invitation": "ICLR.cc/2017/conference/-/paper136/pre-review/question", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer2"], "content": {"title": "Uniformity of the results in Corr 3.2 Theorem 4.1,4.2 and 5.1", "question": "The paper presents some nice ideas on learning over sketched data.\n\n- Corr3.2 is stated for a fixed x and w. Th.4.1 and Th 4.2 are stated uniformly for all x , I would expect a dependency of m and t on \n k log(d/k) to be uniform on B_{d,k}. same thing for w . or the statement in Th 4.1 etc  should be like corr 3.2 , meaning not for all x and all w?\n\n- Sparse linear function: If I understand correctly matrix V is a binary matrix with fixed weights and is not learned, then there is a weight w , that is learned, under a regression or classification cost.  so it seems the paper learns a linear function and not a neural network. N_{Relu} is a deterministic implementation of D^{AND}? or is V learned also in the experiments by back propagation?\n\nMany citations on work on random projections and polynomial kernel approximations are missing and worth the comparison theoretically and practically for instance:\n\n*Random Projections for Linear Support Vector Machines \nhttps://arxiv.org/pdf/1211.6085v5.pdf\n\n* Spherical Random Features for Polynomial Kernels\nhttps://papers.nips.cc/paper/5943-spherical-random-features-for-polynomial-kernels.pdf\n\n- Experiments:  would be interesting to try other datasets than AG news , such as Amazon reviews, to see how m and t depends on the training size and vocabulary size (dimension) in an NLP application. \nalso might be interesting to have some bounds  for m and t taking in account training size and vocabulary size.\n\n- Minor comments:\n proper learning and improper learning are not defined in a clear way in the paper.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959444593, "id": "ICLR.cc/2017/conference/-/paper136/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper136/AnonReviewer1", "ICLR.cc/2017/conference/paper136/AnonReviewer2"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959444593}}}, {"tddate": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1480489577012, "tcdate": 1480483725773, "number": 1, "replyCount": 0, "id": "B1IqQJhzx", "invitation": "ICLR.cc/2017/conference/-/paper136/pre-review/question", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["ICLR.cc/2017/conference/paper136/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper136/AnonReviewer1"], "content": {"title": "Comparison with better hashing schemes (other than Gaussian projections)", "question": "Overall, I like the paper and the ideas. \n\nThe paper is about a better dimensionality reduction techniques for sparse binary data using count sketches. The nice thing about sketching is the possibility of reconstruction, and hence any function approximation can be build using NN (as NN are universal approximator on a reconstructed vector). The arguments in the paper can be simplified for general audiences deferring the math. You can add the equation for gradient descent update (of sketches) for wider appeal.  \n\nThe experiments part can be made much better. Only showing why this is approach is superior over Gaussian projections on sparse binary datasets is not quite enough. It is known that for sparse binary datasets, minwise hashing and its variants are much better than random projections (and even space optimal for binary intersection).  See a series of papers.\n\n1) Hashing algorithms for large scale learning NIPS 2011\n2) Fast near-neighbor search for high dimensional binary data ECML 2012\n3) In Defense of MinHash over SimHash AISTATS 2014\n4) Is Min-Wise Hashing Optimal for Summarizing Set Intersection? PODS 14\n4) On b-bit min-wise hashing for large-scale regression and classification with sparse data. Arxiv 2016\n\nOf course, the reconstruction and polynomial approximation with minwise hashing are not clear. So there is a good argument for sketching and hence the paper has advantages on its own. However, the experiments to show that Gaussian projections are the best benchmark, for sparse datasets, is little obsolete.\n\n \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959444593, "id": "ICLR.cc/2017/conference/-/paper136/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper136/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper136/AnonReviewer1", "ICLR.cc/2017/conference/paper136/AnonReviewer2"], "reply": {"forum": "r1br_2Kge", "replyto": "r1br_2Kge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959444593}}}, {"tddate": null, "tmdate": 1480457848153, "tcdate": 1480457848149, "number": 3, "id": "SJgF0uizl", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "BJXtnlsGx", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "writers": ["~Kunal_Talwar1"], "content": {"title": "Thanks", "comment": "Thanks Olivier!\nYou are right that as stated, our definition of pairwise independent hash functions could be clearer. The probability in the definition (and in the usage) is taken over sampling a random hash function from the set, and we will make this explicit in the next version. Thanks also for pointing out the other typos. We will fix them as well.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1480424570672, "tcdate": 1480424570663, "number": 2, "id": "BJXtnlsGx", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "rkTkPv5zg", "signatures": ["~Olivier_Delalleau1"], "readers": ["everyone"], "writers": ["~Olivier_Delalleau1"], "content": {"title": "Never mind!", "comment": "Sorry, I believe I just had misunderstood the definition of a pairwise independent distribution. I would just suggest to make it a bit clearer, for instance something like this: \"a distribution of hash functions from [d] to [m] is pairwise independent if (..), where the probability is taken over h sampled from this distribution\".\n\nAnd while I'm here, just a couple typos I noticed:\n- At end of proof of th. 3.1 there is the expression Pr[Y_hj(i) != xi], which is missing a j in the subscript (see eq.1)\n- In th. 4.2 the probability should be over G, not h_1:t"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "tmdate": 1480386276742, "tcdate": 1480386276738, "number": 1, "id": "rkTkPv5zg", "invitation": "ICLR.cc/2017/conference/-/paper136/public/comment", "forum": "r1br_2Kge", "replyto": "r1br_2Kge", "signatures": ["~Olivier_Delalleau1"], "readers": ["everyone"], "writers": ["~Olivier_Delalleau1"], "content": {"title": "Question on Theorem 3.1", "comment": "Hi,\n\nIt seems to me something's missing in the definition of a \"pairwise independent set of hash functions\" in order to prove Theorem 3.1. The definition above the theorem says nothing about the relationship between two sampled hash functions, but if they are correlated then the conclusion may not hold (the extreme case would be if they are all equal, which is not forbidden as far as I can tell). Could you please clarify?\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287713761, "id": "ICLR.cc/2017/conference/-/paper136/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1br_2Kge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper136/reviewers", "ICLR.cc/2017/conference/paper136/areachairs"], "cdate": 1485287713761}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478272134239, "tcdate": 1478244408692, "number": 136, "id": "r1br_2Kge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1br_2Kge", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "content": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": ["S1hsDCNFx"], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 17}