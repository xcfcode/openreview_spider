{"notes": [{"id": "ByGUFsAqYm", "original": "rJx2ZEd5KX", "number": 448, "cdate": 1538087806049, "ddate": null, "tcdate": 1538087806049, "tmdate": 1545355430218, "tddate": null, "forum": "ByGUFsAqYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gRzCtVeV", "original": null, "number": 1, "cdate": 1545014806018, "ddate": null, "tcdate": 1545014806018, "tmdate": 1545354486056, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Meta_Review", "content": {"metareview": "This paper studies the question of memorization within overparametrised neural networks. Specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders. \n\nAll reviewers agreed that this is an interesting question that deserves further analysis. However, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. In particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. The AC fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Insufficient Rigor"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper448/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353213697, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper448/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353213697}}}, {"id": "SJlBa66O0X", "original": null, "number": 1, "cdate": 1543196092860, "ddate": null, "tcdate": 1543196092860, "tmdate": 1543196843698, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "content": {"title": "Reviewer Overall Response", "comment": "We thank the three reviewers for their comments.  We have provided a point-by-point response under each of the reviews."}, "signatures": ["ICLR.cc/2019/Conference/Paper448/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623483, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGUFsAqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper448/Authors|ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623483}}}, {"id": "HJgoBxRdRQ", "original": null, "number": 4, "cdate": 1543196739013, "ddate": null, "tcdate": 1543196739013, "tmdate": 1543196739013, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "H1xy_sQJaX", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We answer the reviewer questions in the points below:\n\n\" - Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.\"\n\nFor the scope of our paper, we chose to only consider those initialization schemes that are most often used in practice i.e. Pytorch\u2019s default uniform initialization, Xavier Uniform/Normal, and Kaiming Uniform/Normal.  In Figure 7 in the main text and in Appendix E, we display that for nonlinear downsampling networks, memorization occurs under any of these initialization schemes. \n\n\" - Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix. \"\n\nWe thank the reviewer for the feedback on the flow of our paper.  The extraction of the linear operator is an important contribution to this work; so we felt it would be nice to have it in the main text, but we will make a smoother transition between the sections.   \n\n\"- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then. \"\n\nWe wrote the comment here to emphasize that the Proposition mainly provides strong empirical support for the conjecture as opposed to a full proof of our conjecture on 2 x 2 images.  Namely, for any dataset of 2 x 2 images on which we trained the network in Figure 4a, we found that our conjecture held.  \n\nWe also thank the reviewer for pointing out the minor edits to our paper.   \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623483, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGUFsAqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper448/Authors|ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623483}}}, {"id": "rylzukC_C7", "original": null, "number": 3, "cdate": 1543196522256, "ddate": null, "tcdate": 1543196522256, "tmdate": 1543196522256, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "HklwQZvDa7", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "In the following we provide a point-by-point response to the remarks concerning weaknesses :\n\nIn response to the first two points:  \"+ Not enough theoretical proof is provided to support the hypothesis. Which would be fine but some key experiments are missing to make the paper empirically rigorous. \" \"++ Would be good to see experiments that illustrate the predication that a certain amount of data would allow for learning identity maps, both for linear and non-linear CNNs. \"\n\nFor linear downsampling networks, this experiment is shown in the proof of our proposition, namely we see that for 4 linearly independent training examples the network from Figure 4a learns the identity function exactly.  Depending on the nonlinearity it may be impossible for non-linear CNNs to learn the identity map (for example a single layer with a ReLU activation can never learn the full identity function as it will always zero out the negative values), which is why our conjecture is made only for linear downsampling networks.   \n\nIn response to \"++ In the non-linear CNN setting, I'd like to see the same early-stopping experiment done for linear CNNs whose results are in Fig. 3. I don't see any obvious theoretical reason why that result form Fig. 3 must extend to the non-linear setting. \"\n\nWe have run these experiments and they are identical to those for linear networks in Figure 3.  For example, even after running for 100 iterations, downsampling non-linear networks learn the point map while non-downsampling non-linear networks learn a mapping more similar to the identity function.  \n\nWith regard to  \"+ Initializations are pointed to as effecting the type of function the network learns. The authors give an example of a hand-designed initialization that allows a downsampling linear CNN to learn the identity map but they don't explain how they arrived at this initialization, or its properties that make it a good initialization. In general however, I think it's alright to assign exploration of effect of initialization to future work, since it seems like a non-trivial task.\" \n\nIn regard to the initialization of the identity map, we provide a full description of the initialization in Appendix A.  To provide more intuition around how we arrived at this initialization: the manual initialization is meant to preserve all pixels of the input image through the layers so that they can be rearranged by the final layer to get the identity function.  \n\nLastly, in response to \"+ It is mentioned that \"the results are not observed for linear networks when using Kaiming initialization,\" which I read to mean the downsampling linear CNNs with Kaiming initialization learn the identity map, not point-map. If this is true, it seems like a vital point and should be included in discussions of future work.\" \n\nWe thank the reviewer for pointing out this possible misinterpretation. When we say that the results are not observed for linear networks when using the Kaiming initialization, we meant to say that under the Kaiming initialization, the downsampling linear CNN learns neither the point map nor the identity map.  \n\n\n\"Recommendation: I think this could be a better short paper. There are some interesting contributions, but maybe not enough for a full length paper. For a full length paper, some further exploration of _why_ downsampling leads to (if indeed there is a causality) data memorization is needed.\" \n\nThanks, we will consider this in our future submission.\n\n\"Minor stuff:- Citation \"Gunasekar et al.\" is missing year (conclusions section)\"\n\nAgreed, thanks.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623483, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGUFsAqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper448/Authors|ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623483}}}, {"id": "SJeknR6_R7", "original": null, "number": 2, "cdate": 1543196326743, "ddate": null, "tcdate": 1543196326743, "tmdate": 1543196326743, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "Skljdy0vpX", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "content": {"title": "Response to Reviewer 2 Comments", "comment": "Addressing the first remark:  \"- The main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer. The paper itself states on page 4 that the results depend on the initialization and cite Gunasekar et al. in the conclusion for an analogy, which, however, explores the implicit regularization effect of a gradient descent optimizer.\"\n\nIn regard to initialization: we state on page 4 and in Appendix E that nonlinear downsampling networks learn the point-map (i.e. a low rank solution) under any of the popular initializations (Pytorch\u2019s default uniform, Xavier normal and uniform, and Kaiming normal and uniform), which should eliminate initialization as a confounding factor.  In the conclusion, our citation of Gunasekar et al. is to emphasize that they observe that gradient descent learns a low rank solution for the problem of matrix factorization just as we have observed that gradient descent learns a low rank solution (the point map) for downsampling autoencoders.   We do not draw any analogies to our remarks on initialization in regard to this paper. \n\nAddressing the second remark: \" - There is no clear and proved statement despite the suggestive mathematical nature of the writing (Conjecture, Proposition). The claimed 'proof' of the Proposition is conducted via experiment. In light of the above mentioned confounding factors, the current phrasing of the Proposition will not allow a formal proof as it is unclear what the system 'linear Network DS' even is.\"\n\nThe proposition claims that our linear downsampling network (under the default Pytorch initialization) defined in Figure 4a can learn a solution of rank 1, 2, 3 or 4, which we demonstrate empirically.  As we state in the paper, this is not a proof for our conjecture for images of size 2 x 2, but rather it provides strong empirical evidence that our conjecture holds for this network acting on 2 x 2 images.  \n\nAddressing the last remark: \"- The boundary between conjectured and inferred statements is very vague. For example, the meaning of 'prefers to learn a point map' is unclear. \"\n\nWe respectfully disagree in regard to the vagueness between our conjectured and inferred statements.  The example provided by the reviewer here is for a sentence in our conclusion where we state that \u201cdownsampling CNN architecture[s] with the capacity to learn the identity function prefer[s] to learn a point map.\u201d  As demonstrated in Figure 4, we are merely emphasizing the point that downsampling CNNs learn the point map even though they have the capacity to learn the identity function.  \n\n\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623483, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGUFsAqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper448/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper448/Authors|ICLR.cc/2019/Conference/Paper448/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers", "ICLR.cc/2019/Conference/Paper448/Authors", "ICLR.cc/2019/Conference/Paper448/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623483}}}, {"id": "Skljdy0vpX", "original": null, "number": 3, "cdate": 1542082418776, "ddate": null, "tcdate": 1542082418776, "tmdate": 1542082418776, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "content": {"title": "Promising idea that requires substantial improvement in analysis and presentation", "review": "The authors conjecture that convolutional downsampling is an underlying mechanism behind sample memorization in over-parameterized convolutional autoencoders. They claim that this effect leads the system to converge to a low-rank solution in contrast to the theoretically possible identity mapping. They support their claim with numerical experiments on linear and non-linear convolution autoencoders.\n\nStrengths:\n- The authors develop their idea in close connection to commonly used architectures.\n\nWeaknesses:\n- The main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer. The paper itself states on page 4 that the results depend on the initialization and cite Gunasekar et al. in the conclusion for an analogy, which, however, explores the implicit regularization effect of a gradient descent optimizer.\n- There is no clear and proved statement despite the suggestive mathematical nature of the writing (Conjecture, Proposition). The claimed 'proof' of the Proposition is conducted via experiment. In light of the above mentioned confounding factors, the current phrasing of the Proposition will not allow a formal proof as it is unclear what the system 'linear Network DS' even is.\n- The boundary between conjectured and inferred statements is very vague. For example, the meaning of  'prefers to learn a point map' is unclear.\n\nOverall, the exposition is insufficient in supporting the conjectured effect. The methodology could be strengthened in two directions:\n1) experimentally: designing numerical experiments that exclude confounding factors and surface the conjectured effect\n2) theoretically: abstracting the idea into a clear mathematical statement that can be proved\n\nI encourage the authors to extend their work for submission to a future venue.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "cdate": 1542234459245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726442, "tmdate": 1552335726442, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklwQZvDa7", "original": null, "number": 2, "cdate": 1542054175319, "ddate": null, "tcdate": 1542054175319, "tmdate": 1542054175319, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "content": {"title": "Interesting direction but needs more to be a fully fleshed out paper.", "review": "Summary:-\nThe authors investigates downsampling as one method by which autoencoding CNNs may memorize data. The theoretical motivation provided concentrates on linear CNNs. They show that downsampling linear CNNs tent to learn a point-map of the training data, even though (under certain initializations) they are capable of learning identity maps. However, non-downsampling linear CNNs learn identity maps. Given enough data however, the authors claim that the downsampling CNN will learn the identity map.\n\nStrengths:-\n+ Authors present a good exploration of how linear CNNs memorize data when they do downsampling. \n+ A theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear CNNs is provided, \"Our conjecture also implies that when training a linear downsampling CNN on images of size 3 \u00b7 224 \u00b7 224, which corresponds to the input image size for VGG and ResNet (He et al. (2016), Simonyan & Zisserman (2015)), the number of linearly independent training examples needs to be at least 3 \u00b7 224 \u00b7 224 = 153, 228 before the network can learn the identity function.\" \n\nWeaknesses:-\n+ Not enough theoretical proof is provided to support the hypothesis. Which would be fine but some key experiments are missing to make the paper empirically rigorous.\n++ Would be good to see experiments that illustrate the predication that a certain amount of data would allow for learning identity maps, both for linear and non-linear CNNs.\n++ In the non-linear CNN setting, I'd like to see the same early-stopping experiment done for linear CNNs whose results are in Fig. 3. I don't see any obvious theoretical reason why that result form Fig. 3 must extend to the non-linear setting. \n+ Initializations are pointed to as effecting the type of function the network learns. The authors give an example of a hand-designed initialization that allows a downsampling linear CNN to learn the identity map but they don't explain how they arrived at this initialization, or its properties that make it a good initialization. In general however, I think it's alright to assign exploration of effect of initialization to future work, since it seems like a non-trivial task.\n+ It is mentioned that \"the results are not observed for linear networks when using Kaiming initialization,\" which I read to mean the downsampling linear CNNs with Kaiming initialization learn the identity map, not point-map. If this is true, it seems like a vital point and should be included in discussions of future work.\n\nRecommendation:  I think this could be a better short paper. There are some interesting contributions, but maybe not enough for a full length paper. For a full length paper, some further exploration of _why_ downsampling leads to (if indeed there is a causality) data memorization is needed.\n\nMinor stuff:-\nCitation \"Gunasekar et al.\" is missing year (conclusions section)", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "cdate": 1542234459245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726442, "tmdate": 1552335726442, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xy_sQJaX", "original": null, "number": 1, "cdate": 1541516134747, "ddate": null, "tcdate": 1541516134747, "tmdate": 1541533987219, "tddate": null, "forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "content": {"title": "Interesting idea but stronger supporting theory and more clarity are needed", "review": "The paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. In the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. The authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. Then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.\n\nThe paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.\n\n- Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.\n- Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix.\n- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then.\n\n- Minor comments\n1. \u201c2000 iteration\u201d -> \u201c2000 iterations\u201d\n2. The text says \u201cNetwork ND trained on frog image\u201d while the following next sentence says that \u201cthe network reconstructed the digit 3\u201d. Please clarify.\n3. \u201cNetwork ND reconstructed the digit 3 with a training loss of 10^-4 and Network ND with loss 10^-2\u201d. It seems that one of these should be \u201cNetwork D\u201d.\n4. \u201c(with downsamling)\u201d ->  \u201c(with downsampling)\u201d", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper448/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Downsampling leads to Image Memorization in Convolutional Autoencoders", "abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. \nIn this paper, we link memorization of  images in deep convolutional autoencoders to downsampling through strided convolution.  To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used.  On the other hand, networks without downsampling do not memorize training data.  We provide further evidence that the same effect happens in nonlinear networks.  Moreover, downsampling in nonlinear networks causes the model to not only memorize just linear combinations of images, but individual training images.  Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.  \n", "keywords": ["Memorization in Deep Learning", "Convolutional Autoencoders"], "authorids": ["aradha@mit.edu", "cuhler@mit.edu", "mbelkin@cse.ohio-state.edu"], "authors": ["Adityanarayanan Radhakrishnan", "Caroline Uhler", "Mikhail Belkin"], "TL;DR": "We identify downsampling as a mechansim for memorization in convolutional autoencoders.", "pdf": "/pdf/fe386b94f5c2f7722843c94e559a5dbd62788b9f.pdf", "paperhash": "radhakrishnan|downsampling_leads_to_image_memorization_in_convolutional_autoencoders", "_bibtex": "@misc{\nradhakrishnan2019downsampling,\ntitle={Downsampling leads to Image Memorization in Convolutional Autoencoders},\nauthor={Adityanarayanan Radhakrishnan and Caroline Uhler and Mikhail Belkin},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGUFsAqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper448/Official_Review", "cdate": 1542234459245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGUFsAqYm", "replyto": "ByGUFsAqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper448/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726442, "tmdate": 1552335726442, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper448/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}