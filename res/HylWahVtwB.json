{"notes": [{"id": "HylWahVtwB", "original": "HyxmnXXmwS", "number": 217, "cdate": 1569438905405, "ddate": null, "tcdate": 1569438905405, "tmdate": 1577168249100, "tddate": null, "forum": "HylWahVtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Dd67Q5P5gX", "original": null, "number": 1, "cdate": 1576798690603, "ddate": null, "tcdate": 1576798690603, "tmdate": 1576800944623, "tddate": null, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a method for neural architecture search in embedding space. This is an interesting idea, but its novelty is limited due to its similarity to the NAO approach. Also, the empirical evaluation is too limited; comparisons should have been performed to NAO and other contemporary NAS methods, such as DARTS. \n\nDue the factors above, all reviewers gave rejecting scores (3,3,1). The rebuttal did not remove the main issues, resulting in the reviewers sticking to their scores. I therefore recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730102, "tmdate": 1576800282829, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper217/-/Decision"}}}, {"id": "S1lnWYNBjr", "original": null, "number": 4, "cdate": 1573370115580, "ddate": null, "tcdate": 1573370115580, "tmdate": 1573370115580, "tddate": null, "forum": "HylWahVtwB", "replyto": "Hkl24SyG9B", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank for your comment,\n\n1.  This suggestion is very useful. The reason of only performs experiment on CIFAR10 is the original plan is to only compare with different methods of improving search space (like cell-based). If we performs more dataset will be greater, but it can not deny the contribution of this paper.\n\n2. Yes, we knew excellent algorithms DARTS and NAO (we cited both). However, we still did not compare with it, because of NASES is a method of improving search space. Besides, the performance is not as good as DARTS and ENAS due to we didn\u2019t use cutout, weight sharing and micro search. Strive STOA performance is not ideal goal in this paper, so we only made comparisons of different technicals with macro search and other methods of improving search space (like cell-based).  For example, micro search NAS with Q-Learning,  which spend 96 GPU days and got 3.6 error rate. And macro search ENAS, which spend 0.32 GPU days and got 3.87 error rate.  AND in this paper, macro search NASES, which spend 0.5 GPU days and got 3.71 error rate ( and only use 28.4m params and number of searching to <100 architectures to achieve a final architecture).  Moreover, NAO without weights sharing(and use cell based), it need to spend 200 GPU days. Those experiment results show NASES is very competitive and excellent contribution."}, "signatures": ["ICLR.cc/2020/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylWahVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper217/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper217/Authors|ICLR.cc/2020/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174643, "tmdate": 1576860547480, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment"}}}, {"id": "Sye2ajfriB", "original": null, "number": 3, "cdate": 1573362627902, "ddate": null, "tcdate": 1573362627902, "tmdate": 1573368107737, "tddate": null, "forum": "HylWahVtwB", "replyto": "BJeoxR3Q5H", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for the review and for taking the time to thoroughly read and comment on this paper. \n\n1. Yes, we knew excellent algorithms NAO(we cited it). However, we still did not compare with it, because of NASES is a method of improving search space. In this paper, strive STOA performance is not ideal goal, so we only made comparisons of different technicals with macro search and other methods of improving search space (like cell-based).  We can see the NASES experiments results were very impressive. For example:  This is very competitive when we compare with dimension of improving search space, like micro search NAS with Q-Learning,  which spend 96 GPU days and got 3.6 error rate, and ENAS. Besides, if NAO without weights sharing(and use cell based), it need to spend 200 GPU days. In our experiment, NASES without weights sharing( specially, it is macro search), it just only a half GPU day on more search space. \n\n2.  It is our mistake we will fix it, not Bernoulli distribution, just continuous actions between 0 and 1.\n\n3. Reward function made by generalization error due to performance estimation strategy. Our strategy is less epochs to estimate performance, so if we only estimate\nvalidation performance it will overfit on final architecture, because of NASES will only search for high validation performance on less epoch step( The train loss could be very low). In the final step,  there is not room for improvement when we train 700 epochs on final architecture. \n  \n4.  Yes, I agree autoML makes more progresses and better, and I agree comparison under the same setting is required, that is one of reason we didn\u2019t compared with DARTS and NAO, we want to based on macro search and compared with different methods of improving search space. However, it can not deny contribution of NASES.  NASBench-101 is a very excellent work but parallel with our job."}, "signatures": ["ICLR.cc/2020/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylWahVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper217/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper217/Authors|ICLR.cc/2020/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174643, "tmdate": 1576860547480, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment"}}}, {"id": "B1gSO6FVsB", "original": null, "number": 2, "cdate": 1573326189025, "ddate": null, "tcdate": 1573326189025, "tmdate": 1573326189025, "tddate": null, "forum": "HylWahVtwB", "replyto": "rkg56JppKr", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment", "content": {"title": "Response to Reviewer 3 ", "comment": "Thank for your comment, \n\n1. This paper is very different from Luo et al. 2018. First, Luo et al. 2018 applies gradient descent in the embedded space.  We proposed another technical, which extended embedded space could be applied in NAS with reinforcement learning. Second, for the technical contribution, we droped accuracy predictor, it makes the framework more simply and avoided bias from accuracy predictor. Besides, it is very quick to train pretrained encoder and pretrained decoder by our methods.\n\n\n2. We knew latest and excellent algorithms like DARTS and NAO (we cited both). However, we still did not compare with them, because of NASES is a method of improving search space, so we made comparisons of different technicals with macro search ( like Macro NAS with Q-Learning, Net Transformation) and other methods of improving search space (like cell-based and ENAS). Maybe the performance is far from SOTA( we didn\u2019t apply cell-based and weights sharing and cutout), but we still can see the NASES experiments results were very impressive. For example:\n\n- It is a new method of improving search space: The experiment results were very competitive when we compare with dimension of improving search space, like micro search NAS with Q-Learning,  which spend 96 GPU days and got 3.6 error rate ( we will add into Table 2). And ENAS.\n \n- Spend up:  NAO without weights sharing(and use cell-based), it need to spend 200 GPU days. In our experiment, NASES without weights sharing( specially, it is macro search), it just only a half GPU day on more search space. \n\n- Considerable reduction in searches was achieved by reducing the average number of searching to <100 architectures to achieve a final architecture.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylWahVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper217/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper217/Authors|ICLR.cc/2020/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174643, "tmdate": 1576860547480, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper217/Authors", "ICLR.cc/2020/Conference/Paper217/Reviewers", "ICLR.cc/2020/Conference/Paper217/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Comment"}}}, {"id": "rkg56JppKr", "original": null, "number": 1, "cdate": 1571831745874, "ddate": null, "tcdate": 1571831745874, "tmdate": 1572972623853, "tddate": null, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work searches neural architectures in an embedding space, which is continuous. Overall, it lacks of innovation and experimental results are not strong enough. \n\n\t1. The innovation and technical contribution are weak. The key idea of searching in embedding space has already been proposed and applied in  Luo et al. 2018. The authors do not differentiate this work from Luo et al. 2018, and I didn't find any essential differences between them.\n\n\t2. The proposed method does work well. It is not compared with latest algorithms including NAO and DARTS. The reported numbers in Table 2 are poor, far from SOTA numbers.\n\nBesides, there are many writing issues and typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644867977, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper217/Reviewers"], "noninvitees": [], "tcdate": 1570237755334, "tmdate": 1575644867990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Review"}}}, {"id": "Hkl24SyG9B", "original": null, "number": 2, "cdate": 1572103476153, "ddate": null, "tcdate": 1572103476153, "tmdate": 1572972623811, "tddate": null, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper borrows the idea of word-to-vector from NLP and applies it in reinforcement learning based Neural Architecture Search (NAS). It suggests a pretrained encoder to transform the search space to a dense and continuous architecture-embedding space. First it trains the architecture-embedding encoder and decoder with self-supervision learning like Auto-Encoder.  Then it performs reinforcement learning based Neural Architecture Search(NAS) in the architecture-embedding space.\n\nStrength:\nThere is no architecture prior, such as cell, in the searching process. Thus it's more general and can explore more architectures possibilities.\nBecause it performs architecture search in a continuous space, a CNN based controller is used instead of a RNN controller. \nThe result of the proposed method on CIFAR-10 is comparable with other popular NAS approaches.\nIt reduces the number of searching architectures to <100 in <12 GPU hours without using tricks such as cell or parameter sharing.\n\nWeakness:\nThe evaluations are highly insufficient. It only performs experiment on CIFAR-10, and the generalization ability on other datasets is unclear. In many NAS works. CIFAR-100 and ImageNet are commonly used to evaluate the performance. \nBesides, there is no comparison with more recent and related important methods such as DARTS and the method proposed by Luo et al. (2018). Actually its performance is not as good as Darts or the best performance reported in ENAS.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644867977, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper217/Reviewers"], "noninvitees": [], "tcdate": 1570237755334, "tmdate": 1575644867990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Review"}}}, {"id": "BJeoxR3Q5H", "original": null, "number": 3, "cdate": 1572224499177, "ddate": null, "tcdate": 1572224499177, "tmdate": 1572972623752, "tddate": null, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "invitation": "ICLR.cc/2020/Conference/Paper217/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an interesting idea to perform Neural Architecture Search: first, an auto-encoder is pre-trained to encode/decode an neural architecture to/from a continuous low-dimensional embedding space; then the decoder is fixed but the encoder is copied as an agent controller for reinforcement learning. The controller is optimized by taking actions in the embedding space. The reward is also different from previous works which usually only considered validation accuracy but this work also considers the generalization gap.\n\nThe idea is interesting, but there are some problems on both the method's and the experimental sides:\n1. NAO [1] also embeds neural architectures to a continuous space. Different from NAO which applies gradient descent in the embedded space, this paper uses RL. I double that RL can work better than gradient descent in a continuous space. The paper should compare with NAO. Ideally, this paper might work better than NAO if the accuracy predictor in the NAO is not accurate, while this paper uses real accuracy as a reward for search. However, this is not soundly compared.\n2. It is unreasonable to discretize continuous actions to a Bernoulli distribution. Many RL methods, such as DDPG, can handle continuous actions;\n3. The paper uses Eq. 1 as a reward. It's interesting, but it's unclear why the generalization error is needed. Ablation study is required.\n4. As the community makes more progresses in AutoML, a better and better (smaller and smaller) search space is used. It doesn't make much sense to compare the search time under different search spaces. Comparison under the same setting (e.g. NASBench-101) is required. \n\n\nMinors:\n1. missing a number in \"and T0 = epochs\"\n2. missing \"x\" in \"32 32 RGB in 100 classes\", and \"100\" should be \"10\"\n\n[1] Luo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. \"Neural architecture optimization.\" In Advances in neural information processing systems, pp. 7816-7827. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper217/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Architecture Search in Embedding Space", "authors": ["chun-ting liu"], "authorids": ["jimliu741523@gmail.com"], "keywords": ["neural architecture search", "nas", "automl"], "TL;DR": "This paper proposed a novel neural architecture search framework, which enables reinforcement learning to search in an embedding space by using architecture encoders and decoders.", "abstract": "The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "pdf": "/pdf/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "code": "https://anonymous.4open.science/r/b5cee050-c345-4acf-bc34-4d7233edbe80/", "paperhash": "liu|neural_architecture_search_in_embedding_space", "original_pdf": "/attachment/bae56aaca50c310a698df1eaf5c6bc2fef8cc51d.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Architecture Search in Embedding Space},\nauthor={chun-ting liu},\nyear={2020},\nurl={https://openreview.net/forum?id=HylWahVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylWahVtwB", "replyto": "HylWahVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper217/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644867977, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper217/Reviewers"], "noninvitees": [], "tcdate": 1570237755334, "tmdate": 1575644867990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper217/-/Official_Review"}}}], "count": 8}