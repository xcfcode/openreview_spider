{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028566208, "tcdate": 1490028566208, "number": 1, "id": "HyRfOKpjg", "invitation": "ICLR.cc/2017/workshop/-/paper43/acceptance", "forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028566826, "id": "ICLR.cc/2017/workshop/-/paper43/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028566826}}}, {"tddate": null, "tmdate": 1489620444377, "tcdate": 1489620444377, "number": 3, "id": "H1N1ArDil", "invitation": "ICLR.cc/2017/workshop/-/paper43/public/comment", "forum": "SkvQFOmtg", "replyto": "H1lrb2Hsx", "signatures": ["~Alexander_G_Anderson1"], "readers": ["everyone"], "writers": ["~Alexander_G_Anderson1"], "content": {"title": "revision submitted", "comment": "I submitted a revision to the abstract (and some additional SI) that is my response to your helpful feedback.  One key point that I've added is that the high dimensionality makes it easy for the learning algorithm to approximate the informative directions with a binary vector.  It is also important to note that the magnitude of the dot products are scaled out by both the batch norm (additive constant set to zero), and the subsequent binarization of the activations. While in a typical network, the network may rely on the magnitude of the dot products to code useful information, these networks are trained with that being possible, and is forced to find a solution where the magnitudes of dot products are not used. \n\nThe experiments that you suggest to disentangle the learning algorithm and the high dimensions will be important to think about when we take this workshop abstract submission and flesh it out into a full paper. \n\nIf you have other thoughts and suggestions, I'm all ears. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487272224103, "tcdate": 1487272224103, "id": "ICLR.cc/2017/workshop/-/paper43/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper43/reviewers"], "reply": {"forum": "SkvQFOmtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487272224103}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489619215221, "tcdate": 1487272223530, "number": 43, "id": "SkvQFOmtg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "SkvQFOmtg", "signatures": ["~Alexander_Grant_Anderson1"], "readers": ["everyone"], "content": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489514808274, "tcdate": 1489514808274, "number": 2, "id": "H1lrb2Hsx", "invitation": "ICLR.cc/2017/workshop/-/paper43/public/comment", "forum": "SkvQFOmtg", "replyto": "Byjqo1Ejl", "signatures": ["~Alexander_G_Anderson1"], "readers": ["everyone"], "writers": ["~Alexander_G_Anderson1"], "content": {"title": "Reply to high dimensions or learning dynamics", "comment": "Thanks for your thoughtful comments \u2013 it is very helpful for me to get this feedback so I can continue to further this work.  \n\n-\tI agree that I should clarify that this analysis is intended to be about neural networks with binary weights and activations of the flavor of that is detailed in Coubariaux, Hubara et al 2016. I made the mistake of referring to the neural networks with binary weights and activations as BNNs, which is misleading. [As an aside, I wasn\u2019t aware of an alternative method of training such networks from scratch until you told me about the exhaustive search method]. I\u2019ll make the abstract more clear. \n-\tBuilding off of what you are saying, the connection between the learning dynamics and the high dimensionality is that it is easy to satisfy w^b x = w^c x when you are in high dimensions. I\u2019ll make this more clear.\n-\tI agree that doing more experiments to separate out the effects of high dimensions and the learning would be useful for pushing this work further. \n\nAgain, thanks a lot for your comments and I\u2019ll be updating the paper (I\u2019ll submit the revision by tomorrow at the latest). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487272224103, "tcdate": 1487272224103, "id": "ICLR.cc/2017/workshop/-/paper43/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper43/reviewers"], "reply": {"forum": "SkvQFOmtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487272224103}}}, {"tddate": null, "tmdate": 1489398674992, "tcdate": 1489398674992, "number": 1, "id": "Byjqo1Ejl", "invitation": "ICLR.cc/2017/workshop/-/paper43/official/comment", "forum": "SkvQFOmtg", "replyto": "SJEc-jgig", "signatures": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1"], "content": {"title": "High dimensions or special learning dynamics?", "comment": "I still think there are some conceptual issues here.\n\n-The paper purports to give a general explanation for why binary weights work, based on high dimensional spaces. Eg, \"binary neural networks work because of the geometry of high-dimensional binary vectors.\" The contention is that, usually, binarizing a continuous vector will yield a vector close in angle to the original. If this is in fact the basis for good performance, then binarizing standard continuous nets should yield high performance, but it does not in general. Moreover, as discussed earlier, this only preserves the angle but not the magnitude of the resulting vector, which can be highly significant with nonlinear activation functions.\n\n-In light of the above, the paper and author response quickly retreats from this position to say that one particular scheme for training BNNs, which includes several features such as batch norm, manages to enforce w_binary \\dot x = w_continuous \\dot x. So on this view, it is something distinctive about the learning process which yields good performance, not the fact that the vectors are high dimensional. Indeed it seems quite likely that w_binary \\dot x = w_continuous \\dot x is a sufficient condition for good performance, even in low dimensions, and so it is not surprising that successful learning schemes would approximately satisfy this constraint. This is an entirely different explanation, unrelated to high dimensions.\n\nThe paper might benefit by being reframed from addressing BNNs in general to this specific learning scheme. And it could also be strengthened by more directly investigating the role of high dimensions vs the role of the learning scheme. Empirical investigations which separate these two effects would be very interesting.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487272224103, "tcdate": 1487272224103, "id": "ICLR.cc/2017/workshop/-/paper43/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "SkvQFOmtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper43/reviewers", "ICLR.cc/2017/workshop/paper43/areachairs"], "cdate": 1487272224103}}}, {"tddate": null, "tmdate": 1489184852374, "tcdate": 1489184852374, "number": 2, "id": "r13U_jxsl", "invitation": "ICLR.cc/2017/workshop/-/paper43/official/review", "forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "signatures": ["ICLR.cc/2017/workshop/paper43/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper43/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper attempts to explain the recent practical success of binary weight neural networks by using the relationships between angles of binarized or non-binarized vectors in high dimensional spaces. Even though the paper is short, I think the main idea presented in the paper is important and insightful, and it can lead to interesting follow-up work and would be a good contribution to the workshop track.\n\nI agree with all of the other reviewer's points (including the questions that the paper addresses however does not answer, which I consider to be okay for a workshop paper considering the ideas that it contributes), yet I think the paper is sufficient for acceptance. The exposition is a bit short and you can include your derivations even though you think it is straightforward, for the sake of completeness and ease of reading.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489184853168, "id": "ICLR.cc/2017/workshop/-/paper43/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1", "ICLR.cc/2017/workshop/paper43/AnonReviewer2"], "reply": {"forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489184853168}}}, {"tddate": null, "tmdate": 1489183115575, "tcdate": 1489183115575, "number": 1, "id": "SJEc-jgig", "invitation": "ICLR.cc/2017/workshop/-/paper43/public/comment", "forum": "SkvQFOmtg", "replyto": "Hy69c_lsx", "signatures": ["~Alexander_G_Anderson1"], "readers": ["everyone"], "writers": ["~Alexander_G_Anderson1"], "content": {"title": "Response to Official Review", "comment": "Thank you very much for your comments.  We\u2019ll definitely consider these things as we put together a full write-up of this work.  To directly respond to your questions / points:\n\n-\tIn the caption of the second figure, we state that the entries are chosen uniformly from the interval [-1, 1]. We didn\u2019t contain the full calculations in the write-up due to space constraints, but the calculations are straightforward since each component of the vectors in the dot products are independent.  E.g. if p(x) is the pdf of one of the components of the random vector, the p(z) = sum_{x,y} delta(z-xy) p(x)p(y) is the distribution of one of the components in the dot product of two random vectors. The normalized dot product is Z_n = (z_1+z_2+\u2026+z_n)/n. Using the CLT, we just need the mean and variance of p(z) to get the large n behavior. Likewise, the continuous binary dot product is computed by replacing z=xy with z=x * sign(y). The angles are found by taking the inverse cosine of the normalized dot products. \n-\t\n-\tAs far as the magnitude of the dot products are concerned, in the caption of the third figure addresses this issue.  After each dot product, there is a batch norm layer so any scaling constant is normalized out. For the sake of clarity, we will adjust the equations to say approximately equal up to a constant, which is subsequently normalized out. \n-\t\n-\tWhy we cannot just post-hoc binarize the weights of a network is an important question that is worth thinking about in more detail and is something that we\u2019ve thought a lot about.   We briefly comment on this on the last paragraph on page two, mentioning that the dot product relation isn\u2019t going to be true in general, but is a consequence of the learning dynamics set up by the algorithm. Moving beyond what we wrote in the abstract, there are two methods for addressing this. First, as we show in Fig 1b, on the backward pass, we treat w_b = w_c when they are in [-1, 1] [so w_b * x = w_c * x]. We will include this point directly in an updated version of the paper. The connection to exactly how this impacts the learning dynamics hasn\u2019t been worked out fully.  Second, there are going to be many solutions that could be found that are in the form of a multilayer perceptron. Some of those are going to use more than one bit of precision per activation. However, when we use a learning algorithm that forces the activations to be one bit, this means that the network cannot achieve a solution and is forced to spread informative information to be spread across activations instead of concentrated in one activation. \n-\t\n-\tI agree that a more extensive review of previous work would be good, but was omitted due to space constraints.  Rastegari et al. 2016 (which was cited) already contains an extensive literature review. Thanks for the reference to that paper as I have not seen that particular paper. I agree that it would be an interesting experiment to study a comparison where you learn a continuous perceptron and binarize the weights. I am unsure if this will work for the reasons that you are suggesting above. This method you suggest would be different than what is currently being done in this paper where the weights are binary in the forward pass of the network, and in the backward pass, the errors are propagated to a latent continuous variable that can accumulate small changes from stochastic gradient descent. \n-\t\n-\tThat is an interesting question to think about the sparsity.  Perhaps this would be better treated in a framework were the weights were either -1, 0, +1.  In the binary case, it isn\u2019t clear how to discuss sparsity. \n\nThanks again for your feedback, and we\u2019d be happy to answer any more questions that you have and to consider any other suggestions for new experiments that you might have. After your response, I'd be happy to update the abstract with your comments in mind. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487272224103, "tcdate": 1487272224103, "id": "ICLR.cc/2017/workshop/-/paper43/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper43/reviewers"], "reply": {"forum": "SkvQFOmtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487272224103}}}, {"tddate": null, "tmdate": 1489173141287, "tcdate": 1489173141287, "number": 1, "id": "Hy69c_lsx", "invitation": "ICLR.cc/2017/workshop/-/paper43/official/review", "forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "signatures": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "Deep networks typically rely on relatively high precision arithmetic. There are a variety of advantages to reducing this precision requirement, by coming up with high performance networks with, eg, binary weights. Recent methods have been able to achieve good performance with binary weights (though they make use of continuous weights during training). This paper attempts to explain this performance with the simple but potentially important observation that the angle between a continuous weight vector and its binarized version is typically much larger than the angle between two random vectors in high dimensions.\n\n-The theory could be more explicitly spelled out. Suppose the weight vectors are drawn randomly from a Gaussian or uniform(-1,1) distribution. What is the resulting distribution of the angle with the binarized vector?\n\n-The argument for the prediction that the inner product with the binarized weights should be equal to the inner product with the continuous weights would appear to be neglecting the potential change in the norm of the weights. Eg, w^c \\dot x will only equal w^b \\dot x if the norm of w^c and w^b are similar. One possibility is that this highlights that a binarized network can only be sensitive to the angle to an input, but will struggle to be sensitive to the magnitude of an input. The magnitude can be extremely important: it may decide whether the activity lies in the saturating regime of a sigmoid, for instance. \n\n-It seems like a prediction of the theory is that, if the binarized copy is giving the same value as the original, you should be able to just train a standard continuous network and afterward binarize all the weights and obtain good performance. That this does not work may point to the significance of the magnitude of the weights.\n\n-The paper could benefit from a more extensive overview of related work. Seung, Sompolinksy, & Tishby Physical Review A 1992, for instance, work out the training and generalization error of a perceptron with binary synapses. It would be useful to contrast this solution (where they directly search in the discrete weight space), with the gen/train error achieved by first learning a continuous perceptron and then binarizing the weights, using the arguments developed here.\n\n-How does this interact with sparsity in the weights? ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "pdf": "/pdf/a49c2176212a05dd685b1eab46cab374a8e8517f.pdf", "TL;DR": "Theory and experiments explaining how Binary Neural Networks work based on the geometry of high-dimensional binary vectors", "paperhash": "anderson|the_highdimensional_geometry_of_binary_neural_networks", "conflicts": ["berkeley.edu"], "keywords": ["Theory", "Deep learning"], "authors": ["Alexander G. Anderson", "Cory P. Berg"], "authorids": ["aga@berkeley.edu", "cberg500@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489184853168, "id": "ICLR.cc/2017/workshop/-/paper43/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper43/AnonReviewer1", "ICLR.cc/2017/workshop/paper43/AnonReviewer2"], "reply": {"forum": "SkvQFOmtg", "replyto": "SkvQFOmtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper43/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489184853168}}}], "count": 8}