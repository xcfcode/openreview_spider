{"notes": [{"id": "SJeFNoRcFQ", "original": "rkeksuoOKm", "number": 22, "cdate": 1538087729454, "ddate": null, "tcdate": 1538087729454, "tmdate": 1545355416005, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkgUinThy4", "original": null, "number": 1, "cdate": 1544506526263, "ddate": null, "tcdate": 1544506526263, "tmdate": 1545354498861, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Meta_Review", "content": {"metareview": "While it appears that the authors have done significant amount of work to investigate this topic, there are concerns that the theorems are not rigorously/precisely presented, and it is unclear how they can guide the design and training of neural network models in practice. The response and revision of the authors do not provide sufficient materials to address these concerns. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Concerns on the justifications and practical values of the proposed theorems"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper22/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353364478, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353364478}}}, {"id": "r1gqFveZJ4", "original": null, "number": 9, "cdate": 1543731074510, "ddate": null, "tcdate": 1543731074510, "tmdate": 1543731074510, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "rJxwUCJZJE", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "Yes, our theory is very practical", "comment": "We can do better than that ... we can show that the Universality of the Heavy Tails, combined with the specific form of their tail statistics, can be used to construct a VC-like data-dependent metric that extends recent results by Hidary and Poggio and can be used to predict the generalization accuracy of a common series of pretrained, production level DNNs (i.e, the VGG series, the ResNet series, etc.) without peeking at the test data.  We'll be posting this soon, and this clearly can be used for improved training algorithms.  We are glad to post an illustrative figure if there is you need more details practical value of our approach."}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "rJxwUCJZJE", "original": null, "number": 8, "cdate": 1543728719146, "ddate": null, "tcdate": 1543728719146, "tmdate": 1543728719146, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "r1eY-YgcAm", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "Question on the practical values", "comment": "Dear authors, \n\nI am curious whether you had thought about proposing new training algorithms that can benefit from the insights extracted from your theoretical and empirical analyses. \n\nThanks,\nAC"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "rkglhFxc0Q", "original": null, "number": 7, "cdate": 1543272871585, "ddate": null, "tcdate": 1543272871585, "tmdate": 1543272871585, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "Bylje82KC7", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "More details: Specific comments in response to AnonReviewer2", "comment": "\"The paper attempts to examine the reasons behind the strong generalisation performance of DNNs trained via SGD. The authors propose an analysis which offers a fresh view to this problem. This view has been articulated very well, and is based on sound mathematical arguments.\"\n\n(*) Thank you. \n\n\"there is no formal theorem to support the introduced assumptions, the authors have attempted to provide empirical evidence through experiments with standard DNN architectures and benchmark datasets. However, this is where the weakness of this paper lies:\"\n\n(*) We disagree.  We actually regard this as a strength.  We decided how to construct our theory after we looked in detail at empirical results.  We did not construct our theory based on existing theory.  \n\n\"The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups.\"\n\n(*) Space did not permit us to present all the results, but see additional empirical results on: pages 12 and 21-25 of appendix (for MLP3, LeNet5, AlexNet, InceptionV3, and MiniAlexNet, see page 21 of appendix for details on these); pages 27-27 of appendix, including Tables 5 and 6 (for alexnet, densenet121, densenet121, densenet161, densenet169, densenet201, inception_v3, resnet101, resnet152, resnet18, resnet34, resnet50, vgg11, vgg11_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn, see page 27 of appendix for details on these); and pages 37-46 of appendix and pages 46-48 of appendix (for MiniAlexNet).  While not the final word, we hope that these empirical results will convince the critical reader.  We have still more empirical results on a more diverse set of data domains and architectures that we will make report on soon.\n\n\"Although I accept the claim of the authors concerning the lack of space, they could also trim the Introduction so as to free up some space, as well as provide an indefinite number of extra supporting evidence in the form of Supplementary Material/Appendices.\"\n\n(*) We now have a 59 page appendix.  We will be glad to adjust the introduction in the final accepted version.  Thank you. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "HJgyDYxcAQ", "original": null, "number": 6, "cdate": 1543272791129, "ddate": null, "tcdate": 1543272791129, "tmdate": 1543272791129, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "Bylje82KC7", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "More details: Specific comments in response to AnonReviewer3", "comment": "\"the paper seems a little bit handwavy to me, without any serious theoretical justification. For example, why are \\mu=2 and 4 chosen as the threshold between weakly/moderately/very heavy-tailed? In addition, the paper is build upon o the 5+1 model as in Figure 2 and the graphical comparison between the empirical ESD and the expected ESD of the five models in Table 1, and they lack any mathematical/rigorous definition---see table 2. The simulations are performs over a particular data set and a particular setting, and I wonder if the observations would be different for a different data set and a different setting.\"\n\n(*) Please see the \"Response to AnonReviewer3\" who summarizes several important points:\n1. The mu ranges are well-known in RMT.  We summarize it Section 3 of the appendix and the discussion and references on page 16-17 of the appendix.\n2. The RMT we use comes with extremely strong theory.  Again, we summarize this in Section 3 of the appendix.\n3. We used a large range of different architectures and data, and we presented detailed empirical evidence.  The appendix presents much more.  See Section 4 and pages 20-29 of the appendix.  \n\n\"it may give some important intuition, but the content is not sufficiently rigorous to my knowledge\"\n\n(*) See out general comment above.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "SkxRNYgqA7", "original": null, "number": 5, "cdate": 1543272758445, "ddate": null, "tcdate": 1543272758445, "tmdate": 1543272758445, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "Bylje82KC7", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "More details: Specific comments in response to AnonReviewer4", "comment": "\"the results are far from rigorous theory\"\n\n(*) Please see our general comment above.\n\n\"it is not clear how recent results in MP theory yields the statements made in the paper\"\n\n(*) Please see Sections 3 and 5 of the appendix for details on this.\n\n\"It would be interesting to see more details about how these models are trained, as training greatly affects the value of the weights.\"\n\n(*) Our theory can be applied to pre-trained models, and in this sense there are no details to be provided.  We have done this for LeNet5, AlexNet, InceptionV3 (as reported in the appendix), as well as many other pre-trained models.  We also trained MLP3, retraiend LeNet5, and trained many times MiniAlexNet, and then applied our theory to those models.  Details are provided in Section 2.2, 4.1, and 6.1 of the appendix.  One of the main claims of the paper is that the qualitative properties we report are \\emph{very} robust to the details of the training process, in the sense that nearly every state-of-the-art pretrained model exhibits heavy-tailed structure.  For example, see Table 5 and Figure 12 in the appendix.\n\n\"Theoretical results are not clearly stated\"\n\n(*) We have summarized the known results in the literature for RMT.  At least as importantly, we have also addressed where the know results are deficient by doing numerical simulations.  See Section 3 of the appendix, and, in particular, Table 3 of the appendix for the summary of known results of RMT, and see Figure 6 of the appendix for numerical simulations testing the RMT results.\n\n\"it is not clear how to derive the theory in this paper based on the MP theory\"\n\n(*) We did not derive the theory.  Instead, based on our initial empirical results, we posited that the bulk and edge properties of RMT should be relevant, and we used existing results from RMT to taxonomize trained models in terms of 5+1 phases.  Admittedly, the relevant RMT is somewhat exotic and likely not familiar to many readers.  For that reason, although we can not fit it into ten pages, we provide a background summary of RMT in Section 3 of the appendix, and we provide a detailed description of heavy-tailed RMT in Section 5 of the appendix.  That theory was derived in the papers we cited in detail in those sections of the appendix, the main ones of which were also cited in the main paper.\n\n\"The definition of these 6 phases is not even explicitly given in this paper.\" \n\n(*) The definition is the visual taxonomy provided in Figure 2 of the main paper and the operational definition provided in Table 2 of the main paper.  Both of these are described in much more detail in Section 5 of the main paper.  Our theory is a practical theory, and thus it is operationalized by our code.  Our code is publicly-available.  We will link to it in a publicly-available technical report, and we will also link to it in the final version of this ICLR paper.\n\n\"the theory of all these phases seems to depend on equation (3)\"\n\n(*) No, it does not depend on Equation (3).  It is not clear to us that the Bulk Decay phase can be represented in this additive form.  We provided that equation simply to guide the reader.  We are happy to remove it if it adds confusion.\n\n\"there are no lemmas or propositions that gives a rigourious theoretical guarantee\"\n\n(*) The heavy tailed RMT provides these guarantees.  We show HOW these guarantees are observed in practice across every known pre-trained model we could find.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "r1eY-YgcAm", "original": null, "number": 4, "cdate": 1543272705472, "ddate": null, "tcdate": 1543272705472, "tmdate": 1543272705472, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "Bylje82KC7", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "More details: General comment to all reviewers", "comment": "Thanks for your careful reading.  We admit that our methods are less familiar to the community than other popular machine learning methods, but our paper contains empirical results and theoretical results that suggest that our methods should be more familiar (which was noted by the two non-official public reviewers who signed their names).  Our main empirical results are a detailed set of empirical plots and measurements on a wide range of networks/data.  Mathematical rigor is not their justification, but their ubiquity and robustness across a range of networks/data suggest they are important nonetheless.  Our theory is phenomenological and it is based on rigorous RMT.  Our main theoretical claim is that we can use existing rigorous RMT, both bulk statistics and edge properties, to construct a visual taxonomy of 5+1 phases of training that provides a predictive theory.  Then we used that theoretical insight to exhibit all phases of training simply by changing the batch size.  We have eschewed an overly-mathematical description of our theory.  That is important follow-up work, but our phenomenological theory is important and practical nonetheless.  Please see our specific comments in response to each reviewer below."}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "Bylje82KC7", "original": null, "number": 3, "cdate": 1543255539016, "ddate": null, "tcdate": 1543255539016, "tmdate": 1543255539016, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "ryxLKEntRX", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "To clarify, for the revision, the main part of the paper is identical, and we added a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.", "comment": "To clarify, for the revision, the main part of the paper is identical, and we added a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "SyxRrIzX0X", "original": null, "number": 1, "cdate": 1542821445584, "ddate": null, "tcdate": 1542821445584, "tmdate": 1542821659213, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "content": {"title": "Please read our full response/comment to reviewer comments, or read \"Response to AnonReviewer3\".", "comment": " \nThanks to the two positive reviewers who were willing to write their names.\n\nRegarding AnonReviewer3, we will simply point to the rebuke from anonymous \"Response to AnonReviewer3\".  We could not have said it better ourselves.\n\nActually, some of these comments also are relevant for AnonReviewer4.\n\nAnonReviewer4 thinks \"The empirical studies seem interesting.\" but would like to see \"more details about how these models are trained\".  One of the points of our methods it that we can apply them to already-trained models, as we did.  In particular, if you train a model, e.g., in a way that is not reproducible by others, then we can still apply our theory to that model.  (We also tested the theory on models we trained with mini-AlexNet, and more details on that are provided in the 59 page supplementary material.)  AnonReviewer4 also says that \"training greatly affects the value of the weights\".  This is not true.  One of our main empirical observations is that nearly every state-of-the-art pre-trained network exhibits these properties.  (Again, see the 59 page supplementary material for more.)  This empirical observation itself should merit publication of this paper.\n\nOur theory is a phenomenological predictive theory.  That means it does not include lemmas.  Also our theory does not depend on Eqn. (3).  That equation is just a heuristic guide to understand our theory, for readers familiar with models of that form.  We are not interested in gratuitous rigor.  We are interested in a predictive theory, which is what we have used RMT to construct.  Our theory predicts 5 phases of learning, and we are able to exhibit all 5 phases of learning by changing a single knob, the batch size.  That theoretical prediction and empirical validation itself should merit publication of this paper.\n\nAnonReviewer2 has said \"The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups.\"  That's hard to do in the page limitation.  For that reason, we have uploaded a revised version of the paper, identical to what we submitted, but with a 59 page supplementary material, which can also serve as a technical report version of this page-limited submission.\n\nDeep learning is by its very nature an empirical science.  It seems to violate a number of basic notions, most notably that a non-convex optimization problem can be applied successfully in such a wide number of problem domains and generalize so well.  It seems likely that it is not possible to understand this, i.e. why deep learning even works, relying purely on very loose, very general theoretical techniques.  We suggest that the reviewers and senior reviewers eschew false rigor, and instead focus on the empirical contributions and the theory that makes predictions that we validate.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616484, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJeFNoRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper22/Authors|ICLR.cc/2019/Conference/Paper22/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616484}}}, {"id": "H1gn4x1MaX", "original": null, "number": 3, "cdate": 1541693491541, "ddate": null, "tcdate": 1541693491541, "tmdate": 1541693491541, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "content": {"title": "Although the experiments seem interesting,  theoretical results are not clearly presented.", "review": "This manuscript studies the implicit regularization of neural networks from the perspective of random matrix theory. The authors provide both empirical and theoretical results that aim to show that the empirical spectral density of weights of DNN captures the implicit regularization phenomenon. However, the results are far from rigorous theory and it is not clear how recent results in MP theory yields the statements made in the paper. \n\n\nDetailed comments:\n\n1. The empirical studies seem interesting. It seems that two kinds of results are shown. The first one is that ESD fits perfectly for small models, and the second one is that deep models fit heavy-tailed random matrices class. It would be interesting to see more details about how these models are trained, as training greatly affects the value of the weights.\n\n2. Theoretical results are not clearly stated. In section 2, the authors introduce the basics of MP theory. However, it is not clear how to derive the theory in this paper based on the MP theory. It seems that the main theory is the \"5+1 phases of training\". The definition of these 6 phases is not even explicitly given in this paper. Moreover, the theory of all these phases seems to depend on equation (3) , but there are no lemmas or propositions that gives a rigourious theoretical guarantee.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "cdate": 1542234555702, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631464, "tmdate": 1552335631464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syx_tA0-6X", "original": null, "number": 3, "cdate": 1541693055712, "ddate": null, "tcdate": 1541693055712, "tmdate": 1541693189147, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "r1lZZNN5nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "content": {"comment": "I seriously question the good-faith of this reviewer and even s/he read the paper in detail. AnonReviewer3 is declining to accept empirical evidence. If you are going to make an \"educated guess\" without basic understanding, don't review highly technical papers.   \n\n1.   \\mu ranges are basically lack of reviewers RMT knowledge. Paper rigorously cites the findings where how this range is applied. These are established solidly in RMT.\n2.  Reviewer complaint on Figure 2 and Table 1 saying \"they lack any mathematical/rigorous definition\". This is absolutely not true. Spectral theory is very well established in mathematical theory along with RMT this paper uses.\n3. \"if the observations would be different for a different data set and a different setting. \" This is totally absurd. There is no way that any single machine learning paper can test all datasets and different settings. Paper used large range of different architectures and presented the empirical evidence as any published machine learning paper.\n\nI strongly recommend Chairs to not give any further reviews to this reviewer.  Reviewing without good-faith is a disservice to the community.", "title": "Response to AnonReviewer3"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311937233, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJeFNoRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311937233}}}, {"id": "r1lZZNN5nQ", "original": null, "number": 2, "cdate": 1541190649497, "ddate": null, "tcdate": 1541190649497, "tmdate": 1541534353468, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "content": {"title": "Interesting idea but not made rigorous/precise", "review": "The paper analyzes the empirical spectral density of DNN layher matrices and compares them with the traditionally-regularized statistical models, and develop a theory to identify 5+1 phases of training based on it. The results with different batch sizes illustrate the Generalization Gap pheneomena, and explains it as being causes by implicit self-regularization.\n\nHowever, the paper seems a little bit handwavy to me, without any serious theoretical justification. For example, why are \\mu=2 and 4 chosen as the threshold between weakly/moderately/very heavy-tailed? In addition, the paper is build upon o the 5+1 model as in Figure 2 and the graphical comparison between the empirical ESD and the expected ESD of the five models in Table 1, and they lack any mathematical/rigorous definition---see table 2. The simulations are performs over a particular data set and a particular setting, and I wonder if the observations would be different for a different data set and a different setting. \n\nAs a result, it may give some important intuition, but the content is not sufficiently rigorous to my knowledge.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "cdate": 1542234555702, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631464, "tmdate": 1552335631464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgsDBCZsQ", "original": null, "number": 1, "cdate": 1539593570887, "ddate": null, "tcdate": 1539593570887, "tmdate": 1541534353262, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "content": {"title": "A valuable contribution; yet, not well polished", "review": "The paper attempts to examine the reasons behind the strong generalisation performance of DNNs trained via SGD. The authors propose an analysis which offers a fresh view to this problem. This view has been articulated very well, and is based on sound mathematical arguments. \n\nOn the other hand, since there is no formal theorem to support the introduced assumptions, the authors have attempted to provide empirical evidence through experiments with standard DNN architectures and benchmark datasets. However, this is where the weakness of this paper lies: The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups. \n\nAlthough I accept the claim of the authors concerning the lack of space, they could also trim the Introduction so as to free up some space, as well as provide an indefinite number of extra supporting evidence in the form of Supplementary Material/Appendices. \n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper22/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Official_Review", "cdate": 1542234555702, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper22/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631464, "tmdate": 1552335631464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper22/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gvacTwiX", "original": null, "number": 2, "cdate": 1539984063499, "ddate": null, "tcdate": 1539984063499, "tmdate": 1539984616201, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "content": {"comment": "Authors have shown a clearly original contribution by directly mapping a classification of Random Matrix Theory (RMT) to resulting weight matrices of deep architectures empirically. The resulting taxonomy would help in understanding under what circumstances trained networks generalised better. This approach is novel because it avoids causality objections while it classify networks without any response metric.  Moreover, the intimate connection between statistical mechanics and deep learning architectures are often omitted by practitioners up to now, but it may be a mainstream approach in the near future. I strongly recommend this paper for publication.  ", "title": "Spectral properties of deep learning architectures are key to understand generalisation problems"}, "signatures": ["~Mehmet_Suezen1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["~Mehmet_Suezen1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311937233, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJeFNoRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311937233}}}, {"id": "r1xqJr5IjQ", "original": null, "number": 1, "cdate": 1539904737939, "ddate": null, "tcdate": 1539904737939, "tmdate": 1539906126803, "tddate": null, "forum": "SJeFNoRcFQ", "replyto": "SJeFNoRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "content": {"comment": "I believe the only people who will advance ML are those with an understanding of statistical mechanics.  Your ideas of how to treat the energy landscape are spot on!  You are doing some great work.  My intuition tells me that self organization plays a large roll in how to treat the weight matrices.  ", "title": "Great paper that goes into detail regarding the statistical mechanics of machine learning and explains the energy landscape"}, "signatures": ["~Gershon_Mathew_Wolfe1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper22/Reviewers/Unsubmitted"], "writers": ["~Gershon_Mathew_Wolfe1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Traditional and Heavy Tailed Self Regularization in Neural Network Models", "abstract": "Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.", "paperhash": "martin|traditional_and_heavy_tailed_self_regularization_in_neural_network_models", "keywords": ["statistical mechanics", "self-regularization", "random matrix", "glassy behavior", "heavy-tailed"], "authorids": ["charles@calculationconsulting.com", "mmahoney@stat.berkeley.edu"], "authors": ["Charles H. Martin", "Michael W. Mahoney"], "TL;DR": "See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)", "pdf": "/pdf/1d06ef44c0deafa874c86dbf29dbbb814ee58765.pdf", "_bibtex": "@misc{\nmartin2019traditional,\ntitle={Traditional and Heavy Tailed Self Regularization in Neural Network Models},\nauthor={Charles H. Martin and Michael W. Mahoney},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeFNoRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper22/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311937233, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJeFNoRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper22/Authors", "ICLR.cc/2019/Conference/Paper22/Reviewers", "ICLR.cc/2019/Conference/Paper22/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311937233}}}], "count": 16}