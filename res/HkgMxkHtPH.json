{"notes": [{"id": "HkgMxkHtPH", "original": "SJeFmfsuwS", "number": 1497, "cdate": 1569439465857, "ddate": null, "tcdate": 1569439465857, "tmdate": 1577168244648, "tddate": null, "forum": "HkgMxkHtPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZG27KFOTGM", "original": null, "number": 1, "cdate": 1576798724799, "ddate": null, "tcdate": 1576798724799, "tmdate": 1576800911700, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposed to improve the quality of underwater images, specifically color distortion and haze effect, by an unsupervised generative adversarial network (GAN). An end-to-end autoencoder network is used to demonstrate its effectiveness in comparing to existing works, while maintaining scene content structural similarity. Three reviewers unanimously rated weak rejection. The major concerns include unclear difference with respect to the existing works, incremental contribution, low quality of figures, low quality of writing, etc. The authors respond to Reviewers\u2019 concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718667, "tmdate": 1576800269182, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Decision"}}}, {"id": "S1gemwT6YH", "original": null, "number": 1, "cdate": 1571833623786, "ddate": null, "tcdate": 1571833623786, "tmdate": 1574650535855, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "[Update after rebuttal period]\nIn response, the authors cannot clearly clarify the difference between this work with existing works integrating the physical model into the network. Thus I stay my original score.\n\n\n[Original reviews]\nThis paper proposed an unsupervised generative adversarial network for underwater generating realistic underwater images and haze removal, which can simultaneously deal with the color restoration and haze in the realistic underwater environment.\n\nFirstly, according to the widely used physical model in the image processing area, employed the UnderwaterGAN to trained parameters in advanced, and then use U-Net for color restoration and haze removal of underwater images. However, many existing works used the physical model to represent the imaging principles and using deep network to learn prior knowledge. Thus, I think the proposed idea is a little bit incremental.\n\nFor the experimental part, the experimental results fully demonstrate the effectiveness of the proposed method in comparison with state-of-the-art methods. Additionally, the ablation studies in the appendix also give us the intuition by using the different loss functions. Also, I suggest the authors demonstrate the proposed method on not only low-level, but also high-level vision tasks, e.g., underwater image target detection. \n\nFinally, the paper is well organized and sentence expression is also clear, but small errors that are correctable. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575597748751, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Reviewers"], "noninvitees": [], "tcdate": 1570237736518, "tmdate": 1575597748764, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review"}}}, {"id": "SJx8OG8Fsr", "original": null, "number": 4, "cdate": 1573638766483, "ddate": null, "tcdate": 1573638766483, "tmdate": 1573638964838, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "Skl-8kPxjr", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for pointing out some problems in our work. Please find our response to your questions below. We have updated the paper and uploaded a revision on Nov 13.\n\n**1) The literature is limited.** \n\n**Response:** We have cited and listed some new references in the background. In section 4, the method we chose to compare with ours can be roughly divided into three types: model-free algorithms, model-based algorithms, and deep-learning-based algorithms. These methods are classical and representative, we can't list too much due to paper length limit.\n\n> Anwar S, Li C, Porikli F. Deep underwater image enhancement[J]. arXiv preprint arXiv:1807.03528, 2018.\n>\n> Ancuti C, Ancuti C O, De Vleeschouwer C. D-hazy: A dataset to evaluate quantitatively dehazing algorithms[C]//2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016: 2226-2230.\n>\n> Uplavikar P, Wu Z, Wang Z. All-In-One Underwater Image Enhancement using Domain-Adversarial Learning[J]. arXiv preprint arXiv:1905.13342, 2019.\n>\n> Anwar S, Li C. Diving Deeper into Underwater Image Enhancement: A Survey[J]. arXiv preprint arXiv:1907.07863, 2019.\n>\n> Ding X, Wang Y, Yan Y, et al. Jointly Adversarial Network to Wavelength Compensation and Dehazing of Underwater Images[J]. arXiv preprint arXiv:1907.05595, 2019.\n>\n> Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.\n\n**2) The underwater imaging model presented in this paper derives from the Jaffe-McGlamery model, which is a common sense in this field. The authors use a generator to produce underwater images that only implements the common model by a neural network. Moreover, the statement of section 2.2 is not clear. Please rewrite this section.**\n\n**Response:** We have rewritten section 2.1 and 2.2. Inspired by in-air images dehazing algorithms, we improved the underwater imaging model in this paper. Then, we employed GAN for generating more realistic underwater-style images based on the improved model (taken both light attenuation and haze effect in real-world underwater images into consideration), which can be found in Sections 2.1 and 2.2 in this paper.\n\n**3) The authors used U-Net without any improvement to enhance the results generated from UWGAN, which is the integration of existing models.**\n\n**Response:** U-Net is an efficient tool for the proposed pipeline. We employed U-Net as an enhancement network structure, but not only that, we studied the effect of different loss functions in U-Net (The detailed content can be found in Page 11, section APPENDIX), which could provide a new idea for further research about loss functions on underwater image enhancement. Considering the inference speed and Flops, U-Net is better than other networks and could run on real-time compared to other deep-learning-based methods mentioned in this paper.\n\n**4) The authors claimed that their model is better than others, while there is no evidence to indicates that.**\n\n**Response:** We have revised some imprecise sentences of the result analysis part in section 4, \u201cTable 1 and Table 2 quantitatively show the scores of sample images in Figure 5 and Figure 6 respectively. It can be seen that our proposed method has achieved the highest scores in (a), (c) and (f). In addition, the average quantized scores evaluated on RealA, RealB, and RealC datasets are shown in Table 3. Our model achieves the best score in terms of color restoration.\u201d Besides, we add FLOPs of deep-learning-based methods in Table 5.\n\n**5) Please carefully check the references.**\n\n**Response:** We have revised small errors in references. \n\n> \u201cHummel R. Image enhancement by histogram transformation[J]. Computer Graphics and Image Processing, 1977, 6(2):184-195.\u201d\n\n**6) High-resolution figures should be given in the manuscript.**\n\n**Response:**  We have improved the resolution of images in this paper. It should support higher magnifications."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1497/Authors|ICLR.cc/2020/Conference/Paper1497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155142, "tmdate": 1576860549314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment"}}}, {"id": "SkxGSfLtsH", "original": null, "number": 3, "cdate": 1573638714412, "ddate": null, "tcdate": 1573638714412, "tmdate": 1573638941158, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "SyeZlLA6Yr", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for pointing out some problems in our work. Please find our response to your questions below. We have updated the paper and uploaded a revision on Nov 13.\n\n**1) This paper points out that the previous work (i.e. WaterGAN) generates color noise and the camera model is not suitable, how does this proposed method overcome these points?**\n\n**Response:** The network structure of WaterGAN can be found here (https://github.com/kskin/WaterGAN). \n\nThe image synthesized by WaterGAN suffers color noise due to the input of noise vector z, which was observed when we tested WaterGAN.\n\n\u201cOne limitation of our model is in the parameterization of the vignetting model, which assumes a centered vignetting pattern. This is not a valid assumption for the MHL dataset, so our restored images still show some vignetting though it is partially corrected.\u201d, This sentence is mentioned in the original paper of WaterGAN (In page 6, above VI. Conclusion). It is our mistake to call vignetting model as camera model in Introduction part.\n\nIn order to solve the color noise problem, noise vector z is no longer necessary in our UWGAN model, UWGAN takes color image and its depth map as input, so our model can avoid color noise problem. And, we didn't use \u201cvignetting model\u201d in our model. Inspired by in-air images dehazing algorithms, we improved the underwater imaging model in this paper. Then, we employed GAN for generating more realistic underwater-style images based on the improved model (taken both light attenuation and haze effect in real-world underwater images into consideration), which can be found in Sections 2.1 and 2.2 in this paper. \n\n**2) The figures in this paper are too blurry to see them**\n\n**Response:** We have improved the resolution of images in this paper. It should support higher magnifications.\n\n**3) The technical contribution of the proposed method is not clear**\n\n**Response:** As mentioned in the first paragraph, inspired by in-air images dehazing algorithms, we improved the underwater imaging model in this paper. Then, we employed GAN for generating more realistic underwater-style images based on the improved model (taken both light attenuation and haze effect in real-world underwater images into consideration), which can be found in Sections 2.1 and 2.2 in this paper. We employed U-Net as an enhancement network structure, but not only that, we studied the effect of different loss functions in U-Net (The detailed content can be found in Page 11, section APPENDIX), which could provide a new idea for further research about loss functions on underwater image enhancement. Considering the inference speed and Flops, U-Net is better than other networks and could run on real-time compared to other deep-learning-based methods mentioned in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1497/Authors|ICLR.cc/2020/Conference/Paper1497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155142, "tmdate": 1576860549314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment"}}}, {"id": "Bken178tjr", "original": null, "number": 5, "cdate": 1573638883787, "ddate": null, "tcdate": 1573638883787, "tmdate": 1573638883787, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment", "content": {"title": "Summary of Revision", "comment": "Following reviewers' suggestions, we have updated the paper and uploaded a revision on Nov 13. Here we give a summary of the major changes.\n\n1. We have rewritten section 2.1 and section 2.2. Now we clearly state the improved underwater imaging model and our technical approaches.\n2. In section 4, we add the results of underwater target detection,  which demonstrates that our proposed model can help in underwater high-level computer vision tasks.\n3. We have replaced all figures with higher resolution versions in this paper.\n4. We add FLOPs in Table 5.\n5. We fixed small errors and inappropriate sentence expression."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1497/Authors|ICLR.cc/2020/Conference/Paper1497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155142, "tmdate": 1576860549314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment"}}}, {"id": "rJl7F-UFjS", "original": null, "number": 2, "cdate": 1573638523183, "ddate": null, "tcdate": 1573638523183, "tmdate": 1573638649703, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "S1gemwT6YH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for pointing out some problems in our work. Please find our response to your questions below. We have updated the paper and uploaded a revision on Nov 13.\n\n**1) Many existing works used the physical model to represent the imaging principles and using deep network to learn prior knowledge**\n\n**Response:** Inspired by in-air images dehazing algorithms, we improved the underwater imaging model in this paper, but it has not been stated clearly before. Then, we employed GAN for generating more realistic underwater-style images based on the improved model (taken both light attenuation and haze effect in real-world underwater images into consideration), which can be found in Sections 2.1 and 2.2 in this paper. \n\nWe employed U-Net as an enhancement network structure, but not only that, we studied the effect of different loss functions in U-Net (The detailed content can be found in Page 11, section APPENDIX), which could provide a new idea for further research about loss functions on underwater image enhancement. Considering the inference speed and Flops, U-Net is better than other networks and could run in real-time compared to other deep-learning-based methods mentioned in this paper.\n\n**2) High-level vision tasks **\n\n**Response:** We applied YOLO v3 target detector on degraded underwater images and their enhanced versions generated by our model. The performance of underwater target detection is better on enhanced versions of degraded images\uff0cwhich demonstrated our proposed method on high-level underwater computer-vision tasks. This part can be found in section 4, Page 8.\n\n**3) Small errors** \n\n**Response:** We have modified some sentence expressions and small errors in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1497/Authors|ICLR.cc/2020/Conference/Paper1497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155142, "tmdate": 1576860549314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment"}}}, {"id": "Skl-8kPxjr", "original": null, "number": 3, "cdate": 1573052232785, "ddate": null, "tcdate": 1573052232785, "tmdate": 1573052439055, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "In this article, the authors propose a generative adversarial network named UWGAN to generate realistic underwater images from the pairs of in-air images and depth images. Then, a U-Net was leveraged to enhance the results. \nHowever, the text suffers from too many language problems. The authors should consult professional proofreading services. As a courtesy towards referees, the quality of writing needs meticulous attention before a scientific paper should be submitted. \n\tOther comments:\n1.\tThe literature is limited. I found some novel works being done in the field that must be addressed and listed in the background and experiments.\n2.\tThe underwater imaging model presented in this paper derives from the Jaffe-McGlamery model, which is a common sense in this field. The authors use a generator to produce underwater images that only implements the common model by a neural network. Moreover, the statement of section 2.2 is not clear. Please rewrite this section.\n3.\tThe authors used U-Net without any improvement to enhance the results generated from UWGAN, which is the integration of existing models. \n4.\tThe authors claimed that their model is better than others, while there is no evidence to indicates that. For example, 1) in (page 5, line 4 from bottom), \u201cIt can be seen that our proposed method has achieved a higher score.\u201d, can we observe this from the Table 1 and 2? 2) \u201cThe method we proposed has the fastest processing speed compared to other methods. Moreover, the method proposed in this paper has the fewest parameters compared to other deep-learning-based methods.\u201d, it is suggested that a study about the parameters and FLOPs of the involved methods should be given.\n5.\tPlease carefully check the references. For example, \u201cHummel R. Image enhancement by histogram transformation[J]. Unknown, 1975.\u201d lacks the journal name.\n6.\tHigh-resolution figures should be given in the manuscript.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575597748751, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Reviewers"], "noninvitees": [], "tcdate": 1570237736518, "tmdate": 1575597748764, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review"}}}, {"id": "SyeZlLA6Yr", "original": null, "number": 2, "cdate": 1571837417314, "ddate": null, "tcdate": 1571837417314, "tmdate": 1572972460855, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper uses U-net for underwater image restoration and enhancement. But, it is difficult to obtain realistic underwater images, thus this paper introduces a GAN-based method to generate realistic underwater images from in-air image and depth map pairs.\n\n- Although this paper points out that the previous work (i.e. WaterGAN) generates color noise and the camera model is not suitable, how does this proposed method overcome these points? Please make it clear.\n\n- The figures in this paper are too blurry to see them. To evaluate the effectiveness of the proposed method, the figures are important, thus, it would be better to make them clear.\n\n- The technical contribution of the proposed method is not clear. The proposed method seems to be just using the existing techniques."}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575597748751, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Reviewers"], "noninvitees": [], "tcdate": 1570237736518, "tmdate": 1575597748764, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Review"}}}, {"id": "HJei8hV35H", "original": null, "number": 1, "cdate": 1572781138670, "ddate": null, "tcdate": 1572781138670, "tmdate": 1572781138670, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HyegdV42cB", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment", "content": {"title": "Reply to \"author names are shown in paper\"", "comment": "This is our first-time submission to ICLR. I am very sorry to have made a mistake. Is there anything we can do to correct this mistake? Can we resubmit our paper with hiding the authors' names?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1497/Authors|ICLR.cc/2020/Conference/Paper1497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155142, "tmdate": 1576860549314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Official_Comment"}}}, {"id": "HyegdV42cB", "original": null, "number": 1, "cdate": 1572779111525, "ddate": null, "tcdate": 1572779111525, "tmdate": 1572779111525, "tddate": null, "forum": "HkgMxkHtPH", "replyto": "HkgMxkHtPH", "invitation": "ICLR.cc/2020/Conference/Paper1497/-/Public_Comment", "content": {"title": "Author names are shown in paper", "comment": "Ain't this a violation of double blind reviewing policy?"}, "signatures": ["~Chenxu_John_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chenxu_John_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "authors": ["Nan Wang", "Yabin Zhou", "Fenglei Han", "Lichao Wan", "Haitao Zhu", "Yaojing Zheng"], "authorids": ["nanwangmail@hrbeu.edu.cn", "zyb0977@163.com", "fenglei_han@hrbeu.edu.cn", "wanlch1203@hrbeu.edu.cn", "zhuhaitao_heu@163.com", "yaojingzheng_heu@163.com"], "keywords": ["underwater image", "image restoration", "image enhancement", "GAN", "CNNs"], "TL;DR": "A new apporach to enhance underwater images based on GAN and CNNs", "abstract": "In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "pdf": "/pdf/f71080dccef9ab4aa1c2e97f320d7f52fa560edc.pdf", "code": "https://github.com/infrontofme/UWGAN_UIE", "paperhash": "wang|uwgan_underwater_gan_for_realworld_underwater_color_restoration_and_dehazing", "original_pdf": "/attachment/31683db901c6382fde11e867fa626f1cfe6d4ad8.pdf", "_bibtex": "@misc{\nwang2020uwgan,\ntitle={{\\{}UWGAN{\\}}: {\\{}UNDERWATER{\\}} {\\{}GAN{\\}} {\\{}FOR{\\}} {\\{}REAL{\\}}-{\\{}WORLD{\\}} {\\{}UNDERWATER{\\}} {\\{}COLOR{\\}} {\\{}RESTORATION{\\}} {\\{}AND{\\}} {\\{}DEHAZING{\\}}},\nauthor={Nan Wang and Yabin Zhou and Fenglei Han and Lichao Wan and Haitao Zhu and Yaojing Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgMxkHtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgMxkHtPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504193899, "tmdate": 1576860582598, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1497/Authors", "ICLR.cc/2020/Conference/Paper1497/Reviewers", "ICLR.cc/2020/Conference/Paper1497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1497/-/Public_Comment"}}}], "count": 11}