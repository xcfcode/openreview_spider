{"notes": [{"id": "awnQ2qTLSwn", "original": "Hnlae1HtZF4", "number": 3486, "cdate": 1601308386859, "ddate": null, "tcdate": 1601308386859, "tmdate": 1614985705820, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Cxxy7oUT1n", "original": null, "number": 1, "cdate": 1610040437864, "ddate": null, "tcdate": 1610040437864, "tmdate": 1610474038670, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Although there was some initial disagreement on this paper, the majority of reviewers agree that this work is not ready for publication and can be improved in various manners. After the discussion phase there is also serious concern that the experiments need more work (statistically), to verify if they hold up. More comparisons with baselines are required as well. The paper could also be better put in context with the SOTA and related work. The paper does contain interesting ideas and the authors are encouraged to deepen the work and resubmit to another major ML venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040437851, "tmdate": 1610474038654, "id": "ICLR.cc/2021/Conference/Paper3486/-/Decision"}}}, {"id": "gvJLIDDlYrL", "original": null, "number": 3, "cdate": 1603772171075, "ddate": null, "tcdate": 1603772171075, "tmdate": 1606780808571, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review", "content": {"title": "An interesting paper on learning to share rewards", "review": "Summary\nThe paper considers the cooperative MARL setting where agents get local rewards and they are interconnected as a graph where neighbors can communicate. The paper specifically considers the communication of reward sharing, that is, an agent shares (part of) its reward to its neighbors, such that each agent optimizes its local reward plus rewards from its neighbors. This motivates a bi-level optimization framework where the high-level policy decides how the rewards are shared and the low-level policy locally optimizes the shared rewards given the high-level\u2019s decision. The paper\u2019s flow motivates such a framework well. The experimental results demonstrate the method\u2019s effectiveness. I think it is a strong paper (accept), but my confidence is low due to the following confusions I have.\n \nComments/Questions\n \n1. I have a high-level comment on the reward sharing mechanism. It seems that the proposed method does not support multi-hop sharing because rewards can only be shared to neighbors. Why is this single-hop sharing effective in the experiments? Is it because of domain-specific reasons, or it\u2019s because that single-hop sharing is in principle equally effective, why?\n\n2. The derivation of (18) using taylor expansion is unclear to me. Could the authors explain it with more details?\n\n3. I don\u2019t fully understand the proof of Proposition 4.2. Specifically, does \u201cphi can be learned in a decentralized manner\u201d mean that the *optimal* phi can be based on only the local observation for each agent, instead of based on global state? Could the authors comment on the approximation error induced by the mean-field approximation? Why the proof begins with phi_i based on o_i and ends with phi_i based on global state s.\n\n4. In Equation (17) and (20), should phi^* be just phi (i.e. no * here)?\n\n5. The low-level policy is to optimize the shared rewards. My understanding is that any (single-agent) RL algorithm can be used for optimizing the shared rewards, e.g. DQN, DDPG, A2C, etc. Why would the authors choose DGN, a rather less popular RL algorithm? Have the authors tried more popular algorithms as the low-level policy?\n\n6. For fixed LToS,  how do we determine the fixed sharing weights?\n\n---\nThanks for the response. I've increased my confidence. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074953, "tmdate": 1606915783177, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3486/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review"}}}, {"id": "2J1Lj5YxCeI", "original": null, "number": 1, "cdate": 1603258408339, "ddate": null, "tcdate": 1603258408339, "tmdate": 1606737084123, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review", "content": {"title": "Some interesting ideas, but issues with formalizing the problem setting, theory and unconvincing results. ", "review": "Update: I appreciate the detailed replies to my questions. Indeed, some of the points I raised were addressed well and the paper updated accordingly. \n\nHowever, some new concerns were also raised by the replies:\n- Using 3 seeds for the experimental evaluation is an extremely questionable evaluation protocol. There is no way to know if any of the results are going to hold up. \n- It's also clear now that none of the experiments are comparing to benchmark numbers from other publications. It would have been more confidence inspiring if the method was tested on a set of tasks where external benchmarks have already been established. \n- This is particularly true for the new results that were added to the paper, e.g. the QMIX results. It's difficult to make sense of them and the instability points towards a potential hyperparameter issue.   \n- All baselines for the 'prisoners' case should at least compare to the fully cooperative case of adding up the rewards. Comparing to a DQN baseline that maximizes individual rewards is a red herring. \n- It's odd that all experiments require less than 1000 episodes to train. This is very unusual for challenging multi-agent RL problems. It would be great to understand if the main selling point of LToS is sample-complexity/learning speed or if there is something else going on.\n\nI also agree with the concern raised by other reviewers that the paper is currently not positioned clearly. \nAll things considered, I believe my score is still appropriate for the paper. However, I also believe that a future version of the paper with clarified positioning and more thorough experimental evaluation could make for a compelling contribution.\n\nOriginal review:\n==========\n-\"Obviously, CTDE cannot address such problems due to the curse of dimensionality.\". CTDE means that there is the *option* to use centralized information at training time. Clearly, some ways of using centralized information will scale better than others and claiming that none of them scale is simply unfounded. \n\n-\"One is that the reward function.. tragedy of the commons.\". I am struggling to make sense of this paragraph. Please work on the clarity of the writing. \n\n-\"However, they are learned in a centralized way and hence not scalable.\" These methods have been scaled to large numbers of agents in complex environments. Please provide citations when making a claim that something doesn't scale. For example, the \"The StarCraft Multi-Agent Challenge\", Samvelyan et al 2020, includes results for numbers of agents comparable to the largest experiments in this paper.  \n\n-\"Moreover, the observation of each agent oi \u2208 Oi can be enhanced to the Cartesian product of agent i and its neighbors (Jiang et al.,\n2020) or the observation history (Chu et al., 2020)\". I don't follow this. If the observation of each agent includes the observation of all neighbors (which includes the observation of their neighbors), then shouldn't everyone observe everything? \n\n-Equation (1) is wrong. The left-hand side conditions on 'oi_', but the right-hand side conditions on 's'. This also affects all following equations. \n\n-\"The simple way to optimize the global objective is that each agent maximizes its own expected return, which is known as Markov game. \". This is wrong. When each agent optimizes their own expected return this is typically not a means of optimizing the global objective. \n\n-\". In networked MARL, as the reward of an agent is assumed to depend on the actions of neighbors, we allow reward\nsharing between neighboring agents\": The reward function also depends on the global state, 's', which is a function of the joint action of all of the agents. So this local reward sharing seems clearly insufficient in general. \n\n- Eqn 6 to 15: This proof seems unnecessarily cumbersome. W only redistributes the rewards, so the sum of total rewards is unchanged, qed.\n\n-\"Unlike existing hierarchical RL methods, we can directly construct the value function and action value function of \u03c6 based on the value function of \u03c0 at each agent.\": Constructing the value function isn't really the problem, but approximating and learning it is challenging.\n\nTheory:\n-4.3: \"Each vertex has its own local policy \u03c6ij (wij |oi), and we can verify their independence by means of Markov Random Field.\" This is not clear to me. Furthermore, given that the transition function conditions on the joint action and that the reward function depends on the central state, this seems wrong. Unless I am mistaken, the dependency on the central state should break any locality assumptions. \n\nExperiments:\n- The results on the prisoner's dilemma are misleading. Clearly, if there is an ability to change the reward functions of individual agents (which is assumed by LToS), there is no more social dilemma. As such, only baselines that maximize the total reward are credible comparisons (and seem to be missing completely). \n- The \"traffic\" and \"ROUTING\" experiments seem more interesting. A few caveats: None of the results include uncertainty estimates. It is furthermore unclear, how many seeds were used. Furthermore, the fixed LToS baseline (\"For ablation, we keep the sharing weights fixed for each agent, named fixed LToS\") seems odds. Did you try a baseline where all agents simply share their reward equally with their neighbors? Also, centralized baselines are missing. E.g: https://arxiv.org/pdf/1910.00091.pdf. \n- In \"ROUTING\" Fixed LToS (ie. not learning to share) and LToS seem indistinguishable. \n\n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074953, "tmdate": 1606915783177, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3486/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review"}}}, {"id": "f9TS3KA-XLG", "original": null, "number": 9, "cdate": 1606034724039, "ddate": null, "tcdate": 1606034724039, "tmdate": 1606034724039, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "To all the reviewers", "comment": "We thank all the reviewers for the insightful comments. For the main concerns regarding CTDE methods, we have revised our claims in the context of literature and performed additionally experiments of CTDE (i.e., QMIX) in traffic and routing. The results show QMIX does not perform well. Please refer to the revision for details. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "x7ZgcFBCeKb", "original": null, "number": 8, "cdate": 1606034517624, "ddate": null, "tcdate": 1606034517624, "tmdate": 1606034517624, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "-JHJijUyJE4", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 5", "comment": "> \"it isn't obvious why and how relevant the cited works are... mentioning VDN, QMIX, and QTRAN (which together are some of the latest works in the factorization methods) does not seem to serve any further purpose, as they are no longer compared quantitatively or qualitatively to LToS... there appears to be no evidence whatsoever presented in the latter sections of the paper to show, let alone prove, the superior scalability of LToS.\"\n\nWe position LToS at a particular line of research on networked MARL, where agents form a graph, have restricted communication (limited to neighboring agents), and cooperate on the objective of maximizing the average of cumulative rewards of all agents, following the setting of Zhang et al. (ICML\u201918), Qu et al. (NeurIPS\u201919), Chu et al. (ICLR\u201920), Qu et al. (NeurIPS\u201920). Actually, we do not argue that our method is very scalable while these factorization methods are totally not. We have revise the claims to make them precise and clear.\n\nNetworked MARL focuses on decentralized learning as well as maximizing average return of all agents, while factorization methods are centralized training and focus the case all agents share a reward. Factorization methods can certainly be applied networked MARL. But in the literature it is empirically shown that QMIX performs poorly in large-scale networked MARL (Qu et al., 2020a). We also additionally performed the experiment of QMIX in 1) *traffic*, and it shows QMIX does not perform well as illustrated in Figure 4 and Table 2, and 2) *routing*, and it shows QMIX does not perform well either as illustrated in Figure 7 and Table 4.\n\n> Furthermore, some of the cited works have been left out at the evaluation stage, which leaves the reviewer puzzled as to which baselines LToS really hopes to outshine. The work needs some justification over why the following studies have not been compared to in the evaluation\n\nThe main strength of LToS lies in \"its capability to resolve selfishness and assign credits appropriately to bring about a harmonious cooperation in social dilemmas\". LToS aims to bring a harmonious cooperation by reward sharing in networked MARL. Therefore, we compared LToS to the methods for networked MARL, such as ConseNet and NeurComm. Moreover, as the communication is limited in neighborhood, the methods of communication are not quite related. Additionally, DGN (Jiang et al. ICLR\u201920) is employed to properly handle the communication within neighborhood. That also makes us free from comparison with some methods of communication like CommNet (Sukhbaatar et al. NeurIPS 2016), because DGN already showed its advantage over CommNet by experiments when proposed.\n\nEccles et al. (CoRR 2019) introduced two types of agents: innovator and imitator. There is an intrinsic reward added to the environment reward, so it is still one of the approaches that rely on hand-crafted reward designs, as summarized in Related Work. Moreover, the imitator needs to use the action of the innovator at each timestep to compute the intrinsic reward, which however is unrealistic in practice as they admitted in the paper.\n\nFor BAD, all the experiments are performed in two-play cooperative games. It is non-trivial to extend it to more than two players. Besides, its hierarchical mechanism requires global information which is not realistic in our scenario.\n\nHostallero et al. (AAMAS 2020) aim at maximizing social welfare, too. But unlike our work, they simply use temporal difference error for reward shaping instead of real reward sharing, and there is no explicit optimization for social welfare.\n\nYang et al. (ICML 2018) just use mean-field method and neighbors' information to guarantee scalability and convergence to Nash equilibrium, but their goals don't contain global return optimization. Besides, DGN already showed its advantage over their MFQ by experiments when proposed.\n\n> Synchronization is definitely not cost-free; all the more so if the synchronized RNG is used to sample an experience from the agents' replay buffers. How do the agents synchronize their RNG in a decentralized manner?\n\nAs agents cooperate on maximizing social welfare (not competitive setting), they can simply use a pre-defined RNG. Or, do we misunderstand your question?\n\n> In the Routing evaluation. has overhead been taken into account? How does LToS fare with respect to varied communications channel? What if the network were sparser? Do you observe any trends as you vary the extent of network connectivity?\"\n\nRouting is a very complex problem and we only test LToS in a simplified scenario. We did not investigate varied communication channel of backbone network and network connectivity. More thorough investigation in routing will be considered in future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "hp8I06MOTMh", "original": null, "number": 7, "cdate": 1606034456323, "ddate": null, "tcdate": 1606034456323, "tmdate": 1606034456323, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "60wI5bcMhON", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 3", "comment": "> At the end of Introduction, the sentence \u2018LToS is easy to implement and currently realized by DDPG\u2026\u2019 can be misleading because of the word \u2018realized\u2019 and the fact that authors argue that LToS is a newly proposed method. Does this mean LToS simply combines DDPG and DGN? Do Figure 5 and 6 represent selfishness of agents when LToS is used? \n\nWe consider our LToS more of a new hierarchical MARL framework than a new method, so it is not restricted to DDPG+DGN but can be realized by diverse combinations of methods. It is just \"currently realized\" by DDPG+DGN in the experiments. Yes, Figure 5 and 6 are trying to represent temporal and spatial pattern of agents' selfishness of LToS."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "C6-usLCnHCO", "original": null, "number": 6, "cdate": 1606034424303, "ddate": null, "tcdate": 1606034424303, "tmdate": 1606034424303, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "gvJLIDDlYrL", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 2", "comment": "> Why is this single-hop sharing effective in the experiments? Is it because of domain-specific reasons, or it is because that single-hop sharing is in principle equally effective, why?\n\nThere is an implicit assumption in networked MARL that each agent can only perform single-hop communication in one timestep. That is, two-hop communication requires two timesteps, which could make communicated information outdated. But, how to deal with this is not the focus of this paper.\n\n> The derivation of (18) using taylor expansion is unclear to me. Could the authors explain it with more details?\n\nIt is not suitable to be explained as a Taylor expansion. It is just alternative update as in DARTS (Liu et al. ICLR'19) to settle the problem of bi-level optimization. We have corrected this in the revision.\n\n> I don\u2019t fully understand the proof of Proposition 4.2. Specifically, does \u201cphi can be learned in a decentralized manner\u201d mean that the optimal phi can be based on only the local observation for each agent, instead of based on global state? Could the authors comment on the approximation error induced by the mean-field approximation? Why the proof begins with $\\phi_i$ based on $o_i$ and ends with $\\phi_i$ based on global state s.\n\nSorry for the confusion. We have revised the paper to address this. We have deferred the approximation of state by observations (or history) to the end, which makes the mathematical part simple and clear. About the approximation error, we are afraid that we could not give a generic quantitative error analysis at this stage, because it is extremely hard to model and analyze the error brought in by the reduction of action dependency.\n\nWe notice that there are some works that study on the theoretical foundation and error analysis of MARL, but they usually rely on some strong and special assumptions, like exponential decay property and independent state (local observation)  transition (Qu et al. 2019, arXiv, 1912.02906) which do not hold in many real applications and thus become not general but limited (Qu et al. NeurIPS'20). \n\n> In Equation (17) and (20), should phi^* be just phi (i.e. no * here)?\n\nTypos, $\\phi^*$ is supposed to be $\\phi$ here. We have corrected this. \n\n> The low-level policy is to optimize the shared rewards. My understanding is that any (single-agent) RL algorithm can be used for optimizing the shared rewards, eg DQN, DDPG, A2C, etc. Why would the authors choose DGN, a rather less popular RL algorithm? Have the authors tried more popular algorithms as the low-level policy?\"\n\nThe understanding is correct. We consider our LToS more of a new hierarchical MARL framework than a new method, so it's not restricted to DDPG+DGN but can be realized by diverse combinations of methods. We currently employed DGN in our experimental because it is capable to handle communication while others (DQN, DDPG, A2C) are not, and it has shown its advantage over others like CommNet (Sukhbaatar et al. NeurIPS 2016).\n\n> For fixed LToS, how do we determine the fixed sharing weights?\n\nFor *prisoner*, we set selfishness specially to average global return (i.e., 0.5). For *traffic* and *routing*, we used the best fixed selfishness found by grid search. We have made this clear in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "BFC28lltF-e", "original": null, "number": 5, "cdate": 1606034349674, "ddate": null, "tcdate": 1606034349674, "tmdate": 1606034349674, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "b9O_b_yBHBo", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 4", "comment": "> The contribution of the paper is mainly in formulating the problem in the actor-critic setup of DDPG method which leads to a limited novelty.\n\nOur main contribution is how to learn reward sharing so as to maximize the average return of all agents in networked MARL. LToS is a new hierarchical MARL framework rather than an actor-critic setup, so it is not restricted to current instantiation of DDPG+DGN but can be realized by diverse combinations of methods.\n\n> A key concern about the paper is how to decompose the reward in the first place. The paper aims at optimizing a global objective and assumes (also in the propositions) that this objective has additive connection with the decentralized rewards. Nevertheless, this is a strong assumption, particularly in real-world applications. A global reward can be decomposed into summation of smaller rewards, but not necessarily the other way around. As long as there is a global objective, we need a way to distribute the reward among the agents via learning or reward reshaping (or even manually). How can we properly define the reward of each agent in such scenarios?\n\nIn networked MARL, we do not decompose the reward, but each agent naturally has an individual reward, following the setting of Zhang et al. (ICML\u201918), Qu et al. (NeurIPS\u201919), Chu et al. (ICLR\u201920), Qu et al. (NeurIPS\u201920). For example, in traffic, the reward of each agent is the negative of the queue lengths. In networked MARL, the main problem is just how to advocate cooperating on the objective of maximizing the average of cumulative rewards of all agents, which serves as the global objective.\n\n> It is also unclear what is the benefit of sharing only with the neighbors. The method learns a weight vector of size |N_i| for every agent. Does it make a difference in the architecture/algorithm if we learn the weights of all the other agents (size |N|) instead? \n\nIn networked MARL, a common assumption is that the reward of each agent just depends on its action and the actions of its neighbors (Qu et al., 2020a). Thus, LToS only learns to share reward with neighbors, which is also limited by communication. For the case of |N|, each agent needs to output the weight for all other agents and take as input the weights from all other agents, and thus it becomes more centralized, which contradicts to the decentralized learning of networked MARL.\n\n> Formulating the weights as finite discrete values looks unnatural. If the method is designed for continuous action space, it is expected to have the notations to be continuous as well. Can we just simply convert the summations into integration in the propositions!?\n\nYes, we use summations only to ease the presentation.\n\n> The authors claim that the problem with the related work is that they cannot scale up with the number of agents. However, there is no (empirical) support that how the proposed approach deals with large-scale problems.\n\nActually, we do not argue that our method is very scalable while these factorization methods are totally not. We have amended the claims. Networked MARL focuses on decentralized learning as well as maximizing average return of all agents, while factorization methods are centralized training and focus the case all agents share a reward. Factorization methods can certainly be applied networked MARL. But in the literature it is empirically shown that QMIX performs poorly in large-scale networked MARL (Qu et al., 2020a). We also additionally performed the experiment of QMIX in 1) *traffic*, and it shows QMIX does not perform well as illustrated in Figure 4 and Table 2, and 2) *routing*, and it shows QMIX does not perform well either as illustrated in Figure 7 and Table 4.\n\n> In general, the experiments are small and based on simulation, and simulated scenarios are not considered real-world (which is claimed otherwise in the paper). I would recommend to incorporate more supportive empirical evaluation.\n\nWe have removed these in the revision.\n\n> Minor: What is $\\phi_{-i}$ in eq 17\n\nAs stated in Equation (2), it means the joint policy of all agents except agent $i$."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "NcVed6uL74f", "original": null, "number": 3, "cdate": 1606034153891, "ddate": null, "tcdate": 1606034153891, "tmdate": 1606034285343, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "2J1Lj5YxCeI", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 1: part 2", "comment": "> \"Unlike existing hierarchical RL methods, we can directly construct the value function and action value function of $\\boldsymbol{\\phi}$ based on the value function of $\\boldsymbol{\\phi}$ at each agent.\": Constructing the value function isn't really the problem, but approximating and learning it is challenging.\"\n\nYes, it is no problem to construct a new value function. What we want to avoid is learning the new value function. By this property, we are able to reuse the value function of the low-level policy as we do in the current implementation of LToS. \n\n> The results on the prisoner's dilemma are misleading. Clearly, if there is an ability to change the reward functions of individual agents (which is assumed by LToS), there is no more social dilemma. As such, only baselines that maximize the total reward are credible comparisons (and seem to be missing completely).\n\nYes, for credible comparisons, we exactly use fixed LToS and NeurComm as baselines that maximize the total return. As can be seen in Appendix, we specially set their selfishness and $\\alpha$ to direct agents to maximize average return. As a result, they are able to cooperate eventually but converge much slower.\n\n> None of the results include uncertainty estimates. It is furthermore unclear how many seeds were used. Furthermore, the fixed LToS baseline (\"For ablation, we keep the sharing weights fixed for each agent, named fixed LToS\") seems odds. Did you try a baseline where all agents simply share their reward equally with their neighbors? Also, centralized baselines are missing. E.g: https://arxiv.org/pdf/1910.00091.pdf\n\nFirst, we do not include uncertainty estimates since the test does not contain any random factor. That is because we generated the vehicle and data packet flows once and kept that throughout the whole experiment, partly because vehicle routes have to be fixed in CityFlow. We use three random seeds and show in the tables the average results over them. For fixed LToS, we fix the selfishness and all neighbors have the same sharing weight. For example, if we fix the selfishness of each agent at 0.2 and each agent has four neighbors, then each neighbor gets 0.2. As we performed the grid search to find the best selfishness for fixed LToS in traffic and routing, the experiment should cover the case \"all agents simply share their reward equally with their neighbors.\" For centralized baselines, we additionally compared with QMIX, we can see that in the revision QMIX does not perform well in *traffic*.\n\n> in \"ROUTING\" Fixed LToS (ie. not learning to share) and LToS seem indistinguishable.\n\nWe believe that the figure could show some difference. We admit that for *routing*, the gap between LToS and other baselines is smaller compared to *prisoner* and *traffic*."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "4aEJ6A6IvBw", "original": null, "number": 4, "cdate": 1606034213340, "ddate": null, "tcdate": 1606034213340, "tmdate": 1606034250800, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "2J1Lj5YxCeI", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment", "content": {"title": "Responses to Reviewer 1: part 1", "comment": "> \"Obviously, CTDE cannot address such problems due to the curse of dimensionality.\" CTDE means that there is the option to use centralized information at training time. Clearly, some ways of using centralized information will scale better than others and claiming that none of them scale is simply unfounded.\n\n> \"However, they are learned in a centralized way and hence not scalable.\" These methods have been scaled to large numbers of agents in complex environments. Please provide citations when making a claim that something doesn't scale. For example, the \"The StarCraft Multi-Agent Challenge\", Samvelyan et al 2020, includes results for numbers of agents comparable to the largest experiments in this paper.\n\nWe have revised these claims to make them accurate. We position LToS at a particular line of research of networked MARL, where CTDE methods may not easily scale up with the number of agents as empirically demonstrated in Qu et al., 2020a, including MADDPG, QMIX. We also additionally performed the experiment of QMIX in 1) *traffic*, and it shows QMIX does not perform well as illustrated in Figure 4 and Table 2, and 2) *routing*, and it shows QMIX does not perform well either as illustrated in Figure 7 and Table 4.\n\n> \"One is that the reward function... tragedy of the commons.\" I am struggling to make sense of this paragraph. Please work on the clarity of the writing.\n\n> \"The simple way to optimize the global objective is that each agent maximizes its own expected return, which is known as Markov game. \" This is wrong. When each agent optimizes their own expected return this is typically not a means of optimizing the global objective.\n\nWe have revised Introduction and Background to make them precise and clear.\n\n> \"Moreover, the observation of each agent $o_i \\in O_i$ can be enhanced to the Cartesian product of agent i and its neighbors (Jiang et al., 2020) or the observation history (Chu et al., 2020)\". I don't follow this. If the observation of each agent includes the observation of all neighbors (which includes the observation of their neighbors), then shouldn't everyone observe everything?\"\n\nThe observations of neighbors are obtained by communication, not naturally, and $o_i$ denotes the information available after communication. We have revised this part to make it clear. \n\n> \"In networked MARL, as the reward of an agent is assumed to depend on the actions of neighbors, we allow reward sharing between neighboring agents\": The reward function also depends on the global state, 's', which is a function of the joint action of all of the agents. So this local reward sharing seems clearly insufficient in general.\n\nAlthough the joint action of all agents determines the state transition, hence state distribution, only the actions in the neighborhood of an agent determine its reward at a particular state. Therefore, it is easy for an agent to learn to improve the return by sharing reward with neighbors, since the change of actions of neighbors directly affects the reward. However, the agents outside the neighborhood can only affect the return of the agent by the change of state distribution. It is hard for an agent to learn to amount the effect of reward sharing on the state distribution. Therefore, we consider only local reward sharing. \n\n> Equation (1) is wrong. The left-hand side conditions on 'o_i', but the right-hand side conditions on 's'. This also affects all following equations.\n\n> Theory: -4.3: \"Each vertex has its own local policy \u03c6ij (wij |oi), and we can verify their independence by means of Markov Random Field.\" This is not clear to me. Furthermore, given that the transition function conditions on the joint action and that the reward function depends on the central state, this seems wrong. Unless I am mistaken, the dependency on the central state should break any locality assumptions.\n\nWe have revised the paper to clearly present the mathematical workflow behind LToS. Specifically, in (1), $o_i$ should be $s$, and in Proposition 2, $\\phi_{ij} (w_{ij} |o_i)$ should be $\\phi_{ij} (w_{ij} |s)$. The main difference is we deferred the approximation of state by observations (or history) to the end, which makes the mathematical part simple and clear.\n\n> Eqn 6 to 15: This proof seems unnecessarily cumbersome. W only redistributes the rewards, so the sum of total rewards is unchanged, qed.\n\nYes. We just want to put it more rigorous."}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "awnQ2qTLSwn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3486/Authors|ICLR.cc/2021/Conference/Paper3486/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837040, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Comment"}}}, {"id": "b9O_b_yBHBo", "original": null, "number": 2, "cdate": 1603274716340, "ddate": null, "tcdate": 1603274716340, "tmdate": 1605023991674, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review", "content": {"title": "Needs some improvement", "review": "The paper addresses multi-agent RL problems by presenting a decentralized approach where the agents learn to share their reward with their neighbors. In this method, a high-level policy determines a weight vector for weighting the reward of neighboring agents, and then each agent learns their own independent policy. The learning is thus conducted locally in a partially connected network toward a common goal and without the knowledge of global state and actions.\n\nOverall, the approach is intuitive and interesting for decentralized learning in MARL tasks. However, I have some comments/questions for improving the paper that are summarized below.  Hence, I vote to reject at this stage.\n\nPros:\n+ Intuitive design of communication among agents in decentralized setting\n+ Clever adaption of algorithms\n+ Well written paper and properly organized\n\nComments:\n- The contribution of the paper is mainly in formulating the problem in the actor-critic setup of DDPG method which leads to a limited novelty.\n\n- A key concern about the paper is how to decompose the reward in the first place. The paper aims at optimizing a global objective and assumes (also in the propositions) that this objective has additive connection with the decentralized rewards. Nevertheless, this is a strong assumption, particularly in real-world applications. A global reward can be decomposed into summation of smaller rewards, but not necessarily the other way around. As long as there is a global objective, we need a way to distribute the reward among the agents via learning or reward reshaping (or even manually). How can we properly define the reward of each agent in such scenarios?\n\n- It is also unclear what is the benefit of sharing only with the neighbors. The method learns a weight vector of size |N_i| for every agent. Does it make a difference in the architecture/algorithm if we learn the weights of all the other agents (size |N|) instead?\n\n- Formulating the weights as finite discrete values looks unnatural. If the method is designed for continuous action space, it is expected to have the notations to be continuous as well. Can we just simply convert the summations into integration in the propositions!?\n\n- The authors claim that the problem with the related work is that they can not scale up with the number of agents. However, there is no (empirical) support that how the proposed approach deals with large-scale problems.\n\n- In general, the experiments are small and based on simulation, and simulated scenarios are not considered real-world (which is claimed otherwise in the paper). I would recommend to incorporate more supportive empirical evaluation.\n\n\nMinor:\nWhat is \\phi_{-i} in eq 17", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074953, "tmdate": 1606915783177, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3486/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review"}}}, {"id": "60wI5bcMhON", "original": null, "number": 4, "cdate": 1603868254630, "ddate": null, "tcdate": 1603868254630, "tmdate": 1605023991612, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review", "content": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "review": "The paper present a new method, called LToS which enables agents to share rewards in MARL. Two levels of policies, high-level and low-level, determines rewards and optimize global objectives. Three diverse scenarios were used to test the performance of LToS compared to other baseline methods. LToS consistently outperforms other methods. In the second scenario, authors also show the need for high-level policy by introduction fixed LToS. \n\n- At the end of Introduction, the sentence \u2018LToS is easy to implement and currently realized by DDPG\u2026\u2019 can be misleading because of the word \u2018realized\u2019 and the fact that authors argue that LToS is a newly proposed method. Does this mean LToS simply combines DDPG and DGN?\n- Do Figure 5 and 6 represent selfishness of agents when LToS is used?\n- Minor editorial errors in Appendix", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074953, "tmdate": 1606915783177, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3486/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review"}}}, {"id": "-JHJijUyJE4", "original": null, "number": 5, "cdate": 1604681537175, "ddate": null, "tcdate": 1604681537175, "tmdate": 1605023991490, "tddate": null, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "invitation": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review", "content": {"title": "On Positioning and Evaluation", "review": "The paper proposes a hierarchical multi-agent reinforcement learning method for the restricted communication setting and verifies the algorithm performance in a number of useful applications. The hierarchical approach to the networked MARL problem proves novel, effective, and interesting.\n\n+ Strengths:\n\n+ The work targets an arguably less explored area by focusing on the restrictions on inter-agent communication that may be present in realistic scenarios.\n\n+ Evaluation setup is varied, explained in detail and visualized in an intuitive manner.\n\n+ Niche is well-identified, and the contribution is clear.\n\n\n- Major Concerns:\n\n- The reviewer had issues positioning the paper among the different lines of research. Although the research gap itself is clear (scalable MARL methods in a restricted communication setting), it isn't obvious why and how relevant the cited works are. For example, mentioning VDN, QMIX, and QTRAN (which together are some of the latest works in the factorization methods) does not seem to serve any further purpose, as they are no longer compared quantitatively or qualitatively to LToS. The authors' claim that they are not scalable leads the reviewer to anticipate that LToS naturally is scalable, but there appears to be no evidence whatsoever presented in the latter sections of the paper to show, let alone prove, the superior scalability of LToS, with, for example, growing numbers of agents and training times.\n\n-  Furthermore, some of the cited works have been left out at the evaluation stage, which leaves the reviewer puzzled as to which baselines LToS really hopes to outshine. The work needs some justification over why the following studies have not been compared to in the evaluation:\n\nIf the main strength of LToS lies in its capability to function effectively and efficiently in restricted communications setting, comparison to one or more of the following works should be of great advantage in illustrating that edge:\nDIAL/RIAL by  Foerster 2016 - Learning to Communicate with Deep Multi-Agent Reinforcement Learning\nBiCNet by Peng 2017 arXiv - Multiagent Bidirectionally-Coordinated Nets\nCommNet by Sukhbaatar 2016 NeurIPS - Learning Multiagent Communication with Backpropagation\nIC3Net by Singh 2019 ICLR - Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks\nSchedNet by Kim 2019 ICLR - Learning to Schedule Communication in Multi-agent Reinforcement Learning\n\nIf the main strength of LToS lies in its capability to resolve selfishness and assign credits appropriately to bring about a harmonious cooperation in social dilemmas, analysis with respect to the this work should be helpful:\nEccles 2019 CoRR - Learning Reciprocity in Complex Sequential Social Dilemmas\n\nIt would be interesting to draw some parallels between LToS and BAD, as both draw inspiration from a hierarchical decomposition:\nBAD by Foerster 2019 ICML - Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning\n\nThis recent AAMAS paper is based on peer evaluation and exchanging evaluation messages computed from recently obtained rewards:\nPED-DQN by Hostallero 2020 AAMAS - Inducing Cooperation through Reward Reshaping based on Peer Evaluations in Deep Multi-Agent Reinforcement Learning\nSome of the potential issues to discuss are: bandwidth usage of message exchange, message overhead in sharing the neighbors' rewards.\n\nUsing neighbors' information to achieve scalability in MARL most likely requires discussion of mean-field methods, such as:\nYang 2018 ICML - Mean Field Multi-Agent Reinforcement Learning.\n\n- Going through the Appendices spurred a great deal of curiosity, as the authors mention that all agents share the same, synchronized random number generator with the same seed across all the agents. This leads me to believe that the philosophy of decentralized learning is lost in LToS. Synchronization is definitely not cost-free; all the more so if the synchronized RNG is used to sample an experience from the agents' replay buffers. How do the agents synchronize their RNG in a decentralized manner?\n\n- In the Routing evaluation. has overhead been taken into account? How does LToS fare with respect to varied communications channel? What if the network were sparser? Do you observe any trends as you vary the extent of network connectivity?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3486/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3486/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Share in Multi-Agent Reinforcement Learning", "authorids": ["~Yuxuan_Yi1", "~Ge_Li2", "~Yaowei_Wang1", "~Zongqing_Lu2"], "authors": ["Yuxuan Yi", "Ge Li", "Yaowei Wang", "Zongqing Lu"], "keywords": [], "abstract": "In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where a number of agents are deployed as a partially connected network. Networked MARL requires all agents make decision in a decentralized manner to optimize a global objective with restricted communication between neighbors over the network. We propose a hierarchically decentralized MARL method, \\textit{LToS}, which enables agents to learn to dynamically share reward with neighbors so as to encourage agents to cooperate on the global objective. For each agent, the high-level policy learns how to share reward with neighbors to decompose the global objective, while the low-level policy learns to optimize local objective induced by the high-level policies in the neighborhood. The two policies form a bi-level optimization and learn alternately. We empirically demonstrate that LToS outperforms existing methods in both social dilemma and two networked MARL scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|learning_to_share_in_multiagent_reinforcement_learning", "pdf": "/pdf/4d5792165c0bb7bd5a710c4d1ca6dc24060b16aa.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=fHzOZ477eH", "_bibtex": "@misc{\nyi2021learning,\ntitle={Learning to Share in Multi-Agent Reinforcement Learning},\nauthor={Yuxuan Yi and Ge Li and Yaowei Wang and Zongqing Lu},\nyear={2021},\nurl={https://openreview.net/forum?id=awnQ2qTLSwn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "awnQ2qTLSwn", "replyto": "awnQ2qTLSwn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074953, "tmdate": 1606915783177, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3486/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3486/-/Official_Review"}}}], "count": 14}