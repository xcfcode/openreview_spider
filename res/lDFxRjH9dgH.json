{"notes": [{"id": "lDFxRjH9dgH", "original": "Ggk9mA86Lcz", "number": 10, "cdate": 1615310249850, "ddate": null, "tcdate": 1615310249850, "tmdate": 1615313019263, "tddate": null, "forum": "lDFxRjH9dgH", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Demonstration-Guided Reinforcement Learning with Learned Skills", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper10/Authors"], "authors": ["Anonymous"], "keywords": ["Reinforcement Learning", "Imitation Learning", "Transfer Learning"], "TL;DR": "We propose an algorithm that extracts learned skills from large, task-agnostic datasets and uses them for efficient demonstration-guided reinforcement learning on long-horizon tasks.", "abstract": "Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. Prior approaches for demonstration-guided RL treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator's exact muscle movements. Naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. In this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided RL. We first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. We then propose an algorithm for demonstration-guided RL that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided RL approaches. We validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.\n", "pdf": "/pdf/18bf7ac75b873d06f2cc1681a17ad2f9f759e8ce.pdf", "paperhash": "anonymous|demonstrationguided_reinforcement_learning_with_learned_skills", "_bibtex": "@inproceedings{\nanonymous2021demonstrationguided,\ntitle={Demonstration-Guided Reinforcement Learning with Learned Skills},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=lDFxRjH9dgH},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}