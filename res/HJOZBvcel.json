{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396552520, "tcdate": 1486396552520, "number": 1, "id": "H1x93GI_e", "invitation": "ICLR.cc/2017/conference/-/paper380/acceptance", "forum": "HJOZBvcel", "replyto": "HJOZBvcel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. \n \n While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396553014, "id": "ICLR.cc/2017/conference/-/paper380/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJOZBvcel", "replyto": "HJOZBvcel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396553014}}}, {"tddate": null, "tmdate": 1484253767049, "tcdate": 1484253767049, "number": 4, "id": "rJyUcwHUx", "invitation": "ICLR.cc/2017/conference/-/paper380/public/comment", "forum": "HJOZBvcel", "replyto": "HJOZBvcel", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "writers": ["~Eugene_Belilovsky1"], "content": {"title": "General Comments", "comment": "We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at https://github.com/eugenium/LearnGraphDiscovery. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599094, "id": "ICLR.cc/2017/conference/-/paper380/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJOZBvcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper380/reviewers", "ICLR.cc/2017/conference/paper380/areachairs"], "cdate": 1485287599094}}}, {"tddate": null, "tmdate": 1483316655111, "tcdate": 1483316655111, "number": 3, "id": "Hyw3TGwre", "invitation": "ICLR.cc/2017/conference/-/paper380/public/comment", "forum": "HJOZBvcel", "replyto": "SkyCWALEe", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "writers": ["~Eugene_Belilovsky1"], "content": {"title": "Response", "comment": "Thank you for your review and comments. We are largely in agreement regarding the contributions of the paper.\n\nWith regards to permutation invariance of the architecture, we did indeed consider several possibilities prior to our final model. For example we looked at architectures with recurrent computations and attention mechanisms that can potentially be more invariant. However, at least at this point such approaches are less efficient especially in larger graphs, and more importantly we have found that our current architecture naturally provides us an easy way to perform ensembling to additionally improve results. We can see in a new appendix (A.4) we have added that in fact the outputs for permutation have uncorrelated errors, while we have seen individually they still produce strong classification results. At the same time the speed of our method makes ensembling a very practical approach. \n\nWith regards to interpretability of the method in practice, it is notable that graphical lasso for example has guarantees under restricted distribution assumptions. On the other hand our method could be made to give empirical guarantees, potentially under more loose distribution assumptions or ones more interesting to the user, if they can construct a way to sample from them or obtain ground truth real data.  There are also notably many recent approaches which attempt to understand the predictions of neural networks and other \u201cblack box\u201d models, including convolutional neural networks. Recent examples include [1] LIME and [2] the DeepVis toolbox. These techniques and future ones being developed in the community could be utilized with our method potentially helping explain for example which correlations help explain others.  \n\nIt is also of note that in many applications one might be willing to sacrifice interpretability of the deriving process for speed as well as accuracy guarantees under a specific model. These include applications such as real-time connectivity analysis [3], robust estimation and portfolio optimization in finance. In other cases, there may not be an existing method that is scalable and allows the needed prior, thus an approach such as ours may be the only route. \n\n[1]Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. \u201cWhy should I trust you?\u201d Explaining The Predictions of Any Classifier.  KDD 2016\n[2] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding Neural Networks Through Deep Visualization. http://yosinski.com/deepvis\n[3] Smith, Anne.  Near-real-time connectivity estimation for multivariate neural data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599094, "id": "ICLR.cc/2017/conference/-/paper380/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJOZBvcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper380/reviewers", "ICLR.cc/2017/conference/paper380/areachairs"], "cdate": 1485287599094}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1483315827913, "tcdate": 1478288640351, "number": 380, "id": "HJOZBvcel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJOZBvcel", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "content": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": ["H1GvR47Ye"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482566662198, "tcdate": 1482566662198, "number": 2, "id": "HJCbhjoEg", "invitation": "ICLR.cc/2017/conference/-/paper380/public/comment", "forum": "HJOZBvcel", "replyto": "BkF-pCWVl", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "writers": ["~Eugene_Belilovsky1"], "content": {"title": "Responses", "comment": "Thank you for your review and comments. \n\nWith regards to clarity of the network architecture description in Section 2.3, we have made several revisions in the paper to improve the explanations and wording. You can find these in Figure 1 and where o^k_{i,j} is first introduced. \n\nFor o^k_{i,j} it is indeed a d-dimensional vector for all layers indexed above 0 (i.e l>0).  However in the initial layer it is simply from o^0_{i,j} = p_{i,j}, this is analogous to how CNNs in dense prediction tasks take in images while intermediate layers can be multidimensional. Here the dimensions of o^k_{i,j} correspond to filter outputs in a CNN. Thus indeed d corresponds to channels in the CNN implementation. \n\nFor Figure 1 we have updated the description of (a) to be more explicit.  (a) is illustrating the nodes accessible by intermediate output o^1_{4,13}. In this first layer only edges from 6 nodes are used to construct o^1_{4,13}. Here the edge of interest (4,13) is shown in blue in Figure (a) and (b) and the edges processed to get o^1_{4,13} are shown in grey on both (a) and (b). This figure motivates pulling the entries near the diagonal as well instead of simply following the standard CNN structure since these allow enough information to determine, from o^1_{4,13}, a conditional independence of 4,13 given the adjacent nodes (5,14,3,12). \n\nFinally we have decided to release a first version of the code (at this link https://github.com/eugenium/LearnGraphDiscovery), so that the implementation details can be ascertained. \n\nWith regard to training data. For the real-world data, we use the same training data and in fact for one case the same trained model as in the synthetic experiments. These models are trained using uniform sparsity. We considered this as performance on the synthetic task was quite strong and we wanted to show the generality of a trained model, reusing it for a variety of synthetic tasks and even real. In the case of real world data, both for our method and existing methods such as the graphical lasso, assumptions on the generating distribution must be (implicitly) made. In many cases these assumptions can come from prior knowledge about the problem (e.g. neuroscientific findings in the case of brain connectivity).  One possible interpretation of our results is that the generating distribution used seems to match well the real data as compared to the assumptions in graphical lasso.\n\nLet me know if there are any other clarifications or questions. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599094, "id": "ICLR.cc/2017/conference/-/paper380/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJOZBvcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper380/reviewers", "ICLR.cc/2017/conference/paper380/areachairs"], "cdate": 1485287599094}}}, {"tddate": null, "tmdate": 1482397285060, "tcdate": 1481884677117, "number": 1, "id": "S1pWES-El", "invitation": "ICLR.cc/2017/conference/-/paper380/public/comment", "forum": "HJOZBvcel", "replyto": "B1aSUyJEg", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "writers": ["~Eugene_Belilovsky1"], "content": {"title": "Advantages of Method and Other Clarifications", "comment": "Thank you for your comments. We address each of them below\n\n\u201cIn introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?\u201d\nIn the Introduction Page 2, after Equation 1, we point to several problems  with a method such as the graphical lasso. a) It is difficult to embed new structured priors (e.g. small world graphs) within the penalty term and design optimization for them  b) model selection is unintuitive as it is controlled by the term \\lambda in Equation 1. A final point (c) , from the abstract and Section 2.1, is that the edge recovery of graphical lasso and other methods is an indirect consequence of the objective, whereas we directly optimize an objective on edge recovery. This allows us to greatly outperform the graphical lasso.\n\nWith regard to (b), our point is simply that in many practical problems one has a reasonable range they can assume for the signal sparsity (or other hyperparameters such as number of hubs in small world graphs), yet there is no easy way to correspond these ranges to regularization terms and thus one often is forced to do cross-validation over the full range of lambda. In our setting, parameters such as the sparsity can be directly specified in an intuitive way (one can indicate for example they expect the signal to be 90-99% sparse). \n\nWe found that our method was extremely robust to model selection effects, but we do not rule out the possibility that further work on model selection in our framework could improve results even further. In our experiments, we were able to achieve excellent results, outperforming the graphical lasso and monte carlo based methods with a single trained model. We also note that for model selection we biased the graphical lasso against us in our experiments showing results for the optimal regularization parameter (on the test set).\n\n\u201cAnother concern is that this paper is unorganized. \u201c\nWe believe the organization to be reasonable but we are very open to correcting this if one can cite specific organizational issues.  We welcome further comments that can improve clarity.\n\n\u201cIn Algorithm 1, first, G_i and \\Sigma_i are sampled, and then x_j is sampled from N(0, \\Sigma). Here, what is \\Sigma? Is it different from \\Sigma_i? \u201c\nWe thank the reviewer for noting this indeed typographical mistake in Algorithm 1. Indeed N(0,\\Sigma) should read N(0, \\Sigma_i.) We have corrected this error in the revision. \n\n\u201cFurthermore, how do you construct (Y_i, \\hat{\\Sigma}_i) from (G_i, X_i )?\u201d\nWe try to maintain generality in the algorithm description. Specifically in  the case of our experiments we construct  \\hat{\\Sigma}_i = X_i^TX_i/n and Y_i is defined from the graph G_i as in Equation (2). To put simply Y_i corresponds to the presence or absence of edges in the graph.\n\n\u201c Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?\u201d\nHere is an important point of the paper. Algorithm 1 describes how to train the edge estimator using a prior distribution and does not utilize the final target input data at all.  At test time we show that this trained model can compute edges on a variety of data it has not seen before, and in fact in all our experiments we do not use any real data at all for training but obtain very impressive performance (in order magnitude less time) using both real data as well as synthetic data from completely different sampling packages that are often used to evaluate these algorithms.\n\n\u201cWhat is the definition of the receptive field in Proposition 2 and Proposition 3?\u201d\nWe have added a definition to the revision. Here we refer to the part of the input seen at a \"neuron\" at a given layer. This is a commonly used term in some CNN literature but indeed should be defined. \n\nThank you again for your questions/comments and let me know if there is any other clarifications needed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599094, "id": "ICLR.cc/2017/conference/-/paper380/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJOZBvcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper380/reviewers", "ICLR.cc/2017/conference/paper380/areachairs"], "cdate": 1485287599094}}}, {"tddate": null, "tmdate": 1482248646941, "tcdate": 1482248646941, "number": 3, "id": "SkyCWALEe", "invitation": "ICLR.cc/2017/conference/-/paper380/official/review", "forum": "HJOZBvcel", "replyto": "HJOZBvcel", "signatures": ["ICLR.cc/2017/conference/paper380/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper380/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "I sincerely apologize for the late-arriving review. \n\nThis paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. \n\nThe main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. \n\nI rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. \n\nIt is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. \n\nIn summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603965, "id": "ICLR.cc/2017/conference/-/paper380/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper380/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper380/AnonReviewer1", "ICLR.cc/2017/conference/paper380/AnonReviewer2", "ICLR.cc/2017/conference/paper380/AnonReviewer3"], "reply": {"forum": "HJOZBvcel", "replyto": "HJOZBvcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603965}}}, {"tddate": null, "tmdate": 1481924212086, "tcdate": 1481923840927, "number": 2, "id": "BkF-pCWVl", "invitation": "ICLR.cc/2017/conference/-/paper380/official/review", "forum": "HJOZBvcel", "replyto": "HJOZBvcel", "signatures": ["ICLR.cc/2017/conference/paper380/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper380/AnonReviewer2"], "content": {"title": "Interesting algorithm to estimate sparse graph structure", "rating": "7: Good paper, accept", "review": "The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.\n\nIn general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.\n\nThe experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. \n\nHowever, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?\n\nFigure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.\n\nFor real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603965, "id": "ICLR.cc/2017/conference/-/paper380/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper380/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper380/AnonReviewer1", "ICLR.cc/2017/conference/paper380/AnonReviewer2", "ICLR.cc/2017/conference/paper380/AnonReviewer3"], "reply": {"forum": "HJOZBvcel", "replyto": "HJOZBvcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603965}}}, {"tddate": null, "tmdate": 1481729605208, "tcdate": 1481729605200, "number": 1, "id": "B1aSUyJEg", "invitation": "ICLR.cc/2017/conference/-/paper380/official/review", "forum": "HJOZBvcel", "replyto": "HJOZBvcel", "signatures": ["ICLR.cc/2017/conference/paper380/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper380/AnonReviewer1"], "content": {"title": "Advantage of the proposed method", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?\n\nAnother concern is that this paper is unorganized. In Algorithm 1, first, G_i and \\Sigma_i are sampled, and then x_j is sampled from N(0, \\Sigma). Here, what is \\Sigma? Is it different from \\Sigma_i? Furthermore, how do you construct (Y_i, \\hat{\\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?\n\nWhat is the definition of the receptive field in Proposition 2 and Proposition 3?\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Discover Sparse Graphical Models", "abstract": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ", "pdf": "/pdf/84bd49390e7f5de0a7a6911fc3b31fad4884434a.pdf", "TL;DR": "Sparse graphical model structure estimators make restrictive assumptions.  We show that empirical risk minimization can yield SOTA estimators for edge prediction across a wide range of graph structure distributions.  ", "paperhash": "belilovsky|learning_to_discover_sparse_graphical_models", "keywords": [], "conflicts": ["inria.fr", "ecp.fr", "centralesupelec.fr", "cs.toronto.edu", "umontreal.ca", "esat.kuleuven.be", "google.com", "ibm.com", "ucl.ac.uk"], "authors": ["Eugene Belilovsky", "Kyle Kastner", "Gael Varoquaux", "Matthew B. Blaschko"], "authorids": ["eugene.belilovsky@inria.fr", "kyle.kastner@umontreal.ca", "gael.varoquaux@inria.fr", "matthew.blaschko@esat.kuleuven.be"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603965, "id": "ICLR.cc/2017/conference/-/paper380/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper380/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper380/AnonReviewer1", "ICLR.cc/2017/conference/paper380/AnonReviewer2", "ICLR.cc/2017/conference/paper380/AnonReviewer3"], "reply": {"forum": "HJOZBvcel", "replyto": "HJOZBvcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper380/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603965}}}], "count": 9}