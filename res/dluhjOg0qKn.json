{"notes": [{"id": "dluhjOg0qKn", "original": "WklYjX_haxh", "number": 3626, "cdate": 1601308403481, "ddate": null, "tcdate": 1601308403481, "tmdate": 1614985650162, "tddate": null, "forum": "dluhjOg0qKn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "UrLHrV84mz1", "original": null, "number": 1, "cdate": 1610040512812, "ddate": null, "tcdate": 1610040512812, "tmdate": 1610474120801, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers recommend that the paper be rejected.  The reviewers appreciate the line of research and is worthwhile, but find that the paper lacks in technical novelty and insight.  The AC is in consensus with their reviews due to the concerns raised regarding novelty and insight and recommends rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040512799, "tmdate": 1610474120786, "id": "ICLR.cc/2021/Conference/Paper3626/-/Decision"}}}, {"id": "drXRBBkGeKk", "original": null, "number": 1, "cdate": 1603711208449, "ddate": null, "tcdate": 1603711208449, "tmdate": 1606728721379, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review", "content": {"title": "Deep Ensembles for Low-Data Transfer Learning", "review": "Update after author response: I appreciate the authors' efforts to address my concerns and to raise some interesting points I missed. I still find the paper's insights are lacking some novelty to be published, but I think that this line of research is worth it! \n\n\n------------------------\n\n\nIn the low data regime, the use of transfer learning techniques collides with a widely used strategy: training multiple models for different purposes, being the main obstacle the lack of a clear diversity source.\n\nThis paper proposes a simple method to circumvent this problem: identifying pre-training itself as an easily accessible and valuable form of diversity and proposing the greedy combination of several pre-trained models.  Experiments show that the proposed strategy can achieve state-of-the-art performance on 19 different tasks. One necessary assumption of the method is the availability of a large pool of related models and the ability to look at the target data to make a decision on which models to fine-tune.\n\nOne of the critical points of the method is the use of cheap proxy metrics which assess the suitability of a pre-trained model before training it.  To this end, the paper proposes the use of leave-one-out nearest-neighbour accuracy.\n\nPros:\n+ The paper takes one of the most important issues of deep learning: training high-performance models in the low data regime. \n+ The results section is well structured and experiments are convincing. The proposed method is evaluated from several points of view.\n\nCons:\n- The paper refers to a previous publication (Puigcerver et al., 2020) and from this point of view, the proposal represents only an incremental step, with a low level of novelty. \n- The description of the method is not very specific and it refers to other existing methods as the main steps (Puigcerver et al. (2020) and  Caruana et al. (2004))\n- The proposed method is based on heuristics and there are no hints about why it does work. Diversity is a generic concept and there is a large number of papers that have explored several measures of diversity in order to understand \"when\" and \"why\" it is helpful. I miss some references to this previous knowledge. See, for example: Bian, Yijun, and Huanhuan Chen. \"When does Diversity Help Generalization in Classification Ensembles?.\" arXiv (2019): arXiv-1910.\n\nMy main concern is not about the results, which I think are good, but about the level of novelty with respect to some existing publications (mainly Puigcerver et al. (2020)) and the lack of experiments devoted to understanding the role of diversity. It is a well-known fact that diversity per se is not sufficient to build strong multiple classifiers, and different kinds of diversity measures are helpful to diagnose it.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072497, "tmdate": 1606915803737, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review"}}}, {"id": "Per1wOXRrIt", "original": null, "number": 2, "cdate": 1605606479000, "ddate": null, "tcdate": 1605606479000, "tmdate": 1605608891771, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "_lKBjNTlPJs", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment", "content": {"title": "Reply to review by AnonReviewer1", "comment": "We thank the reviewer for the thorough response! We are glad you enjoyed the writing and found the explanation/experiments of a good standard, and hope we can address your comments below:\n\n* About novelty of the work: We do believe this paper has many valuable insights, and will aim to make that clearer in the updated version.\n  * We would first like to note that as far as we could find, there were very few research efforts focussed on ensembles of modern neural networks in the low data regime - we reiterate that on downstream tasks, we only have 1000 data points available. For tasks such as CIFAR100 or CalTech101 this means only 10 data points per class. We believe it is a solid contribution to demonstrate the efficacy of different ensembling methods (both utilising \u2018downstream\u2019 sources of diversity and \u2018upstream\u2019 sources of diversity). For context, [recent work](https://openreview.net/pdf?id=_77KiX2VIEg) studies ensembling in a similar regime - their results on CIFAR100 (10-shot per class i.e. ~1k data points) achieve around ~20% top-1 accuracy, but our approaches achieve 70%+. Considering production-scale models, on realistic and diverse classification tasks, with very small amounts of data, is a very useful regime to work on and we believe there are many useful insights here for practitioners and researchers alike.\n     * We recognise the kNN-score for model selection component of the algorithm isn\u2019t a novel contribution, but believe that the application to selecting a set of models, to be combined later, and stress testing it (picking 15 from over 2000 models!) are highly valuable contributions.   \n    * We further note that the simple yet highly performant heuristic for combining upstream and downstream diversity is also a novel technical contribution.\n  * As you noted, yes; one of the key messages of this paper is that in this regime, creating ensembles which leverage differences in pretraining perform better than models which exploit diversity that is generated on the downstream task. As far as we are aware, there is no other concurrent work which demonstrates this conclusion. We also propose heuristics for computationally efficient ways to get the best of both worlds - again, we do not find parallels in the literature.\n  * Departing from the low-data regime and top-1 accuracy, the robustness on ImageNet results are arguably even more important than raw top-1 accuracy in a controlled setting, and we believe it is a valuable contribution to show ways to tackle it.\n* Points about related literature:\n  * Many thanks for pointing out some of these other papers! We will update our work to properly compare and contrast against these works.\n  * As far as we understand, LEEP, DEPARA, Dual Diagram Similarity, and Task2Vec all propose different ways of selecting models - but they all focus on selecting a single model for a given task. Those methods could all act as a drop-in replacement for the KNN selection phase of the proposed algorithm, and are thus complementary to our work - for example, if using LEEP in the model selection phase improved ensemble performance on downstream tasks, then we believe that this would only further verify the efficacy/performance of our approach, as opposed to being a literature baseline that we did not evaluate against.\n  * The performance of our approach does not directly necessitate the use of the KNN / the \u2018pre-selection\u2019 phase; we feel it was a valuable contribution to show that these sorts of approaches can help narrow down the pool of potential models, thus making the algorithm computationally feasible for many practitioners. However, one of our best results was just using a pool consisting of models pretrained upstream with different random seeds. This is analogous to practitioners using multiple different pretrained models (e.g. on ImageNet), which are widely available online.\n    * Lastly, the main findings and contributions of our paper - in relation to the superiority of upstream/combined diversity instead of downstream diversity, the impact on robustness to distribution shift and the proposed algorithm itself are all novel with respect to the papers mentioned.\n* Comparing with simple baseline of fine-tuning with early stopping: This is effectively what the \u2018single model\u2019 baselines are. Previous efforts fine-tune a single model on each downstream task, and found that such a strategy applied to very large scale models was the most performant approach. We achieve higher performance using smaller models with a significantly lower inference time. Note that those models do not require early stopping as after significant study on VTAB they developed a hyperparameter heuristic rule which defines schedule length as a function of the downstream dataset; this is arguably an even stronger baseline than finetuning with early stopping. Furthermore, early stopping could be applied to the baseline as well as our ensemble models, and therefore we consider that a separate direction."}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dluhjOg0qKn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3626/Authors|ICLR.cc/2021/Conference/Paper3626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835542, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment"}}}, {"id": "GTz9qP_UGMF", "original": null, "number": 4, "cdate": 1605608070174, "ddate": null, "tcdate": 1605608070174, "tmdate": 1605608675147, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "m4ZmWnuGnpk", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment", "content": {"title": "Reply to review by AnonReviewer3", "comment": "Many thanks for the comments; we hope we can address some of them here.\n\n1. We understand the reviewers concerns relating to novelty in terms of technique contributions; we combine a number of disparate techniques (in particular, KNN selection from Puigcerver et al. and greedy ensembling from Caruana et al.) in order to build the proposed algorithm.\n\n    We would like to note a few technical contributions however:\n    * Previous work did not consider selecting multiple models for finetuning on new tasks (we also stress-test the KNN, picking only 15 models from a pool of over 2002!). Indeed, as far as we are aware, previous work found that combining different experts models trained on different data is hard, and it did not work out of the box. From Puigcerver et al.: \u201cSelecting and combining multiple experts for any downstream task is a natural extension of our work. This could be especially useful for tasks that require understanding several concepts, not necessarily captured by a single expert.\u201d In this work, we found a way to make it work nicely.\n    * We propose a heuristic based on KNN accuracy (Section 2.3.1) which allows a performant balance between upstream and downstream diversity by automating not only which pretrained models to select, but how many to select for each downstream task.\n\n    _However, aside from technical novelty, we believe there are a number of valuable contributions from this work:_\n    * To the best of our knowledge, we found no other works comparing or suggesting approaches to building ensembles of modern deep networks in the low data regime. We would like to re-iterate that there are only 1000 datapoints available per task downstream; for example, [concurrent work](https://openreview.net/pdf?id=_77KiX2VIEg) which developed ensembles in the low data regime achieved ~20% accuracy on CIFAR100, whereas our models achieve ~70+% accuracy (with only 10 data points per class). We further believe that our work is distinguished in considering production-scale models on a very diverse range of tasks, instead of experimenting with small-scale models on tasks such as MNIST. Furthermore, we find very little comparable work studying transfer learning and the low data regime in the context of ensembles.\n        * We note that even the performance of our baseline ensembles - which use \u2018downstream\u2019 diversity - are a contribution. Though we consider it a baseline, we know very few other works that study such ensembling techniques with such little data. Showing the competitiveness of hyperparameter or augmentation ensembles in this setting is in and of itself a contribution.\n    * Our main conclusion is that **upstream diversity is more useful than downstream diversity**; we know of no other concurrent or previous works that demonstrate this. Similarly, our demonstration of the superiority of combining both forms of diversity - and a proposed heuristic to do so efficiently - is also novel as far as we are aware.\n    * We believe that the results showing significantly increased distribution to domain shift, by assessing on the 7 ImageNet variants, are also a valuable contribution and a very interesting insight. Even if our models were not more accurate, we believe that showing such a significant boost on multiple robustness metrics is a very compelling result.\n\n2. Concerning how models are compared:\n    * We are not sure we fully follow the point relating to fairness of comparison. We chose to compare inference cost, as this was the clearest thing that was comparable across settings. We do not make any claims in relation to the total training time - though given that, for example, the R152x4 requires ~45x as many FLOPs for a forward pass as a single R50x1, we suspect they are at least in the same ballpark range for training cost.\n    The core assumption of this work is that there is a number of available pre-trained upstream models, something which is increasingly true (see https://www.tensorflow.org/hub). In this setup, the downstream training cost of our ensemble methods is negligible, and most likely not the reason why we see the nice large gaps shown in Figure 2 a).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dluhjOg0qKn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3626/Authors|ICLR.cc/2021/Conference/Paper3626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835542, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment"}}}, {"id": "CZXQHVDlGj2", "original": null, "number": 5, "cdate": 1605608586154, "ddate": null, "tcdate": 1605608586154, "tmdate": 1605608605477, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "drXRBBkGeKk", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment", "content": {"title": "Reply to review by AnonReviewer4", "comment": "We would like to thank the reviewer for the response - we are glad you recognise the importance of the work, and that the writing and experiments were of a sufficient standard!\n\nIn response to some of the concerns:\n* __Novelty__\n    * On the topic of novel technical contributions, we agree with the reviewer that the work proposes an algorithm that builds off multiple preceding works, specifically the use of model selection to discriminate between pretrained models and the greedy algorithm for constructing ensembles. However, we argue that combining disparate efforts in order to present a computationally feasible and performant approach to tackle a problem of high importance is itself valuable progress.\n    * We also believe that our work has many contributions aside from the proposed algorithm:\n        * We found very few related works studying ensembling of modern neural networks in this data regime. We believe that studying different approaches is itself a solid contribution. For example, just demonstrating that the combination of transfer learning from a single pretrained initialisation + downstream diversity (from augmentations, hyperparameters, etc) can yield good performance in this type of task is itself a valuable finding, as we do not know of other works that show this in this data regime.\n            * For example, we know of only one other work that studies [hyperparameter ensembles](https://arxiv.org/pdf/2006.13570.pdf); they do not use production-scale architectures, do not restrict the data to the low data regime, and do not assess models on such a wide array of new tasks; and there is no transfer learning involved.\n        * Ultimately, we concluded that - whether the source of variation was due to upstream pretraining on different datasets, or simply upstream pretraining with multiple random seeds - that sources of upstream diversity are more performant than sources of downstream diversity. We do not find any concurrent works that demonstrate the same conclusion.\n            * We believe that the improvements to robustness demonstrated by the evaluation on ImageNet variants is a particularly interesting and valuable insight that makes this conclusion even more convincing, departing from the tunnel-vision focus on top-1 accuracy.\n            * Furthermore, demonstrating the superiority of combining the two sources of diversity is also a unique finding, and proposing a simple heuristic for doing so in a computationally efficient and performant way is a novel technical contribution.\n\n* __Why is it helpful - and what is the role of diversity?__\n    * Firstly, we think in hindsight that the term diversity was used very informally in this effort - it is tricky as there are multiple definitions and it isn\u2019t something the field has agreed upon, but really when discussing upstream/downstream \u2018diversity\u2019, what we meant was sources of model variation as opposed to diversity in the sense of diversity of predictions/errors etc.\n    * That being said, we totally agree with the reviewer here - we read many papers on the topic of quantifying and understanding diversity and spent a lot of effort applying them to analyse our ensembles. Though we generally saw that the ensembles using upstream/combined diversity were more \u2018diverse\u2019, and that they were marginally more optimal on the diversity/accuracy tradeoff, we didn\u2019t find any systematic and convincing trends, and didn\u2019t wish to engage in data dredging till an attractive/presentable looking trend emerged. We also found this disappointing, as were hoping to find some clear insights to back up why the different sources of model variation help.\n    * The paper the reviewer linked is fascinating! It may explain the strong performance on the ImageNet variants. To our understanding, the role of diversity in ensemble performance still seems contested in literature, with many works proposing different diversity metrics and making contradictory claims about its importance; given this, and the fact that we already had a lot of strong results to present, we decided not to include it. We believe that in lieu of such analysis, thoroughly evaluating the different approaches from so many angles (19 diverse classification tasks for accuracy and 7 different variants of ImageNet for multiple robustness metrics) was a more useful contribution.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dluhjOg0qKn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3626/Authors|ICLR.cc/2021/Conference/Paper3626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835542, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment"}}}, {"id": "ZPIJPYlxpT", "original": null, "number": 3, "cdate": 1605607217248, "ddate": null, "tcdate": 1605607217248, "tmdate": 1605607217248, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "WZdsAdbDuSs", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment", "content": {"title": "Reply to review by AnonReviewer2", "comment": "Many thanks for the clear review! We will correct some of the minor formatting errors.\nIn response to the comments:\n\n1. Apologies for the lack of clarity in writing! We will definitely clear this up in the paper, but to clarify here:\n      * {Aug/Hyper} Ensembles are ensembles which utilise only downstream diversity (from augmentations/hyperparameter variation in the downstream finetuning).\n      * {Generalist/Expert} Ensembles are ensembles which utilise only upstream diversity (from pretraining models with multiple random seeds, or on different upstream datasets, respectively).\n      * {Aug/Hyper}{Generalists/Experts} combine both - e.g. HyperExperts utilise experts as an upstream source of diversity, combined with hyper parameterization as a downstream source of diversity. The proposed thresholding heuristic automatically decides the balance between the two forms of diversity.\n\n2. This is a good point! We would like to mention a few things:\n      * The assumption underpinning many works in Transfer Learning is that the pretraining cost is only incurred once, and therefore not considered in the finetuning phase. We therefore split computation costs in \u2018upstream pretraining\u2019 (ignored - analogous to practitioners downloading pretrained models from the web), \u2018downstream finetuning\u2019 (consistent between all our ensembles), and \u2018inference\u2019 (arguably the most important)\n      * Assumedly, one needs to separately compare finetuning cost vs. VTAB-1K score, and inference cost vs VTAB-1K score, as a model is only trained once but may be used many times. \n        * Inference cost: We predominately compared based on inference budget as this info is readily available (we can use FLOPS as a proxy, which is likely fair due to the similarity in architecture). We believe inference cost is the fairer comparison given the differences in setup, and also of more practical relevance, and hope that the benefits in this quarter are clear in the paper.\n        * Fine-tuning cost: Here it is harder to say; these numbers are not readily available to us for the baseline, but the single-SOTA models were also fine-tuned downstream with a hyper-parameter sweep. \n            * As a back of the envelope calculation, the best single model is a R152x4 which has ~44x flops for a forward pass vs a single R50x1. Our ensembles train 60 ResNet-50s downstream. Assuming a training pass scales linearly in compute time w.r.t. an inference pass (a very conservative assumption), this would perhaps put our ensembles at 1.36x the cost of fine-tuning once the strongest single model we compare against.\n            * We showed (Section 4.4) that our methods perform very well even with reduced fine-tuning budget; our ensembles that train 20 ResNet-50s downstream still beat the R152x4 baseline. Thus we suspect our ensembles compare favourably here too.\n\n3. As mentioned at the start of Section 4, all of our ensembles use the same finetuning/inference compute budget - Downstream, Upstream and Combined diversity are therefore on a fair playing field in this respect - we will adjust the text to make this clearer. The only difference lies in which (models+hyperparameters) we fine-tune in each case, but compute-wise they all get the same budget. One of the contributions of this work is to suggest a simple heuristic based on KNN accuracy which allows one to combine the two sources of diversity under a fixed finetuning budget.\n    * This again assumes the pretraining is \u2018free\u2019 - from a practical perspective, this is not an unreasonable assumption; there are many widely available pretrained checkpoints using a variety of architectures, pre-training methods, datasets etc.\n\n4. Interesting question! \n    * The default hyperparameter sweep (2 learning rates, 2 learning schedules) is used for ensembling approaches (augmentation, upstream diversity) which don\u2019t include a hyperparameter sweep already (e.g. AugEnsembles, ExpertEnsembles). It is in fact the same hyperparameter sweep used in the original Visual Task Adaptation Benchmark paper. Concerning patterns between these hyperparameters and the wider sweep used to generate HyperEnsembles, we didn\u2019t notice any systematic trends between the hyperparameters across the 19 tasks; it was highly task dependent, with per-task preferences relating to dropout, learning rate, schedule length etc varying significantly.\n    * The baseline numbers are copied from literature and were not replicated by us. They propose a hyperparameter heuristic which adaptively sets learning rates/schedule lengths/resolution/etc as a function of the downstream task\u2019s properties (number of images, nature of the images and so on); downstream, they therefore only use one hyperparameter per task. However, this hyperparameter heuristic was defined after much experimentation on VTAB-1k, whereas we use the default suggested in the original Visual Task Adaptation Benchmark paper; therefore, it\u2019s not simple to compare the ours to the baseline on this front.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dluhjOg0qKn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3626/Authors|ICLR.cc/2021/Conference/Paper3626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835542, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Comment"}}}, {"id": "m4ZmWnuGnpk", "original": null, "number": 2, "cdate": 1603790199153, "ddate": null, "tcdate": 1603790199153, "tmdate": 1605023966465, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review", "content": {"title": "Three main \"contributions\" in its framework are all from exisiting (related) works!", "review": "This paper does achieve good performance but its method is quite about engineering using very intuitive training tricks that everybody could be able to use given a lot of GPU machines. I would not like to encourage such work to be published as a research paper.\n\nPros:\n\n1. The proposed framework achieves a good performance compared to its related works.\n\n2. It is a good organization of a lot of training techniques, and a good reference for engineering.\n\nCons:\n\n1. No technique contribution. The main framework of this submission is very similar to the existing work [Scalable Transfer Learning with Expert Models] which has not been officially published but only on arXiv. Besides the common methods of pre-training and ensembling, it involves three \"new\" methods in its main framework: the first one is kNN selection on pre-trained models (referred to the same technique in the work [Scalable Transfer Learning with Expert Models]); the second is the hyperensembles by fine-tuning multiple diverse copies of the models (referred to the hyperparameter sets used in another related work [Big transfer (BiT): General visual representation learning]); and the last is greedy ensemble (referred to the third related word [Ensemble selection from libraries of models]). Not sure what is the contribution of this submission.\n\n2. The paper is quite about engineering tricks or combinations of tricks. In addition, in terms of engineering, it is not fair to compare to related methods under the condition of using the same numbers of pre-trained models. A better way may be based on the total computational COSTS such as the max running epochs, the network architectures, the total training time under the same usage of GPU machines.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072497, "tmdate": 1606915803737, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review"}}}, {"id": "WZdsAdbDuSs", "original": null, "number": 3, "cdate": 1603943856514, "ddate": null, "tcdate": 1603943856514, "tmdate": 1605023966336, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review", "content": {"title": "Papers needs more clarity, better writing and total computational cost justification. ", "review": "Summary:\nPaper proposed an ensemble learning approach for the low-data regime. Paper uses various sources of diversity - pre-training, fine-tuning and combined to create ensembles. It then uses nearest-neighbor accuracy to rank pre-trained models, fine-tune the best ones with a small hyper-parameter sweep, and greedily construct an ensemble to minimize validation cross-entropy. Paper claims to achieve state-of-the art performance with much lower inference budget. \n\nRecommendation: Based on my understanding of the paper I recommend a clear rejection. Please look at the details below: \n\nStrength: \n1) Authors have tried to lot of experiments and give summary of conclusion/results in section 4. \n\n2) Experimental setup is clear and the motivation is valid. \n\nWeakness/Questions: \n1) Paper was very hard to read. I had to go back and forth between pages to make sense of what\u2019s defined and make my own definitions in many cases. In some cases, terms are defined but never used and in other cases terns are never defined. For example, \na) AugEnsembles: Where is this used?\nb) ExpertEnsembles: Where is this defined? \nc) HyperExperts: Where is this defined?\nd) AugExperts: Where is this defined? \n\n2) In figure 2, Single-model SOTA has only one model. Do you have a graph for total cost (training + inference) vs VTAB_{1K} performance for all the models that are shown in figure 2? Only showing an inference budget may not tell the entire picture here. \n\n3) In Table 2, how is computational cost different for different sources of diversity (D, U and C)? If C needs more computational cost than U and D then is the comparison fair? \n\n4) Appendix A.2 mentions the hyper parameters used when using \u201chyper ensembles\u201d and then there is a default hyper parameter sweep - \u201cDefault Hyper Parameter Sweep\u201d in appendix A.1. \nDid you find any pattern in the hyperparameters with the best model?  How were the hyperparameters chosen for baselines in table 1? \n\n\nminor: \n1) VTAB should have been defined just before listing contributions - \n\u201cnew form of diversity improves on the Visual Task Adaptation Benchmark (VTAB) SOTA by 1.8% (Zhai et al., 2019).\n\n2) Paper repeatedly cites Puigcerver et al 2020 [1] to justify experimental framework or as a follow up paper which is also very similar to the current paper in terms of motivation. \n\n[1] Puigcerver, Joan, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr\u00e9 Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. \"Scalable transfer learning with expert models.\" arXiv preprint arXiv:2009.13239 (2020).", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072497, "tmdate": 1606915803737, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review"}}}, {"id": "_lKBjNTlPJs", "original": null, "number": 4, "cdate": 1604208551499, "ddate": null, "tcdate": 1604208551499, "tmdate": 1605023966275, "tddate": null, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "invitation": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review", "content": {"title": "Recommendation to Reject based on limited novelty and lack of convincing experiments", "review": "[Summary] This paper presents different ways of creating ensembles from pre-trained models. Specifically, authors first utilize nearest-neighbor accuracy to to rank pre-trained models, then fine-tune the best ones with a small hyperparameter sweep, and finally greedily construct an ensemble to minimize validation cross-entropy. Experiments on the Visual Task Adaptation Benchmark show the efficacy of the approach in selecting few models within a computational budget.\n\n[Score] Overall, I found the paper is well-written with experiments using large-scale benchmarks such as JFT, ImageNet21K and VTAB datasets. I like the problem of model selection for transfer learning. However, my major concern is about the novelty of the paper including concerns regarding prior works. Given the lack of novelty and convincing experiments, I vote for rejecting the paper. Hopefully the authors can address my concerns in the rebuttal period. \n\n[Weaknesses] The technical novelty of the paper is very limited. Besides combining few prior methods (e.g., Puigcerver et al. (2020); Caruana et al. (2004)) and then performing large scale experiments on JFT/ImageNet21K datasets, what are the main contributions of the paper are not clear. Although I admit that papers on analysis or study of different methods are quite interesting, I failed to find any major insights from the study of different diverse ensemble techniques. Is the upstream pre-training achieves better accuracy than that from the downstream fine-tuning stage the major take away message of the paper? Authors should clearly explain the major contributions of the paper.\n\nThere are few recent papers which discuss model selection for transfer learning. E.g., Duality Diagram Similarity: a generic framework for initialization selection in task transfer learning, ECCV 2020; DEPARA: Deep Attribution Graph for Deep Knowledge Transferability, CVPR 2020. How is the proposed approach related to these prior works? These paper should be clearly discussed with proper comparison in the experiments. \n\nComparison with prior methods is not satisfactory. Authors should clearly discuss what are the different ways of selecting models and creating ensembles out of that in the experiments. Specifically, what are the different alternatives to KNN and greedy approach used to construct ensembles? What about the performance of those methods? How is the proposed simple approach comparable to them in terms of performance vs complexity. E.g., how is the proposed approach comparable to the pretrained model selection strategy based on Task2Vec: see TASK2VEC: Task Embedding for Meta-Learning?\n\nHow is the proposed method related to Leep: A new measure to evaluate transferability of learned representations? Furthermore, how is the current approach comparable to a simple baseline on fine-tuning with early stopping?\n\nFigure 1 is not clear and it is not described clearly anywhere in the paper. I would like the authors to clearly explain this figure either in the caption or text in the introduction section.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3626/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3626/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Ensembles for Low-Data Transfer Learning", "authorids": ["~Basil_Mustafa1", "~Carlos_Riquelme_Ruiz1", "~Joan_Puigcerver1", "~Andr\u00e9_Susano_Pinto1", "~Daniel_Keysers2", "~Neil_Houlsby1"], "authors": ["Basil Mustafa", "Carlos Riquelme Ruiz", "Joan Puigcerver", "Andr\u00e9 Susano Pinto", "Daniel Keysers", "Neil Houlsby"], "keywords": ["transfer learning", "representation learning", "computer vision", "ensembles"], "abstract": "In the low-data regime, it is difficult to train good supervised models from scratch.\nInstead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.", "one-sentence_summary": "A study of ensembling pre-trained models for downstream computer vision classification tasks with minimal data, and a proposal for an approach that utilises diversity from pre-training to improve accuracy and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mustafa|deep_ensembles_for_lowdata_transfer_learning", "supplementary_material": "/attachment/1c1b9291dbdd6fb1639d046844d5206fa458a6df.zip", "pdf": "/pdf/8e814cf5467e37b83b22456d09a5b0e35ab97888.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KSwfRwvzrd", "_bibtex": "@misc{\nmustafa2021deep,\ntitle={Deep Ensembles for Low-Data Transfer Learning},\nauthor={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Andr{\\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},\nyear={2021},\nurl={https://openreview.net/forum?id=dluhjOg0qKn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dluhjOg0qKn", "replyto": "dluhjOg0qKn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072497, "tmdate": 1606915803737, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3626/-/Official_Review"}}}], "count": 10}