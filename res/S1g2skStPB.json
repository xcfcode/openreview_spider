{"notes": [{"id": "NdZnF_RegQ", "original": null, "number": 14, "cdate": 1577411980595, "ddate": null, "tcdate": 1577411980595, "tmdate": 1584624157863, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "About Codes and Datasets", "comment": "[update: 03/19/2020]\n\nWe have released our codes, along with datasets and training logs in the paper, at https://github.com/huawei-noah/trustworthyAI/tree/master/Causal_Structure_Learning/Causal_Discovery_RL . \n\nPlease file an issue if you have any questions.\n\n---------------\n\nHi all, \n\nOur codes and datasets are currently undergoing the regular open-source process of Huawei Noah's Ark Lab, and will be made available as a repository at https://github.com/huawei-noah. We will also release the training logs of the experimental results that are reported in the paper.\n\nWe will let you know once the codes are released.\n\nBest Regards,\nShengyu"}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "S1g2skStPB", "original": "HJe3VJ1YDr", "number": 1929, "cdate": 1569439651868, "ddate": null, "tcdate": 1569439651868, "tmdate": 1583912026849, "tddate": null, "forum": "S1g2skStPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "uELYHwr-UE", "original": null, "number": 15, "cdate": 1580700138676, "ddate": null, "tcdate": 1580700138676, "tmdate": 1580700138676, "tddate": null, "forum": "S1g2skStPB", "replyto": "aF1c8IofC7", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "Re: Related work", "comment": "Thanks for letting us know. My first impression is that the first one is indeed very relevant. We will have a careful read of both papers."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "aF1c8IofC7", "original": null, "number": 1, "cdate": 1580676408530, "ddate": null, "tcdate": 1580676408530, "tmdate": 1580676408530, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Public_Comment", "content": {"title": "Related work", "comment": "This is a very interesting paper on using reinforcement learning to learn to solve combinatorial optimization problems involving graphs, in this particular case, the structure of the graphical models.  \n\nThere are two highly relevant papers which are worthwhile discussing in context and can enrich the current paper: \n\n1. Learning Combinatorial Optimization Algorithms over Graphs. Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song. NeurIPS 2017. \n\n2. GLAD: Learning Sparse Graph Recovery. Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinivas Aluru, Han Liu, Le Song. ICLR 2020. \n\n"}, "signatures": ["~Le_Song1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Le_Song1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187629, "tmdate": 1576860572986, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Public_Comment"}}}, {"id": "AZ_pu0JgN_", "original": null, "number": 1, "cdate": 1576798736127, "ddate": null, "tcdate": 1576798736127, "tmdate": 1576800900243, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This paper proposes an RL-based structure search method for causal discovery. The reviewers and AC think that the idea of applying reinforcement learning to causal structure discovery is novel and intriguing. While there were initially some concerns regarding presentation of the results, these have been taken care of during the discussion period. The reviewers agree that this is a very good submission, which merits acceptance to ICLR-2020.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721986, "tmdate": 1576800273185, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Decision"}}}, {"id": "HklEzhj6FB", "original": null, "number": 2, "cdate": 1571826700491, "ddate": null, "tcdate": 1571826700491, "tmdate": 1574379730525, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Update: after the revision, I have decided to increase my score to 8.\n\nOriginal comments:\n\nIn this paper, the authors proposed a new reinforcement learning based algorithm to learn causal graphical models. Simulations on real and synthetic data also shows promise.\n\nPros\n\n1. It's great to see the authors has done a comprehensive comparison with the other methods, especially under different simulation scenarios.\n\n2. The novel idea of applying reinforcement learning to DAG search sounds intriguing. Reinforcement learning offers a powerful tool for policy evaluation and decision making. It\u2019s good to see that the author can successfully extend such toolbox to the field of causal structure learning. To the best of the author\u2019s knowledge, such idea has never been considered by previous work in causal graphical models.\n\nCons. \n\n1. In the introduction section, the authors claimed that \u201cGES is not guaranteed in the finite sample regime\u201d. This seems to be incorrect. For example, the Nandy et al. paper tackles exactly the finite sample problem. \n\nIn conclusion, overall this is a sensible idea, although some of the preliminaries still remain to be polished.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836003614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Reviewers"], "noninvitees": [], "tcdate": 1570237730243, "tmdate": 1575836003632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review"}}}, {"id": "H1eabJol9B", "original": null, "number": 3, "cdate": 1572019973242, "ddate": null, "tcdate": 1572019973242, "tmdate": 1574376552014, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "In this paper, the authors propose an RL-based structure searching method for causal discovery. The authors reformulate the score-based causal discovery problem into an RL-format, which includes the reward function re-design, hyper-parameter choose, and graph generation. To my knowledge, it\u2019s the first time that the RL algorithm is applied to causal discovery area for structure searching.\n \nThe authors\u2019 contributions are:\n(1) re-design the reword function which concludes the traditional score function and the acyclic constraint\n\n(2) Theoretically prove that the maximizing the reward function is equivalent to maximizing the original score function under some choices of the hyper-parameters.\n\n(3) Apply the reinforce gradient estimator to search the parameters related to adjacency matrix generation.  \n\n(4) In the experiment, the authors conduct experiment on datasets which includes both linear/non-linear model with Gaussian/Non-gaussian noise.\n\n(5) The authors public their code for reproducibility.\n \nOverall, the idea of this paper is novel, and the experiment is comprehensive. I have the following concerns.\n \n(1) In page 4 Encoder paragraph, the authors mention that the self-attention scheme is capable of finding the causal relationships. Why? In my opinion, the attention scheme only reflects the correlation relationship. The authors should give more clarifications to convince me about their beliefs.\n \n(2) The authors first introduce the h(A) constraint in eqn. (4), and mentioned that only have that constraint would result in a large penalty weight. To solve this, the authors introduce the indicator function constraint. What if we only use the indicator function constraint? In this case, the equivalence is still satisfied, so I am confused about the motivation of imposing the h(A) constraint.\n \n(3) In the last paragraph of page 5, why the authors adjust the predefined scores to a certain range?\n \n(4) Whether the acyclic can be guaranteed after minimizing the negative reward function (the eqn.(6))? I.e., After the training process, whether the graph with the best reward can be theoretically guaranteed to be acyclic?\n \n(5) In section 5.3, the authors mention that the generated graph may contain spurious edges? Whether the edges that in the cyclic are spurious? Whether the last pruning step contains pruning the cyclic path?\n \n \n(6) In the experiment, the authors adopt three metrics. For better comparison, the author should clarify that: the smaller the FDR/SHD is, the better the performance, and the larger the TPR is, the better the performance.\n\n(7) From the experimental results, the proposed method seems more superiors under the non-linear model case. Why? Could the authors give a few sentences about the guidance of the model selection in the real-world? i.e., when to select the proposed RL-based method? And under which case to choose RL-BIC, and which case to selection RL-BIC2?\n \n(8) What\u2019s training time, and how many samples are needed in the training process?\n \n \nMinor:\n1. In the page 4 decoder section, the notation of enc_i and enc_j is not clarified.\n\n2. On page 5, the \\Delta_1 and \\Delta_2 are not explained.\n\n3. For better reading experience, in table 1,2,3,4, the authors should bold value that has the best performance.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836003614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Reviewers"], "noninvitees": [], "tcdate": 1570237730243, "tmdate": 1575836003632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review"}}}, {"id": "SJguv9wTtB", "original": null, "number": 1, "cdate": 1571809887656, "ddate": null, "tcdate": 1571809887656, "tmdate": 1573778868786, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This work addresses the task of causal discovery. The proposed contribution is to apply prior work which uses reinforcement learning for combinatorial optimization to structure learning. Specifically, the proposed optimization problem seeks to maximize a penalized score criterion subject to the acyclicity constraint proposed by Zheng, et al. Empirical results show the proposed method performing favorably in contrast to prior art. \n\nOverall I think this is a sensible idea, and the authors do a nice job of exposition, and empirical evaluation. \n\nMy concerns are as follows:\n\n* The novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning.\n\n* The paper is loose with technical points. Specifically, the authors claim to use the additive noise model, but then make no restrictions on f(). In this setting, it is fairly well known that we can only hope to learn up to the Markov equivalence class (not the fully directed graph), but there is no mention of this in the paper. \n\nWith all of this said, I think overall the paper is an interesting addition to the causal discovery literature. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836003614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Reviewers"], "noninvitees": [], "tcdate": 1570237730243, "tmdate": 1575836003632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Review"}}}, {"id": "BklImw8Msr", "original": null, "number": 3, "cdate": 1573181213545, "ddate": null, "tcdate": 1573181213545, "tmdate": 1573527723289, "tddate": null, "forum": "S1g2skStPB", "replyto": "H1eabJol9B", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We greatly appreciate the reviewer's comments/suggestions [Author Response 3/3]", "comment": "(8) 'What\u2019s training time, and how many samples are needed in the training process?'\n \nWe did not include the training time because we used different machines for our experiments. The implementation of benchmark methods can also be optimized to reduce time (e.g., DAG-GNN's codes did not work with GPU) and the results may be somehow inaccurate. Here we just provide a rough description with 12-node linear data models:\n\n- Traditional methods PC and GES were run on a laptop with Intel 4-core i7 CPU, and produced the estimated result within 10 seconds; \n- NOTEARS and ICA-LiNGAM were also run on the laptop and can be finished in 1~3 minutes (we set the maximum number of iterations of the ICA algorithm to be 20,000, ten times of the default number used by the ICA-LiNGAM authors); \n- CAM was run on the same laptop and typically required 7~8 minutes; \n- Our algorithms RL-BIC and RL-BIC2 were run with Intel Xeon 3.20GHz CPU and Nvidia Quadro RTX 5000 GPU. Both methods took about 30~40 minutes with 12-nodes graphs and 20,000 iterations. For 30-node graphs and 30,000 iterations, they needed around 3 hours;\n- DAG-GNN took about 1 hour with the same Intel Xeon 3.20GHz CPU (their codes with GPU option did not work; the algorithm in fact did not require such a long time to reach convergence, yet no early stopping choice was provided in the codes); \n- GraN-DAG with the same CPU and GPU took about 20~30 minutes.\n \nRegarding the sample number, we have given the number of samples in each experiment description.\n \n(9) Minor: \n \n1. 'In the page 4 decoder section, the notation of enc_i and enc_j is not clarified. '\n\nActually $enc_i$ is given in the last sentence of the encoder part. \n \n2. 'On page 5, the \\Delta_1 and \\Delta_2 are not explained.'\n\nThanks for pointing this out. We will add a definition for the two notations.\n \n3. 'For better reading experience, in table 1,2,3,4, the authors should bold value that has the best performance.'\n \nThanks for this suggestion. We have considered doing so, but it is usually the case that a method that has the best TPR does not achieve the lowest FDR, and only making one in bold seems insufficient to evaluate the overall performance of a method. If possible, can the reviewer give further suggestion on this part? Thanks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "SklquiIIiH", "original": null, "number": 6, "cdate": 1573444466442, "ddate": null, "tcdate": 1573444466442, "tmdate": 1573446620502, "tddate": null, "forum": "S1g2skStPB", "replyto": "S1g2skStPB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We have uploaded a revised version ", "comment": "Dear reviewers,\n\nWe have uploaded a revised version of our paper, following the suggestions/comments from all the reviewers. Some changes are:\n\n- Section 3, Page 3: we add a sentence on the identifiability of Markov equivalence class, following Reviewer 1's comment;\n- Section 4, Page 4: we revise the statement on self-attention scheme being capable of finding causal relationships, following Reviewer 2's suggestion; \n- Section 5, Page 5: we add a sentence on the necessity of the acyclicity constraint from Zheng et al., according to Reviewer 1's comment;\n- Section 5, Page 5: we add a sentence on the effect of picking $\\lambda_2=0$ or using only the indicator function in the reward, with a more detailed discussion given in Appendix C in the revised manuscript, according to Reviewer 2's comment;\n- Section 5, Page 5: we add definitions for $\\Delta_1$ and $\\Delta_2$ and rephrase the paragraph for a better presentation, following Reviewer 2's comment;\n- Section 6, Page 6: we add a sentence to state that a lower SHD indicates a better performance, according to Reviewer 2's comment.\n\nTo Reviewer 3, we have not revised our statement on the finite sample result of GES in the new manuscript. We are eager to learn this finite sample result of GES and are happy to modify the statement if we misunderstand or omit related results in Nandy et al. paper.\n\nWe once again thank all the reviewers for their effort and many helpful comments/suggestions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "ryesqDUfsH", "original": null, "number": 4, "cdate": 1573181331162, "ddate": null, "tcdate": 1573181331162, "tmdate": 1573188191781, "tddate": null, "forum": "S1g2skStPB", "replyto": "H1eabJol9B", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We greatly appreciate the reviewer's comments/suggestions  [Author Response 2/3] ", "comment": "(3) \u2018In the last paragraph of page 5, why the authors adjust the predefined scores to a certain range?\u2019\n \nWe observe that the acyclicity penalty terms do not depend on the particular score functions. Consequently, even we have started with small penalty weights and then gradually increase them, the initial penalty weights may still be too high for other scores on the same problem, e.g., the independence based score function, which in the ideal case shall be zero, or if one wants to use the sample average BIC score. Therefore, we adjust the score to a certain range so that the RL algorithm with some choice of penalty weights is likely to work for other score functions as well.\n \n(4) 'Whether the acyclic can be guaranteed after minimizing the negative reward function (the eqn.(6))? I.e., After the training process, whether the graph with the best reward can be theoretically guaranteed to be acyclic? '\n \nThis is also a good point. In theory, no, since policy gradient methods only guarantee local convergence. But with the proposed strategy for penalty weights, the inferred graphs from RL algorithms are all DAGs in our experiments. In practice, if the graph from the training process is not acyclic, we may rerun the algorithm, possibly add more penalty weights, and/or try different NN weights as well. Post-processing method like pruning can also be used to make the inferred graph acyclic.\n \n(5) 'In section 5.3, the authors mention that the generated graph may contain spurious edges? Whether the edges that in the cyclic are spurious? Whether the last pruning step contains pruning the cyclic path? '\n \nWe do not fully understand this question, but we try to address it as much as we can.\n\nSpurious edge means false discovery, i.e., an edge in the estimated graph does not exist in the true graph. Using BIC, negative log likelihood, or other reconstruction error based score functions is very likely to result in spurious edge in finite sample regime. For example, the least squares loss would not increase, and usually decreases, if we include a non-parental node when fitting a causal relation. If this additional edge caused by this node does not violate the acyclicity constraint, we indeed get a better reward. So in practice with finite samples, spurious edges are hardly avoided and post-processing is needed. \n \nWe consider majority vote to somehow remove spurious edges based on the observation that the top few graphs, ranked by their rewards, are usually structurally similar. However, a majority vote of several DAGs is not necessarily a DAG. As per our assumption that the true graph is a DAG, a cyclic path must contain at least one spurious edge, but spurious edge does not necessarily lie in a cyclic path. Since the pruning methods with a decreasing tolerance or an increasing threshold can lead to the empty graph, i.e., graphs that have no edges, we believe that, with proper tolerance or threshold, the methods will result in DAGs so that the cyclic path is removed.\n \nPlease let us know if we have addressed your concern.\n \n(6) 'In the experiment, the authors adopt three metrics. For better comparison, the author should clarify that: the smaller the FDR/SHD is, the better the performance, and the larger the TPR is, the better the performance.' \n \nThanks. We will add this clarification in the revised version.\n \n(7) 'From the experimental results, the proposed method seems more superiors under the non-linear model case. Why? Could the authors give a few sentences about the guidance of the model selection in the real-world? i.e., when to select the proposed RL-based method? And under which case to choose RL-BIC, and which case to selection RL-BIC2?'\n \nThis is an acute observation. We did not notice it previously. We do not think that this should be the case, although it appears so. Different methods may have different assumptions on data generating procedures, and if the ground truth meets the assumptions, these methods usually perform very well (but may still incur estimation errors due to finite samples). For example, ICA-LiNGAM recovers all the true edges without any false discoveries for LiNGAM data. For non-linear model cases, it may be because we use Gaussian process regression which is nonparametric and can fit causal relations well.\n \nAs to the model selection, we believe that this is related to what score functions perform well here. For example, if we know that the true data model does not follow an additive model, then it is very likely that the least squares loss or BIC is not appropriate. For RL-BIC or RL-BIC2, model selection then reduces to whether we shall use the least squares or negative log likelihood as our loss function."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "HyesVTrGoS", "original": null, "number": 1, "cdate": 1573178674963, "ddate": null, "tcdate": 1573178674963, "tmdate": 1573186701260, "tddate": null, "forum": "S1g2skStPB", "replyto": "HklEzhj6FB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We thank the reviewer for the positive feedback and would like the reviewer to provide more details", "comment": "We thank the reviewer for the positive feedback on our work.\n \nRegarding 'GES is not guaranteed in the finite sample regime' and Nandy et al. paper 'High-dimensional consistency in score-based and hybrid structure learning': \n \nHere we aimed to state the consistency result of GES established by Chickering, and we find that Nandy et al. paper is also about consistency of GES but w.r.t. high dimension settings. In our understanding, consistency means that the probability of correct estimation of the ground truth goes to one as the number of samples approaches infinity, and we believe that this is also the case with Nandy et al. paper (please find below some quoted sentences where $n$ denotes the number of samples).  We however do not find a result or claim regarding guaranteed performance in the finite sample regime. In case we may misunderstand or miss certain results, can the reviewer please give more details on 'the Nandy et al. paper tackles exactly the finite sample problem', and if possible, the corresponding theorems or claims in Nandy et al. paper and other papers as well? \n \nAgain we greatly appreciate the reviewer's effort. We would definitely revise our statement if we misunderstand/omit the result of GES in the finite sample regime.\n \n-------------\n \nQuoted sentences from the arxiv version of Nandy et al. paper, available at https://arxiv.org/pdf/1507.02608.pdf:\n \nPage 3: 'In this paper, we prove high-dimensional consistency of GES, and we propose new hybrid algorithms based on GES that are consistent in several sparse high-dimensional settings and scale well to large sparse graphs. To the best of our knowledge, these are the first results on high-dimensional consistency of score-based and hybrid methods.'\n \nPage 7: 'Consistency of $\\mathcal S$ assures that $\\mathcal G_0$ has a lower score than any DAG that is not in the Markov equivalence class of $\\mathcal G_0$, with probability approaching one as $n\\to\\infty$ (Proposition 8 of Chickering [2002b]).'  \n \nPage 19, Theorem 5.2: 'Assume (A1)-(A6). Let $\\hat{\\mathcal C}_n$, $\\breve{\\mathcal C}_n$ and $\\tilde {\\mathcal C}_n$ be the outputs of ARGES-CIG based on $\\hat{\\mathcal I}_n$, ARGES-skeleton based on $\\hat{\\mathcal U}_n$ and GES respectively, with the scoring criterion $\\mathcal S_{\\lambda_n}$. Then there exists a sequence $\\lambda_n\\to 0$ such that $\\lim_{n\\to\\infty}\\mathbb P(\\hat{\\mathcal C}_n=\\mathcal C_{n0})=\\lim_{n\\to\\infty}\\mathbb P(\\breve{\\mathcal C}_n=\\mathcal C_{n0})=\\lim_{n\\to\\infty}\\mathbb P(\\tilde{\\mathcal C}_n=\\mathcal C_{n0})=1.$'"}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "rkeDLyLGor", "original": null, "number": 2, "cdate": 1573179215451, "ddate": null, "tcdate": 1573179215451, "tmdate": 1573186425409, "tddate": null, "forum": "S1g2skStPB", "replyto": "SJguv9wTtB", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We are grateful to the reviewer's effort and the positive comment", "comment": "We are grateful to the reviewer's effort and the positive comment on our paper. We are revising the paper by taking into accounts all the reviewers' comments/suggestions, and the revised version will be uploaded at a later time within this week.\n\n* Regarding 'The novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning':\n \nThese two ideas are indeed important to our RL based approach to causal discovery. Here we would like to briefly discuss the necessity of the acyclicity constraint from Zheng et al. With the proposed penalty weights in our work, Zheng et al.\u2019s acyclicity constraint $h(A)$ is used to guide the RL agent to generate directed graphs \u2018closer\u2019 to be acyclic and the indicator function w.r.t. acyclicity aims to induce exact DAGs. The major benefit of $h(A)$, or more precisely, $h(W\\circ W)$ ($W$ denotes the weighted adjacent matrix if it exists, e.g., for linear models, and $\\circ$ denotes Hadamard product), is its smoothness that enables continuous optimization for structure learning. This property is not utilized in our approach, and we believe other acyclicity functions, which measure certain \u2018distance\u2019 of a directed graph to be acyclic and do not need to be differentiable, can also be used here. We will add more discussions on this point in the revision.\n \n* Regarding 'The paper is loose with technical points. Specifically, the authors claim to use the additive noise model, but then make no restrictions on f(). In this setting, it is fairly well known that we can only hope to learn up to the Markov equivalence class (not the fully directed graph), but there is no mention of this in the paper':\n \nThanks for this helpful comment. We will add a sentence in Section 2 to state this result, along with the fact that we use fully identifiable models to generate observations in our experiments.\n \nWe once again appreciate the reviewer\u2019s effort on reviewing our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}, {"id": "HJllWOIMir", "original": null, "number": 5, "cdate": 1573181432275, "ddate": null, "tcdate": 1573181432275, "tmdate": 1573183339417, "tddate": null, "forum": "S1g2skStPB", "replyto": "H1eabJol9B", "invitation": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment", "content": {"title": "We greatly appreciate the reviewer's comments/suggestions [Author Response 1/3]", "comment": "We greatly appreciate the reviewer's comments/suggestions, many of which will lead to a more readable and self-contained version of our paper. We attempt to address all the concerns in the following. In case we may omit certain places, please do let us know. The revised manuscript will be uploaded at a later time within this week.\n\n(1) 'In page 4 Encoder paragraph, the authors mention that the self-attention scheme is capable of finding the causal relationships. Why? In my opinion, the attention scheme only reflects the correlation relationship. The authors should give more clarifications to convince me about their beliefs.'\n \nThis is a good point. The statement in the submitted manuscript is indeed vague and confusing. We agree with the reviewer that the attention scheme reflects only correlation or association. Many existing score based methods exploit correlations together with structural constraints to discover the causal relations. For example, NOTEARS uses linear regression for fitting the causal function with least squares as loss function. Clearly, using only linear regression could not find causal relationships, and what enables linear regression to find causal graphs is the acyclicity constraint. Since the self-attention scheme is very powerful in capturing the (correlated or associated) relations amongst variables, we believe that it, together with the acyclic constraint, is capable of finding causal relationships. We will revise the statement accordingly. Thanks very much for this comment that makes our paper more rigorous. \n \n(2) \u2018The authors first introduce the h(A) constraint in eqn. (4), and mentioned that only have that constraint would result in a large penalty weight. To solve this, the authors introduce the indicator function constraint. What if we only use the indicator function constraint? In this case, the equivalence is still satisfied, so I am confused about the motivation of imposing the h(A) constraint.\u2019 \n \nThis is very insightful. With only the indicator function term, problems (1) and (6) can still be equivalent. Yet this fact does not imply that an RL algorithm would also work well. Actually our initial reward consisted of the score function and only the indicator term, which worked well for small graphs (with $\\leq 6$ nodes or so) but very poorly for larger ones. We observed that the RL algorithm, with randomly initialized NN weights, could hardly generate DAGs in this case when only the indicator term was used. We now attempt to illustrate why this is the case:\n \n(a) the directed graphs in our approach are randomly generated according to Bernoulli distributions, and without loss of generality, consider that each edge is drawn independently according to Bern(0.5). For small graphs (with $\\leq 6$ nodes), a few hundreds of samples of directed graphs are very likely to contain a DAG. Yet for large graphs, the probability of sampling a DAG is a lot lower. If no DAG is generated during training, then the RL agent can hardly learn to generate DAGs. Thus, we need the reward to guide the agent to produce DAGs. This, however, is difficult for large graphs with only the indicator term; see below.\n\n(b) for a cyclic directed graph with all possible directed edges in place and a cyclic directed graph with only two edges (that is, $i\\to j$ and $j\\to i$ for some $i\\neq j$), the latter is 'closer' to be acyclic in some sense, e.g., number of edge operations to make it acyclic. However, the first one is likely to have a lower BIC score when using linear regression for fitting causal relations, and yet the penalty terms of acyclicity are the same. In other words, the first graph usually has a better reward, which does not help the agent to tend to generate DAGs. This fact motivates us to include the other penalty term that measures some 'distance' to be a DAG, so that the agent can be trained to produce graphs closer to acyclicity and finally to generate exact DAGs. With initialized NN weights, the generated graphs at early iterations can be 'far' from acyclicity for large problems, and we believe that using only the indicator function is insufficient.\n \nA question is then what if we start with a DAG, e.g., by initializing the probability of generating each edge to be very small. This setting did not lead to good performance, either. The generated directed graphs at early iterations can be very different from the true graphs in missing many true edges, and the resulting score is much higher than the optimum under the DAG constraint. With small penalty weights of the acyclicity terms, the agent would produce cyclic graphs with lower scores, which then reduces to case (b). On the other hand, large penalty weights, as we have discussed in the paper, limit exploration of the RL agent and usually result in DAGs whose scores are far from optimum.\n \nWe hope that the above discussion has addressed the reviewer\u2019s concern. We will add more discussions to make this point clear in the revised paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1929/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Discovery with Reinforcement Learning", "authors": ["Shengyu Zhu", "Ignavier Ng", "Zhitang Chen"], "authorids": ["zhushengyu@huawei.com", "ignavierng@cs.toronto.edu", "chenzhitang2@huawei.com"], "keywords": ["causal discovery", "structure learning", "reinforcement learning", "directed acyclic graph"], "TL;DR": "We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets", "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint. ", "pdf": "/pdf/55c1ba462c99a668beb7d886577e3d3dce136788.pdf", "paperhash": "zhu|causal_discovery_with_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020Causal,\ntitle={Causal Discovery with Reinforcement Learning},\nauthor={Shengyu Zhu and Ignavier Ng and Zhitang Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g2skStPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dd54e05123e3fea8562954777ae620e0ecfc764a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g2skStPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1929/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1929/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1929/Authors|ICLR.cc/2020/Conference/Paper1929/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148826, "tmdate": 1576860539373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1929/Authors", "ICLR.cc/2020/Conference/Paper1929/Reviewers", "ICLR.cc/2020/Conference/Paper1929/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1929/-/Official_Comment"}}}], "count": 14}