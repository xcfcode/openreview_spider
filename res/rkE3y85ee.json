{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487870554225, "tcdate": 1478283179713, "number": 281, "id": "rkE3y85ee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkE3y85ee", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "content": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396478185, "tcdate": 1486396478185, "number": 1, "id": "S1LB3MLul", "invitation": "ICLR.cc/2017/conference/-/paper281/acceptance", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper proposes a neat general method for relaxing models with discrete softmax choices into closely-related models with continuous random variables. The method is designed to work well with the reparameterization trick used in stochastic variational inference. This work is likely to have wide impact.\n \n Related submissions at ICLR:\n \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\" by Maddison et al. contains the same core idea. \"Discrete variational autoencoders\", by Rolfe, contains an alternative relaxation for autoencoders with discrete latents, which I personally find harder to follow.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396478687, "id": "ICLR.cc/2017/conference/-/paper281/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkE3y85ee", "replyto": "rkE3y85ee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396478687}}}, {"tddate": null, "tmdate": 1484874031464, "tcdate": 1484274205219, "number": 8, "id": "BJr753rIl", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "SJ1R_ieEg", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "writers": ["~Eric_Jang1"], "content": {"title": "Reply: Review4", "comment": "Thank you for the comments. Unlike previous approaches to learning with categorical latent variables that rely on complex score function estimators, Gumbel-Softmax is easy to implement in any modern deep learning framework. Furthermore, the idea of developing continuous relaxations for reparameterization gradients with categorical variables is novel, and represents a large step forward in our ability to develop estimators for discrete variables.\n\nThe purpose of the semi-supervised experiments was to demonstrate that stochastic inference using Gumbel-Softmax results in tremendous speedups of semi-supervised architectures without compromising accuracy. \n\nEvaluating on a simple dataset such as MNIST makes the analysis and interpretation of results simpler; the feasibility of the technique comes from stochastic inference with Gumbel-Softmax, and we did not have to tweak the inference, discriminative, or generative components of the model to reproduce reasonable semi-supervised results. \n\nAs for scaling up to large class sizes, the interior of the simplex (where the continuous relaxations lie) does indeed get smaller as the number of classes go up, resulting in vanishing gradients. We ran some experiments and found that unbiased estimators like MuProp converge faster than Gumbel-Softmax when the dimensionality of the sample is large (K>800). However, Gumbel-Softmax remains useful for differentiating through many kinds of lower-dimensional categorical samples (mixture models, discrete action spaces, character sequences). In practice, large class sizes are not an issue anyway, because categorical samples with large number of classes K can be encoded in base M << K rather than a single one-hot K-vector. For instance, stochastic binary layer with log2(K) units."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1484274017113, "tcdate": 1484274017113, "number": 7, "id": "rkYwK3rUl", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "HJf3GfM4e", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "writers": ["~Eric_Jang1"], "content": {"title": "Reply: Review3", "comment": "Thank you for the comments. Please see our response to Review 2 for discussion of the temperature parameter. \n\nOur proposed straight-through Gumbel-Softmax estimator can be used to produce samples that are exactly one-hot and can be re-encoded as a K-ary categorical variable. \n\nWe have updated our paper with a reference and comparison to the Discrete VAE paper by Rolfe (2016) (Section 3.1). They consider a substantially different model from other prior work; they use a discrete latent space augmented with auxiliary continuous variables, a hierarchical posterior, and a RBM prior. Because the models are so different, it\u2019s difficult to compare quantitative results directly.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1484273954755, "tcdate": 1484273954755, "number": 6, "id": "Syj7K3SIx", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "Sk0G5NVEg", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "writers": ["~Eric_Jang1"], "content": {"title": "Reply: Review 2", "comment": "Thank you for the comments. We agree with the reviewer that Gumbel-Softmax is useful in semi-supervised situations, and also find it to be useful in learning discrete latent variable models in the purely unsupervised setting. The learned discrete latent spaces often yield more interpretable and semantically meaningful latent spaces than a corresponding continuous latent variable model. Additionally, sampling from Gumbel-Softmax can be used to select discrete \u201cactions\u201d (and then differentiated to obtain policy gradients for Reinforcement Learning), or used in sequential sampling of discrete samples corresponding to character tokens (language models), or sampling from mixture models in a re-parameterizable fashion.\n\nWe have made the correction from \u201csampling\u201d => \u201capproximate sampling\u201d (P1) and the \u201cbackpropagtion\u201d typo (P3).\n\nWe made a typo in describing the architecture of the VAE - our experiments use a learned categorical prior (consistent with prior work from Gu et al. 2016), not a uniform categorical prior, and we have made a correction to this as well.\n\nFor our experiments, fixed temperatures between 0.5 and 1.0 yield good results for both structured output prediction and VAE tasks. The concurrent submission by Maddison et al. 2016 (https://openreview.net/forum?id=S1jE5L5gl) uses a fixed temperature of 2/3 for all experiments. We found that annealing the temperature yielded slightly better results that also converged faster for VAEs. The takeaway is that annealing improves performance, but is not absolutely critical to yield good results. \n\nWe ran some follow-up experiments to determine whether the temperature parameter can indeed be learned. Surprisingly, learning the temperature causes the temperature parameter to increase and saturate, rather than decrease and saturate. We suspect that when posterior inference does not do a good job, i.e. sampling q(z|x) chooses a \u201cbad sample\u201d, the models can improve the reconstruction locally (and thus the autoencoding term) simply by increasing the temperature (i.e. smoothing the discrete sample to a more uniform one). Optimizing the model parameters with this higher temperature results in a lower training loss, but because Gumbel-Softmax samples are no longer constrained to be sparse, we do poorly on the original discrete objective. We still observe overfitting behavior on the validation and test sets when training using Straight-Through Gumbel Softmax with learning temperature.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1482078807183, "tcdate": 1482078742380, "number": 3, "id": "Sk0G5NVEg", "invitation": "ICLR.cc/2017/conference/-/paper281/official/review", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["ICLR.cc/2017/conference/paper281/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper281/AnonReviewer2"], "content": {"title": "Interesting idea, encouraging results", "rating": "7: Good paper, accept", "review": "This paper introduces a continuous relaxation of categorical distribution,  namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients. The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models. The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments. The paper is well-written, and I applaud the details provided in the appendix. The main application seems to be semi-supervised situations where you really want categorical variables.\n\n - P1: \"differentiable sampling mechanism for softmax\". \"sampling\" => \"approximate sampling\", since it's technically sampling from the Gumbal-softmax.\n \n - P3: \"backpropagtion\"\n \n - Section 4.1: Interesting experiments.\n \n - It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported.\n \n - It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637511, "id": "ICLR.cc/2017/conference/-/paper281/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper281/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper281/AnonReviewer4", "ICLR.cc/2017/conference/paper281/AnonReviewer3", "ICLR.cc/2017/conference/paper281/AnonReviewer2"], "reply": {"forum": "rkE3y85ee", "replyto": "rkE3y85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637511}}}, {"tddate": null, "tmdate": 1481937682496, "tcdate": 1481937578041, "number": 2, "id": "HJf3GfM4e", "invitation": "ICLR.cc/2017/conference/-/paper281/official/review", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["ICLR.cc/2017/conference/paper281/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper281/AnonReviewer3"], "content": {"title": "Review: Categorical Reparameterization with Gumbel-Softmax", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a method for reparameterization gradients with categorical distributions. This is done by using the Gumbel-Softmax distribution, a smoothened version of the Gumbel-Max trick for sampling from a multinomial.\n\nThe paper is well-written and clear. The application to the semi-supervised model in Kingma et al. (2014) makes sense for large classes, as well as its application to general stochastic computation graphs (Schulman et al., 2015).\n\nOne disconcerting point is that (from my understanding at least), this does not actually perform variational inference for discrete latent variable models. Rather, it changes the probability model itself and performs approximate inference on the modified (continuous relaxed) version of the model. This is fine in practice given that it's all approximate inference, but unlike previous variational inference advances either in more expressive approximations or faster computation (as noted by the different gradient estimators they compare to), the probability model is fundamentally changed.\n\nTwo critical points seem key: the sensitivity of the temperature, and whether this applies for non-one hot encodings of the categorical distribution (and thus sufficiently scale to high dimensions). Comments by the authors on this are welcome.\n\nThere is a related work by Rolfe (2016) on discrete VAEs, who also consider a continuous relaxed approach. This is worth citing and comparing to (or at least mentioning) in the paper.\n\nReferences\n\nRolfe, J. T. (2016). Discrete Variational Autoencoders. arXiv.org.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637511, "id": "ICLR.cc/2017/conference/-/paper281/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper281/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper281/AnonReviewer4", "ICLR.cc/2017/conference/paper281/AnonReviewer3", "ICLR.cc/2017/conference/paper281/AnonReviewer2"], "reply": {"forum": "rkE3y85ee", "replyto": "rkE3y85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637511}}}, {"tddate": null, "tmdate": 1481844935227, "tcdate": 1481844935222, "number": 1, "id": "SJ1R_ieEg", "invitation": "ICLR.cc/2017/conference/-/paper281/official/review", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["ICLR.cc/2017/conference/paper281/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper281/AnonReviewer4"], "content": {"title": "The paper is well written but the novelty of the paper is less clear", "rating": "6: Marginally above acceptance threshold", "review": "The paper combines Gumbel distribution with the popular softmax function to obtain a continuous distribution on the simplex that can approximate categorical samples. It is not surprising that Gumbel softmax outperforms other single sample gradient estimators. However, I am curious about how Gumbel compares with Dirichlet experimentally. \n\nThe computational efficiency of the estimator when training semi-supervised models is nice. However, the advantage will be greater when the number of classes are huge, which doesn't seem to be the case in a simple dataset like MNIST. I am wondering why the experiments are not done on a richer dataset. \n\nThe presentation of the paper is neat and clean. The experiments settings are clearly explained and the analysis appears to be complete. \n\nThe only concern I have is the novelty of this work. I consider this work as a nice but may be incremental (relatively small) contribution to our community. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637511, "id": "ICLR.cc/2017/conference/-/paper281/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper281/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper281/AnonReviewer4", "ICLR.cc/2017/conference/paper281/AnonReviewer3", "ICLR.cc/2017/conference/paper281/AnonReviewer2"], "reply": {"forum": "rkE3y85ee", "replyto": "rkE3y85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637511}}}, {"tddate": null, "tmdate": 1481519232342, "tcdate": 1481519232211, "number": 5, "id": "S1_tl2smg", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "Symj05vmx", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "writers": ["~Eric_Jang1"], "content": {"title": "Gumbel-Softmax ha sstraightforward reparameterization", "comment": "Gumbel-Softmax and Dirichlet are both continuous distributions on the simplex; thus, samples from both distributions can be interpreted as class (categorical) probabilities. \n\nOne distinction is that samples from Gumbel-Softmax can be sampled via a straightforward reparameterization trick, while the sampling from a Dirichlet (via stick-breaking or urn drawing) makes it harder to exploit the reparameterization trick for estimating gradients (though recent work https://arxiv.org/abs/1610.05683 enables this via rejection sampling, and a reparameterizable Dirichlet approximation might be possible using https://arxiv.org/abs/1505.05770). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1481259396457, "tcdate": 1481259396450, "number": 4, "id": "By3FY3v7x", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "rJhKOuwXx", "signatures": ["~Eric_Jang1"], "readers": ["everyone"], "writers": ["~Eric_Jang1"], "content": {"title": "re: batch sizes", "comment": "minibatch size = 100 for all experiments"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1481252506620, "tcdate": 1481252506616, "number": 1, "id": "Symj05vmx", "invitation": "ICLR.cc/2017/conference/-/paper281/pre-review/question", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["ICLR.cc/2017/conference/paper281/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper281/AnonReviewer4"], "content": {"title": "dirichlet distribution", "question": "How does this relate to dirichlet distribution?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481252507116, "id": "ICLR.cc/2017/conference/-/paper281/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper281/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper281/AnonReviewer4"], "reply": {"forum": "rkE3y85ee", "replyto": "rkE3y85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper281/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481252507116}}}, {"tddate": null, "tmdate": 1481242756071, "tcdate": 1481242756067, "number": 3, "id": "rJhKOuwXx", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Batch sizes used", "comment": "I am trying to reproduce the results in the paper. What are the batch sizes used in the experiments ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}, {"tddate": null, "tmdate": 1479945812024, "tcdate": 1479945812019, "number": 2, "id": "BJ2URiQfg", "invitation": "ICLR.cc/2017/conference/-/paper281/public/comment", "forum": "rkE3y85ee", "replyto": "rkE3y85ee", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Applicability over large class sizes", "comment": "It would be interesting to see experiments over large class sizes(K>10k). Have the authors already tried that ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "pdf": "/pdf/14d989bed02fa62e71fbb485fcec6e26d6c91ccf.pdf", "TL;DR": "Simple, differentiable sampling mechanism for categorical variables that can be trained in neural nets via standard backprop.", "paperhash": "jang|categorical_reparameterization_with_gumbelsoftmax", "conflicts": ["google.com"], "keywords": ["Deep learning", "Semi-Supervised Learning", "Optimization", "Structured prediction"], "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "authorids": ["ejang@google.com", "sg717@cam.ac.uk", "poole@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642562, "id": "ICLR.cc/2017/conference/-/paper281/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkE3y85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper281/reviewers", "ICLR.cc/2017/conference/paper281/areachairs"], "cdate": 1485287642562}}}], "count": 13}