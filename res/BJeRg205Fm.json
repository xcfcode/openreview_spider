{"notes": [{"id": "BJeRg205Fm", "original": "HJeJL44PKQ", "number": 1129, "cdate": 1538087926482, "ddate": null, "tcdate": 1538087926482, "tmdate": 1545355417200, "tddate": null, "forum": "BJeRg205Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skl_Aig2kN", "original": null, "number": 1, "cdate": 1544453071601, "ddate": null, "tcdate": 1544453071601, "tmdate": 1545354497710, "tddate": null, "forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Meta_Review", "content": {"metareview": "This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet-Multinomial likelihood. This paper is clearly written with a sound main idea. However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. All the reviewers therefore considered this paper to be of limited novelty. Reviewer 2 also had a concern about the mixed experimental results of the proposed method.\n\nReviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Limited novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper1129/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1129/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352954355, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1129/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1129/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1129/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352954355}}}, {"id": "H1leTlOyAQ", "original": null, "number": 2, "cdate": 1542582456138, "ddate": null, "tcdate": 1542582456138, "tmdate": 1542582456138, "tddate": null, "forum": "BJeRg205Fm", "replyto": "rJgovF952X", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Official_Comment", "content": {"title": "Relationship to variational autoencoder models", "comment": "Thank you for taking the time to review our paper.\n\nFor each input, the proposed model provides a distribution over the possible target values, not just a point estimate. A variational autoencoder is able to model more complex output distributions by replacing a fixed output distribution with a neural network, but it is fundamentally doing the same thing --- it is just another parameterized model trained to maximize the conditional likelihood of the targets. The models described in this paper are simpler and have practical advantages over variational autoencoder models: 1) training can be performed using the true gradients rather than approximations, 2) the form of the output (posterior) distribution is easy to interpret, and 3) it is easy to integrate the output distribution over the target space."}, "signatures": ["ICLR.cc/2019/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1129/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1129/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625559, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeRg205Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1129/Authors", "ICLR.cc/2019/Conference/Paper1129/Reviewers", "ICLR.cc/2019/Conference/Paper1129/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1129/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1129/Authors|ICLR.cc/2019/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1129/Reviewers", "ICLR.cc/2019/Conference/Paper1129/Authors", "ICLR.cc/2019/Conference/Paper1129/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625559}}}, {"id": "B1lBOWKbpX", "original": null, "number": 3, "cdate": 1541669228918, "ddate": null, "tcdate": 1541669228918, "tmdate": 1541669228918, "tddate": null, "forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "content": {"title": "Unoriginal and unfortunately unfocused contributions", "review": "The authors use neural networks to parameterize conditional probability distributions. This is well-known and has been applied in the literature since extensions to generalized linear models beyond their canonical link function in the 70s. Their transformation from real-valued network output to, say, strictly positive concentration parameters in a Dirichlet are worth studying; but they don't analyze this in any detail.\n\nIn addition, while lacking novelty may be fine in and of itself, the purpose of applying these ideas doesn't have a focused purpose. For example, the authors argue in the abstract this quantifies uncertainty. That's only true if you care about data noise, but the end-result is still point estimation for the parameters with uncalibrated probabilities. In the rest of the paper, they write primarily about simplex-valued outputs (i.e., soft one-hot labels).", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1129/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "cdate": 1542234299481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1129/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879149, "tmdate": 1552335879149, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1129/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgovF952X", "original": null, "number": 2, "cdate": 1541216610867, "ddate": null, "tcdate": 1541216610867, "tmdate": 1541533397499, "tddate": null, "forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "content": {"title": "No novelty, conceptually problematic, and exceeding the page limit", "review": "The paper shows how to model the outputs of neural networks via likelihoods other than commonly used ones. The likelihoods discussed include Beta, Dirichlet and Dirichlet-Multinomial. The paper introduces the gradient computation of these likelihoods and test them in several datasets. \n\nThis paper lacks novelty and has conceptual mistakes. It is a common practice, in Bayesian learning, to model different types of data with different likelihoods. The examples discussed in this paper are very basis and the gradient computation is standard. I do not see anything new. And the authors misunderstand that if you involve some likelihood in training, you can quantify the uncertainty. It is wrong. Uncertainty should be estimated in the posterior inference framework --- you need to integrate the posterior distribution of the (latent) random variables into the test likelihood to obtain the predictive distribution, from which you can identify the confidence levels. That\u2019s why auto-encoding variational Bayes framework is useful and popular.  \nWhat the paper is doing is still the point estimation. \n\nBesides, the paper exceeds the 8-page limit for the content. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1129/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "cdate": 1542234299481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1129/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879149, "tmdate": 1552335879149, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1129/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJx6p1W53m", "original": null, "number": 1, "cdate": 1541177285111, "ddate": null, "tcdate": 1541177285111, "tmdate": 1541533397288, "tddate": null, "forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "content": {"title": "Reasonable proposal and well-written paper, but no new insights and inconclusive empirical results", "review": "This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. Overall, I found the writing very clear, the main idea sound, and paper generally well executed, but I have serious concerns about the significance of the contributions that lead me to recommend rejection. It would be very useful to me if the authors would provide a concise list of what they consider the main contributions to be and why they are significant. As I see it, the paper does three main things:\n\n1. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions. The simplifications and gradient derivations are well known and appear in many places (e.g. http://jonathan-huang.org/research/dirichlet/dirichlet.pdf, https://arxiv.org/pdf/1405.0099.pdf) and should not be considered contributions in the age of automatic differentiation (see Justin Domke's blog post on autodiff).\n\n2. In section 3, the authors consider the unique challenges of using the proposed networks. They propose targeted activation functions that will improve the stability of learning. I found this to be the most interesting portion of the paper and the most significant contribution. Unfortunately, it is short on details and empirical results are referenced that do not appear in the paper (i.e. the second to last paragraph on page 5). If I were to rewrite this paper, I would focus on answering the question \"What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?\", replacing section 2 with an expanded section 3.\n\n3. In section 4, the authors evaluate the proposed networks on a collection of synthetic and real tasks. In the end, the results are mixed, with the Dirichlet network performing best on the XENON1T task and the standard softmax network performing best on the CIFAR-100 task. In general, I don't mind mixed results and I appreciate that the authors included both sets of experiments; however, it is important that there is a convincing argument for why one would prefer the proposed solution even when accuracy is the same (e.g. it is faster, it is interpretable, etc.). The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. This may be true, but they only perform evaluations on tasks where the primary goal is accuracy. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that.\n\nIn summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or (2) present a convincing evaluation that strongly motivates the proposed model's use or that provides some novel insight into the model's behavior. I think that the authors are on their way to achieving (1), but do not achieve (2). I would suggest finding an application that requires uncertainty estimates for the distribution and centering the paper around that application.\n\nMinor comments:\n\n- Figure 2 (right) should include a y-axis label (e.g. \"parameter value\").\n\n- In Figure 3 (right), it is not obvious what the \"Sigmoid\" line corresponds to. \n\n- It is not clear what the authors are trying to show in section 4.1. The EL activation function is smooth and monotone and the likelihood is convex, so there should be no question that the distribution will concentrate around y.\n\n- Section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1129/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Official_Review", "cdate": 1542234299481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1129/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879149, "tmdate": 1552335879149, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1129/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyehQhujcm", "original": null, "number": 1, "cdate": 1539177508173, "ddate": null, "tcdate": 1539177508173, "tmdate": 1539177738596, "tddate": null, "forum": "BJeRg205Fm", "replyto": "BJeRg205Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1129/Public_Comment", "content": {"comment": "Hello!\n\nI find your investigation of the construction and training of models which parameterise the Dirichlet family of distributions to be relevant to our work, especially your investigation into improving the trainability and stability of such models. \n\nIn our (due to appear at NIPS 2018 -  https://arxiv.org/pdf/1802.10501.pdf  ) we parameterise a Dirichlet distribution using a DNN in order to derive measures of uncertainty from 'distributions over distributions' for detecting misclassifications and out-of-distribution inputs.\n\nI'm excited by your work and looking forward to any follow up :) .\n\nBest Regards,\nAndrey Malinin", "title": "Related work"}, "signatures": ["~Andrey_Malinin1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1129/Reviewers/Unsubmitted"], "writers": ["~Andrey_Malinin1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs", "abstract": "We propose a method for quantifying uncertainty in neural network regression models when the targets are real values on a $d$-dimensional simplex, such as probabilities. We show that each target can be modeled as a sample from a Dirichlet distribution, where the parameters of the Dirichlet are provided by the output of a neural network, and that the combined model can be trained using the gradient of the data likelihood. This approach provides interpretable predictions in the form of multidimensional distributions, rather than point estimates, from which one can obtain confidence intervals or quantify risk in decision making. Furthermore, we show that the same approach can be used to model targets in the form of empirical counts as samples from the Dirichlet-multinomial compound distribution. In experiments, we verify that our approach provides these benefits without harming the performance of the point estimate predictions on two diverse applications: (1) distilling deep convolutional networks trained on CIFAR-100, and (2) predicting the location of particle collisions in the XENON1T Dark Matter detector.", "keywords": ["regression", "uncertainty", "deep learning"], "authorids": ["peter.sadowski@hawaii.edu", "pfbaldi@ics.uci.edu"], "authors": ["Peter Sadowski", "Pierre Baldi"], "TL;DR": "Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.", "pdf": "/pdf/307f854d16a91d35691110152d1995c9dfb8a767.pdf", "paperhash": "sadowski|neural_network_regression_with_beta_dirichlet_and_dirichletmultinomial_outputs", "_bibtex": "@misc{\nsadowski2019neural,\ntitle={Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs},\nauthor={Peter Sadowski and Pierre Baldi},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeRg205Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1129/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311671892, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJeRg205Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1129/Authors", "ICLR.cc/2019/Conference/Paper1129/Reviewers", "ICLR.cc/2019/Conference/Paper1129/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1129/Authors", "ICLR.cc/2019/Conference/Paper1129/Reviewers", "ICLR.cc/2019/Conference/Paper1129/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311671892}}}], "count": 7}