{"notes": [{"id": "Hke-JhA9Y7", "original": "rylSWld5YX", "number": 959, "cdate": 1538087896802, "ddate": null, "tcdate": 1538087896802, "tmdate": 1550876789920, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1edbrPEeV", "original": null, "number": 1, "cdate": 1545004287695, "ddate": null, "tcdate": 1545004287695, "tmdate": 1545354486912, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Meta_Review", "content": {"metareview": "The reviewers all feel that the paper should be accepted to the conference.  The main strengths that they noted were the quality of writing, the wide applicability of the proposed method and the strength of the empirical evaluation.  It's nice to see experiments across a large number of problems (100), with corresponding code, where baselines were hyperparameter tuned as well.  This helps to give some assurance that the method will generalize to new problems and datasets.    Some weaknesses noted by the reviewers were computational cost (the method is significantly slower than the baselines) and they weren't entirely convinced that having more concise representations would directly lead to the claimed interpretability of the approach.  Nevertheless, they found it would make for a solid contribution to the conference.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Well written paper on learning concise representations for regression with strong empirical evaluation"}, "signatures": ["ICLR.cc/2019/Conference/Paper959/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper959/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353020491, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353020491}}}, {"id": "BJgNl0PKnX", "original": null, "number": 2, "cdate": 1541139947705, "ddate": null, "tcdate": 1541139947705, "tmdate": 1543345417604, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "content": {"title": "This paper lacks technical novelty and experimental result is incomplete.", "review": "This paper introduces a genetic algorithm that maintains an archive of representations that are iteratively evolved and selected by comparing validation error. Each representation is constructed as a syntax tree consists of elements that are common in neural network architectures. The experimental results showed that their algorithm is competitive to the state-of-the-art while achieving much smaller model size.\n\nComments:\n1. I think this paper lacks technical novelty. I'm going to focus on experimental result in the following two questions.\n2. FEAT is a typical genetic algorithm that converges slowly. In the appendix, one can verify that FEAT converges at least 10x slower than XGBoost. Can FEAT achieve lower error than XGBoost when they use the same amount of time? \nCan the authors provide a convergence plot of their algorithm (i.e. real time vs test error)?\n3. From Figure 3 it seems that the proposed algorithm is competitive to XGBoost, and the model size is much smaller than XGBoost. Have the authors tried to post-processing the model generated by XGBoost? How's the performance compare?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper959/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "cdate": 1542234338060, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841691, "tmdate": 1552335841691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxS9k3-Am", "original": null, "number": 5, "cdate": 1542729613430, "ddate": null, "tcdate": 1542729613430, "tmdate": 1542729613430, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "ryxU37agA7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "content": {"title": "regarding the parameter expansion", "comment": "2. We expanded the parameter space for XGBoost to give it a larger computational budget. This larger budget compensates for the fact that fitting a single model using XGBoost is quicker than with Feat. The extra tuning made the XGBoost results slightly better; in the pdfdiff for Figure 3 of the revision, one can see a slight improvement in the boxplot for XGBoost. However, XGBoost's accuracy was still not significantly different than Feat over all problems (p = 1.0). Interestingly, the new XGBoost results did significantly outperform MLP, unlike the original results. "}, "signatures": ["ICLR.cc/2019/Conference/Paper959/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619803, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper959/Authors|ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619803}}}, {"id": "ryxU37agA7", "original": null, "number": 4, "cdate": 1542669230052, "ddate": null, "tcdate": 1542669230052, "tmdate": 1542669230052, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "S1lEd9Oqa7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "content": {"title": "Some Updates after reading authors' comments and other reviews", "comment": "1. I should say I'm biased since the techniques that the authors used actually sounds familiar to me. I'll take this into consideration.\n\n2. Why was the parameter expansion necessary? Does it reduce the error?\n\n3. This addresses my question. Thanks."}, "signatures": ["ICLR.cc/2019/Conference/Paper959/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper959/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619803, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper959/Authors|ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619803}}}, {"id": "rkgGkod5pQ", "original": null, "number": 3, "cdate": 1542257369682, "ddate": null, "tcdate": 1542257369682, "tmdate": 1542257369682, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "SkgGi8lYnX", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "content": {"title": "Revision expands hyperparameters, and a note on the choice of constant optimization", "comment": "We thank the reviewer for their comments, and address a few minor points below. \n\n1) \"only very limited hyperparameter tuning for the other methods was performed\"\n\n  - We have extended the hyperparameter space for XGBoost, the closest competitor, in our revision. Hopefully this addresses the reviewer's concern.\n\n2) The reviewer correctly points out that size is only a proxy for interpretability in this experiment. We do not have a better way to assess lebility outside of an application with expert analysis. Nevertheless, simpler models are generally (but not always) easier to interpret. Our goal with the illustrative example is to show this, and we state similar caveats as the reviewer has suggested. \n\n3) Regarding adjustment of weights, weights are only adjusted for features that are composed of differentiable operators because this is a limitation of the chain rule with gradient descent. It is important to note that all of the floating point operators we considered were differentiable; the only non-differentiable nodes were boolean operators, which don't include weights. It would also be possible to use another method to tune the weights such as stochastic hillclimbing, although previous symbolic regression research on this subject tends to favor gradient descent for weight tuning weights [1,2]. Hopefully this addresses the reviewer's question; if not we are happy to clarify further. \n\n[1] Kommenda, M. et. al. (2013, July). Effects of constant optimization by nonlinear least squares minimization in symbolic regression. In Proceedings of the 15th annual conference companion on Genetic and evolutionary computation (pp. 1121-1128). ACM.\n[2] Topchy, A., & Punch, W. F. (2001, July). Faster genetic programming based on local gradient search of numeric leaf values. In Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation (pp. 155-162). Morgan Kaufmann Publishers Inc..\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper959/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619803, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper959/Authors|ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619803}}}, {"id": "S1lEd9Oqa7", "original": null, "number": 2, "cdate": 1542257259732, "ddate": null, "tcdate": 1542257259732, "tmdate": 1542257259732, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "BJgNl0PKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "content": {"title": "On the novelty of this work, and a discussion of updated experiments ", "comment": "We thank the reviewer for the critiques, which have led to some improvements to our experiment and hopefully more convincing analysis. \n\n1. It is hard for us to respond to the reviewer's contention that our work lacks technical novelty without more specific critiques. However, we will restate what is novel here. \n\nFirst, FEAT represents models in the population as sets of syntax trees/equations. This representation is novel both in neural network literature and genetic algorithm literature. Second, we use the feedback of model weights to guide variation probabilities; to our knowledge this is a new approach. FEAT also uses multiple type representations, meaning it can learn boolean and continuous functions in the same representation, something we believe to be novel as well. Finally, the composition of syntax trees using NN activation functions along with other operations is rarely seen in GA/GP literature, much less the edge-based encoding of weights. Taken as a whole, there are several novel technical aspects of the algorithm. \n\nIn addition to the methodological aspects, few if any previous works in neural architecture search / neuroevolution focus on regression with the goal of intelligibility. In this regard we believe our results are novel and important: by establishing a new state-of-the-art, they point to a new area of application for this field of research.  \n\n2. We completely agree with the reviewer's point that FEAT converges more slowly than XGBoost. We should expect a randomized, population-based heuristic search method to be slower than a greedy, single-model heuristic-based method. To address this point, we have added text to the experiments and discussion, and reworked the XGBoost analysis . \n\nOur stated goal is to produce simplest possible models without sacrificing accuracy, and we contend that our method achieves this. Although computation time suffers as a result, we believe it is reasonable to consider a 60 minute cutoff for optimization time on every problem, some of which contain millions of samples. \n\nThe reviewer also asks whether FEAT can achieve lower error than XGBoost given the same amount of time. Based on the reviewer's comments we have expanded the hyperparameter space for XGBoost in our revision, from 9 hyperparameter combinations to 1925. This extension results in wallclock runtimes closer to those of FEAT and MLP. Under these conditions, the accuracy comparisons do not change much. We still see no significant differences between FEAT and XGBoost in terms of accuracy.\n\n3. To address the reviewer's suggestion regarding complexity, we have generated our XGBoost results in this revision with a pruning step after tree construction. We have also optimized the minimum split loss criterion (gamma) that controls the amount of pruning. Under these conditions, we observe very similar size comparisons as before. \n\nWe hope the updated manuscript addresses the reviewer's concerns."}, "signatures": ["ICLR.cc/2019/Conference/Paper959/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619803, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper959/Authors|ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619803}}}, {"id": "Skl9kud9p7", "original": null, "number": 1, "cdate": 1542256610286, "ddate": null, "tcdate": 1542256610286, "tmdate": 1542256610286, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "rJeoSzM9h7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "content": {"title": "Computational tradeoff now discussed", "comment": "We thank the reviewer for their positive comments. We agree with the reviewer's assessment of the tradeoff between interpretability and computational cost. Many applications with interpretability as a main focus can stand the additional burden (in this case, 60 minutes maximum). It is also worth noting that this method is parallelizable, although that functionality has not been exploited in our benchmarking. \n\nBased on the reviewer's comments and other comments, we have made the following changes:\n\n- we explicitly mention the termination criteria in the experiments section and the computation times in the results\n - a discussion of the tradeoff of computational cost has been added to the discussion\n - we have added a validation loss terminal criterion (a.k.a. early stopping) to Feat to improve the runtimes a bit\n\nThanks for the helpful comments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper959/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619803, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke-JhA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper959/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper959/Authors|ICLR.cc/2019/Conference/Paper959/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers", "ICLR.cc/2019/Conference/Paper959/Authors", "ICLR.cc/2019/Conference/Paper959/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619803}}}, {"id": "rJeoSzM9h7", "original": null, "number": 3, "cdate": 1541182018890, "ddate": null, "tcdate": 1541182018890, "tmdate": 1541533545545, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "content": {"title": "A solid method for learning interpretable networks, though with a large computational cost", "review": "# Summary\nThe paper presents a method for learning network architectures for regression tasks. The focus is on learning interpretable representations of networks by enforcing a concise structure made from simple functions and logical operators. The method is evaluated on a very large number of regression tasks (99 problems) and is found to yield very competitive performance.\n\n# Quality\nThe quality of the paper is high. The method is described in detail and differences to previous work are clearly stated. Competing methods have been evaluated in a fair way with reasonable hyperparameter tuning.\n\nIt is very good to see a focus on interpretability. The proposed method is computationally heavy, as can be seen from figure 7 in the appendix, but I see the interpretability as the main benefit of the method. Since many applications, for which interpretability is key, can bear the additional computational cost, I would not consider this a major drawback. However, it would be fair to mention this point in the main paper.\n\n# Clarity\nThe paper reads well and is nicely structured. The figures and illustrations are easy to read and understand.\n\n# Originality\nThe paper builds on a large corpus of previous research, but the novelties are clearly outlined in section 3. However, the presented method is very far from my own field of research, so I find it difficult to judge exactly how novel it is.\n\n# Significance\nThe proposed method should be interesting to a wide cross-disciplinary audience and the paper is clearly solid work. The focus on interpretability fits well with the current trends in machine learning. However, the method is far from my area of expertise, so I find it difficult to judge the significance.\n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper959/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "cdate": 1542234338060, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841691, "tmdate": 1552335841691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgGi8lYnX", "original": null, "number": 1, "cdate": 1541109402488, "ddate": null, "tcdate": 1541109402488, "tmdate": 1541533545129, "tddate": null, "forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "content": {"title": "Interesting method with very promising results.", "review": "The paper proposes a method for learning regression models through evolutionary\nalgorithms that promise to be more interpretable than other models while\nachieving similar or higher performance. The authors evaluate their approach on\n99 datasets from OpenML, demonstrating very promising performance.\n\nThe authors take a very interesting approach to modeling regression problems by\nconstructing complex algebraic expressions from simple building blocks with\ngenetic programming. In particular, they aim to keep the constructed expression\nas small as possible to be able to interpret it easier. The evaluation is\nthorough and convincing, demonstrating very good results.\n\nThe presented results show that the new method beats the performance of existing\nmethods; however, as only very limited hyperparameter tuning for the other\nmethods was performed, it is unclear to what extent this will hold true in\ngeneral. As the main focus of the paper is on the increased interpretability of\nthe learned models, this is only a minor flaw though.\n\nThe interpretability of the final models is measured in terms of their size.\nWhile this is a reasonable proxy that is easy to measure, the question remains\nto what extent the models are really interpretable by humans. This is definitely\nsomething that should be explored in future work, as a small-size model does not\nnecessarily imply that humans can understand it easily, especially as the\ngenerated algebraic expressions can be complex even for small trees.\n\nThe description of the proposed method could be improved; in particular it was\nunclear to this reviewer why the features needed to be differentiable and what\nthe benefit of this was (i.e. why was this the most appropriate way of adjusting\nweights).\n\nIn summary, the paper should be accepted.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper959/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning concise representations for regression by evolving networks of trees", "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features.", "keywords": ["regression", "stochastic optimization", "evolutionary compution", "feature engineering"], "authorids": ["lacava@upenn.edu", "tilakraj@seas.upenn.edu", "surisr@seas.upenn.edu", "jhmoore@upenn.edu"], "authors": ["William La Cava", "Tilak Raj Singh", "James Taggart", "Srinivas Suri", "Jason H. Moore"], "TL;DR": "Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. ", "pdf": "/pdf/dbc8b24939c7edf937197c9847a336725d0349c1.pdf", "paperhash": "cava|learning_concise_representations_for_regression_by_evolving_networks_of_trees", "_bibtex": "@inproceedings{\ncava2018learning,\ntitle={Learning concise representations for regression by evolving networks of trees},\nauthor={William La Cava and Tilak Raj Singh and James Taggart and Srinivas Suri and Jason Moore},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke-JhA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper959/Official_Review", "cdate": 1542234338060, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke-JhA9Y7", "replyto": "Hke-JhA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper959/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841691, "tmdate": 1552335841691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper959/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}