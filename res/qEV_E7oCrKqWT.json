{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363754820000, "tcdate": 1363754820000, "number": 1, "id": "ddIxYp60xFd0m", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "qEV_E7oCrKqWT", "replyto": "qEV_E7oCrKqWT", "signatures": ["Richard Socher"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their feedback.\r\n\r\nI have not seen references to similarity learning, which can be used to say if two images are of the same class. These can obviously be used to determine if an image is of a known class or not, without having seen any image of the class. \r\n- Thanks for the reference. Would you use the images of other classes to train classification similarity learning? These would have a different distribution than the completely unseen images from the zero shot classes? In other words, what would the non-similar objects be?\r\n\r\n* I wonder if the standard deviation will not be biased (small) since it is estimated on the training samples. How important is that? \r\n- We tried fitting a general covariance matrix and it decreases performance.\r\n\r\n* I wonder if the threshold does not depend on things like the complexity of the class and the number of training examples of the class. \r\n- It might be and we notice that different thresholds should be selected via cross validation.\r\n\r\n\r\nIn general, I am not convinced that a single threshold can be used to estimate if a new image is of a new class. \r\n- Right, we found a better performance by fitting different thresholds for each class. We will include this in follow-up paper submissions.\r\n\r\n\r\nI did not understand what to do when one decides that an image is of an unknown class. How should it be labeled in that case?\r\n- Using the distances to the word vectors of the unknown classes.\r\n\r\nI did not understand why one needs to learn a separate classifier for the known classes, instead of just using the distance to the known classes in the embedding space.\r\nreply.\r\n- The discriminative classifiers have much higher accuracy than the simple distances for known classes. \r\n\r\n\r\n\r\nI do wonder why the authors claim that they 'further extend [the] theoretical analysis [of Palatucci et a.] ... and weaken their strong assumptions'. \r\n- Thanks, we will take this and the other typo out and uploaded a new version to arxiv (which should be available soon)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning Through Cross-Modal Transfer", "decision": "conferenceOral-iclr2013-workshop", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "pdf": "https://arxiv.org/abs/1301.3666", "paperhash": "socher|zeroshot_learning_through_crossmodal_transfer", "keywords": [], "conflicts": [], "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["richard@socher.org", "mganjoo@stanford.edu", "hsridhar@stanford.edu", "obastani@stanford.edu", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363754760000, "tcdate": 1363754760000, "number": 4, "id": "SSiPd5Rr9bdXm", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "qEV_E7oCrKqWT", "replyto": "qEV_E7oCrKqWT", "signatures": ["Richard Socher"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their feedback.\r\n\r\nI have not seen references to similarity learning, which can be used to say if two images are of the same class. These can obviously be used to determine if an image is of a known class or not, without having seen any image of the class. \r\n- Thanks for the reference. Would you use the images of other classes to train classification similarity learning? These would have a different distribution than the completely unseen images from the zero shot classes? In other words, what would the non-similar objects be?\r\n\r\n* I wonder if the standard deviation will not be biased (small) since it is estimated on the training samples. How important is that? \r\n- We tried fitting a general covariance matrix and it decreases performance.\r\n\r\n* I wonder if the threshold does not depend on things like the complexity of the class and the number of training examples of the class. \r\n- It might be and we notice that different thresholds should be selected via cross validation.\r\n\r\n\r\nIn general, I am not convinced that a single threshold can be used to estimate if a new image is of a new class. \r\n- Right, we found a better performance by fitting different thresholds for each class. We will include this in follow-up paper submissions.\r\n\r\n\r\nI did not understand what to do when one decides that an image is of an unknown class. How should it be labeled in that case?\r\n- Using the distances to the word vectors of the unknown classes.\r\n\r\nI did not understand why one needs to learn a separate classifier for the known classes, instead of just using the distance to the known classes in the embedding space.\r\nreply.\r\n- The discriminative classifiers have much higher accuracy than the simple distances for known classes. \r\n\r\n\r\n\r\nI do wonder why the authors claim that they 'further extend [the] theoretical analysis [of Palatucci et a.] ... and weaken their strong assumptions'. \r\n- Thanks, we will take this and the other typo out and uploaded a new version to arxiv (which should be available soon)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning Through Cross-Modal Transfer", "decision": "conferenceOral-iclr2013-workshop", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "pdf": "https://arxiv.org/abs/1301.3666", "paperhash": "socher|zeroshot_learning_through_crossmodal_transfer", "keywords": [], "conflicts": [], "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["richard@socher.org", "mganjoo@stanford.edu", "hsridhar@stanford.edu", "obastani@stanford.edu", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362080640000, "tcdate": 1362080640000, "number": 3, "id": "UgMKgxnHDugHr", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "qEV_E7oCrKqWT", "replyto": "qEV_E7oCrKqWT", "signatures": ["anonymous reviewer cfb0"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Zero-Shot Learning Through Cross-Modal Transfer", "review": "*A brief summary of the paper's contributions, in the context of prior work*\r\nThis paper introduces a zero-shot learning approach to image classification. The model first tries to detect whether an image contains an object from a so-far unseen category.  If not, the model relies on a regular, state-of-the art supervised classifier to assign the image to known classes. Otherwise, it attempts to identify what this object is, based on a comparison between the image and each unseen class, in a learned joint image/class representation space. The method relies on pre-trained word representations, extracted from unlabelled text, to represent the classes. Experiments evaluate the compromise between classification accuracy on the seen classes and the unseen classes, as a threshold for identifying an unseen class is varied. \r\n\r\n*An assessment of novelty and quality*\r\nThis paper goes beyond the current work on zero-shot learning in 2 ways. First, it shows that very good classification of certain pairs of unseen classes can be achieved based on learned (as opposed to hand designed) representations for these classes. I find this pretty impressive.\r\n\r\nThe second contribution is in a method for dealing with seen and unseen classes, based on the idea that unseen classes are outliers. I've seen little work attacking directly this issue. Unfortunately, I'm not super impressed with the results: having to drop from 80% to 70% to obtain between 15% and 30% accuracy on unseen classes (and only for certain pairs) is a bit disappointing. But it's a decent first step. Plus, the proposed model is overall fairly simple, and zero-shot learning is quite challenging, so in fact it's perhaps surprising that a simple approach doesn't do worse.\r\n\r\nFinally, I find the paper reads well and is quite clear in its methodology.\r\n\r\nI do wonder why the authors claim that they 'further extend [the] theoretical analysis [of Palatucci et a.] ... and weaken their strong assumptions'. This sentence suggests there is a theoretical contribution to this work, which I don't see. So I would remove that sentence.\r\n\r\nAlso, the second paragraph of section 6 is incomplete.\r\n\r\n*A list of pros and cons (reasons to accept/reject)*\r\nThe pros are:\r\n- attacks an important, very hard problem\r\n- goes significantly beyond the current literature on zero-shot learning\r\n- some of the results are pretty impressive\r\n\r\nThe cons are:\r\n- model is a bit simple and builds quite a bit on previous work on image classification [6] and unsupervised learning of word representation [15]    (but frankly, that's really not such a big deal)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning Through Cross-Modal Transfer", "decision": "conferenceOral-iclr2013-workshop", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "pdf": "https://arxiv.org/abs/1301.3666", "paperhash": "socher|zeroshot_learning_through_crossmodal_transfer", "keywords": [], "conflicts": [], "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["richard@socher.org", "mganjoo@stanford.edu", "hsridhar@stanford.edu", "obastani@stanford.edu", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362001800000, "tcdate": 1362001800000, "number": 2, "id": "88s34zXWw20My", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "qEV_E7oCrKqWT", "replyto": "qEV_E7oCrKqWT", "signatures": ["anonymous reviewer 310e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Zero-Shot Learning Through Cross-Modal Transfer", "review": "summary:\r\nthe paper presents a framework to learn to classify images that can come either from known\r\nor unknown classes. This is done by first mapping both images and classes into a joint embedding\r\nspace. Furthermore, the probability of an image being of an unknown class is estimated using\r\na mixture of Gaussians. Experiments on CIFAR-10 show how performance vary depending on the threshold use to\r\ndetermine if an image is of a known class or not.\r\n\r\nreview:\r\n- The idea of learning a joint embedding of images and classes is not new, but is nicely explained\r\nin the paper.\r\n- the authors relate to other works on zero-shot learning. I have not seen references to similarity learning,\r\n  which can be used to say if two images are of the same class. These can obviously be used to determine\r\n  if an image is of a known class or not, without having seen any image of the class.\r\n- The proposed approach to estimate the probability that an image is of a known class or not is based\r\n  on a mixture of Gaussians, where one Gaussian is estimated for each known class where the mean is\r\n  the embedding vector of the class and the standard deviation is estimated on the training samples of\r\n  that class. I have a few concerns with this:\r\n  * I wonder if the standard deviation will not be biased (small) since it is estimated on the training\r\n    samples. How important is that?\r\n  * I wonder if the threshold does not depend on things like the complexity of the class and the number\r\n    of training examples of the class. In general, I am not convinced that a single threshold can be used\r\n    to estimate if a new image is of a new class. I agree it might work for a small number of well\r\n    separate classes (like CIFAR-10), but I doubt it would work for problems with thousands of classes\r\n    which obviously are more interconnected to each other.\r\n- I did not understand what to do when one decides that an image is of an unknown class. How should it\r\n  be labeled in that case?\r\n- I did not understand why one needs to learn a separate classifier for the known classes, instead of\r\n  just using the distance to the known classes in the embedding space."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning Through Cross-Modal Transfer", "decision": "conferenceOral-iclr2013-workshop", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "pdf": "https://arxiv.org/abs/1301.3666", "paperhash": "socher|zeroshot_learning_through_crossmodal_transfer", "keywords": [], "conflicts": [], "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["richard@socher.org", "mganjoo@stanford.edu", "hsridhar@stanford.edu", "obastani@stanford.edu", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358436600000, "tcdate": 1358436600000, "number": 9, "id": "qEV_E7oCrKqWT", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "qEV_E7oCrKqWT", "signatures": ["richard@socher.org"], "readers": ["everyone"], "content": {"title": "Zero-Shot Learning Through Cross-Modal Transfer", "decision": "conferenceOral-iclr2013-workshop", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "pdf": "https://arxiv.org/abs/1301.3666", "paperhash": "socher|zeroshot_learning_through_crossmodal_transfer", "keywords": [], "conflicts": [], "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["richard@socher.org", "mganjoo@stanford.edu", "hsridhar@stanford.edu", "obastani@stanford.edu", "manning@stanford.edu", "ang@stanford.edu"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}