{"notes": [{"id": "Syx4wnEtvH", "original": "rJxWUpe-QS", "number": 1, "cdate": 1569438812057, "ddate": null, "tcdate": 1569438812057, "tmdate": 1583912045523, "tddate": null, "forum": "Syx4wnEtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "__jo5mffx6", "original": null, "number": 1, "cdate": 1576798684431, "ddate": null, "tcdate": 1576798684431, "tmdate": 1576800950282, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents a range of methods for over-coming the challenges of large-batch training with transformer models.  While one reviewer still questions the utility of training with such large numbers of devices, there is certainly a segment of the community that focuses on large-batch training, and the ideas in this paper will hopefully find a range of uses. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730750, "tmdate": 1576800283612, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1/-/Decision"}}}, {"id": "H1eSuSc9YS", "original": null, "number": 1, "cdate": 1571624300614, "ddate": null, "tcdate": 1571624300614, "tmdate": 1574259354149, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "\nIn this paper, the authors made a study on large-batch training for the BERT, and successfully trained a BERT model in 76 minutes. The results look quite exciting, however, after looking into the details of the paper, I would say that this is just a kind of RED AI \u2013 the results were mostly achieved by putting together a huge number of TPUs, without necessary technical innovation and fundamental contributions.\n\n1)\tThe work used 1024 TPUs to achieve 76-min training. If we compare this with the original BERT training (16 TPU for 81 hours), there is no algorithmic speed up at all (only system speedup). Not to mention that making a single BERT training faster by using more resources does not seem to be a big thing \u2013 one can do multiple BERT training experiments in parallel or in pipeline, which will correspond to similar innovation speed.\n\n2)\tThe theoretical analysis is not very impressive, and to certain degree, is not helpful. The theory just says that in certain conditions, both LARS and LAMB converge fasters than SGD. However, LAMB ha no advantage over LARS at all, which cannot well explain the experimental observations. Furthermore, when \\beta_2 > 0, the convergence rate of LAMB is even slower than LARS, which delivers some contradictory message. As we know, \\beta_2>0 is very important, otherwise the optimization algorithm will not be ADAM at all.\n\nOverall speaking, I am afraid that such work do not have sufficient theoretical or algorithmic contributions. And I doubt the true value of adding a huge number of computational resources to achieve speedup. \n\n\n**I read the author rebuttal. Thanks for the clarification on the algorithmic contribution of LAMB. However, my other concerns still remain. I have adjusted my rating by a little, but I can hardly move to the positive side.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575406395826, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1/Reviewers"], "noninvitees": [], "tcdate": 1570237758639, "tmdate": 1575406395841, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Review"}}}, {"id": "SJxDlO0mjB", "original": null, "number": 3, "cdate": 1573279726981, "ddate": null, "tcdate": 1573279726981, "tmdate": 1573279726981, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "H1eSuSc9YS", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thanks for your comments. We believe there is some confusion regarding the large batch setting in our work, which we clarify in detail below. \n\nRegarding the quote from first paragraph and comment 1: \u201cThe results were mostly achieved by putting together a huge number of TPUs, without necessary technical innovation and fundamental contributions.\u201d \u201cThere\u2019s no algorithm speed up (only system speedup).\u201d \u201cOne can do multiple BERT training experiments in parallel or in pipeline, which will correspond to similar innovation speed.\u201d\n\nWe want to re-emphasize that the central goal of our paper is to develop an optimization technique that enables very large batch training for achieving the largest possible speedup (minimize the end-to-end wall time). Large batch training is a very challenging problem (please see references e.g., Keskar et al., 2016, Goyal et al., 2017) since naive approach of large batch optimization typically leads to poor generalization even after extensive hyperparameter tuning. Thus, simply increasing the batch size with more computational resources does not translate into good performance. This can also be seen from our experiments where simply increasing the batch size for Adam or LARS does not yield good performance (see discussion in Section 4.1 and Table 2).\n\nHowever, an appropriately designed optimization technique can make better use of larger batch size and reduce end-to-end wall time without hurting the accuracy. By using carefully selected per-parameter and layerwise learning rates, the proposed LAMB technique scales training to very large batch sizes without accuracy loss, and requires less tuning over differing batch sizes, loss functions, etc, which makes scaling up training much easier. With these novel normalization strategies, LAMB has enabled training many SOTA models in a scalable manner (which were previously hard to train). It is not clear if providing the relevant works will lead to some violation of double blind policy. But we will be happy to provide the references to relevant works if this is not the case.\n\nRegarding the quote from comment 2: \u201cit is just a direct use of ADAM in the optimization process. No specific customization for BERT.\u201d\n\nLAMB uses careful per-parameter and layerwise normalization for achieving good performance with large batches. The update rule is different from Adam: the proposed layer-wise adaptive learning rate can be found in eq(3), where g_t is the SGD or Adam update. This approach is motivated by the general layerwise adaptation strategy proposed in the paper for large batch settings. LAMB is robust and, as shown in the paper, works across a wide range of applications without much tuning or customization. This should be seen in positive light since LAMB can be used out-of-the-box for various applications without much customization or tuning. In fact, LAMB has been used for training a state-of-the-art model leading to best known results on the GLUE, RACE, and SQuAD benchmarks.\n\nWe don\u2019t intend to do any customization on BERT. Our goal is to provide a generalized optimization technique which can be applied on a wide range of models and problems.\n\nRegarding the comment (3): We would like to highlight the theoretical contributions of the paper. First, we provide a general adaptation framework specially catered for large batch settings in deep learning. LARS, which achieved SOTA results for large batch training on ResNet, is a special instantiation of the framework. Based on this framework, we propose a more robust layerwise adaptation strategy, LAMB.  Second, our paper provides convergence analysis for LARS and LAMB methods in the context of large batch learning. To our knowledge, ours is the first work to study even the convergence of LARS rigorously in the context of large batch learning for deep learning. We show that LARS and LAMB (with beta2 = 0) can converge faster than SGD (see discussion above Section 4 for more details). We believe LAMB (with beta2 > 0) should behave even better. It is an interesting and important open problem to improve the analysis given that LAMB is used for various SOTA tasks recently (again, we will be happy to provide references to these works if it does not violate double blind policy) .\n\nOverall, given the necessity of large-batch optimization for modern large-scale machine learning applications, we think this paper is very useful for both industry and academia alike.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx4wnEtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1/Authors|ICLR.cc/2020/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177872, "tmdate": 1576860550313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment"}}}, {"id": "S1gvIwRQjB", "original": null, "number": 2, "cdate": 1573279566619, "ddate": null, "tcdate": 1573279566619, "tmdate": 1573279566619, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "H1ggL7XhKS", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment", "content": {"title": "Response to review #3", "comment": "We thank the reviewer for the positive feedback and for highlighting the significance of our work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx4wnEtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1/Authors|ICLR.cc/2020/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177872, "tmdate": 1576860550313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment"}}}, {"id": "B1x_XvR7oH", "original": null, "number": 1, "cdate": 1573279519543, "ddate": null, "tcdate": 1573279519543, "tmdate": 1573279519543, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "r1g_3CFatr", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for giving us constructive feedback.\n\nLAMB, unlike LARS, uses both per-parameter and layerwise normalization which enables to achieve good performance for very large batches. This approach is motivated by the general layerwise adaptation strategy proposed in the paper for large batch settings. We would like to  highlight that our paper provides the first theoretical analysis for these methods in the context of large batch learning.\n\nRegarding reviewer\u2019s comment \u201cthe authors are encouraged to report LAMB optimization results on transformer-based models such as GPT, RoBERTa, and XLNet.\u201d\n\nLAMB has been used for training a state-of-the-art model leading to best known results on the GLUE, RACE, and SQuAD benchmarks. Another recent model has been trained using our LAMB optimizer on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark.  This model can also match the performance of RoBERTa while using less than 1/4 of the total compute. LAMB has also been reported to scale the transformer model to 128 GPUs without losing accuracy. It is not clear if providing the relevant works will lead to some violation of double blind policy. But we will be happy to provide references to the relevant works if this is not the case.\n\nIn addition to BERT, we did not just show the results of MNIST and CIFAR, we also have ResNet-50/ImageNet results. To the best of our knowledge, all the previous adaptive optimizers fail to achieve state-of-the-art performance on ResNet-50/ImageNet task for large-batch training.\n\nRegarding the reviewer\u2019s questions:\n\nDoes the training take advantage of FP16 half-precision training?\nNo, because we want to keep it the same with the BERT baseline implementation. The goal is to provide a generalized optimization technique, not to customize for a specific model.\n\nHow does the training process handle overflow and NaN gradients?  \nWe used the gradient clipping technique which is the same as the BERT author\u2019s implementation.\n\nWhat is the significance of the range [\u03b1_l, \u03b1_u] in Theorem 2 and Theorem 3, and how to choose the value for them in practice?\nThese parameters ensure that the layerwise learning rates are not small or large. This is required for the convergence analysis. We chose \u03b1_l = 1e-6 and \u03b1_u was not required for all our experiments. We also observed that LAMB is quite robust to changes in these parameters."}, "signatures": ["ICLR.cc/2020/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx4wnEtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1/Authors|ICLR.cc/2020/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177872, "tmdate": 1576860550313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1/Authors", "ICLR.cc/2020/Conference/Paper1/Reviewers", "ICLR.cc/2020/Conference/Paper1/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Comment"}}}, {"id": "H1ggL7XhKS", "original": null, "number": 2, "cdate": 1571726151983, "ddate": null, "tcdate": 1571726151983, "tmdate": 1572972651160, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper developed a novel layerwise adaptation strategy, LAMB, that allows training BERT model with large mini-batches (32k vs baseline 512). This significantly speeds up the status quo in training BERT model, and effectively reduces the training time from original 3 days to only 76 minutes. In addition to demonstrating superior results across various tasks in practice, the paper also provides theoretical convergence analysis on LAMB optimizer.  \n\nOverall, this paper has made sound contributions that enables BERT-alike language to be trained with significant speedups, which is not otherwise achievable through LARS. The paper is well written and structured. I recommend acceptance. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575406395826, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1/Reviewers"], "noninvitees": [], "tcdate": 1570237758639, "tmdate": 1575406395841, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Review"}}}, {"id": "r1g_3CFatr", "original": null, "number": 3, "cdate": 1571819184294, "ddate": null, "tcdate": 1571819184294, "tmdate": 1572972651127, "tddate": null, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "invitation": "ICLR.cc/2020/Conference/Paper1/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a learning rate adaptation mechanism, called LAMB, for large-batch distributed training. The goal is to stabilize the training as the batch size increases. The idea is simple and straightforward -- there should be a layerwise learning rate adjusted by normalizing the layer weights and gradients at each layer so that layers with larger weights take larger learning steps, and vice versa. The authors perform empirical studies on BERT-large and ResNet to conclude that LAMB can scale up training batch size while still being able to converge in time with comparable accuracy.\n\nStrengths: \n+ Demonstrate the scalability of large-batch training (up to 64K) on BERT-Large with comparable accuracy. \n+ A leap from the prior work LARS that demonstrates the layer-wise learning rate adjustment scheme also works with Adam for NLP tasks.\n+ The re-warmup technique for stabilizing the second phase of mixed sequence training is neat.\n\nWeaknesses:\n- Although the authors' analysis is based on a large set of models and clearly outperforms the prior work LARS, it is still hard to assess the generality of the obtained results. The authors made an effort to show evaluation results on MNIST and CIFAR-10, but they are much less challenging tasks. \n- Technical novelty over LARS seems to be incremental, where a large portion of the work is essentially applying LARS to Adam and demonstrate its effectiveness on BERT and ResNet.\n\nOverall, the LAMB technique seems to be simple to apply yet very useful in practice for large scale training. The work can potentially help the practitioner to scale-out large model training to hundreds or even thousands of GPUs/TPUs with good scalability. Moving forward, the authors are encouraged to report LAMB optimization results on transformer-based models such as GPT, RoBERTa, and XLNet.\n\nQuestion:\nDoes the training take advantage of FP16 half-precision training?\nHow does the training process handle overflow and NaN gradients?  \nWhat is the significance of the range [\u03b1_l, \u03b1_u] in Theorem 2 and Theorem 3, and how to choose the value for them in practice?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youyang@cs.berkeley.edu", "jingli@google.com", "sashank@google.com", "jhseu@google.com", "sanjivk@google.com", "bsrinadh@google.com", "xiaodansong@google.com", "demmel@berkeley.edu", "keutzer@berkeley.edu", "chohsieh@cs.ucla.edu"], "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "authors": ["Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh"], "pdf": "/pdf/857c7f5fd13b287d2aa9ecea6712812c11f75f37.pdf", "TL;DR": "A fast optimizer for general applications and large-batch training.", "abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.", "keywords": ["large-batch optimization", "distributed training", "fast optimizer"], "paperhash": "you|large_batch_optimization_for_deep_learning_training_bert_in_76_minutes", "code": "https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "_bibtex": "@inproceedings{\nYou2020Large,\ntitle={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},\nauthor={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx4wnEtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/fba406f629ae63d9ffc7183dc68397ea2cb3955c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx4wnEtvH", "replyto": "Syx4wnEtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575406395826, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1/Reviewers"], "noninvitees": [], "tcdate": 1570237758639, "tmdate": 1575406395841, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1/-/Official_Review"}}}], "count": 8}