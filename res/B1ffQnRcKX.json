{"notes": [{"id": "B1ffQnRcKX", "original": "rJlr9aj5Y7", "number": 1340, "cdate": 1538087962408, "ddate": null, "tcdate": 1538087962408, "tmdate": 1557321630579, "tddate": null, "forum": "B1ffQnRcKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJggy6LTy4", "original": null, "number": 1, "cdate": 1544543448322, "ddate": null, "tcdate": 1544543448322, "tmdate": 1545354483159, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Meta_Review", "content": {"metareview": "\npros:\n- the paper is well-written and presents a nice framing of the composition problem\n- good comparison to prior work\n- very important research direction\n\ncons:\n- from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al]\n- as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi-lingual arithmetic case, and a pre-determined set of operations (scaling, translation, rotation, identity) in the MNIST case.\n\nAs the authors state, there are three core ideas in this paper (my paraphrase):\n\n(1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization. \n(2) treating the problem of selecting functions for composition as a sequential decision-making problem in an MDP\n(3) jointly learning the parameters of the functions and the (meta-level) composition policy.\n\nAs discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al.  However CRL offers insights and improvements over RN algorithmically in a several ways:\n\n(1) CRL uses a curriculum learning strategy.  This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems.\n(2) The focus in RN was on using the architecture to solve multi-task problems in object recognition. The solutions learned in image domains while \"compositional\" are less clearly interpretable.  In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation.\n(3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.  In this paper (CRL) recursion is given a clear, prominent role.\n\nI appreciate that the authors' engagement in the discussion period. My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.  I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg. 2 in light of the discussion on routing nets.\n\nRouting Networks, Adaptive Selection of Non-Linear Functions for Multi-task Learning, ICLR 2018", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Nice framing of the problem; architecturally somewhat incremental over routing nets"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1340/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352873424, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352873424}}}, {"id": "r1gbHlap3X", "original": null, "number": 3, "cdate": 1541423161088, "ddate": null, "tcdate": 1541423161088, "tmdate": 1544479816739, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "content": {"title": "Well-written paper; second experiment could be made stronger.", "review": "Summary: This paper is about trying to learn a function from typed input-output data so that it can generalize to test data with an input-output type that it hasn't seen during training. It should be able to use \"analogy\" (if we want to translate from French to Spanish but don't know how to do so directly, we should translate from French to English and English to Spanish). It should also be able to generalize better by learning useful \"subfunctions\" that can be composed together by an RL agent. We set up the solution as having a finite number of subfunctions, including \"HALT\" which signifies the end of computation. At each timestep an RL agent chooses a subfunction to apply to the current representation until \"HALT\" is chosen. The main idea is we parameterize these subfunctions and the RL agent as neural networks which are learned based on input -output data. RL agent is also penalized for using many subfunctions. The algorithm is called compositional recursive learner (CRL). Both analogy and meaningful subfunctions should arise purely because of this design.\n\nMultilingual arithmetic experiment. I found this experiment interesting although it would be helpful to specify that it is about mod-10 arithmetic. I was very confused for some time since the arithmetic expressions didn't seem to be evaluated correctly. It also seems that it is actually the curriculum learning that helps the most (vanilla CRL doesn't seem to perform very well) although authors do note that such curriculum learning doesn't help the RNN baseline. It also seems that CRL with curriculum doesn't outperform the RNN baseline that much on test data with the same length as training data. The difference is larger when tested on longer sequences. However here, the CRL learning curve seems to be very noisy, presumably due to the RL element. The qualitative analysis illustrates well how the subfunctions specialize to particular tasks (e.g. translation or evaluating a three symbol expression) and how the RL agent successively picks these subfunctions in order to solve the full task.\n\nImage transformations experiment. This experiment feels a bit more artificial although the data is more complicated than in the previous experiment. Also, in some of the examples in Figure 2, the algorithms seems to perform translation (action 2) twice in a row while it seems like this could be achieved by only one translation. How does this perform experimentally in comparison to an RNN (or other baseline)?\n\nI found this paper to be well-written. Perhaps it could be stronger if the \"image transformations\" experiment quantitatively compared to a baseline. I'm not an expert in this area and don't know in detail how this relates to existing work (e.g. by Rosenbaum et al; 2018).\n\nEdit: change score to 7 in light of revisions and new experiment.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "cdate": 1542234251194, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335925667, "tmdate": 1552335925667, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJl1SmLj3m", "original": null, "number": 2, "cdate": 1541264182944, "ddate": null, "tcdate": 1541264182944, "tmdate": 1544196117378, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "content": {"title": "Trying to learn composition", "review": "This is a good review paper. I am not sure how much it adds to the open question of how to learn representation with high structure. \n\nI would like to see more detail on what is communicated between the controller and the evaluator. Is it a single function selected or a probability distribution that is sent? How does the controller know how many function the evaluator has created? Or visa versa. \n\nThere is a penalty for the complexity of the program, is there a penalty for the number of functions generated? \n\nHaving just read Hudson and Manning's paper using a separate controller and action/answer generator they make strong use of attention. It is not clear if you use attention? Maybe in that you can operate on a portion of X. What role does attention play in your work?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "cdate": 1542234251194, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335925667, "tmdate": 1552335925667, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygCi7pr14", "original": null, "number": 12, "cdate": 1544045478250, "ddate": null, "tcdate": 1544045478250, "tmdate": 1544045478250, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "Byg3F7Qap7", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Quantitive Evaluation of MNIST transformations ", "comment": "Below is a quantitative evaluation of how CRL compares with a CNN baseline.\n\nThe dataset contains MNIST digits that have been scaled (S), rotated (R), and translated (T). There are two types of scaling: large and small. There are two types of rotation: left and right. There are four types of translation: left, right, up, and down. The set of depth-2 compositions (20 total) we considered are scale->translate (2*4 possible), rotate->translate (2*4 possible), scale->rotate (2*2 possible). \u201cscale->translate\u201d means that the image was first scaled, then translated. The set of depth-3 compositions we considered are scale->rotate->translate (2*2*4 possible). \n\nThe training set is 16 out of the 20 depth-2 compositions, the first hold-out set is the remaining 4 out of the 20 depth-2 compositions, and the second hold-out set is the set of depth-3 compositions. The first hold-out set tests extrapolation to a disjoint set of transformation combinations of the same depth as training; the second hold-out set tests extrapolation to a set of transformation combinations of longer depth than in training.\n\nThe CNN baseline was pre-trained to classify canonical MNIST digits, and it continued training on transformed MNIST digits.\nCRL used the same pre-trained MNIST classifier as a decoder (whose weights are frozen), and learned a set of Spatial Transformer Networks (STN) constrained to rotate, scale, or translate.\nWe noticed instability in training the STNs to model drastic translations (where the digit was translated more than 15% the width of the images). A potential reason for this is that because the weights of CRL\u2019s decoder (pre-trained MNIST classifier) are frozen, the classifier acts as a more complex loss functions for the upstream STNs. We addressed this challenge by defining a curriculum for the translated data, where initially the digit was translated by a small amount, and at the end of the curriculum, the digit is translated to the far edge of the image. We applied this curriculum to both CRL and the baseline.\n\nThe results are as follows (over 5 random seeds):\n\nTraining set accuracy:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nCNN\nmedian: 0.98\n10% quantile: 0.98\n90% quantile: 0.98\nCRL\nmedian: 0.89\n10% quantile: 0.87\n90% quantile: 0.90\n\nHold-out set (same depth)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nCNN\nmedian: 0.22\n10% quantile: 0.19\n90% quantile: 0.23\nCRL\nmedian: 0.67\n10% quantile: 0.59\n90% quantile: 0.71\n\nHold-out set (longer depth)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nCNN\nmedian: 0.26\n10% quantile: 0.26\n90% quantile: 0.27\nCRL\nmedian: 0.69\n10% quantile: 0.60\n90% quantile: 0.71\n\nWe notice that CRL performs a bit worse on the training set because it is constrained to go through the bottleneck of only using Spatial Transformation Networks, whereas the CNN is free to fit the training set without such constraints. In the hold-out sets, it is clear that the CNN overfits to the training set and is unable to classify MNIST digits that have been transformed by a set of transformation combinations it has not seen before. CRL, on the other hand, generalizes significantly better because it re-uses the primitive spatial transformations it had learned during training to re-represent the image into a canonical MNIST digit."}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "BJeiNE7TaQ", "original": null, "number": 10, "cdate": 1542431794992, "ddate": null, "tcdate": 1542431794992, "tmdate": 1542431794992, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "Hkx25xk6hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Comparison with Routing Networks", "comment": "Based on OP\u2019s suggestions, we have included a paragraph in Section 3.4 (\u201cDiscussion of Design Choices\u201d) that features a discussion that compares CRL with Routing Networks.\n\nTo avoid misrepresenting Routing Networks, we have revised the wording of the experiment of Appendix D.2 to compare with a mixture-of-expert- inspired baseline, rather than Routing Networks, because as OP points out, 1) RN does not necessarily have a separate controller per time step and 2) RN does not necessarily use a different set of functions per computation step. The purpose of this experiment is to show the benefits of reusing modules across computation steps and to show the benefit of allowing a flexible computation horizon.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "H1ei6m7TaX", "original": null, "number": 9, "cdate": 1542431682960, "ddate": null, "tcdate": 1542431682960, "tmdate": 1542431682960, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1eifeX9n7", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank Reviewer 2 for their constructive review, which helped us improve the paper in the following aspects. We would be happy to incorporate any other suggestions Reviewer 2 may have for the paper.\n\n1. We have revised Section 3.1 and the introductory paragraph of Section 3 to be more precise about the domain-specific assumptions CRL makes about the problem distribution. In particular, we included a discussion about restricting the representational vocabulary and the functional form of the modules as a way to incorporate as an inductive bias domain-specific knowledge of the problem distribution. \n\n2. We agree with Reviewer 2 that the \u201crecursive\u201d/\u201dtranslational\u201d terminology should be clearer. Therefore, we have revised the \u201cProblems\u201d and \u201cThe goal\u201d paragraphs in Section 2 to remove the discussion on translational problems and only focus on recursive problems, where the input and output representations are drawn from the same vocabulary.\n\n3. Further, we agree with and appreciate Reviewer 2\u2019s analysis that our paper is only a first step towards the full general problem of discovering subproblem decomposition. Accordingly we have revised the end of Section 6 (Discussion) to acknowledge this. We also revised \u201cThe challenge\u201d paragraph in Section 2 to be more precise that we are not solving the general subproblem decomposition problem, but rather solving the problem of learning to compose partial solutions to subproblems when the general form of the subproblem decomposition of a task distribution is known.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "Byg3F7Qap7", "original": null, "number": 8, "cdate": 1542431620461, "ddate": null, "tcdate": 1542431620461, "tmdate": 1542431620461, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "r1gbHlap3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank Reviewer 3 for their constructive review, which helped us improve the paper in various aspects. We would be happy to incorporate any other suggestions Reviewer 3 may have for the paper. We would like to make the following clarifications:\n\n1. We have clarified in Section 4.1 that arithmetic problems are modulo-10.\n\n2. With regards to how CRL compares to the RNN on test data with the same length as the training data, Figure 2b shows that there is a substantial difference between CRL (red curve) and RNN (purple curve). It is only with 10x more data does the RNN (yellow curve) reach comparable performance with CRL.\n\n3. Reviewer 3 noted that in the right half of Figure 4, the top-two examples showed that CRL performs transformation twice, when in fact this can be achieved by only translation. This is true. For simplicity, we had fixed the number of transformations to two transformations. That CRL finds alternate ways of achieving the same end representation (using two translations instead of one) illustrates a core feature of the CRL framework: that it is possible to solve a problem (e.g. a large translation) by composing together partial solutions (two small translation).\n\n4. We will have the baseline experiments Reviewer 3 requested in time for the final, and will endeavor to add these in to the paper during the discussion period.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "B1eifeX9n7", "original": null, "number": 1, "cdate": 1541185554949, "ddate": null, "tcdate": 1541185554949, "tmdate": 1541533215500, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "content": {"title": "Interesting approach to compositionally", "review": "==== Summary ====\n\nThis paper proposes a model for learning problems that exhibit compositional and recursive structure, called Compositional Recursive Learner (CRL). The paper approaches the subject by first defining a problem as a transformation of an input representation x from a source domain t_x to a target domain t_y. If t_x = t_y then it is called a recursive problem, and otherwise a translational problem. A composite problem is the composition of such transformations. The key observation of the paper is that many real-world problems can be solved iteratively by either recursively transforming an instance of a problem to a simpler instance, or by translating it to a similar problem which we already know how to solve (e.g., translating a sentence from English to French through Spanish). The CRL model is essentially composed of two parts, a set of differential functions and a controller (policy) for selecting functions. At each step i, the controller observes the last intermediate computation x_i and the target domain t_y, and then selects a function and the subset of x_i to operate on. For each instance, the resulting compositional function is trained via back-propagation, and the controller is trained via policy gradient. Finally, the paper presents experiments on two synthetic datasets, translating an arithmetic expression written in one language to its outcome written in another language, and classifying MNIST digits that were distorted by an unknown random sequence of affine transformations. CRL is compared to RNN on the arithmetic task and shown to be able to generalize both to longer sequences and to unseen language pairs when trained on few examples, while RNN can achieve similar performance only using many more examples. On MNIST, it is qualitatively shown that CRL can usually (but not always) find the sequence of transformations to restore the digit to its canonical form.\n\n==== Detailed Review ====\n\nI generally like this article, as it contains a neat solution to a common problem that builds on and extends prior work. Specifically, the proposed CRL model is a natural evolution of previous attempts at solving problems via compositionally, e.g. Neural Programmer [1] that learns a policy for composing predefined commands, and Neural Module Networks [2] that learns the parameters of shared differential modules connected via deterministically defined structure (found via simple parse tree). The paper contains a careful review of the related works and highlights the similarities and differences from prior approaches. Though the experiments are mostly synthetic, the underlying method seems to be readily applicable to many real-world problems.\n\nHowever, the true contributions of the paper are somewhat muddied by presenting CRL as more general than what is actually supported by the experiments. More specifically, the paper presents CRL as a general method for learning compositional problems by decomposing them into simpler sub-problems that are automatically discovered, but in practice, a far more limited version of CRL is used in the experiments, and the suggested translational capabilities of CRL, which are important for abstract sub-problem discovery, are not properly validated:\n\n1. In both experiments, the building-block functions are hand-crafted to fit to the prior knowledge on the compositionally of the problem. For the arithmetic task, the functions are limited to operate each step just on a single window of encompassing 3 symbols (e.g., <number> <op> <number>,  <op> <number> <op>) and return a distribution over the possible symbols, which heavily forces the functions to represent simple evaluators for simple expressions of the form <number> <op> <number>. For the distorted MNIST task, the functions are limited to neural networks which choose the parameter of predetermined transformations (scaling, translation, or rotation) of the input. In both cases, CRL did not *found* sub-problems for reducing the complexity of the original instance but just had to *fine tune* loosely predefined sub-problems. Incorporating expert knowledge into the model like so is actually an elegant and useful trick for solving real problems, and it should be emphasized far clearly in the article. The story of \u201cdiscovering subproblems\u201d should be left for the discussion / future research section, because though it might be a small step towards that goal, it is not quite there yet.\n2. The experiments very neatly show how recursive transformations offer a nice framework for simplifying an instance of a problem. However, the translation capabilities of the model are barely tested by the presented experiments, and it can be argued that all transformations used by the model are recursive in both experiments. First, only the arithmetic task has a translation aspect to it, i.e., the task is to read an expression in one language and then output the answer in a different language. Second, this problem is only weakly related to translation because it is possible to translate the symbols independently, word by word, as opposed to written language that has complex dependencies between words. Third, the authors report that in practice proper translation was only used in the very last operation for translating the computed value of the input expression to the requested language, and not as a method to translate one instance that we cannot solve into another that we can. Finally, all functions operate and return on all symbols and not ones limited to a specific language, and so by the paper\u2019s own definition, these are all recursive problems and not translational ones.\n\nIn conclusion, I believe this paper should be accepted even with the above issues, mostly because the core method is novel, clearly explained, and appears to be very useful in practice. Nevertheless, I strongly suggest to the authors to revise their article to focus on the core qualities of their method that can be backed by their current experiments, and correctly frame the discussion on possible future capabilities as such.\n\n[1] Reed et al. Neural Programmer-Interpreters. ICLR 2016.\n[2] Andreas et al. Neural Module Networks. CVPR 2016.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Review", "cdate": 1542234251194, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335925667, "tmdate": 1552335925667, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxemWyanX", "original": null, "number": 4, "cdate": 1541366040057, "ddate": null, "tcdate": 1541366040057, "tmdate": 1541366040057, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "SJx54U9i27", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Response to AreaChair1", "comment": "We apologize for the delay. We have now posted a detailed response."}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "r1x-kZyT2X", "original": null, "number": 3, "cdate": 1541365977065, "ddate": null, "tcdate": 1541365977065, "tmdate": 1541365977065, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "SJg_qTBs3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Response to \"Update to concerns above\"", "comment": "Question 1\n\nAlthough we have acknowledged the similarities in \"Response to Relation of the Compositional Recursive Learner to Routing Networks\", we respectfully disagree with OP that \u201cCRL is in effect a Routing Network.\u201d To make such a statement would be to mischaracterize the difference between the generative nature of CRL and the routing-based nature of RN and to ignore the respective problem domains that CRL and RN tackles.\n\nWe focus on the extrapolation problem (Sec 2 and 3), for which learning on multiple tasks is a means to this end, whereas Rosenbaum et al. focus on task interference, for which multi-task learning is the end itself (see abstract of\u00a0Rosenbaum et al.). Because our focus is on subproblem decomposition, CRL restricts the representation space such that harder problems can be expressed in the same vocabulary as easier problems. RN do not focus on subproblem decomposition, so it is not clear whether their modules learn any interpretable atomic functionality or whether their representations capture semantic boundaries between subproblems that comprise a larger problem. Therefore, RN does not have the inductive bias for extrapolation problems that require the learner to re-represent the new problem in terms of problems the learner has seen during training.\n\nThe key methodological difference between CRL and RN lies in the generative nature of CRL and the routing-based nature of RN. RN and other work such as PathNet (Fernando et al. 2017) route input-dependent paths through a large fixed architecture. In contrast, the extrapolation problem necessitates CRL be generative, meaning that it incrementally builds module on top of module without a fixed computational horizon. This is necessary for the problem domain we consider, in which we want to train and extrapolate to different problems that require various computation depths. Therefore, variable-length computation horizon, the restrictions on the representational vocabulary, and the emergent semantic functionality of its submodules as solutions to subproblems within a larger problem (see Figure 3) are crucial design considerations for the capability of CRL that RN does not incorporate in their approach.\n\nQuestion 2\n\nBased on the crucial difference between the generative nature of CRL and the routing-based nature of RN, the variable computation horizon is a crucial feature of CRL, not a minor difference, as we discussed above and in the Related Work. Because of the variable computation horizon, it is not possible to have a separate controller at each timestep/depth because the number of time steps of computation unknown; therefore this is also not a minor difference.\u00a0\n\nWe agree with OP that the particular RL algorithm (PPO vs MARL-WPL) is not particularly relevant to the central focus of our paper, which is extrapolation in compositionally structured problems, and we indeed did not claim so. Nevertheless, our work represents an algorithmic improvement that does make the single controller architecture more effective (above 90% extrapolation accuracy for multilingual arithmetic) than Rosenbaum et al.\u2019s architecture (Figure 4 and Figure 5 of Rosenbaum et al. shows < 50% accuracy, whereas their best method achieves around 60%).\n\nCRL\u2019s focus on capturing interpretable atomic functionality in its modules and using representations capture semantic boundaries between subproblems that comprise a larger problem are important ingredients for CRL\u2019s analogical reasoning: literally re-representing a problem in terms of problems it has already seen. This is another key difference between RN and CRL, because the architectural design of RN do not have the inductive bias (restrictions on the modules and representations) that encourage it re-represent problems in literally terms of previously-seen problems.\n\nQuestion 3: Novelty\n\nThe novelty of our work (with respect to RN) lies in the generative nature of CRL because we reframe of the extrapolation problem as a problem of learning algorithmic procedures over transformations between representations, as discussed in the abstract, intro, and discussion. CRL generates function composition, in contrast to how RN routes through function paths. As shown in the experiments section, the transformations CRL learns have interpretable, atomic functionality and the representations capture semantic boundaries between subproblems that comprise a larger problem. These features of the CRL architecture crucially differentiate it from other routing-based architectures, including RN and PathNet."}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "Hkx25xk6hQ", "original": null, "number": 2, "cdate": 1541365908094, "ddate": null, "tcdate": 1541365908094, "tmdate": 1541365908094, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "HJlzO5a1nm", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "content": {"title": "Response to \"Relation of the Compositional Recursive Learner to Routing Networks\"", "comment": "We are grateful to the Anonymous Commenter (OP) for their detailed and insightful comment.\n\nIt is true, as OP points out, that there is a close connection to Routing Networks (RN), an important and interesting paper that seeks to mitigate task interference in multi-task learning by routing through the modules of a convolutional neural network. Like RN, a feature of our work is that the learner creates and executes a different computation graph for different inputs, where this computation graph consists of a series of functions applied according to a controller. Therefore, it is possible to see CRL as taking a step beyond the single-controller (which they refer to as \u201csingle-agent\u201d in Rosenbaum et al.) version of RN by incorporating several algorithmic improvements that make the single controller version not only effective for solving the task (c.f. Figure 4 and Figure 5 of Rosenbaum et al.) but also effective for extrapolation, a problem domain that Rosenbaum et al. does not consider.\u00a0\n\nWe will follow OP\u2019s recommendation and make the comparison with RN more salient in the experiments and related work section. However, we would like to emphasize that the problem that RN tackles (mitigating task interference in multi-task learning) is not the central focus of the paper. That CRL and RN started from significantly different motivations and problem domains but converged to a similar architecture design serves as encouraging evidence in support of an old idea that exploiting modularity and encapsulation yield help more efficiently capture the modalities of a task distribution, and we are excited that both we and Rosenbaum et al. are actively pushing this front.\n\nWe thank OP for pointing out it is indeed true that 1) RN does not necessarily have a separate controller per time step and 2) RN does not necessarily use a different set of functions per computation step; we will follow OP\u2019s recommendation and clarify this in the next version of the paper to avoid potential misunderstanding. One source for our misunderstanding is that the exposition of RN in section 3 of Rosenbaum et al. (e.g. \u201cIf the number of function blocks differs from layer to layer in the original network, then the router may accommodate this by, for example, maintaining a separate decision function for each depth\u201d (page 4, Rosenbaum et al.) and \u201cThe approximator representation can consist of either one MLP that is passed the depth (represented in 1-hot), or a vector of d MLPs, one for each decision/depth\u201d (page 5, Rosenbaum et al.)) seems to heavily suggest the two assumptions we made on page 15 of our manuscript, so we thought that the single-controller or shared function cases were included in Rosenbaum et al. mostly for the sake of comparison. The reason that our submission discussed points (1) and (2) was not intended to misrepresent RN. Rather it was because we interpreted Figure 4, Figure 5, Table 3, Table 4 of Rosenbaum et al. as claiming the routing-all-fc (one-agent-per-task, separate controller per depth, different functions-per-layer) as the flag bearer of their results. To make the comparison that most fairly represents RN\u2019s claims, we had conducted our comparison based on the best version of RN reported in Rosenbaum et al. (routing-all-fc), which uses a separate controller per depth and a different set of functions per depth (according to Table 3 and 4 in Rosenbaum et al.)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1340/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618623, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ffQnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1340/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1340/Authors|ICLR.cc/2019/Conference/Paper1340/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618623}}}, {"id": "SJg_qTBs3X", "original": null, "number": 2, "cdate": 1541262735839, "ddate": null, "tcdate": 1541262735839, "tmdate": 1541262735839, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "HJlzO5a1nm", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Public_Comment", "content": {"comment": "Now that the review period is officially over, I was hoping to get a response to the issues raised above. I ask the authors to address the following questions in particular:\n1. Do the authors agree with the assessment that the CRL is in effect a Routing Network? (I might point out that the authors even hint at that in the arxiv version of this paper)\n2. Do the authors agree that the only two minor differences (apart from the training schedule) are (1) that the CRL has infinite horizon recurrence, while RNs only have limited horizon recurrence, and (2) the RL algorithm chosen? (this implies a mischaracterization of RNs on the authors part)\u00a0\n3. In light of the previous two points, why do the authors claim that their architecture is novel? (this critique does not extend to the other parts of their paper)\u00a0", "title": "Update to concerns above"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311620655, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1ffQnRcKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311620655}}}, {"id": "HJlzO5a1nm", "original": null, "number": 1, "cdate": 1540508265541, "ddate": null, "tcdate": 1540508265541, "tmdate": 1540508445532, "tddate": null, "forum": "B1ffQnRcKX", "replyto": "B1ffQnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1340/Public_Comment", "content": {"comment": "I have read the paper \"Automatically Composing Representation Transformations as a Means for Generalization\" with great pleasure. I particularly enjoyed how the paper tries to link compositionality to analogical reasoning. I think an architecture for compositional reasoning that can solve even complex tasks elegantly is of great value.  \nI do though have some concerns about the relationship between the \"Compositional Recursive Learner\" (CRL) and \"Routing Networks\" (RN).  Specifically, it seems to me that the CRL is an example of a single agent recursive routing network, as described in (Rosenbaum et al, ICLR 2018). In particular, the design of a compositional computation and learning framework that combines trainable function blocks with a reinforcement learning meta learner (as described in section 3.2 and 3.3) is highly similar (section 3.2) or nearly identical (section 3.3) to the formulation in the routing networks paper.\nThe main difference is that while (Rosenbaum et al) focused on a limited-horizon recurrence (see pages 1, 3, 4, 7, and particularly 14 in the appendix), CRL uses an infinite-horizon recurrence.\nSurprisingly, this relationship is not discussed in the paper in any detail. Routing Networks are more closely examined in the appendix only. Additionally, there are two stated assumptions (on p. 15) on routing networks that I do not think are true: (1) Routing Networks necessarily have a separate controller per computation step and (2) Routing Networks necessarily use a different set of functions per computation step.  The idea of an RN with a single controller applied across computation steps is discussed on page 5 of (Rosenbaum et al).  The idea of re-using function blocks across computation steps is discussed on pages 1, 3, 4, 7 and 14.\n\nGiven the obviously close relationship between these two works, I feel that the connection should be more emphasized and the comparison more central to the paper. And indeed, the results shown for routing networks are somewhat hard to believe (at least for smaller problems as routing networks are not expected to scale to inputs of the same size). Is the routing networks implementation compared to actually also recurrent? Does the routing network receive the same curriculum learning strategy training?\n\nThe link to Rosenbaum et al in ICLR 2018: https://openreview.net/forum?id=ry8dvM-R-", "title": "Relation of the Compositional Recursive Learner to Routing Networks"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1340/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatically Composing Representation Transformations as a Means for Generalization", "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage  the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.", "keywords": ["compositionality", "deep learning", "metareasoning"], "authorids": ["mbchang@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu", "tom_griffiths@berkeley.edu"], "authors": ["Michael Chang", "Abhishek Gupta", "Sergey Levine", "Thomas L. Griffiths"], "TL;DR": "We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.", "pdf": "/pdf/086663869ea0366468eec4c42a0c6eec539e154c.pdf", "paperhash": "chang|automatically_composing_representation_transformations_as_a_means_for_generalization", "_bibtex": "@inproceedings{\nchang2018automatically,\ntitle={Automatically Composing Representation Transformations as a Means for Generalization},\nauthor={Michael Chang and Abhishek Gupta and Sergey Levine and Thomas L. Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ffQnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1340/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311620655, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1ffQnRcKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1340/Authors", "ICLR.cc/2019/Conference/Paper1340/Reviewers", "ICLR.cc/2019/Conference/Paper1340/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311620655}}}], "count": 14}