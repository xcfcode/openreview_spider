{"notes": [{"id": "6IVdytR2W90", "original": "nlM78ta_uom", "number": 2760, "cdate": 1601308306069, "ddate": null, "tcdate": 1601308306069, "tmdate": 1614985642351, "tddate": null, "forum": "6IVdytR2W90", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "vFmDH4d6zOL", "original": null, "number": 1, "cdate": 1610040519354, "ddate": null, "tcdate": 1610040519354, "tmdate": 1610474127890, "tddate": null, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This submission proposes an approach for fusing representations at multiple scales to improve object detection systems. Reviewers thought the paper was well-written and showed positive results on COCO, a common object detection benchmark. However, reviewers agreed that there was not sufficient methodological novelty or empirical improvement over existing approaches to warrant acceptance at ICLR: several prior works have addressed multiscale fusion and reviewers did not find the evaluation/ablations sufficient to demonstrate the approach yielded substantial improvements over these existing approaches. I hope the authors will consider resubmitting the paper after refining it based on the reviewers' feedback."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040519341, "tmdate": 1610474127875, "id": "ICLR.cc/2021/Conference/Paper2760/-/Decision"}}}, {"id": "bYfKpqYXuRD", "original": null, "number": 2, "cdate": 1603104060204, "ddate": null, "tcdate": 1603104060204, "tmdate": 1606483227427, "tddate": null, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review", "content": {"title": "Strong results, limited technical contribution & analysis", "review": "In this paper, the authors study the problem of scale-friendly feature fusion for object detection. Specifically, the authors propose to process features at each layer of a feature pyramid network at multiple scales and fuse them back into a single scale. To be specific, they resize features at a layer into multiple scales, process these rescaled features independently, rescale them back into the original scale and combine them with the original features.\n\nStrengths:\n- Scale is an important problem in object detection and the paper addresses an important issue.\n- Strong results showing significant improvements, around ~2AP, over baselines, including strong detectors like RepPoints.\n- Overall, the paper is very well written. I didn't find any typos or grammatical errors, which is very rare for a thorough reviewer like me. \n\nWeaknesses:\n- The novelty is limited. Multi-scale processing at a layer has been extensively studied with ResNext-type and inception-type architectures. The paper just takes such ideas and use them with FPN without any technical or theoretical insights or contributions.\n\nIn other words, this appears to be just FPN with inception module. It would have been nicer for the authors to motivate how/whether the novelty goes beyond this.\n\n- The paper does not make a comparison with multi-scale backbone networks such as ResNext. It has been shown that such multi-scale architectures improve the detection performance compared to ResNet type architectures, and this comparison is very crucial for the reader to grasp the significance of the contribution, if any.\n\n- The paper does not make a comparison with respect to methods trying to address limitations of FPN. There are many papers that extend FPN to address scaling issues. It is very crucial for the reader to see how the proposed solution performs in comparison with those methods.\n\n- It would have been nicer to see in Table 4 the details on the type of the backbone used. This might be a very crucial factor in analysing the differences in the gap among the different models.\n\n***AFTER AUTHOR RESPONSE***\n\nI have read the comments of the other reviewers, which revealed that all reviewers identified the same major issues with the paper (novelty and evaluation). The authors did not provide a rebuttal but kindly thanked the reviewers and stated their intention for improving the paper with the reviewer comments and submitting it for a future venue. Therefore, I changed my overall rating to rejection.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089196, "tmdate": 1606915805549, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2760/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review"}}}, {"id": "DsPfTyOB54_", "original": null, "number": 6, "cdate": 1606228172520, "ddate": null, "tcdate": 1606228172520, "tmdate": 1606228172520, "tddate": null, "forum": "6IVdytR2W90", "replyto": "yHGFg6cjMt8", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment", "content": {"title": "Thanks for your advice!", "comment": "Thank you very much for your advice. We will keep working on it based on your comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6IVdytR2W90", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2760/Authors|ICLR.cc/2021/Conference/Paper2760/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844755, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment"}}}, {"id": "4fDxYerty5e", "original": null, "number": 5, "cdate": 1606228015797, "ddate": null, "tcdate": 1606228015797, "tmdate": 1606228015797, "tddate": null, "forum": "6IVdytR2W90", "replyto": "bYfKpqYXuRD", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment", "content": {"title": "Thanks for your advice!", "comment": "Thank you very much for your advice. We will keep working on it based on your comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6IVdytR2W90", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2760/Authors|ICLR.cc/2021/Conference/Paper2760/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844755, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment"}}}, {"id": "veRSu22GcL", "original": null, "number": 4, "cdate": 1606227940998, "ddate": null, "tcdate": 1606227940998, "tmdate": 1606227940998, "tddate": null, "forum": "6IVdytR2W90", "replyto": "n2YSiKuodII", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment", "content": {"title": "Thanks for your advice!", "comment": "Thank you very much for your advice. We will keep working on it based on your comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6IVdytR2W90", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2760/Authors|ICLR.cc/2021/Conference/Paper2760/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844755, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment"}}}, {"id": "B6qoZ23WB1s", "original": null, "number": 3, "cdate": 1606227827780, "ddate": null, "tcdate": 1606227827780, "tmdate": 1606227827780, "tddate": null, "forum": "6IVdytR2W90", "replyto": "csoADX-1jKc", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment", "content": {"title": "Thanks for your advice!", "comment": "Thank you very much for your advice. We will keep working on it based on your comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6IVdytR2W90", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2760/Authors|ICLR.cc/2021/Conference/Paper2760/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844755, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Comment"}}}, {"id": "yHGFg6cjMt8", "original": null, "number": 1, "cdate": 1602915174349, "ddate": null, "tcdate": 1602915174349, "tmdate": 1605024137934, "tddate": null, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review", "content": {"title": "Lack of novelty is the main problem", "review": "This paper proposes to obtain multi-scale features by `resize -> convolution -> resize (inverse)\u2019. Extensive experimental results on COCO validate the effectiveness of the proposed approach. \n\nPros:\n\nExperimental results on the widely used benchmarking dataset.\nComparison with recent approaches.\nPaper is easy to understand, simply because the proposed method is simple.\n\nCons:\nLack of novelty. The use of multi-scale features is not new [a, b]. The formulation in this paper is not very different from that in [b]. It has been used for lots of applications. In object detection, dilated conv in TridentNet is not the only approach using multiple branches. Approaches like [c, d] also used multi-scale-multi-layer features by resizing. \n\n[a] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Spatial pyramid pooling in deep convolutional networks for visual recognition.\" IEEE transactions on pattern analysis and machine intelligence 37, no. 9 (2015): 1904-1916.\n\n[b] Yang, Wei, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. \"Learning feature pyramids for human pose estimation.\" In proceedings of the IEEE international conference on computer vision, pp. 1281-1290. 2017.\n\n[c] Gidaris, Spyros, and Nikos Komodakis. \"Object detection via a multi-region and semantic segmentation-aware cnn model.\" In Proceedings of the IEEE international conference on computer vision, pp. 1134-1142. 2015.\n\n[d] Zeng, Xingyu, Wanli Ouyang, Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu et al. \"Crafting gbd-net for object detection.\" IEEE transactions on pattern analysis and machine intelligence 40, no. 9 (2017): 2109-2123.\n\n\nThe ablation study in the experimental results did not compare with existing works, like TridentNet, and [c, d] to justify why another multi-scale approach is needed.\n\nSymbols are not illustrated well (Authors need not answer this in the rebuttal but need to revise in the revised version).\n`S() is the squeeze module: What is the meaning of squeeze module?\nThere are no common definitions on `makes the input x thinner\u2019, `combination function\u2019, `unsqueeze module\u2019.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089196, "tmdate": 1606915805549, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2760/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review"}}}, {"id": "n2YSiKuodII", "original": null, "number": 3, "cdate": 1603940880228, "ddate": null, "tcdate": 1603940880228, "tmdate": 1605024137852, "tddate": null, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review", "content": {"title": "Solid work on experiments but need to improve on writing", "review": "This paper proposes a new general feature fusion operation, Multi-Scale Fusion Module (MSFM). By adding MSFM layers between feature extraction layers, it is observed that the detection result is improved with minor added parameters.\n\nPros:\nGood to see new work to explore various of ways to perform feature fusion.\n\nCons:\nMajor comments:\n(1) The quality and clarify of the paper needs to be improved, for example, Table 3 has shifted horizontal line. \n(2) Ablation studies clarifies on the effects of the change of configurations, but does provide much evidence on why MSFM module helps with the detection task.\n\nSome minor comments:\n(1) Figure1a. could be further clarified by adding the notations mentioned in the equations to to the figure. \n(2) Good to report the variances/confidence-intervals of the metrics as well.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089196, "tmdate": 1606915805549, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2760/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review"}}}, {"id": "csoADX-1jKc", "original": null, "number": 4, "cdate": 1604232790018, "ddate": null, "tcdate": 1604232790018, "tmdate": 1605024137717, "tddate": null, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "invitation": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review", "content": {"title": "The multi-scale feature fusion block does not have enough novelty and is computationally expensive", "review": "\nThe paper proposes a multi-scale feature fusion block and inserts the block into ResNet backbones for object detection. It is very similar to the inception block in IneceptionNets. The only difference is the proposed feature fusion contains feature map upsampling and downsampling (resize and resize^{-1}) for different branches. The paper has some merits as follows.\n\n1. The method has evaluated on different object detection frameworks, such as Faster R-CNN, Cascade R-CNN, Grid R-CNN, Dynamic R-CNN, RepPoints, etc.\n2. The method obvious performance gains on different frameworks with a few additional parameters.\n\nHowever, the flaws are obvious as follows.\n1. The novelty is very limited. Inception block is widely used in deep learning. The paper is only a small modification over Inception. The novelty is far below the bar of ICLR.\n2. The proposed feature fusion block is very computation expensive. The upsampling procedure makes the computation cost of the 3$\\times$3 convolutions very high. The paper only reports additional parameters but does not report the additional computation cost (e.g., additional FLOPs and running time).\n3. Related methods are not compared. At least, inserting inception blocks into ResNets can be compared.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2760/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2760/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MSFM: Multi-Scale Fusion Module for Object Detection", "authorids": ["~Xuesong_Wang2", "~Caisheng_Wang1"], "authors": ["Xuesong Wang", "Caisheng Wang"], "keywords": ["Feature Fusion", "Object Detection", "Multi-Scale"], "abstract": "Feature fusion is beneficial to object detection tasks in two folds. On one hand, detail and position information can be combined with semantic information when high and low-resolution features from shallow and deep layers are fused. On the other hand, objects can be detected in different scales, which improves the robustness of the framework. In this work, we present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer. Specifically, the input of the module will be resized into different scales on which position and semantic information will be processed, and then they will be rescaled back and combined with the module input. The MSFM is lightweight and can be used as a drop-in layer to many existing object detection frameworks. Experiments show that MSFM can bring +2.5% mAP improvement with only 2.4M extra parameters on Faster R-CNN with ResNet-50 FPN backbone on COCO Object Detection minival set, outperforming that with ResNet-101 FPN backbone without the module which obtains +2.0% mAP with 19.0M extra parameters. The best resulting model achieves a 45.7% mAP on test-dev set. Code will be available.", "one-sentence_summary": "We present a Multi-Scale Fusion Module (MSFM) that extracts both detail and semantical information from a single input but at different scales within the same layer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|msfm_multiscale_fusion_module_for_object_detection", "pdf": "/pdf/bce8dd10364cb6d95d0b4d145aa84aef44f09352.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IblXk1C75Q", "_bibtex": "@misc{\nwang2021msfm,\ntitle={{\\{}MSFM{\\}}: Multi-Scale Fusion Module for Object Detection},\nauthor={Xuesong Wang and Caisheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=6IVdytR2W90}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6IVdytR2W90", "replyto": "6IVdytR2W90", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089196, "tmdate": 1606915805549, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2760/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2760/-/Official_Review"}}}], "count": 10}