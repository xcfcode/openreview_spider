{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396699384, "tcdate": 1486396699384, "number": 1, "id": "B1m7TzLdg", "invitation": "ICLR.cc/2017/conference/-/paper599/acceptance", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396699929, "id": "ICLR.cc/2017/conference/-/paper599/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396699929}}}, {"tddate": null, "tmdate": 1481939219385, "tcdate": 1481939172776, "number": 3, "id": "Sy6yFzzEe", "invitation": "ICLR.cc/2017/conference/-/paper599/official/review", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer4"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.\n\nThe experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528130, "id": "ICLR.cc/2017/conference/-/paper599/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528130}}}, {"tddate": null, "tmdate": 1481914261204, "tcdate": 1481914261204, "number": 2, "id": "Bya5vnbVg", "invitation": "ICLR.cc/2017/conference/-/paper599/official/review", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer5"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.\n\nThe topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528130, "id": "ICLR.cc/2017/conference/-/paper599/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528130}}}, {"tddate": null, "tmdate": 1481890035186, "tcdate": 1481890035186, "number": 1, "id": "ryolYUbVe", "invitation": "ICLR.cc/2017/conference/-/paper599/official/review", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer3"], "content": {"title": "Interesting questions but very limited experiments", "rating": "3: Clear rejection", "review": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528130, "id": "ICLR.cc/2017/conference/-/paper599/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528130}}}, {"tddate": null, "tmdate": 1481584362398, "tcdate": 1481584362393, "number": 3, "id": "B1zgkh2mg", "invitation": "ICLR.cc/2017/conference/-/paper599/public/comment", "forum": "HJrDIpiee", "replyto": "Hkh8mMJXx", "signatures": ["~Jean_Harb1"], "readers": ["everyone"], "writers": ["~Jean_Harb1"], "content": {"title": "Parameter details", "comment": "1. We didn't tune the hyper-parameters, as such a search is quite expensive and would overfit to the specific games we trained on. As for the difference in parameters, RMSprop was kept consistent with DQN parameters, while the Adam parameters are simply the suggested parameters from the original paper.\n\n2. Indeed, we truncate the traces as in Watkins Q(\\lambda).\n\n3. We haven't tried comparing lambda-returns with n-step returns. It would be interesting to compare the effects of each method."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505403, "id": "ICLR.cc/2017/conference/-/paper599/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJrDIpiee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper599/reviewers", "ICLR.cc/2017/conference/paper599/areachairs"], "cdate": 1485287505403}}}, {"tddate": null, "tmdate": 1481583850982, "tcdate": 1481583850976, "number": 2, "id": "By7g6ohQg", "invitation": "ICLR.cc/2017/conference/-/paper599/public/comment", "forum": "HJrDIpiee", "replyto": "By_SyRkml", "signatures": ["~Jean_Harb1"], "readers": ["everyone"], "writers": ["~Jean_Harb1"], "content": {"title": "implementation details", "comment": "1. Lambda was not tuned. We selected 0.8 as it decays to a weight of 0.01 in just over 20 steps, allowing for a good truncated sequence length. For every new algorithm, a hyper-parameter search should be completed, but it's quite expensive to do so.\n\n2. We have not yet tested the algorithm on other games. This remains as future work.\n\n3. DQN runs at 1700 fps, while the RNN without traces runs at 560 fps and the RNN with traces runs at 500 fps. The code computing the traces could be further optimized to speed up the algorithm to speeds closer to the trace free model.\n\n4. This is kept consistent with DQN, where an epoch corresponds to 250k steps (actions) or 1M frames."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505403, "id": "ICLR.cc/2017/conference/-/paper599/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJrDIpiee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper599/reviewers", "ICLR.cc/2017/conference/paper599/areachairs"], "cdate": 1485287505403}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481582553725, "tcdate": 1478379101269, "number": 599, "id": "HJrDIpiee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJrDIpiee", "signatures": ["~Jean_Harb1"], "readers": ["everyone"], "content": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481582396819, "tcdate": 1481582396814, "number": 1, "id": "ryBSwoh7e", "invitation": "ICLR.cc/2017/conference/-/paper599/public/comment", "forum": "HJrDIpiee", "replyto": "Bk2DXskmx", "signatures": ["~Jean_Harb1"], "readers": ["everyone"], "writers": ["~Jean_Harb1"], "content": {"title": "games and architecture", "comment": "1.  If I understand correctly, you're asking for results on all Atari games. Unfortunately, it's simply too computationally demanding to perform training on all of them, right now. As such, it remains as future work.\n\n2. The architecture was kept as close as possible with DRQN, with the exception that we have the feed forward layer with 512 neurons, used in DQN, before feeding it into the RNN. The training was also slightly different as we used multiple frames as input to the RNN that aren't used for error, but only to fill the hidden state."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505403, "id": "ICLR.cc/2017/conference/-/paper599/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJrDIpiee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper599/reviewers", "ICLR.cc/2017/conference/paper599/areachairs"], "cdate": 1485287505403}}}, {"tddate": null, "tmdate": 1480740671644, "tcdate": 1480740671637, "number": 3, "id": "By_SyRkml", "invitation": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer4"], "content": {"title": "Some questions", "question": "1. Was the value of lambda tuned? How would other values of lambda compare?\n\n2. How does this method work on other Atari games? Are there any scenarios where elibility traces, recurrence, or Adam decreases performance?\n\n3. How does the wall-clock time compare between each of the three methods?\n\n4. How much experience (e.g. in terms of roll-outs or timesteps) does one epoch correspond to?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959194012, "id": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959194012}}}, {"tddate": null, "tmdate": 1480729443882, "tcdate": 1480729443878, "number": 2, "id": "Bk2DXskmx", "invitation": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer5"], "content": {"title": "Comparison with other methods", "question": "1. Can you please show how the presented method compares with of the box state of the art for the Atari games?\n2. Can you please clarify the differences between the used architecture used and the in Hausknecht and Stone (2015), aside from adding the eligibility traces step?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959194012, "id": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959194012}}}, {"tddate": null, "tmdate": 1480692563858, "tcdate": 1480692563853, "number": 1, "id": "Hkh8mMJXx", "invitation": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "forum": "HJrDIpiee", "replyto": "HJrDIpiee", "signatures": ["ICLR.cc/2017/conference/paper599/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper599/AnonReviewer3"], "content": {"title": "Questions", "question": "1) How did you select the hyperparameters for Adam and RMSprop? Did you tune them individually? It seems potentially odd to use a square gradient momentum of 0.95 in RMSprop and 0.999 in Adam since those hyperparameters are somewhat analogous.\n2) Do you truncate the eligibility traces as Watkin's Q(\\lambda) prescribes?\n2) Have you done any experiments to separate the effect of using n-step returns on faster credit assignment from combining different n-step returns using eligibility traces? The benefits of using n-step returns have already been shown both with and without the use of neural networks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "pdf": "/pdf/0ef00b69b455f1ef1bf93bfacd23b16660df34ba.pdf", "TL;DR": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "paperhash": "harb|investigating_recurrence_and_eligibility_traces_in_deep_qnetworks", "keywords": ["Reinforcement Learning", "Deep learning"], "conflicts": ["cs.mgill.ca", "umontreal.ca"], "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959194012, "id": "ICLR.cc/2017/conference/-/paper599/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper599/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper599/AnonReviewer3", "ICLR.cc/2017/conference/paper599/AnonReviewer5", "ICLR.cc/2017/conference/paper599/AnonReviewer4"], "reply": {"forum": "HJrDIpiee", "replyto": "HJrDIpiee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper599/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959194012}}}], "count": 11}