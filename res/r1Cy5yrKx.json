{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1491934003254, "tcdate": 1487366630445, "number": 142, "id": "r1Cy5yrKx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1Cy5yrKx", "signatures": ["~Yen-Chen_Lin1"], "readers": ["everyone"], "content": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028626748, "tcdate": 1490028626748, "number": 1, "id": "rJoUuY6jg", "invitation": "ICLR.cc/2017/workshop/-/paper142/acceptance", "forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028627399, "id": "ICLR.cc/2017/workshop/-/paper142/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028627399}}}, {"tddate": null, "tmdate": 1489635384162, "tcdate": 1489635384162, "number": 2, "id": "HJgBOYvsl", "invitation": "ICLR.cc/2017/workshop/-/paper142/public/comment", "forum": "r1Cy5yrKx", "replyto": "H1KJ5eBix", "signatures": ["~Yen-Chen_Lin1"], "readers": ["everyone"], "writers": ["~Yen-Chen_Lin1"], "content": {"title": "Re: An application of existing NN attacks in an RL setting", "comment": "Thanks a lot for your comments!\n\nI would love to clarify that the \u201cstrategically-timed attack\u201d we proposed in our paper also determines \u201cwhen to attack\u201d, i.e., it wants to reduce the total rewards gained by the agent by only attacking it at selective timesteps. Therefore, it goes beyond an adaption of misclassification attack to RL tasks. \n\nThe reason why the difference in frames is perceptible is that we enlarge the value of perturbation 250x for visualization, sorry for the confusion. We will clarify it in our future revision.\n\nAbout the comparison, as we mentioned in our abstract and experiment conclusion, our strategically-timed attack (attacking on average only 25% of timesteps) can reach the same effect as attacking the agent at every timesteps (i.e., Huang\u2019s strategy)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366630968, "tcdate": 1487366630968, "id": "ICLR.cc/2017/workshop/-/paper142/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper142/reviewers"], "reply": {"forum": "r1Cy5yrKx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366630968}}}, {"tddate": null, "tmdate": 1489467873496, "tcdate": 1489467873496, "number": 2, "id": "H1KJ5eBix", "invitation": "ICLR.cc/2017/workshop/-/paper142/official/review", "forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "signatures": ["ICLR.cc/2017/workshop/paper142/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper142/AnonReviewer1"], "content": {"title": "An application of existing NN attacks in an RL setting ", "rating": "7: Good paper, accept", "review": "This paper explains the adaptation of a (Carlini & Wagner, 2016) (mis)classification attack to making the agent choose it's worse (lowest Q score or lower prob for \\pi) action instead of best. It also explains an extension of the single time step (s, a, r, s') version to a sequence version, through the use of a forward model (Oh et al., 2015). Side note: the \\delta (attack vectors) seem quite significant (the difference in frames in perceptible, e.g. Figure 2).\n\nIt is an interesting application of a \"classic\" attack, the comparison (in terms of performance) to (Huang et al., 2017) is unclear. The experimental evaluation is weak, but sufficient for a workshop.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489467874366, "id": "ICLR.cc/2017/workshop/-/paper142/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper142/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2", "ICLR.cc/2017/workshop/paper142/AnonReviewer1"], "reply": {"forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489467874366}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489222932185, "tcdate": 1489005985843, "number": 1, "id": "SJ5o6kCqx", "invitation": "ICLR.cc/2017/workshop/-/paper142/official/review", "forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "signatures": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2"], "content": {"title": "Interesting and relevant topic, clearly work in progress", "rating": "7: Good paper, accept", "review": "Thank you for pointing me to this work; I was not aware of work in this area, and the topic is quite exciting.\n\nThe main problem with this paper is that it is still very clearly work in progress. The problem is not very well-motivated, and the authors rush right into the content without giving any context (it is almost as if they outright assume the reader has read Huang et al. 2016 immediately before reading the current paper or are very well familiar with it.) This work needs proper introduction and motivation, a summary of the related work it builds on, a smoother narrative, etc. I would reject this paper as a conference submission for these reasons: it is just not ready.\n\nDespite this, the authors have results and the topic is very interesting. This is the kind of paper that I think makes an ideal workshop paper: the topic is worth considering and relevant, and results are preliminary but interesting; it could stimulate some discussion which could (a) influence the direction of the work (b) lead to a broader interest in this type of work. So for these reasons, I am recommending accept.\n\nPossible discussion point: identifying the flaws/vulnerabilities with deep RL-trained policies is only the first step. How do we then modify our deep RL algorithms to produce policies that are robust to these type of attacks? Even some speculation on this point would be nice, as I expect it to be a major discussion point once work in this area matures.\n\n------ Edit post-response from authors:\n\nThe authors told me about the strict 3-page limit, which I was not aware of. With this limit in mind, I think the authors did a fairly good job of compressing the main ideas and results into the space they had. The page limit does unfortunately still detract from the smoothness of the intro/setup, but the description is still clear enough to understand what follows.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489467874366, "id": "ICLR.cc/2017/workshop/-/paper142/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper142/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2", "ICLR.cc/2017/workshop/paper142/AnonReviewer1"], "reply": {"forum": "r1Cy5yrKx", "replyto": "r1Cy5yrKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489467874366}}}, {"tddate": null, "tmdate": 1489222541379, "tcdate": 1489222541379, "number": 1, "id": "SkSqiNbjg", "invitation": "ICLR.cc/2017/workshop/-/paper142/official/comment", "forum": "r1Cy5yrKx", "replyto": "ByuHsCkix", "signatures": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper142/AnonReviewer2"], "content": {"title": "Thank you for the clarification and update", "comment": "Sorry, I was not aware of the strict 3-page limit. I will update my review accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366630975, "tcdate": 1487366630975, "id": "ICLR.cc/2017/workshop/-/paper142/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "r1Cy5yrKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper142/reviewers", "ICLR.cc/2017/workshop/paper142/areachairs"], "cdate": 1487366630975}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489217561032, "tcdate": 1489132351732, "number": 1, "id": "ByuHsCkix", "invitation": "ICLR.cc/2017/workshop/-/paper142/public/comment", "forum": "r1Cy5yrKx", "replyto": "SJ5o6kCqx", "signatures": ["~Yen-Chen_Lin1"], "readers": ["everyone"], "writers": ["~Yen-Chen_Lin1"], "content": {"title": "Re: Interesting and relevant topic, clearly work in progress", "comment": "We thank the reviewer for detail comments. Unfortunately, due to a strict 3 pages limit for ICLR workshop this year, we have to go straight to our method and results in this submission. Similarly due to space, we focus on the attack tactics for this submission. \n\nBased on the reviews, we added the following ideas about defending in section C of our Appendix: (1) train RL agent with adversarial example, (2) detect adversarial example first and then try to mitigate the effect. We hope to have enough interesting results on defending attacks to share in the future.\n\nWe are indeed working on a 6-8 pages conference submission which will include proper introduction and motivation, and a summary of the related work. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "abstract": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.", "pdf": "/pdf/5b455f367833c6fdd390a0386ebf0bfe9f627ea6.pdf", "TL;DR": "We propose two tactics of adversarial attacks for deep reinforcement learning and show their strength.", "paperhash": "lin|tactics_of_adversarial_attack_on_deep_reinforcement_learning_agents", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["nvidia.com", "nthu.edu.tw", "merl.com"], "authors": ["Yen-Chen Lin", "Zhang-Wei Hong", "Yuan-Hong Liao", "Meng-Li Shih", "Ming-Yu Liu", "Min Sun"], "authorids": ["yenchenlin1994@gmail.com", "williamd4112@gapp.nthu.edu.tw", "s102061137@m102.nthu.edu.tw", "shihsml@gapp.nthu.edu.tw", "sean.mingyul@nvidia.com", "sunmin@ee.nthu.edu.tw"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366630968, "tcdate": 1487366630968, "id": "ICLR.cc/2017/workshop/-/paper142/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper142/reviewers"], "reply": {"forum": "r1Cy5yrKx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366630968}}}], "count": 7}