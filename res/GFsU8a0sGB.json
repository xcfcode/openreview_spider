{"notes": [{"id": "GFsU8a0sGB", "original": "EiZzNbYUuZ", "number": 1852, "cdate": 1601308204170, "ddate": null, "tcdate": 1601308204170, "tmdate": 1614864335627, "tddate": null, "forum": "GFsU8a0sGB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CB_jIq400H_", "original": null, "number": 1, "cdate": 1610040491440, "ddate": null, "tcdate": 1610040491440, "tmdate": 1610474097361, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviewers raised a number of concerns which are addressed by the authors. The paper provides an interesting/novel perspective for federated learning (as a posterior inference problem rather than an optimization problem) which can potentially allow for faster and more accurate solutions. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491427, "tmdate": 1610474097347, "id": "ICLR.cc/2021/Conference/Paper1852/-/Decision"}}}, {"id": "561sFyfUl-d", "original": null, "number": 7, "cdate": 1605652685026, "ddate": null, "tcdate": 1605652685026, "tmdate": 1605652685026, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment", "content": {"title": "Summary of the revision updates", "comment": "We would like to thank all reviewers for their feedback and helpful comments.\n\nWe have responded to each individual reviewer in the comments below. AnonReviewer2 brought up a few pointers to the literature, and we have additionally provided 1-sentence summaries of all of them in a separate comment for other discussion participants to follow the discussion.\n\nWe have also revised and updated the paper and made the following changes:\n- Expanded Section 4 with a detailed description of ClientMCMC algorithm (as requested by AnonReviewer3).\n- Added an empirical comparison of the computational cost of FedAvg vs. FedPA client updates at the end of Section 4 (as requested by AnonReviewer1).\n- Significantly updated Appendix A, which now analyzes FedAvg and FedPA convergence from the perspective of the bias-variance tradeoff in the client updates. The discussion at the end of Section 4 was updated accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GFsU8a0sGB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1852/Authors|ICLR.cc/2021/Conference/Paper1852/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855058, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment"}}}, {"id": "ShFCjZdKvAs", "original": null, "number": 6, "cdate": 1605651954046, "ddate": null, "tcdate": 1605651954046, "tmdate": 1605651954046, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "8dBWt3UAeMD", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment", "content": {"title": "Thank you for the review and helpful feedback", "comment": "Thank you, we appreciate your feedback and the highlights of the positive sides of our paper.\n\nRegarding the Cons:\n\n1. Thanks for the suggestion to include the empirical analysis of the on-client computation cost. We revised the paper and included an empirical comparison at the end of Section 4 that shows the difference between FedAvg and FedPA in terms of time complexity of the client updates on linear regression tasks of different dimensionality. In practical high-dimensional settings, the difference becomes negligible. We also timed client updates on the benchmark tasks (EMNIST, CIFAR100, StackOverflow), where the difference between FedAvg and FedPA for the same number of local epochs (ranging between 5 and 20) was unnoticeable (i.e., within the margin of error).\n\n2. Regarding hyperparameter tuning, this is a very good point. Generally, to avoid introducing a new hyperparameter, one could imagine using an \u201cadaptive\u201d algorithm that checks whether a client has \u201cburned-in\u201d (e.g., based on the loss decay) and decides whether to start sampling or to keep running FedPA in the FedAvg regime. Our decision to use a fixed hyperparameter for the # of burn-in steps was motivated by simplicity and the fact that tuning this hyperparameter turned out to be fairly easy in practice on the datasets and tasks we considered.\n\n3. The results of FedAvg-1E on the very sparse StackOverflow LR task is indeed puzzling. We believe that the effect might be mainly due to extreme sparsity of the problem (the local models were > 95% sparse): more local epochs lead to better local convergence, which made the models more accurate but also hurt the recall of the resulting global model on the held out clients. FedAvg-1E is included in the main text in Table 2.c, and we updated the submission to emphasize this more at the end of Section 5 and urge the reader to check out the full set of figures provided in the appendix.\n\n4. Regarding local updates for FedAvg and FedPA: this is not quite right, we did actually select different local optimizers (either SGD with or without momentum) depending on which ones worked best for FedAvg and used the same setup for FedPA (see Table 3 in the appendix for details on our setup). Using adaptive optimizers such as Adam for ClientOpt should be possible too, but requires careful treatment of whether and how the local accumulators are treated (e.g., should they be also learned and thus communicated to the server and inferred). Note that Reddi et al. (2020) strictly used SGD for ClientOpt (even without momentum), so we believe that our comparison is fair in that regard. The momentum does not really affect the samples produced by IASG as long as the local learning rates are adjusted, since IASG essentially runs Polyak-averaged SGD which is known to be equivalent to SGD with momentum up to the learning rate (http://proceedings.mlr.press/v40/Flammarion15.pdf)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GFsU8a0sGB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1852/Authors|ICLR.cc/2021/Conference/Paper1852/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855058, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment"}}}, {"id": "cIBzXe1pB_L", "original": null, "number": 5, "cdate": 1605650944843, "ddate": null, "tcdate": 1605650944843, "tmdate": 1605651105535, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "XCbkfYRtUDq", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment", "content": {"title": "Summary of the suggested references", "comment": "In addition to our response below, we also took a careful look at each of the suggested references and provide 1-sentence summaries for other discussion participants (reviewers, ACs, or anyone interested) with brief comments about the relevance of each of them to our work.\n\n---\n\nSummary of the suggested references:\n- Lavancier, F., Rochet, P.: A general procedure to combine estimators. preprint arXiv:1401.6371 (2014):\n  - The paper proposes a weighted averaging method for combining multiple estimators of the same quantity (a random vector), where the optimal weights are computed by solving an optimization problem.\n  - It is unclear how this averaging-based estimator is related to Eq. 3 and whether it can be used for posterior inference and/or FL.\n- Bordley, R.F.: The combination of forecasts: A Bayesian approach. Journal of the Operational Research Society 33(2), 171\u2013174 (1982)\n  - Solves a similar problem as the previous reference.\n  - Unclear how it is related to our method and/or FL more broadly.\n- D. Luengo et al, \"Efficient linear fusion of partial estimators\", Digital Signal Processing, Volume 78, Pages: 265-283, 2018.\n  - The paper derives an optimal way to combine multiple parameter estimators, recovers our Eq. 3 using the minimum mean squared error principle and then provides a couple of alternative estimators.\n  - The connection between FL and the estimation problem in signal processing is nice, but all the proposed estimators seem intractable in very high dimensions.\n- Cattivelli, F.S., Sayed, A.H.: Diffusion LMS strategies for distributed estimation. IEEE Transactions on Signal Processing 58(3), 1035\u20131048 (2010)\n  - Essentially, provides an iterative algorithm for estimating the global posterior mean (as given in our Eq. 3) but using peer-to-peer communication on a network of interconnected nodes. The approach is focused on linear problems.\n  - FL in peer-to-peer settings should be possible. However, it is beyond the classical server-client communication pattern (hub-and-spoke network topology), which is the focus of our paper.\n- R. Craiu et al. Learn from thy neighbor: Parallel-chain and regional adaptive MCMC. Journal of the American Statistical Association, 104(448):1454\u20131466, 2009.\nF. Llorente et al, \"Parallel Metropolis-Hastings Coupler\", IEEE Signal Processing Letters, Volume 26, Number 6, Pages 953-957, 2019.\nJ.Corander et al. Parallel interacting MCMC for learning of topologies of graphical models. Data Mining and Knowledge Discovery, 17(3):431\u2013456, 2008\n  - These papers propose different approaches to parallel-chain MCMC with some sort of coupling between the chains.\n  - In our paper, MCMC chains are decoupled and run independently on each client. Coupling MCMC chains would require peer-to-peer communication, which might be interesting and possible in some FL settings, but is not directly relevant to our current work.\n\n---\n\nFor the reference, Eq. 3 from our submission refers to the following: $\\mu := \\left( \\sum_{i=1}^N q_i \\Sigma_i^{-1} \\right)^{-1} \\left( \\sum_{i=1}^N q_i \\Sigma_i^{-1} \\mu_i \\right)$."}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GFsU8a0sGB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1852/Authors|ICLR.cc/2021/Conference/Paper1852/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855058, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment"}}}, {"id": "-Rz5qjp1KVk", "original": null, "number": 4, "cdate": 1605650614191, "ddate": null, "tcdate": 1605650614191, "tmdate": 1605651054717, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "XCbkfYRtUDq", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment", "content": {"title": "Thank you for the feedback and pointers to the literature", "comment": "Thank you for the feedback, we very much appreciate all the pointers to the literature!\n\nRegarding the raised concerns:\n\n1. **Novelty:** We emphasize that the main focus of this paper is federated learning (FL), not just distributed inference. FL was introduced in 2017 (McMahan et al., 2017), and has not been approached from a distributed inference perspective prior to this work. Our main contributions: (1) showing a connection between FL and distributed inference, (2) designing the first distributed inference algorithm for FL which is practical and tractable from the computation and communication points of view, (3) connect distributed inference with the FedAvg optimization algorithm, which is the default option in FL today. We agree that from the distributed estimation point of view, our method may not be the most advanced possible, but it is the first method designed to work in FL settings (i.e., with limited communication and local computation). Having said that, further improving estimators and sampling techniques is definitely an interesting direction for future work.\n\n2. **Difference between our method and other consensus-based estimation techniques:**\nAgain, our work specifically focuses on FL, while the suggested related work does not directly address the same problem. It would be great to see if other consensus-based algorithms can actually scale to million-dimensional problems and effectively run distributed inference across a very large number of devices. It is not obvious, however, if the suggested consensus-based estimation methods could be adapted to work in federated learning settings.\n\n3. **Regarding parallel MCMC:** Thanks for your pointers to other MCMC schemes that might hold promise for application to FL. In this work, we did a deep dive into one simple MCMC-based strategy and showed how to make it work within the relatively strict computation and communication efficiency requirements of FL.  We think that it would be an interesting future challenge to see if we could make other MCMC techniques work in a similarly efficient way.\n\n4. **Regarding the Laplace approximation:** Indeed, we do need to infer local MAP estimates, which is exactly what is accomplished when we use IASG (see Algorithm 4). To see this, note that we use stochastic gradient descent to get close to the optima and, after a number of burn-in steps, we compute a running average of the SGD iterates. This is essentially a Polyak-averaged SGD and it converges to the corresponding optimum (i.e., local MAP estimate) at an accelerated rate.\n\nIn a separate comment in this thread, we discuss the suggested references and comment about their relationship to our work."}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GFsU8a0sGB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1852/Authors|ICLR.cc/2021/Conference/Paper1852/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855058, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment"}}}, {"id": "sLluvQR7inE", "original": null, "number": 3, "cdate": 1605646728559, "ddate": null, "tcdate": 1605646728559, "tmdate": 1605646728559, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "MwJP-xQX-jG", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment", "content": {"title": "Thank you for the review and feedback", "comment": "Thank you for the feedback and for highlighting the positive sides of our paper.\n\nRegarding the Cons:\n\n1. We agree. Given the extra page provided for the rebuttal and camera ready, we have elaborated the details of the algorithm in the main text (Section 4) and updated the manuscript.\n\n2. This is correct, our main contributions are primarily algorithmic as well as experimental. Having said that, FedPA generalizes FedAvg, and essentially reduces the bias in the client updates by trading it for some extra variance (coming from local sampling). Assuming that the extra variance is bounded, the existing convergence rates known for FedAvg (e.g., from Reddi et al., 2020) can be directly applicable to FedPA. One could potentially improve these rates by making further assumptions about how exactly bias-variance of the client deltas are affected by our local moment estimation techniques. We have extended Appendix A.1 with a discussion of FedAvg and FedPA convergence as SGD methods with biased gradients and empirical analysis of the bias-variance tradeoff in a synthetic setting (see the updated version of the manuscript). We have also updated the discussion in the main text accordingly.\n\n(Also note that the original paper that introduced FedAvg (McMahan et al., 2017) did not have any formal convergence guarantees; the analysis under a variety of different assumptions was only later developed by the optimization community over the span of a couple of years: https://arxiv.org/abs/1907.02189, https://arxiv.org/pdf/1910.06378.pdf, https://arxiv.org/abs/2002.07839, etc.)\n\n3. To the best of our knowledge, distributed MCMC techniques---and more broadly inferential approaches---have not been used in the federated learning (FL) context prior to this work. There are two main challenges that we tackle. The first one is indeed communication, which is orders of magnitude more expensive in FL than in the standard distributed optimization and inference settings (FL is designed for low-bandwidth, potentially faulty network, where clients are mobile devices or different organizations). The second is efficient local and global posterior inference in high dimensions (with millions of parameters, as common in deep learning) while keeping the communication cost linear. We will make sure to emphasize this in the paper and discuss the related MCMC literature in more detail. Analysis of the privacy guarantees is left for future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GFsU8a0sGB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1852/Authors|ICLR.cc/2021/Conference/Paper1852/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855058, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Comment"}}}, {"id": "8dBWt3UAeMD", "original": null, "number": 1, "cdate": 1603307826566, "ddate": null, "tcdate": 1603307826566, "tmdate": 1605024343993, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review", "content": {"title": "A well motivated and effective method for deriving local updates on the client", "review": "The authors propose a new method of generating local (client) updates in Federated Learning (FL), where the clients return an adjusted version of their usual local updates to the server. The authors derive this new local update rigorously from the viewpoint of estimating the posterior distribution of the data (under Gaussianity assumptions). They also provide an efficient method for calculating this new update, and show that it outperforms Federated Averaging on several datasets.\n\nPros:\n\nThe new method (FEDPA) is well motivated and is 'as simple as possible, but not simpler' based on the derivation.\n\nFEDPA has standard Federated Averaging (FEDAVG) as a special case, and suggests a family of new methods based on approximations of the covariance matrix, which likely would exhibit bias-variance-tradeoff-esque behaviour.\n\nThe authors provide a practical way of calculating the required new quantities efficiently.\n\nExperimental results show FEDPA has superior performance compared to FEDAVG, especially in regimes where client compute is high. \n\nCons:\n\nThe big O analysis of the dynamic programming method for computing the local updates is very useful, but it would also be good to have empirical results on the additional cost on the client. It seems like the cost should not be too significant, but evidence of this would be very valuable, since the cost of FEDPA is strictly greater than the cost of FEDAVG. \n\nThe addition of more tuning parameters in FL is never ideal, especially with the knowledge that using too small of a burn in time can lead to arbitrarily bad behaviour. However since the positive effects of FEDPA appear very quickly after the burn in ends, this may be less of a concern in practice. \n\nWould be valuable to include the results of the FEDAVG-1E in the experiments in the main paper, especially since it outperforms both FEDAVG-5E and FEDPA-5E on the StackOverflow LR, which is a surprising and unexpected result. \n\nIn all experiments the FEDAVG locally updated using SGD (likely to maintain the connection and comparison with SGD used in IASG). However for FEDAVG, the optimization procedure CLIENTOPT could be something else, such as adam. It would be valuable to know how this FEDPA compares to FEDAVG when the local optimizer is a more powerful method than SGD, since FEDAVG has the freedom to change the local optimizer. (Of course it is only fair to also empirically compare FEDPA where the CLIENTOPT can be the same method. However since the use of SGD in IASG provides it with certain properties, it is less clear if this substitution can be safely made). \n\nThe authors provide ample justification for their new method and sufficient evidence that it outperforms the existing standard method FEDAVG.  ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109398, "tmdate": 1606915797840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1852/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review"}}}, {"id": "XCbkfYRtUDq", "original": null, "number": 2, "cdate": 1603969040312, "ddate": null, "tcdate": 1603969040312, "tmdate": 1605024343927, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review", "content": {"title": "The paper is nice and seems technically sound. The discussion of the state-of-the-art must be improved.", "review": "The paper is nice and seems technically sound. However, some part must be clarified.\nFor instance, the discussion of the state-of-the-art must be improved.\nMy main concerns is the degree of novelty. It is not clear the difference with other works.\nSee my comments below.\n\n- Regarding Eq. (1): you state that \"For example, least squares loss corresponds to likelihood under a\nGaussian model, cross entropy loss corresponds to likelihood under a categorical model, etc. Thus, Eq. 1 corresponds to maximum likelihood estimation (MLE) of the model parameters \u0012.\" This sentence must be clarified since it is not straightforward to see it from Eq. (1) and it is an important equation and sentence for your work.  Do you mean that F(\\theta) is a log-posterior or a  log-likelihood?\nGenerally,  it is not a mixture of components. Clarify this point.\n\n- Regarding Eq. (3):  the state-of-the-art related to this equation must be improved. For instance, the following relevant contributions must be considered\n\nLavancier, F., Rochet, P.: A general procedure to combine estimators. preprint arXiv:1401.6371 (2014)\n\nD. Luengo et al, \"Efficient linear fusion of partial estimators\", Digital Signal Processing, Volume 78, Pages: 265-283, 2018.\n\nCattivelli, F.S., Sayed, A.H.: Diffusion LMS strategies for distributed estimation. IEEE Transactions on Signal Processing 58(3), 1035\u20131048 (2010)\n\nBordley, R.F.: The combination of forecasts: A Bayesian approach. Journal of the Operational Research Society 33(2), 171\u2013174 (1982)\n\n- It is not clear the difference among your strategy and the consensus one and the others in the papers above. Please clarify.\n\n- Regarding parallel MCMC chains, other schemes could be considered such as:\n\nR. Craiu et al. Learn from thy neighbor: Parallel-chain and regional adaptive MCMC. Journal of the American\nStatistical Association, 104(448):1454\u20131466, 2009.\n\nF. Llorente et al, \"Parallel Metropolis-Hastings Coupler\", IEEE Signal Processing Letters, Volume 26, Number 6, Pages 953-957, 2019.\n\nJ.Corander et al. Parallel interacting MCMC for learning of topologies of graphical models. Data Mining and Knowledge Discovery, 17(3):431\u2013456, 2008\n\n- Regarding the Laplace approximations: you still need the knowledge of the MAP  (maxima a-posteriori). You still need to compute/reach these maxima. Please clarify the computational cost of these previous steps for finding these maxima.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109398, "tmdate": 1606915797840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1852/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review"}}}, {"id": "MwJP-xQX-jG", "original": null, "number": 3, "cdate": 1604042022905, "ddate": null, "tcdate": 1604042022905, "tmdate": 1605024343846, "tddate": null, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "invitation": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review", "content": {"title": "Review on Federated Learning via Posterior Averaging", "review": "This paper introduces a new perspective on federated learning through the lens of posterior inference. The paper designs a computation- and communication-efficient posterior inference algorithm\u2014federated posterior averaging (FEDPA), which generalizes FedAvg. FEDPA is compared with the strong baselines in Reddi et al. (2020) on realistic FL benchmarks, which achieves state-of-the-art results with respect to multiple metrics of interest. \n\nOverall, the paper is well written and easy to follow. I tend to accept the paper. The detailed comments follow. \n\nPros: \n1. Viewing federated learning through the lens of posterior inference is new. The motivating example in Figure 1 gives a nice explanation why FedPA may outperform FedAvg. \n2. The proposed FedPA algorithm enjoys provable performance guarantee.  \n3. The simulations are extensive, which demonstrate the clear advantage of FedPA over FedAvg on multiple benchmark tasks. \n\nCons:\n1. The algorithm itself is not well explained in the first eight pages. Consider moving some key equations from appendix to main text to explain clientMCMC. \n2. The convergence result is weaker than those on optimization for federated learning. Specifically, no finite-time analysis is provided, and the dependence of the number of local update K on the performance is not clear in the current form. \n3. The new challenge relative to distributed MCMC literature is not well explained. Communication? Privacy? or else. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1852/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1852/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms", "authorids": ["~Maruan_Al-Shedivat1", "~Jennifer_Gillenwater1", "~Eric_Xing1", "~Afshin_Rostamizadeh1"], "authors": ["Maruan Al-Shedivat", "Jennifer Gillenwater", "Eric Xing", "Afshin Rostamizadeh"], "keywords": ["federated learning", "posterior inference", "MCMC"], "abstract": "Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm\u2014federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.", "one-sentence_summary": "A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alshedivat|federated_learning_via_posterior_averaging_a_new_perspective_and_practical_algorithms", "supplementary_material": "", "pdf": "/pdf/3c19f2476503b117ff059dbfc938d5efd211ed0f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nal-shedivat2021federated,\ntitle={Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms},\nauthor={Maruan Al-Shedivat and Jennifer Gillenwater and Eric Xing and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GFsU8a0sGB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GFsU8a0sGB", "replyto": "GFsU8a0sGB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1852/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109398, "tmdate": 1606915797840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1852/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1852/-/Official_Review"}}}], "count": 10}