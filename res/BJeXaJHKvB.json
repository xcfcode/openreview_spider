{"notes": [{"id": "BJeXaJHKvB", "original": "ryeDZVkYDH", "number": 1984, "cdate": 1569439675514, "ddate": null, "tcdate": 1569439675514, "tmdate": 1577168276949, "tddate": null, "forum": "BJeXaJHKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1tPaz9X-m_", "original": null, "number": 1, "cdate": 1576798737577, "ddate": null, "tcdate": 1576798737577, "tmdate": 1576800898794, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Decision", "content": {"decision": "Reject", "comment": "This paper addresses the extension of path-space-based SGD (which has some previously-acknowledged advantages over traditional weight-space SGD) to handle batch normalization. Given the success of BN in traditional settings, this is a reasonable scenario to consider.  The analysis and algorithm development involved exploits a reparameterization process to transition from the weight space to the path space.  Empirical tests are then conducted on CIFAR and ImageNet.\n\nOverall, there was a consensus among reviewers to reject this paper, and the AC did not find sufficient justification to overrule this consensus.  Note that some of the negative feedback was likely due, at least in part, to unclear aspects of the paper, an issue either explicitly stated or implied by all reviewers.  While obviously some revisions were made, at this point it seems that a new round of review is required to reevaluate the contribution and ensure that it is properly appreciated.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722605, "tmdate": 1576800273946, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Decision"}}}, {"id": "HygQZEdrir", "original": null, "number": 1, "cdate": 1573385210758, "ddate": null, "tcdate": 1573385210758, "tmdate": 1573800472008, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "ryeSKamyoB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment", "content": {"title": "To Reviewer #4", "comment": "Thank you for your comments. The following is our responses.\n\nQ1: \u201cLet start with Theorem 3.1: I am not sure about the statement of the theorem. Is this result for a linear net? I think for a Relu net, outputs need an additional scaling parameter that depends on all past hidden states (outputs).\u201d\nA1: No, all results in our paper are for non-linear neural networks with ReLU activations. We are sorry that we lost the activation function g in theorem 3.1, and we have fixed it in our new version.\n\nQ2: \u201cTheorem 3.2 and 4.1 do not seem informative to me.\u201d\nA2: Theorem 3.2 and 4.1 provide the norm of gradient w.r.t outputs in every layer, when conventional BN and P-BN are applied to re-parameterized networks, respectively. Please note that the diagonal elements of $\\hat{W}_s$ in Theorem 3.2 are equal to 1 and in Theorem 4.1, we have stated that $\\hat{W}\u2019_s=\\hat{W}_s-I$. Theorem 3.2 demonstrates that gradients explode along network depth (layer index), because the variance term $\\|\\sigma^s\\|$ is less than 1 and there is an identity matrix contained in $\\hat{W}_s$. Theorem 4.1 demonstrates that the gradient exploding problem will be weaken by P-BN, because the identical matrix is separated from $W\u2019_s$, and the variance term $\\|\\sigma^{s,/}\\|$ becomes larger. We provide a comparison with clearer description in Corollary 4.2 in our new version.\n\nQ3: \u201cIn fact, batch normalization naturally remedies this type of singularity since lengths of weights are trained separately from the direction of weights.\u201d\nA3: Batch Normalization cannot fully remedy this type of singularity. Batch normalization can keep the outputs unchanged when weights in one layer and its successive layer is multiplied and divided by a positive constant. However, the gradients w.r.t weights are changed by such rescaling operation because the Lipschitz constants w.r.t weights at different layers have changed. An intuitive explanation is that if a BN network whose weights at different layers have unbalanced magnitudes, stochastic gradient descent will suffer from such \u201cunbalanced\u201d scale of weights. On the other hand, gradients can still keep unchanged when the network is optimized in the path space. Thus, studying batch normalization in the path space is important. \n\nWe hope that we have answered your questions and addressed your concerns. We also hope that you can reconsider your ratings."}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJeXaJHKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1984/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1984/Authors|ICLR.cc/2020/Conference/Paper1984/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148013, "tmdate": 1576860536761, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment"}}}, {"id": "BygbSE_riH", "original": null, "number": 2, "cdate": 1573385272969, "ddate": null, "tcdate": 1573385272969, "tmdate": 1573800328754, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "HkxkpF_j5S", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment", "content": {"title": "To Reviewer #3", "comment": "Thanks for your suggestions. The following is our responses.\n\nQ1: \u201cwhy P-BN helps path optimization is not clear in the paper.\u201d\nA1: The reason that P-BN helps path optimization is described in section 3.2 and section 4. Specifically, in theorem 3.2, we demonstrate that gradients explode along network depth (layer index), because the variance term $\\|\\sigma^s\\|$ is less than 1 and there is an identity matrix contained in $\\hat{W}_s$ (please note that the diagonal elements of $\\hat{W}_s$ in Theorem 3.2 are equal to 1 and in Theorem 4.1, we have stated that $\\hat{W}\u2019_s=\\hat{W}_s-I$). Therefore, we propose P-BN, which only normalize the terms related to the trained coefficients, and exclude the term related to the constant coefficient. Then, as shown in theorem 4.1, the gradient exploding problem will be weaken by P-BN, because the identical matrix (constant coefficients) is separated from W\u2019_s, and the variance term $\\|\\sigma^{s,/}\\|$ becomes larger. We have provided a comparison with clearer description in Corollary 4.2 in our new version.\n\nQ2: \u201cThe experimental part is not convincing.\u201d\nA2: We use the SGD without momentum in our experiments, because the way to utilize momentum in the path space remains unclear now. Our experiments follow the experimental settings of the work (Meng et al., 2019). We have clarified it in Appendix F.1 (Experimental Setting Details) in our updated version.\n\nQ3: \u201cIt is not easy to imagine how the re-parameterization works on CNNs since the kernel is applied over the entire image (\"hidden activations\").\u201d\nA3: We provided some details on CNN in Appendix D.1 in our original version. In that way, the number of hidden nodes in MLP corresponds to the number of channels in CNN, and CNN can be operated similarly with MLP. We have also added some descriptions about this in Appendix D.1 in our new version.\n\nWe sincerely hope that we have addressed your concerns and you can reconsider your ratings.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJeXaJHKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1984/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1984/Authors|ICLR.cc/2020/Conference/Paper1984/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148013, "tmdate": 1576860536761, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment"}}}, {"id": "BJl__VdBiH", "original": null, "number": 3, "cdate": 1573385328301, "ddate": null, "tcdate": 1573385328301, "tmdate": 1573800259498, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "r1leWJ7TYr", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment", "content": {"title": "To Reviewer #2", "comment": "Thanks for your comments. The following is our responses.\n\nQ1: \u201cBy comparing theorem 3.2 and theorem 4.1, it seems P-BN even gives even worse upper bound of gradient norm.\u201d\nA1: This is not a worse upper bound. Theorem 3.2 and 4.1 provide the norm of gradient w.r.t outputs in every layer, when conventional BN and P-BN are applied to re-parameterized networks, respectively. Here, larger gradient norm means greater gradient exploding, so the larger one is the worse one. Please note that the diagonal elements of $\\hat{W}_s$ in Theorem 3.2 are equal to 1 and in Theorem 4.1, we have stated that $\\hat{W}\u2019_s=\\hat{W}_s-I$. Thus, theorem 4.1 demonstrates that the gradient exploding problem will be weaken by P-BN, i.e., the gradient norm can become smaller after applying P-BN, because the identical matrix is separated from $W\u2019_s$, and the variance term $\\|\\sigma^{s,/}\\|$ becomes larger. We have provided a comparison with clearer description in Corollary 4.2 in our new version.\n\nQ2: \u201cwe don't actually care that much about the issue of gradient exploding since one could always do gradient clipping.\u201d\nA2: Gradient clipping is a trick which needs tuning hyper-parameters, and here we want to get rid of this trick and find a well-performed design for the problem. This paper aims to propose a suitable BN method in path space to ensure stable gradient propagation, which is more fundamental and have theoretical guarantee. It is unfair to criticize this paper\u2019s significance to state that gradient exploding is not an important issue.\n\nQ3: \u201cThe formulation of the P-BN seems to be closely related to ResNet \u2026\u201d\nA3: We have some following differences. Frist, our motivation is quite different, as we start from the path space, while ResNet is not motivated by the new parameter space. Second, the novelty of ResNet is adding a skip-connection, but the identical connection is naturally exists in the path space, when the network is re-parameterized. Then, P-BN exclude the term related to the constant coefficient. Third, P-BN can also be used for ResNet since path space for ResNet is also established in previous work, which means that they are compatible. \n\nQ4: \u201cIt would be better to describe the method in a broader sense.\u201d\nA4: We provided some details on CNN in Appendix D.1 in our original version. In that way, the number of hidden nodes in MLP corresponds to the number of channels in CNN, and CNN can be operated similarly with MLP. We have also added some additional details on CNN in the updated version.\n\nQ5: \u201cThe assumption of diagonal elements of matrix w to be all positive is very restrictive and simply removes the effect of ReLU activations.\u201d\nA5: First, we lost the activation g in Theorem 3.1 and we have fixed it in our updated new version. We are sorry for this typo, and it may cause the misunderstanding that our theorem removes the effect of ReLU activations. In fact, ReLU activations still works after re-parameterization. Second, in the remark of theorem 3.1, we show that the positive constraint is not essential for proving theorem 3.1. Third, this constraint will not bring much influence on the model expressiveness, because the number of the constrained weights is tiny compared with the total weights, and according to experiment results in Meng et al., 2019 the practical performances are not harmed by this constrain. Actually, this constraint comes from the optimization algorithm in the path space (Meng et al., 2019), which is not introduced by our paper. \n\nWe sincerely hope that we have addressed your concerns and you can reconsider your ratings after reading the responses and our updated version.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJeXaJHKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1984/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1984/Authors|ICLR.cc/2020/Conference/Paper1984/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148013, "tmdate": 1576860536761, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment"}}}, {"id": "rygGgK_roS", "original": null, "number": 4, "cdate": 1573386474063, "ddate": null, "tcdate": 1573386474063, "tmdate": 1573386474063, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment", "content": {"title": "Summary of the new version", "comment": "Thanks for all reviewers and their comments, which are helpful for us. We have uploaded a new version of our paper, and the main changes include:\n1. We have fixed a typo in Theorem 3.1 by adding the activation g. \n2. We have added Corollary 4.2. \n3. We have added some more descriptions about CNN in Appendix D.1."}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJeXaJHKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1984/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1984/Authors|ICLR.cc/2020/Conference/Paper1984/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148013, "tmdate": 1576860536761, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Authors", "ICLR.cc/2020/Conference/Paper1984/Reviewers", "ICLR.cc/2020/Conference/Paper1984/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Comment"}}}, {"id": "ryeSKamyoB", "original": null, "number": 3, "cdate": 1572973949241, "ddate": null, "tcdate": 1572973949241, "tmdate": 1572973949241, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The proposal is an adapted batch normalization method for path regularization methods used in the optimization of neural networks. For neural networks with Relu activations, there exits a particular singularity structure, called positively\nscale-invariant, which may slow optimization. In that regard, it is natural to remove these singularities by optimizing along invariant input-output paths. Yet, the paper does not motivate this type of regularization for batchnormalized nets. In fact, batch normalization naturally remedies this type of singularity since lengths of weights are trained separately from the direction of weights. Then, the authors motivate their novel batch-normalization to gradient exploding (/vanishing) which is a completely different issue. \nI am not sure whether I understood the established theoretical results in this paper. Let start with Theorem 3.1: I am not sure about the statement of the theorem. Is this result for a linear net? I think for a Relu net, outputs need an additional scaling parameter that depends on all past hidden states (outputs). Theorem 3.2 and 4.1 do not seem informative to me. Authors are saying that if some terms in the established bound in Theorem 4.1 is small, then exploding gradient does not occur for their novel method. The same argument can be applied to the plain batchnorm result in Theorem 3.2. For me, it is not clear to see the reason why the proposed method remedies the gradient exploding (/vanishing). \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575554839043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Reviewers"], "noninvitees": [], "tcdate": 1570237729446, "tmdate": 1575554839062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review"}}}, {"id": "r1leWJ7TYr", "original": null, "number": 1, "cdate": 1571790583752, "ddate": null, "tcdate": 1571790583752, "tmdate": 1572972398259, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzes a reparametrization of the network that migrates from the weight space to the path space. It enables an easier way to understand the batch normalization (BN). Then the authors propose a variant of BN on the path space and empirically show better performance with the new proposal. \n\nTo study BN in the reparameterized space is well-intuited and a natural idea. Theorem 3.1 itself is interesting and has some value in understanding BN. However, the main contribution of the paper, i.e., the proposal of the P-BN, is not motivated enough. It is merely mentioned in the beginning of section 4 and it's not clear why this modification is better compared to conventional BN. This is not verified by theory either. By comparing theorem 3.2 and theorem 4.1, it seems P-BN even gives even worse upper bound of gradient norm. \nPlus we don't actually care that much about the issue of gradient exploding since one could always do gradient clipping. The notorious gradient vanishing problem on the other hand, is not address in the theorems. \nThe formulation of the P-BN seems to be closely related to ResNet, since it sets aside the identity mapping and only normalizes on the other part. It would be better to have some discussions. \nAlso, the reparameterization and P-BN seems only to apply to fully connected layer from Eqn. (3-5) where they are proposed, but the experiments applies to ResNet. It would be better to describe the method in a broader sense. How would you do this P-BN in more complicated networks?\n\nFinally, it's very unclear to me the value of Theorem 3.1 and the proof that takes almost one page in the main context. The assumption of diagonal elements of matrix w to be all positive is very restrictive and simply removes the effect of ReLU activations. \n\nTherefore I think the paper has some room for improvement and is not very suitable for publication right now. \n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575554839043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Reviewers"], "noninvitees": [], "tcdate": 1570237729446, "tmdate": 1575554839062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review"}}}, {"id": "HkxkpF_j5S", "original": null, "number": 2, "cdate": 1572731319491, "ddate": null, "tcdate": 1572731319491, "tmdate": 1572972398214, "tddate": null, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "invitation": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Originality: The paper proposed a new Path-BatchNormalization in path space and compared the proposed method with traditional CNN with BN.\n\nQuality: The theoretical part is messy but intuitive. Also, why P-BN helps path optimization is not clear in the paper. The experimental part is not convincing. All CNN with BN networks have much lower accuracy than people reported, e.g. https://pytorch.org/docs/stable/torchvision/models.html for ResNet on ImageNet.\n\nClarity: The written is not clear enough. It is not easy to imagine how the re-parameterization works on CNNs since the kernel is applied over the entire image (\"hidden activations\").\n\nSignificance: \nSee above."}, "signatures": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1984/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["luoxufang@buaa.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "tyliu@microsoft.com"], "title": "P-BN: Towards Effective Batch Normalization in the Path Space", "authors": ["Xufang Luo", "Qi Meng", "Wei Chen", "Tie-Yan Liu"], "pdf": "/pdf/06a5e296b87f2600dfe1e607dd937e3fc1d1f3b8.pdf", "abstract": "Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi\ufb01cantly outperform the conventional BN in the weight space.", "keywords": [], "paperhash": "luo|pbn_towards_effective_batch_normalization_in_the_path_space", "original_pdf": "/attachment/357b57704bca20bc26e765865d6bc81f74e5fd79.pdf", "_bibtex": "@misc{\nluo2020pbn,\ntitle={P-{\\{}BN{\\}}: Towards Effective Batch Normalization in the Path Space},\nauthor={Xufang Luo and Qi Meng and Wei Chen and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeXaJHKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeXaJHKvB", "replyto": "BJeXaJHKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1984/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575554839043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1984/Reviewers"], "noninvitees": [], "tcdate": 1570237729446, "tmdate": 1575554839062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1984/-/Official_Review"}}}], "count": 9}