{"notes": [{"id": "rkl8sJBYvH", "original": "HkeSZ0RdDS", "number": 1915, "cdate": 1569439645750, "ddate": null, "tcdate": 1569439645750, "tmdate": 1583912043631, "tddate": null, "forum": "rkl8sJBYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "4uudRwsglG", "original": null, "number": 1, "cdate": 1576798735828, "ddate": null, "tcdate": 1576798735828, "tmdate": 1576800900545, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper carries out extensive experiments on Neural Tangent Kernel (NTK) --kernel methods based on infinitely wide neural nets on small-data tasks. I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710913, "tmdate": 1576800260000, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Decision"}}}, {"id": "SJx5zKqaKS", "original": null, "number": 3, "cdate": 1571821842104, "ddate": null, "tcdate": 1571821842104, "tmdate": 1574716082741, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper conducts very interesting and meaningful study of kernels induced by infinitely wide neural networks on small data tasks. They show that on a variety of tasks performance of these kernels are superior to both finite neural networks and Random Forest methods.\n\nWhile neural tangent kernel (NTK) [1] is motivated for studying training dynamics of neural networks, it is also important to ask to find utility of these new powerful kernels that captures functional priors of neural networks. This paper conducted important study on small dataset regime and on a wide range of tasks (90 UCI datasets, small subset of CIFAR-10, few shot image classification task on VOC07. \n\nAuthors introduce a family of generalized NTK kernels interpolating between NNGP kernels [2] to original NTK[1] by fixing first L\u2019 layers and allowing to train remaining layers. Treating L\u2019 as a hyperparameter, the authors try both NNGP/NTK and kernels in between as well. \n\nAnother contribution I observe is applying kernel SVM where one utilizes NTK and shows that it can work well. This paper shows that kernels induced by infinitely wide networks could become useful for real world applications where data size is not so large. \n\nThere are few small concerns regarding experiments which are discussed in detailed comments. Overall I think the message of the paper is clear and well supported therefore I recommend accepting the paper. \n \nDetailed comments\n\t\n1) From reading the paper it was not easy to grasp where point 4 of the abstract was based on. \n2) In the first footnote, small nit is that, in practice one should not invert matrix but just do a linear solve for better numerical stability and efficiency (still O(N^3) but with better constant)\n3) In section 3, there seems to be no bias. Are NTK and NNs considered in this work contain no bias? Or is bias ignored for ease of presentation? \n4) Nit p4 first paragraph in section 4 : multiplayer -> multilayer\n5) Regards to NTK initialization performing better than standard He initialization: It was observed in [3] that for multilayer perceptron both parameterization is on-par but for CNN or WideResNet case standard parameterization performed significantly better.\n6) Note that similar to analysis in section 5,  for CIFAR-10 with fully connected model [1] shows that for all dataset size(100-45k) NNGP performs better than trained neural networks.\n7) One may worry that ResNet-34 is not properly tuned as most hyperparameters were fixed for large dataset. \n8) Regards to hyperparameters for NTK, is there a consistent trend one could find regards to L\u2019? What percentage of tasks that NTK performed well actually have a high L\u2019?\n9) To help the readers, I would suggest adding a little more description on statistics used for comparison as well as what VOC07 task entails. \n\n[1] Jacot et al., Neural Tangent Kernel: Convergence and Generalization in Neural Networks, NeurIPS 2018\n[2] Lee et al., Deep Neural Networks as Gaussian Processes, ICLR 2018\n[3] Park et al., The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study, ICML 2019\n\nEDIT AFTER AUTHOR RESPONSE:\nI have read the response from authors. I appreciate all the efforts to improve the paper. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833322671, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Reviewers"], "noninvitees": [], "tcdate": 1570237730449, "tmdate": 1575833322688, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review"}}}, {"id": "Sklj47KpFS", "original": null, "number": 2, "cdate": 1571816243293, "ddate": null, "tcdate": 1571816243293, "tmdate": 1574485601995, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper evaluates the empirical power of neural tangent kernel (NTK) on small-data tasks. The authors demonstrate the superior performance of NTK for classification/regression tasks on UCI database, small CIFAR-10 dataset and VOC07 testbed.\n\nOverall, this paper is well written and organized. The experimental results are also quite interesting. Besides, some questions and comments are as follows:\n\nOne of the baseline algorithms in Table 1 is NN with NTK initialization. However, this paper does not give the formal definition of NTK initialization.\n\nIn Figures 1-2, it can be observed that NTK cannot universally outperform baselines on all dataset. For some dataset, NTK can be worse than baselines but for some other dataset, NTK can be significantly better than baselines. Therefore, I would like the authors to briefly discuss which kind of data can be more efficiently learned through NTK or other training algorithms.\n\nIn Tables 2-5, it can be observed that for CIFAR10 dataset, increasing the number of layers leads to higher test accuracy. But for VOC07, one can observe the opposite thing. Is there any explanation for this phenomenon?\n\nThe authors should provide a clear description of the experimental setting. For example, do you use batch normalization/weight decay in ResNets? For training NN, which optimization algorithms do you use? Do you use learning rate decay? \n\n======================\nAfter reading authors' response:\n\nThanks for your response, I would like to keep my score.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833322671, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Reviewers"], "noninvitees": [], "tcdate": 1570237730449, "tmdate": 1575833322688, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review"}}}, {"id": "HJeJkFCisr", "original": null, "number": 4, "cdate": 1573804246773, "ddate": null, "tcdate": 1573804246773, "tmdate": 1573804246773, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment", "content": {"title": "General Response and Revision Summary", "comment": "We thank all reviewers for the positive reviews. \nWe have revised our paper to fix typos and add clarifications according to reviewers\u2019 suggestions. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl8sJBYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1915/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1915/Authors|ICLR.cc/2020/Conference/Paper1915/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149034, "tmdate": 1576860543965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment"}}}, {"id": "B1xFj_Rijr", "original": null, "number": 3, "cdate": 1573804192887, "ddate": null, "tcdate": 1573804192887, "tmdate": 1573804192887, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "r1eG5udtKS", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your positive review. We have revised our paper according to your suggestion. Regarding your comment \u201cNTK tunes one more parameter (L\u2019) than NNs\u2026\u201d: Since training all layers in NN is the standard practice, we did not fix the first $L\u2019$ layers. Also note that for experiments on UCI, more hyper-parameters do not necessarily give better performance because we used 4-fold cross-validation."}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl8sJBYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1915/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1915/Authors|ICLR.cc/2020/Conference/Paper1915/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149034, "tmdate": 1576860543965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment"}}}, {"id": "rJgJBORiiH", "original": null, "number": 2, "cdate": 1573804087293, "ddate": null, "tcdate": 1573804087293, "tmdate": 1573804087293, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "Sklj47KpFS", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your positive review. Please find our response to your comments.\n1.\tNTK initialization means a neural network with parameterization defined in Equation 2 with all weighted being initialized to be i.i.d. $\\mathcal{N}(0, 1)$. We have added a sentence after Equation 2 to clarify this.\n2.\t\u201cIn Figures 1-2, it can be observed \u2026..\u201d There is no clear trend on which dataset NTK can be better than other classifiers. We believe that investigating on which dataset NTK gives better performance requires more domain knowledge. Some analyses on pairwise comparisons: NTK vs. RF and NTK vs. Gaussian kernel, are provided in Section 4.2.\n3.\t\u201cIn Tables 2-5, it can be observed that \u2026\u2026\u201d Note for Tables 2-5, CNTKs are used on top of raw images, so to achieve better performance, one needs to use multi-layer CNTKs to extract higher-level features. On the other hand, CNTKs on VOC07 are used on top of extracted features from ResNet-50, which are already high-level features. Therefore, shallow CNTKs suffice for this case.\n4.\tWe have stated the experiment details in the third paragraph in Section B.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl8sJBYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1915/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1915/Authors|ICLR.cc/2020/Conference/Paper1915/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149034, "tmdate": 1576860543965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment"}}}, {"id": "SJg-qDAjjS", "original": null, "number": 1, "cdate": 1573803912743, "ddate": null, "tcdate": 1573803912743, "tmdate": 1573803912743, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "SJx5zKqaKS", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment", "content": {"title": "Response ", "comment": "Thank you for your positive review. We have revised our paper according to your comments. Please find our response to your questions below. \n-\t\u201cpoint 4 in the abstract\u201d:\nFor point 4, we mainly refer to Figure 2(b). We found \"There is no dataset the one classifier is significantly better than the other\".\n-\t\u201cBias in NTKs and NNs\u201d:\nWe did not add bias in NTKs and NNs.\n-\t\u201cResNet-34 is not properly tuned\u201d\nWe agree but note that there is not good way to tweak large nets on small datasets. Also note in the small data regime ($n=10$ to $n=320$), CNTK with 5,8,11 and 14 layers all beat ResNet.\n-\t\u201cis there a consistent trend one could find regards to $L\u2019$\u201d?\nWe did not find a consistent trend. We did not try very deep NTKs ($L \\le 5$ and $L\u2019 \\le 4$ in our experiments).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl8sJBYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1915/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1915/Authors|ICLR.cc/2020/Conference/Paper1915/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149034, "tmdate": 1576860543965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Authors", "ICLR.cc/2020/Conference/Paper1915/Reviewers", "ICLR.cc/2020/Conference/Paper1915/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Comment"}}}, {"id": "r1eG5udtKS", "original": null, "number": 1, "cdate": 1571551369807, "ddate": null, "tcdate": 1571551369807, "tmdate": 1572972407173, "tddate": null, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "invitation": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "[Summary]\nThis paper performs an extensive empirical evaluation of Neural Tangent Kernel (NTK) classifiers---kernel methods that theoretically characterize infinitely wide neural nets---on small-data tasks. Experiments show that NTK classifiers (1) strongly resemble the performance of neural nets on small-data tasks, (2) can beat prior benchmark methods such as Random Forests (RF) on classification tasks in the UCI dataset, and (3) can also outperform standard linear SVM on a few-shot learning task.\n\n[Pros]\nThe question considered in this paper is well motivated, and a very natural extension of Lee et al. (2019) and Arora et al. (2019a). These papers show that NTK performs well on (relatively) large benchmark tasks such as CIFAR-10 but is still a bit inferior to fully trained neural nets. On the other hand, for small-data tasks, the relationship is reversed --- neural nets are slightly inferior to more traditional methods such as random forests (e.g. from Fernandez-Delgado et al. 2014) and Gaussian kernel SVMs. As the NTK gives a limiting characterization for wide neural nets, it is a sensible question to test the performance of NTK on these small datasets, and see if they can improve over neural nets and compare more favorably against the traditional methods.\n\nThe experimental results, in my perspective, is a reasonably convincing evidence that the resemblance between NTK and NN on small-data tasks is stronger than on larger tasks such as CIFAR-10, which agrees with the NTK theory. In addition to the UCI datasets, the paper also tries out NTK in a few-shot learning task and show that SVM with the convolutional NTK does better than the linear SVM as the few-shot learner. I am less familiar with few-shot learning though so am not entirely sure about the strength of this part.\n\nThe paper is well-written and delivers its messages clearly. The results and discussions are easy to follow.\n\n[Cons, and suggestions]\nThe message that \u201cNTK beats RF\u201d seems a bit delicate to me, specifically considering the fact that the average accuracies of (NTK, NN, RF) are all pretty close but the Friedman rank comparison says NTK > RF > NN (somewhat more significantly). This implies the difference between all these methods has to be small and it\u2019s only that NTK happens to win on more tasks. In addition, NTK tunes one more parameter (L\u2019) than NNs, so I guess perhaps NNs can also be tuned to outperform RF in the rank sense if we also tune L\u2019 (by fixing the bottom L\u2019 layers to be not trained) in NNs?\n\nAlso, it would be better if the authors could provide a bit more background on the metrics used in the UCI experiments -- for example, the Friedman rank is not defined in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1915/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["arora@cs.princeton.edu", "ssdu@ias.edu", "zhiyuanli@cs.princeton.edu", "rsalakhu@cs.cmu.edu", "ruosongw@andrew.cmu.edu", "dingliy@cs.princeton.edu"], "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "authors": ["Sanjeev Arora", "Simon S. Du", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang", "Dingli Yu"], "pdf": "/pdf/f377385ec3403aedd89a258487418ac6eda2dc27.pdf", "TL;DR": "We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.\n1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.\n2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.\n3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.\n4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output.", "keywords": ["small data", "neural tangent kernel", "UCI database", "few-shot learning", "kernel SVMs", "deep learning theory", "kernel design"], "paperhash": "arora|harnessing_the_power_of_infinitely_wide_deep_nets_on_smalldata_tasks", "code": "https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS", "_bibtex": "@inproceedings{\nArora2020Harnessing,\ntitle={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},\nauthor={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl8sJBYvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5665acf89ec36323befb93a25ba7d749465db07f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl8sJBYvH", "replyto": "rkl8sJBYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1915/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833322671, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1915/Reviewers"], "noninvitees": [], "tcdate": 1570237730449, "tmdate": 1575833322688, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1915/-/Official_Review"}}}], "count": 9}