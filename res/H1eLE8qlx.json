{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396486573, "tcdate": 1486396486573, "number": 1, "id": "r1yInMLug", "invitation": "ICLR.cc/2017/conference/-/paper294/acceptance", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Most of the reviewers agreed that the proposed budgeted options framework was interesting, but there were a number of serious concerns raised about the work. Many of the reviewers found the assumptions of the approach to be somewhat odd, and while the particular formulation in the paper was generally assessed as novel, it has connections to a number of previous works that were not explored in detail. Finally, the experimental evaluation is conducted on simple tasks with few comparisons, so it is very difficult to make concrete conclusions about how well the method works."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396487126, "id": "ICLR.cc/2017/conference/-/paper294/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396487126}}}, {"tddate": null, "tmdate": 1484847516511, "tcdate": 1484847516511, "number": 5, "id": "HyNitOAIg", "invitation": "ICLR.cc/2017/conference/-/paper294/public/comment", "forum": "H1eLE8qlx", "replyto": "Syo2DYy4g", "signatures": ["~Aur\u00e9lia_L\u00e9on1"], "readers": ["everyone"], "writers": ["~Aur\u00e9lia_L\u00e9on1"], "content": {"title": "deletion of Bi-POMDP and precisions", "comment": "Thank you for your review.\n\nWe deleted the Bi-POMDP framework (see comment made to reviewer AnonReviewer1)\n\nConcerning lines 4 & 6, we modified the paper in that direction:\n\u2022  sigma_t is equal to 0 or 1, randomly taken from P(sigma_t=1) = sigmoid(W*z_{t-1} + W*a_{t-1} + W*x_t) (in our experiments) using a Bernoulli distribution\n\u2022  o_t = f(y_t) = relu(W*y_t) in our experiments\n\u2022  In both cases, the  W  corresponds to different matrix of parameters (see the appendix of the paper)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287635125, "id": "ICLR.cc/2017/conference/-/paper294/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1eLE8qlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper294/reviewers", "ICLR.cc/2017/conference/paper294/areachairs"], "cdate": 1485287635125}}}, {"tddate": null, "tmdate": 1484847440310, "tcdate": 1484847440310, "number": 4, "id": "H1OLY_08g", "invitation": "ICLR.cc/2017/conference/-/paper294/public/comment", "forum": "H1eLE8qlx", "replyto": "r1bD3ZGBl", "signatures": ["~Aur\u00e9lia_L\u00e9on1"], "readers": ["everyone"], "writers": ["~Aur\u00e9lia_L\u00e9on1"], "content": {"title": "comments and deletion of Bi-POMDP", "comment": "Thank you for your detailed review. Here are some comments and modifications we have made based on your remarks:\n\n1.   We deleted the Bi-POMDP framework which was confusing; the BONN model is now just described as a model that can ask for an additional costly observation y_t.\n\n2.   The interest of using recurrent network instead of  classical feedforwards nets is to keep memory of the lastly chosen options in order to guide the choice of the actions.\n\n3.   We fully understand that a comparative evaluation of BONN w.r.t existing options model would be interesting. But such a comparison is not trivial since existing models are not based on the use of two different observations x_t and y_t. Even if we could compare models with the same input setting, such a comparison would necessitate to be able to evaluate if the options chosen by a model are better than the one chosen by another model (the two models being able to both solve the RL task e.g moving to a goal position). As far as we know, there is no such comparative study in the option literature.\nThe model the closest to our is the one presented in Bacon & Precup ,2015b which is described in a short paper without enough details to be implemented correctly"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287635125, "id": "ICLR.cc/2017/conference/-/paper294/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1eLE8qlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper294/reviewers", "ICLR.cc/2017/conference/paper294/areachairs"], "cdate": 1485287635125}}}, {"tddate": null, "tmdate": 1484847328027, "tcdate": 1484847328027, "number": 3, "id": "r1_1YORLe", "invitation": "ICLR.cc/2017/conference/-/paper294/public/comment", "forum": "H1eLE8qlx", "replyto": "Sy98yIwre", "signatures": ["~Aur\u00e9lia_L\u00e9on1"], "readers": ["everyone"], "writers": ["~Aur\u00e9lia_L\u00e9on1"], "content": {"title": "design of observations", "comment": "First, thank you very much for your detailed and interesting review.\n\nConcerning the design of observations x_t and y_t, you\u2019re right in saying that the choice of them is crucial and we added a small paragraph in the paper pointing out this aspect. However, we would like to say that, first, on any RL problem, one can consider the setting where y_t corresponds to the full observation while x_t is an empty observation, the agent learning to acquire observations at relevant time steps. Even with this simple configuration, and as shown in the experiments, our algorithm exhibits an interesting ability to discover options. Second, in many cases, the definition of x_t and y_t is quite natural or at least easier than defining the full catalogue of possible options (for example x_t is the internal state of the agent \u2013 the observation of the room in our case \u2013 and y_t is the full knowledge of the task to solve \u2013 the goal position in our case). The paper by Heess et al. has now been added in the related work section and is effectively based on the same kind of decomposition of the observation (without budget in their case).\n\nWhen x_t corresponds to the empty observation, the option computed from y_t can indeed be seen as a (stochastic) macro action. This has been explained in the new version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287635125, "id": "ICLR.cc/2017/conference/-/paper294/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1eLE8qlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper294/reviewers", "ICLR.cc/2017/conference/paper294/areachairs"], "cdate": 1485287635125}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484847183821, "tcdate": 1478284359917, "number": 294, "id": "H1eLE8qlx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1eLE8qlx", "signatures": ["~Ludovic_Denoyer1"], "readers": ["everyone"], "content": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483329361728, "tcdate": 1483329361728, "number": 4, "id": "Sy98yIwre", "invitation": "ICLR.cc/2017/conference/-/paper294/official/review", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer4"], "content": {"title": "Interesting paper, some issues", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors study the problem of discovering options for reinforcement learning. They introduce the Bi-POMDP model, which is a POMDP where the observations are structured as a pair of elements, with the first element only available to the option-choosing component (the \"option level\") and the second element only to the action-choosing component (the \"action level\") and the termination component (the \"acquisition model\"). They also detail the BONN learning model, which consists of three artificial neural network that implement these three components. Finally, they suggest optimizing a tradeoff between the value achieved by the model and the cost of switching between options (the \"cognitive effort\"), and demonstrate this approach in three simple domains: Cart-Pole, Lunar Lander and two variants of a grid-world maze.\n\nThe paper is interesting, and adds considerably to the increasing body of research in hierarchical reinforcement learning (HRL). I found no critical flaws in the paper, but also no high-impact insights or impressive improvements. This paper offers some good ideas that are moderately novel and may advance the field, but has some issues.\n\n\nThe first issue is that it is unclear how much easier it is to compose Bi-POMDPs than hand-crafted options or subgoals. If Bi-POMDPs are to alleviate the design costs of using HRL with human-defined structure, one needs to show that the splitting of observations into two elements (x, y) is easier to do well enough.\n\nFor example, in Section 5 the authors are correct in pointing out that sequences of actions (\"macro-actions\") are open-loop, and therefore not as expressive as closed-loop options. However, by setting x empty in all but one experiment, the authors also restrict themselves to open-loop sub-policies, albeit stochastic ones. Such sub-policies may be sufficiently expressive for the simple domains in this paper, but this is unlikely so in more realistic domains.\n\nIn the MAZE_2 domain, x is the agent's position relative to the current room. Designing this domain-specific observation model requires domain knowledge, arguably no less than designing relevant subgoals. It is hard to judge the effectiveness of this approach without design principles for these domain-specific observation models, and more realistic experiments to evaluate their quality.\n\nFinally, it is revealing that the drive for hierarchy is only achieved by limiting x. In Section 3.3 the authors mention that acquisition of y is \"crucial for discovering a good policy: an agent only using the observations x_t would be unable to solve the task\", which suggests that the design choice of x directly impacts one side of the trade-off between the value and the cognitive effort. Such an important factor should be addressed explicitly.\n\nThe authors may be interested in the paper \"Learning and Transfer of Modulated Locomotor Controllers\" (Heess et al., 2016), which has a similar split observation model, and suffers from the same issue.\n\n\nThe second issue is that the option space seems to be expressive enough to represent y with high fidelity. If this is the case, and if the learned option model indeed maintains a good image of y, then the algorithm is really solving a different problem: reinforcement learning with costly observability of y. This means that it learns how to act given a stale value of y and when to refresh it. It should then be framed accordingly and compared with the relevant literature. It may or may not be as interesting or novel.\n\nIn contrast, the standard options framework calls for compression of y into the choice of option o (usually in a small finite space). The agent should learn to extract subtask-relevant information from y, in a way that generalizes to unseen states or subtasks.\n\nThat said, the embedding of y in the option space can be interesting in itself, even if it is lossless (1-to-1). Unfortunately, no such analysis was offered by the authors.\n\n\nMinor issues:\n\n- In 3.3: error in citation, (?) appears instead.\n\n- In 4.1: it is confusing to say that \"the environments are more stochastic\" when epsilon is increased. It is the agent's policy, not the environment, that becomes more stochastic, which is useful for exploration. If this hurts performance, some discussion is needed of why too much exploration is detrimental to learning. In particular, the paper does not make explicit the number of iterations in the experiments, and it is not clear whether learning with larger epsilon is worse after some fixed number of iterations or asymptotically.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483329362361, "id": "ICLR.cc/2017/conference/-/paper294/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483329362361}}}, {"tddate": null, "tmdate": 1482984536849, "tcdate": 1482984536849, "number": 3, "id": "r1bD3ZGBl", "invitation": "ICLR.cc/2017/conference/-/paper294/official/review", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer1"], "content": {"title": "Interesting ideas, but the paper requires more work", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents an approach to constructing hierarchical RL representations which relies on assuming agents that need to spend cognitive effort in order to choose their actions.  The paper p[roposes a specific way of formulating option construction via what they call a \"Bi-POMDP\". This idea is potentially very interesting, plausible form a cognitive science point of view, and definitely deserves attention. However, there are some problems which do not make the paper acceptable in its current form. I am listing them here in order of importance.\n1. It is not clear from the description why a Bi-POMDP is not a POMDP. POMDPs allow for vector-based observations.  Suppose the observation vector is (x_t, \\sigma_t * y_t). This seems like it would result in a POMDP which is identical to the proposed model. The paper should include an example of a Bi-POMDP which is *not* a POMDP, or be revised to use specific POMDP terminology (see eg the use of augmented MDPs in hierarchical RL, which *are MDPs* but do not work in the original state space)\n2. The paper make some specific assumptions about the abstractions (eg determinism in certain places). It is not clear why these are needed at all. Similarly, there are some very specific assumptions regarding the form of the approximations used (Relu, GRUs etc).  Are these necessary? In principle one could implement the ideas in the paper with other, simpler architectures. Was this the first set of choices, or was it arrived at after some experimentation? It is important to understand how much of the performance achieved is due to the specific (fairly powerful) architectures and what one could get through simpler means (eg, feedforward nets)\n3. The paper seems quite similar in spirit to Bacon & Precup, 2015b; in fact, it seems that the use of a value function or model that they discuss is a way to provide a y_t. However, there is no direct comparison to that approach. Since it is very related, it would be useful to perform some of those same experiments. Also, their paper works entirely in the MDP, not POMDP framework, so some clarification is needed here regarding the use of POMDPs instead.\n4. The choice of domains is somewhat limited to simple tasks, while some of the recent approaches in hierarchical RL use more complex domains (Atari, Minecraft etc). Ideally, the experiments should be extended to some of these more complex tasks.\n5. What are the theoretical properties of the proposed approach? Eg, is the proposed algorithm convergent? If Bi-POMDP is a POMDP, then one should be able to leverage POMDP results to build some theory here. If it is not a POMDP, then we need some understanding of how easy/hard a Bi-POMDP is to solve\n6. The paper contains many grammar problems and some broken references, and should be proof-read thoroughly \nIn summary, while the proposed approach is quite interesting and definitely worth exploring, the paper is not ready for publication in its current form.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483329362361, "id": "ICLR.cc/2017/conference/-/paper294/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483329362361}}}, {"tddate": null, "tmdate": 1482864748710, "tcdate": 1482864748710, "number": 3, "id": "HJHd_4eHe", "invitation": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer1"], "content": {"title": "No questions", "question": "No questions"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1485138820003, "id": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1485138820003}}}, {"tddate": null, "tmdate": 1481981148075, "tcdate": 1481981148075, "number": 2, "id": "r1VyahzNg", "invitation": "ICLR.cc/2017/conference/-/paper294/official/review", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer3"], "content": {"title": "Learning options in reinforcement learning ", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an approach to learning hierarchical actions or options in reinforcement learning using the so-called BONN model (for budgeted options with neural networks). The approach is an interesting mixture of the old and the new. Some ideas seem very related to previous work in the literature, such as variants of hidden Markov models proposed by Hung Bui and others (abstract HMM, hierarchical POMDP by Theocharous et al., IROS 2005; Murphy et al., NIPS, ICRA). \n\nThe major difference is that unlike the prior work using a graphical model, this paper uses a gated recurrent network neural model to implement the learning of options from data. The approach is based on minimizing some quantity called the \"cognitive effort\", but this is confusingly explained, and not very precise. The basic idea here is to define a budget that modified the immediate reward, and so its minimization is viewed as minimizing cognitive effort. The approach seems a bit ad hoc. \n\nExperiments are reported on a variety of simple discrete and continuous control benchmark domains. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483329362361, "id": "ICLR.cc/2017/conference/-/paper294/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483329362361}}}, {"tddate": null, "tmdate": 1481770931270, "tcdate": 1481770931261, "number": 1, "id": "Syo2DYy4g", "invitation": "ICLR.cc/2017/conference/-/paper294/official/review", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper tackles the very important problem of learning options from data. The introduction of the budget constraint is an interesting twist on this problem, which I had not seen before (though other methods apply other constraints.)\n\nI must say I\u2019m not very convinced by the need to introduce the Bi-POMDP framework, where the conventional POMDP framework would do.  In discussions, the authors suggest this makes for simpler comparison with RL models, but I find that it rather obscures the link to POMDP models.\n\nThe proposed method makes an interesting contribution, distinct from the existing literature as far as I know.  The extension to discover a discrete set of options is a nice feature for practical applications.\n\nIn terms of the algorithm itself, I am actually unclear about lines 4 & 6.  At line 4, I don\u2019t know how \\sigma_t is computed. Can you give the precise equation?  At line 6, I don\u2019t know how the new option o_t is generated. Again, can you give the precise procedure?\n\nThe paper contains several empirical results, on contrasting simulated domains. For some of these domains, such as CartPole, it\u2019s really not clear that options are necessary. In my mind, the lack of comparison to other options learning methods is a limitation of the current draft.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483329362361, "id": "ICLR.cc/2017/conference/-/paper294/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483329362361}}}, {"tddate": null, "tmdate": 1481206656206, "tcdate": 1481206656201, "number": 2, "id": "ryOFjyP7g", "invitation": "ICLR.cc/2017/conference/-/paper294/public/comment", "forum": "H1eLE8qlx", "replyto": "rJwIc9JXl", "signatures": ["~Aur\u00e9lia_L\u00e9on1"], "readers": ["everyone"], "writers": ["~Aur\u00e9lia_L\u00e9on1"], "content": {"title": "Answers", "comment": "Dear reviewer,\n\nFirst of all, thank you for your questions.\nConcerning the definition of Bi-POMDP, you are right, Bi-POMDP can be mapped to a POMDP with different types of actions ('classic' actions and 'acquisition' actions). We decided to introduce the term Bi-POMDP to avoid to define different types of actions, each time step thus corresponding to choosing one 'classic action'. It has the advantage to allow a simpler comparison with classical RL models, and to clearly state that we consider different possible types of observations, at different prices. We have added a small paragraph making the connection between Bi-POMD and POMDP explicit.\n\nThe latent variable o_t computed with y_t in Fig.1 represents the option latent vector. It is indeed related to \\sigma_t : the model doesn\u2019t use y_t if \\sigma_t=0, and uses it (and compute o_t) if \\sigma_t=1. We modified the legend of Fig.1  to make things clearer.\n\nWe are not sure to understand your remark on Algorithm 2. Line 6, \u201coption level\u201d, means that the model acquires the observation y_t, and compute a new option o_t based on y_t. We replaced \u201cgenerate a new option o_t\u201d by \u201ccompute a new option o_t=f(y_t)\u201d that is maybe more explicit.\nLine 7 (if it is what you meant), \u201cactor level\u201d, the state of the actor z_t is then initialized with the new option o_t and the other observation x_t. \n\nThe model has been compared with the classical recurrent policy gradient algorithm using a classical RNN architecture without the option level, but not with DQN that will certainly obtain the same kind of results on these tasks.\n\nAs explained in the paper, the lambda value is fixed to 0 at the beginning of the training and then slowly increased. The performances are not very dependant on the increasing speed for \\lambda which mainly change the learning speed.\n\nWe added the year and publisher for Bacon & Precup, we\u2019re sorry for the oversight."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287635125, "id": "ICLR.cc/2017/conference/-/paper294/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1eLE8qlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper294/reviewers", "ICLR.cc/2017/conference/paper294/areachairs"], "cdate": 1485287635125}}}, {"tddate": null, "tmdate": 1481206544335, "tcdate": 1481206544329, "number": 1, "id": "H1dzjyP7x", "invitation": "ICLR.cc/2017/conference/-/paper294/public/comment", "forum": "H1eLE8qlx", "replyto": "BkO51g1Qx", "signatures": ["~Aur\u00e9lia_L\u00e9on1"], "readers": ["everyone"], "writers": ["~Aur\u00e9lia_L\u00e9on1"], "content": {"title": "AHMM", "comment": "Thank you for your questions and remarks.\n\nConcerning the terminology, our goal was not to redefined well known concepts. We modified Section 2.2 to use standard terminology and to make the section clearer.\n\nConcerning the comparisons with AHMM architectures, we agree that our model has connections with the AHMM and added a paragraph on this point in the related work.  However, there are still many differences between BONN and AHMM:\n\n- contrary to others options models (and AHMM), BONN doesn\u2019t learn a discrete set of options but \u201ccontinuous vectors\u201d computed with neural networks. In BONN each option o_t is a vector computed based on y_t.\n\n- in AHMM, the state space is partitioned into a sequence of partitions P1, P2... corresponding to the levels of abstractions (section 3.2 of \u201cPolicy Recognition in the AHMM\u201d by Hung Bui et al.), which corresponds to our 2 different observations x_t and y_t. Then, each partition Pi is partitioned into regions R1, R2\u2026 and for each region a set of policies is defined. In BONN, only the space partition is defined (with the two levels xt and yt). In some cases, like in the rooms experiments, we can considered that the state space is partitioned into different regions like for AHMM (as the observations brutally change when changing room). However, BONN can be more general as it uses directly the observation x_t and y_t : it would also work for a robot that see some meters in front of him (this observation would be x_t) and do sometimes a scan of the room (the observation would be y_t). \nBONN would work exactly the same in this case, contrary to AHMM that would need to define regions. Moreover, in AHMM one must fix the number of policies in each region, and they terminates when the agent exit the region, which is not the case for BONN (where we really don\u2019t fix anything concerning the policies, and a policy could begin in a room and terminate two rooms after).\n\n- in BONN, options are not pre-defined by a user like in AHMM, but are discovered while minimizing the number of time a new option is computed (=\u2019cognitive effort\u2019)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287635125, "id": "ICLR.cc/2017/conference/-/paper294/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1eLE8qlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper294/reviewers", "ICLR.cc/2017/conference/paper294/areachairs"], "cdate": 1485287635125}}}, {"tddate": null, "tmdate": 1480727119204, "tcdate": 1480727119198, "number": 2, "id": "rJwIc9JXl", "invitation": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer2"], "content": {"title": "Questions", "question": "Why do you state that Bi-POMDP are more general than POMDP?  It seems every problem you describe for the Bi-POMDP case can be equally well modelled in a standard POMDP, introducing an action for choosing the sense y.\n\nSimilarly, the augmented reward (Eqn 3) seems like it can be re-written as a standard POMDP reward function, where there is a cost to applying the \u201cobserve y\u201d action.\n\nWhy is y in Fig.1 modelled as a latent variable?  This seems directly related to \\sigma.\n\nCan you add precise specification of lines 6 & 6 in Algorithm 2?\n\nDid you perform empirical comparison to any other option learning method, e.g. Bacon & Precup?  Also, any comparison to other method that doesn\u2019t use option, e.g. standard DQN?\n\nHow much data is necessary to fit the parameter \\lambda?\n\nWhat is the year and publisher for Bacon & Precup?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1485138820003, "id": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1485138820003}}}, {"tddate": null, "tmdate": 1480683407723, "tcdate": 1480683407716, "number": 1, "id": "BkO51g1Qx", "invitation": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "signatures": ["ICLR.cc/2017/conference/paper294/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper294/AnonReviewer3"], "content": {"title": "Standardize terminology and relate your work to abstract HMMs", "question": "\nFirst, please refrain from making up confusing and obfuscating terminology for concepts that are well defined using standard definitions. For example, equation 1 and the preceding equation need to be discarded. The correct and standard terminology to use here is that R(s1, a1, ....) is known as the discounted sum of rewards, or the return. It is confusingly termed here as the \"reward\". If R is the reward, what is r(s_t,a_t)? Also, J(\\pi) is confusingly NOT the expected reward, but the expected sum of rewards (the little r's, not the big R). The little \"r\" is also called \"reward\" and confuses your definition of R and r! Why make up terms on your own when the literature has standard terms, namely the value function, the return, and the discounted sum of reward etc.? (see Puterman, MDP textbook, or Sutton/Barto RL text, or Bertsekas/Tsitsiklis, NDP book). \n\nSecond, please cite and discuss in detail the work of Hung Bui et al., (Journal of AI Research) on the abstract hidden Markov model (or AHMM). It is *exactly* equivalent to your BONN architecture. How does your work differ from AHMM and please provide a detailed comparison, both experimentally and theoretically. There is also prior work on learning AHMMs, see Johns et al., AAAI 2005. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "pdf": "/pdf/8eebb654909c2389f3176c1b52a8e745402a855f.pdf", "TL;DR": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "paperhash": "l\u00e9on|options_discovery_with_budgeted_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["lip6.fr"], "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1485138820003, "id": "ICLR.cc/2017/conference/-/paper294/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper294/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper294/AnonReviewer3", "ICLR.cc/2017/conference/paper294/AnonReviewer2", "ICLR.cc/2017/conference/paper294/AnonReviewer1", "ICLR.cc/2017/conference/paper294/AnonReviewer4"], "reply": {"forum": "H1eLE8qlx", "replyto": "H1eLE8qlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper294/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1485138820003}}}], "count": 14}