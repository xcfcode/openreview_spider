{"notes": [{"id": "rJgCOySYwH", "original": "rJlhmQ0_DH", "number": 1824, "cdate": 1569439606255, "ddate": null, "tcdate": 1569439606255, "tmdate": 1577168232997, "tddate": null, "forum": "rJgCOySYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "a_0cc8A99b", "original": null, "number": 6, "cdate": 1577068589235, "ddate": null, "tcdate": 1577068589235, "tmdate": 1577068589235, "tddate": null, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment", "content": {"title": "Updated code and Further studies", "comment": "Updated code and further studies, please see:\n\nhttps://github.com/Wanggcong/SolutionSimilarityLearning\n\nThis is one of my favorite works! Suggestions are welcomed!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgCOySYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1824/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1824/Authors|ICLR.cc/2020/Conference/Paper1824/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150363, "tmdate": 1576860554029, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment"}}}, {"id": "k0yT7dKj2H", "original": null, "number": 1, "cdate": 1576798733365, "ddate": null, "tcdate": 1576798733365, "tmdate": 1576800903077, "tddate": null, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Decision", "content": {"decision": "Reject", "comment": "This paper tackles an important problem: understanding if different NN solutions are similar or different. In the current form, however, the main motivation for the approach, and what the empirical results tell us, remains unclear. I read the paper after the updates and after reading reviews and author responses, and still had difficulty understanding the goals and outcomes of the experiments (such as what exactly is being reported as test accuracy and what is meant by: \"High test accuracy means that assumptions are reasonable.\"). We highly recommend that the authors revisit the description of the motivation and approach based on comments from reviewers; further explain what is reported as test accuracy in the experiments; and more clearly highlight the insights obtain from the experiments. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722616, "tmdate": 1576800273960, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Decision"}}}, {"id": "S1x3MHlHiH", "original": null, "number": 2, "cdate": 1573352724049, "ddate": null, "tcdate": 1573352724049, "tmdate": 1573438130573, "tddate": null, "forum": "rJgCOySYwH", "replyto": "SJgHjqzIqr", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the kind review and suggestions. We have revised the paper according to the suggestions and would like to answer the reviewer\u2019s questions as follows. \n\nQ1: What are the findings of the paper? The authors make the assumption that weights across the same layer (say layer #2) are somehow always going to learn similar values?\nA1: We assume that weights across the same chain (say Layer-#1-#2-#3 ) could be affected by the permutation problem in different runs. A chain is defined as a sequence of layers of a neural network that begins with the first layer. Please find the definition in Page 4 of the previous manuscript. That means we compare two nets chain by chain:\nNet1(Layer#1)   <--->  Net2(Layer#1))\nNet1(Layer#1-2)   <--->  Net2(Layer#1-2))\nNet1(Layer#1-2-3)   <--->  Net2(Layer#1-2-3)\n...\nWe also set a baseline that compares two nets layer by layer:\nNet1(Layer#1)   <--->  Net2(Layer#1))\nNet1(Layer#2)   <--->  Net2(Layer#2))\nNet1(Layer#3)   <--->  Net2(Layer#3)\n...\nFigs. 2, 3 and 4 (Section 4.1, 4.2 and 4.3) show that the chain alignment rule of the proposed method works. Fig. 1 (Section 2.2) shows the motivation of the chain alignment rule. \n\nWhat are the findings of the paper? In Section 3.1, we introduce a \u201clearning tells the truth\u201d principle. Given a specific assumption, we label a dataset based on the assumption. The dataset is split into a training set and a test set. If a trained model achieves high accuracy on the test set, we say the assumption is reasonable, and vice\u00a0versa. The findings are listed as follows\n1)In Section 4.1, 4.2 and 4.3, \n--Assumption: the local solution (weights) of the same learning tasks share a highly similar solution structure even though neural networks are non-convex functions.\n--Setting: based on this assumption, for each task (different runs by SGD), we assign a label to these local solutions. The solution set is split into a training set and a test set. \n--Learning and validation: using the weight alignment and projection can achieve high accuracy (98~99% accuracy) on the test set.\nConclusion: the assumption holds and the function feature learning works. \n\n2)In Section 4.4, \n--Assumption:local solutions of different network depths (PlainNet-5 and PlainNet-6) of a learning task share a similar structure. (Learning and validation: yes, high classification/retrieval accuracy)\n--Assumption: different network structures (plain and residual) share similar local solutions of a learning task. (Learning and validation: partially correct, residual nets affect the weights to some extent, moderate classification accuracy)\n--Assumption: different activation functions (ReLU vs. LeakyReLU) lead to similar local solutions of a learning task. (Learning and validation: yes, high accuracy).\n--Assumption: SGD and Adam optimizers lead to similar local solutions of a learning task. (Learning and validation: no, low accuracy).\n\nExcept several findings are listed above, some other potential benefits are given in the response to Reviewer #2.\n\nQ2. What the term solution class means, or what the authors want the reviewer to believe it means. Also solution label? \nA2: Solution class or solution label is assigned based on assumptions that local solutions of a learning task (weights in different runs) share the same class while local solutions of different learning tasks have different class labels. Local solutions (trained weights of neural networks) and their assumptive labels can be regarded as data points, which are used for function feature learning. We have provided a clearer definition for solution label and class in Section 3.1. In Section 4.1~4.3 shows the detail for the generation of solution sets.  \n\nQ3. Can you elaborate more on the goals of the experiments? \nA3: We have added these in the introduction of Section 4. For each experiment, we have added some words to clarify the goals of the experiments. Specifically, Section 4.1~4.3 assume that under the SGD optimizer condition, local solutions of each task (different runs) are highly similar. In Section4.4, we change one factor of the baseline to form one new setting each time. Under a new condition, we investigate if the assumption in Section 4.1~4.3 still holds. We also study if the proposed function feature representation that is trained under the baseline condition is available under another condition.\n\n\nQ4: Can you elaborate the goal of local solution classification? \nA4:  We have added these in the introduction of Section 4. Local solution classification is to validate that under the label assumption the rule/knowledge learning from a training set also holds on the test set. Local solution retrieval aims to validate if function feature representation can be used for unseen solution classes, because solution classes between a training set and a test set are non-overlapping in the retrieval setting. These protocols follow image classification and retrieval and thus have the same motivation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgCOySYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1824/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1824/Authors|ICLR.cc/2020/Conference/Paper1824/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150363, "tmdate": 1576860554029, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment"}}}, {"id": "rkezXwxHiS", "original": null, "number": 3, "cdate": 1573353241529, "ddate": null, "tcdate": 1573353241529, "tmdate": 1573376869058, "tddate": null, "forum": "rJgCOySYwH", "replyto": "HygT-omAtB", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the comments and would like to answer the reviewer\u2019s questions as follows.\n\nQ1: I\u2019m not 100% sure about the usefulness of the method. More insights into neural networks?\nA1: We use the function feature representation together with the \u201cLearning tells the truth\u201d principle to validate some assumptions. We have polished Section 3.1 and Section 4 to make it clear.\n\nIn Section 4.1, 4.2 and 4.3, we validate that local solutions of the same learning tasks share a highly similar solution structure even though neural networks are non-convex functions. In Section 4.4, we found 1) local solutions of different network depths (PlainNet-5 and PlainNet-6) to a learning task share a similar structure. 2) Different network structures (plain and residual) partially share similar local solutions to a learning task. 3) Different activation functions (ReLU and LeakyReLU) lead to similar local solutions to a learning task. 4) SGD and Adam optimizers do not share similar local solutions to a learning task.\n\nDue to the non-convexity of the neural network, one could hardly know the properties of local solutions and even do not know if two given solutions are trained from the same learning task. This paper could take one small step towards this goal. Furthermore, one could use the chain aligned and projected solution to validate other useful assumptions. \n\nBesides, we would like to highlight the importance of weight similarity metric that could be under-explored. Weight similarity metrics could provide many potential benefits for machine learning:\n1)Transfer learning. Transfer learning aims to find similar tasks that contain overlapping knowledge to help the learning of the target task. A good weight similarity metric can tell if two learning tasks are transferable. The low similarity of two tasks leads to negative transfer while high similarity brings positive transfer.\n2)Ensemble learning. Ensemble learning aims to find a set of diverse learners to further boost performance. A good weight similarity metric can measure the similarity of a set of learners and select diverse ones for ensemble learning.\n3)Non-convex optimization. A good weight similarity metric could be used to remove redundant local solutions and discover the underlying relations between different local solutions, which provides better gradient descent direction.\n\nWe have added these as future works in the conclusion section.\n\n\nQ2: The authors used a neural networks - a black box model - to provide insights for other neural networks, also black box models?\nA2: We analyze the similarity of neural networks by using the chain alignment rule and a linear function (FC layer), but not a black-box model. One can use other traditional linear classifiers instead. We are not sure if we get the right point of the reviewer. If not, please correct us. \n\nQ3: One assumption from this paper that networks trained with different initializations for the same subtasks produce the same local solution is wrong. Therefore, I\u2019m not 100% sure whether the results produced from all the experiments are trustable.\n\nA3: Assumptions could be wrong under some conditions (e.g., ADAM optimizer) while reasonable under some conditions (eg., SGD). We use a \u201clearning tells the truth\u201d principle to validate assumptions. High accuracy means the assumptions are reasonable. To eliminate the reviewer\u2019s concerns, we remove that assumption and use the retrieval setting. We directly use aligned function features (without FC layer for projection, no learning process, normalized)  and cosine similarity to compute the similarity of aligned solutions. The solutions are generated like Section 4.1 and 4.2. We found that without that assumption, the results are still promising. We report these results as follows. \n\nTinyImageNet: \ncmc rank 1,5,10: 97.3,1,1 (using Chain 1)\ncmc rank 1,5,10: 98.8,99.8,1  (Chain 2)\nCifar100:\ncmc rank 1,5,10: 95.7,99.3,99.8 (Chain 1)\ncmc rank 1,5,10: 97.3,99.6,99.7 (Chain 2)\n\nThese show that without the learning process the similarity between aligned local solutions to a learning task is naturally higher than that of different learning tasks, leading to high retrieval results. We only compute the first two chains because weight space is too large (without projecting into the low dimension space).  \n\nWe would like to share our code and confirm all of the authors\u2019 information is removed. Some related absolute paths could be invalid. The code could take too much time to create a solution set (e.g., 5000 trained models). As a simple example, we suggest the reviewers focus on these files:\nA.Solution set generation:\n--train.py\n--model/mlp.py\nB.Solution classification/retrieval, includes chain alignment rule and linear projection\n-- train_sup.py\n--model/meta_model_mlp.py\n\nThe result is easy to reproduce since it is naturally a simple classification/retrieve problem. Anonymous code at: https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/"}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgCOySYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1824/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1824/Authors|ICLR.cc/2020/Conference/Paper1824/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150363, "tmdate": 1576860554029, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment"}}}, {"id": "HyxIbqgriS", "original": null, "number": 4, "cdate": 1573353982233, "ddate": null, "tcdate": 1573353982233, "tmdate": 1573359095663, "tddate": null, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment", "content": {"title": "Response to all reviewers", "comment": "We thank all the reviewers for their helpful comments (while we still have a missing review from R3). We have revised the paper as suggested by the reviewers, and summarize the major changes as follows: \n* Clearer explanations about several terminologies required by Reviewer2 are updated in Section 3.1. \n* Clearer goals of the experiments and local solution/retrieval required by Reviewer2 are added in Section 4.\n* Clearer explanations about the findings/insights of this paper required by Reviewer1 and Reviewer2 are updated in Section 3.1 and Section 4.\n* Discussions about the usefulness of the proposed method required by Reviewer1 are added in Section 5.\n* Discussions about the soundness of the proposed method required by Reviewer1.\nWe would like to ask for the reviewers\u2019 suggestions if it is allowed to have one more extra page to include more details and make the paper clearer. We targeted at 8 pages in the initial submission, but according to the reviewers\u2019 comments, it will be helpful to have more details in the main text.\n\nWe also try to eliminate each reviewer\u2019s concerns one by one, which can be seen in the corresponding response."}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgCOySYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1824/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1824/Authors|ICLR.cc/2020/Conference/Paper1824/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150363, "tmdate": 1576860554029, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Authors", "ICLR.cc/2020/Conference/Paper1824/Reviewers", "ICLR.cc/2020/Conference/Paper1824/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Comment"}}}, {"id": "HygT-omAtB", "original": null, "number": 1, "cdate": 1571859204993, "ddate": null, "tcdate": 1571859204993, "tmdate": 1572972418964, "tddate": null, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method called \u2018function feature learning\u2019 which do not learn the data distribution but the parameters distribution of several neural networks types. The main idea is to generate many weights from different NNs trained with different random initializations for different subtasks and use them as training data for \u2018function feature learning\u2019. The experiments were done on three different datasets.\n\nOverall, the idea is quite interesting and new. However, I\u2019m not 100% sure about the usefulness of the method. The authors claimed to provide more insights of neural networks with their method which I did not see when reading the paper. Furthermore, the authors used a neural networks - a black box model - to provide insights for other neural networks, also black box models. It sounds odd, doesn\u2019t it? Moreover, one assumption from this paper that networks trained with different initializations for the same subtasks produce the same local solution is wrong. Therefore, I\u2019m not 100% sure whether the results produced from all the experiments are trustable. \n\nIn sum, I rate this paper as a borderline paper and lean towards rejection due to several aforementioned uncertain points. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576601591355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Reviewers"], "noninvitees": [], "tcdate": 1570237731774, "tmdate": 1576601591370, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Review"}}}, {"id": "SJgHjqzIqr", "original": null, "number": 2, "cdate": 1572379292940, "ddate": null, "tcdate": 1572379292940, "tmdate": 1572972418913, "tddate": null, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "invitation": "ICLR.cc/2020/Conference/Paper1824/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "I first wanted to thank the authors for their proposed approach in this paper. The paper discusses an interesting idea for quantizing the similarity of neural networks, based on weight similarity. However, overall this assumption is based on the fact that similar layers within a particular architecture will learn similar semantics across different runs (and surprisingly authors add tasks to this as well, which I don't understand why this is the case). \n\nUnfortunately, the paper is hard to read. It is not easy to understand the research questions, and experimental setup. Partly due to overloaded terms, such as \u201csolution\u201d being used for describing multiple different concepts across the paper (solution class, local solution classification, local solution retrieval, none of them particularly well defined in the paper). I highly recommend the authors to describe their findings in a more concrete manner. I did not see any attempts at giving *insights*, mostly numerical comparison. \n\nMy concerns with the methodology of the paper are as follows:\n\n1. What are the findings of the paper? Permutation of the neural networks is certainly an area worth studying. However, in this paper, authors make the assumption that weights across the same layer (say layer #2) are somehow always going to learn similar values (except in permutation) across different runs of the model? Major clarification in this area is required. \n\n2. I am not quite sure what the term solution class means, or what the authors want the reviewer to believe it means. Please elaborate. Also solution label? This terminology seems a bit obsolete and cumbersome, unless properly defined at the beginning of the paper. \n\n3. Can you elaborate more on the goals of the experiments? Right now the outcome of the experiments are a bit vague based on lack of hypotheses. \n\n4. Can you elaborate what the goal of local solution classification is? It is not clear if this is simply the classification accuracy of a trained model, or what is vaguely described in section 3.3\n\nI will make a re-evaluation of the paper after the above questions are answered. Overall, I suggest a rewrite of the paper to make claims, experimental hypotheses and design more clear. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1824/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1824/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wanggc3@mail2.sysu.edu.cn", "stsljh@mail.sysu.edu.cn", "wanggrun@mail2.sysu.edu.cn", "liangwq8@mail2.sysu.edu.cn"], "title": "Function Feature Learning of Neural Networks", "authors": ["Guangcong Wang", "Jianhuang Lai", "Guangrun Wang", "Wenqi Liang"], "pdf": "/pdf/62846941f62c2f6a5c5b5fa31287ae314584d411.pdf", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.", "keywords": [], "paperhash": "wang|function_feature_learning_of_neural_networks", "code": "https://anonymous.4open.science/r/74e46ebe-4023-4a85-86b6-19ee20c5070a/", "original_pdf": "/attachment/4cde91e6bc25623bb29517863af8385cc761957b.pdf", "_bibtex": "@misc{\nwang2020function,\ntitle={Function Feature Learning of Neural Networks},\nauthor={Guangcong Wang and Jianhuang Lai and Guangrun Wang and Wenqi Liang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgCOySYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgCOySYwH", "replyto": "rJgCOySYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1824/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576601591355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1824/Reviewers"], "noninvitees": [], "tcdate": 1570237731774, "tmdate": 1576601591370, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1824/-/Official_Review"}}}], "count": 8}