{"notes": [{"id": "SkxG-CVFDH", "original": "S1gvrl4_DB", "number": 956, "cdate": 1569439225678, "ddate": null, "tcdate": 1569439225678, "tmdate": 1577168212043, "tddate": null, "forum": "SkxG-CVFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "B-1LS68jK", "original": null, "number": 1, "cdate": 1576798710662, "ddate": null, "tcdate": 1576798710662, "tmdate": 1576800925673, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Decision", "content": {"decision": "Reject", "comment": "The authors integrate an interpolation based regularization to develop a graph neural network for semi-supervised learning. While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period. The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718998, "tmdate": 1576800269573, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper956/-/Decision"}}}, {"id": "r1e_vEZKor", "original": null, "number": 2, "cdate": 1573618783603, "ddate": null, "tcdate": 1573618783603, "tmdate": 1573767843298, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"title": "Common response for all the reviewers: Summary of the changes made ", "comment": "We thank the reviewers for their helpful comments and suggestions. All the reviewers agreed that Data-augmentation based regularization for Graph Structured data is an important problem and the proposed framework is simple and effective approach to address this problem, with experimental results using a variety of architectures and datasets demonstrating its effectiveness. \n \nWe have incorporated all proposed changes and performed all suggested experiments. We believe that the quality of the paper has been improved and our contribution is more  clear now.\n\nAs pointed out by reviwer 2 and 3, we have made it more clear in the revised version of the paper that the proposed framework is a re-synthesis/adaptation/modification of various existing methods from the literature for Graph structured data, rather than being an entirely new method. However, we would like to point out the major advantages of this work:  1) it can be applied to any underlying GNN that uses a parametric transformation of node feature vectors 2) our experiments show that it improves the test accuracy over a number of underlying state-of-the-art GNNs and datasets 3) It has almost no additional computation cost 4) The framework can be implemented on top of an existing GNN using only a few lines of codes. 5) The framework is not very sensitive to the hyperparameters (it improves the test accuracy over the underlying GNNs with even the minimal hyperparameter search).\n\nThis serves two purposes: (1) Due to its effectiveness, our method paves the way for attracting more attention to designing better data-augmentation techniques for the Graph Neural Networks. (2) Due to its simplicity, no-additional computation cost and state-of-the-art performance, this method can serve as a strong baseline for future research into designing novel architectures for the Graph Neural Networks. \n\nThe summary of the changes made in the revised version is as follows:\n\n1: The writing has been improved to clearly communicate the motivation of the proposed framework.\n2. We have added Graph-U-Net experiments.  Our results show that GraphMix can improve the results over vanilla Graph-U-Net. ( Note that despite our best efforts, we were not able to reproduce the results in the Graph-U-Net paper, so our results for vanilla Graph-U-net are not similar to the Graph-U-Net paper).\n3. To maintain the consistency in the tables, we have added GCN(predicted-targets), GAT and GraphMix(GAT) results in all the tables where necessary.\n4. We have added a t-SNE visualization of GCN(predicted-targets) to demonstrate how much cluster separation happens because of Manifold Mixup training of FCN.\n5. We have added explanation for design choices such as why \u201csharpening\u201d is used instead of \u201cPsuedo-labels\u201d. \n\n\nWe address the concerns of each reviewer individually in the following comments.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "BklJaOZFiB", "original": null, "number": 4, "cdate": 1573619895428, "ddate": null, "tcdate": 1573619895428, "tmdate": 1573655587357, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "HyemZINEcr", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your valuable comments and feedback. We have made made the necessary change suggested by you. Following is the more detailed response on point by point basis:\n\n\n\"Weak technical contribution to the problem\" \n\n(We have addressed this concern in the \"common response to all the reviewers\" comment above, so you can skip the response to this point if you have already read  \"common response to all the reviewers\"comment.) \nWe agree that the proposed framework in not entirely novel, rather it is a re-synthesis/adaptation/modification of various existing methods. However, it has major advantages such as 1) it can be applied to any underlying GNN that uses a parametric transformation of node feature vectors 2) our experiments show that it improves the test accuracy over a number of underlying GNNs and datasets 3) It has almost no additional computation cost 4) The framework can be implemented on top of an existing GNN using only a few lines of codes. 5) The framework is not very sensitive to the hyperparameters (it improves the test accuracy over the underlying GNNs with even the minimal hyperparameter search)\n \n\"The performance gain is from mostly the use of a semi-supervised learning approach based on entropy minimization, which has been developed without consideration of graph-structured data.\"\n\nShchur et .al[1] have demonstrated that the performance of various GNN architectures using the  standard Train/validation/Test split of benchmark datasets Cora/Citeseer/Pubmed is significantly different than the random split of the datasets. Based on their observations, to avoid any bais introduced by the standard(fixed) split, they recommend using random splits. Along these lines we conduct our experiments using 10 random Train/Validation/Test splits of the data. Our results in Table1 demonstrate that GraphMix always performs better than existing Semi-supervised learning based approach (GCN(Predicted-targets)), by a sizeable margin. \n\n\nFurthermore, the success of  existing Semi-supervised learning approach such as GCN(predicted-targets) depends heavily on the quality of the  predicted-targets. The quality of the predicted-targets degrades if the number of labeled samples is lesser. To demonstrate this, we conducted a new ablation study, where we limited the number of per-class labels to 10. Results are in Table 3. Our results suggest that the Manifold Mixup training of FCN via shared parameters plays an important role in the success of the proposed framework. \n\n[1]Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. CoRR\n, abs/1811.05868, 2018. URL http://arxiv.org/abs/1811.0586\n\n\n\u201cThe paper does not compare the computational complexity of the proposed approach to the baseline approaches. Although it seems the additional computation cost for the proposed approach compared to the baseline such as GCN does not increase dramatically, it would be better if the authors could provide its computational cost as well as performance given the number of perturbed samples. \u201c\n\nThe proposed framework does not add any significant additional computation cost over the underlying GNN, because the underlying GNN is trained in the standard way and the FCN training requires trivial additional computation cost for computing the predicted-targets (Section 3.2.1 and Section 3.2.2) and the interpolation function ( Mix(a,b) in Section 2.3). We have added this explanation in the revised paper.\n\n\n\u201cAccording to Table 1, self-learning approaches, i.e., GCN (with predicted targets), works comparable to GraphMix. I do not find the benefit of using such a more complicated approach other than simple approaches such as self-learning in the experiment section.\u201d\n\nAccording to Table 1 ( Table 5 in the revised version) , GCN (with predicted targets) works better than GraphMix only on Pubmed using the standard split. On Cora and Citeseer, GraphMix is significantly better than GCN (with predicted targets) using the standard split.  As mentioned above, a more justifiable experimental setting is random split of data in train/validation/test sets. In these experiments ( Table 1), GraphMix(GCN) improves over GCN (with predicted targets) across all the datasets, by a significant margin.\n\n\n\n\n\u201cManifold Mixup is not explained properly. For example, the authors may need to explain two random samples, i.e., (x, y) and (x\u2019, y\u2019) are drawn from the data distribution D, and how it is used in GraphMix more clearly.\u201d\n\u201cAdjacency matrix A is used without its definition.\u201d\n\nWe have corrected these mistakes. Thanks for pointing this out.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "rJegzPZYiH", "original": null, "number": 3, "cdate": 1573619463952, "ddate": null, "tcdate": 1573619463952, "tmdate": 1573620225995, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SyxexpRpqH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"title": "Response", "comment": "Thankyou for your valuable comments and feedback. In the common response to all the reviewers above,  we have listed the main advantages of this approach and the reasons we believe  this can be a useful work for the research community. In the following we address the concerns point by point:\n\n\u201cRating Justification: Efficient data-augmentation procedures are an important area of research. The relative simplicity and generality of GraphMix is appealing. I would give the paper a higher score if the authors showed that GraphMix(Graph U-net) was an improvement over Graph U-net, or if it was made more clear that some substantial benefit is derived from using Mixup features.\u201d\n\nWe have added results for Graph-U-Net in Table 1. Our experiments demonstrate the GraphMix(Graph-U-Net) improves the results of the vanilla Graph-U-Net.  For \u201csubstantial benefit is derived from using Mixup features\u201d, please see below.\n \n\n\n\"1. Based on the ablation study it seems that Mixup actually plays a very minor role in the overall success of the procedure. I would be curious to see the t-SNE visualization of the GraphMix(GCN) \\ Mixup features in order to determine how much of the cluster separation is due to Mixup specifically. I understand that previous work has suggested that Mixup features are superior for discrimination than normal features, but in this work specifically the evidence for this assertion is fairly weak.\"\n\nWe have added Figure 2B which has the t-SNE visualization of  GCN (trained with the predicted targets), which is essentially  GraphMix(GCN) \\ Mixup features.  Figure 2C has   t-SNE visualization of  GraphMix(GCN). By comparing these two plots, we can observe that the GraphMix(GCN) obtains better cluster separation. This can be also demonstrated with the soft-rank of class-specific features ( Figure 2D) .\n\nFurthermore, we would like to point out that, Mixup plays even more important role when the labeled samples are very small.  To demonstrate this, we conducted a new ablation study, where we limited the number of per-class labels to 10. Results are in Table 3. Our results suggest that the Manifold Mixup training of FCN via shared parameters plays an important role in the success of the proposed framework. \n\n\n \"2. A clearer distinction between GraphMix(GCN) and GCN (with predicted targets) would be very helpful, especially since GCN (with predicted targets) actually performs the best on the standard PubMed splits.\"\n\nWe have improved the writing to  make this clear in the revised version. \n\n\n\"3. Why were the results for GCN (with predicted targets) not included in Table 2?\"\n\nWe have added the results for GCN (with predicted targets) in Table 2 ( Table 1 in the revised version). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "SJeUhKZYsS", "original": null, "number": 6, "cdate": 1573620141849, "ddate": null, "tcdate": 1573620141849, "tmdate": 1573620141849, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "BklncFZtor", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"title": "Response (Part 2) ", "comment": "5. In Table 2, 3, and 4, it is not clear on why GAT and GraphMix (GAT) are missing. This inconsistency with regard to Table 1 should be justified.\n\nThe reason behind having GAT and GraphMix(GAT) in Table1 was to show that GraphMix can improve over other standard architecture (other than GCN such as GAT) as well. Table 2, 3 and investigate other aspects such as \u201cless labeled data\u201d and \u201crandom split\u201d  rather than trying to compare against other standard architectures. For completeness, we have added GAT and GraphMix (GAT) results in Table2 and 3 (Table 1 and Table 6 in revised version). \n\nFor the datasets of Table 4 (Table 2 in the revised version), we were unable to perform GAT and GraphMix(GAT) using a 32GB GPU RAM, due the prohibitive memory requirements of GAT. GAT has O(V^2) memory complexity, V being the number of nodes in the graph.\n\n 6. In the ablation analysis, if the reason for the suboptimal performance when using EMA is the lack of hyperparameter search, then it is suggested to perform comprehensive hyperparameter search so that the results are more solid. Otherwise, the design choice cannot be well justified.\n\nWe have run the extensive hyperparameter search now and we observe that EMA can indeed achieve better results (Table 3).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "BklncFZtor", "original": null, "number": 5, "cdate": 1573620116429, "ddate": null, "tcdate": 1573620116429, "tmdate": 1573620116429, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "B1g6Dm86FH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"title": "Response (part 1) ", "comment": "Thankyou for your valuable comments and feedback. In the following, we address the concerns on point by point basic :\n\n\u201c1. The motivation to integrate interpolation based regularization for GNN is not intuitively clear. The authors may want to further clarify on how can Eq. (2) help improve the classification power of GNN in an intuitive manner. \u201c\n\nThanks for pointing this out. We have made it more clear in the revised version. The main motivation of this work is to propose a data-augmentation technique (such as Mixup or Manifold Mixup) for Graph Structured data. A naive way to apply such data-augmentation would be to mix the nodes features and their corresponding labels to synthesize new nodes. However, how to connect these synthesized nodes to other nodes of the graph, such that they confirm to the structure of the graph, remains unclear. To alleviate this issue, we proposed to train an auxiliary Fully-connected-net (FCN)     using Manifold Mixup. Note that the FCN only uses the node features ( not the graph structure), thus the Manifold mixup loss in Eq. (2) can be directly used for training the FCN. The Manifold Mixup training of FCN facilitates learning more discriminative node features. How to transfer  these discriminative node features learned by the FCN to the GCN? We apply parameter sharing between FCN and GCN to do so. Using these more discriminative features of nodes and the graph structure, GCN loss is computed in the usual way. Further, since the parameters of GCN and FCN are shared, in the next learning update for training the FCN, the FCN receives more refined representations learned by GCN. In this way both FCN and GCN improve each-others learning akin to Co-training framework. This is discussed in 3.2.3 ( Connection to Co-training).\n\n\n2. Some algorithm design choices, such as sharpening for entropy minimization, are not well justified. It is not clear why sharpening is selected over psudolabels. \n\nPseudolabeling technique constructs hard(1-hot) labels for the unlabeled data which has \u201chigh-confidence predictions\u201d. Since many of  the unlabeled data samples may have \u201clow-confidence predictions\u201d, they can not be used  in the Pseudolabeling technique. On the other hand, Sharpening [1] does not require \u201chigh-confidence predictions\u201d , and thus it can be used for all the unlabelled samples. Thanks for pointing this out, we have made more clear in the revised version of the paper why sharpening is selected over the pseudolabels. \n\n[1]David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. MixMatch: A Holistic Approach to Semi-Supervised Learning.\narXiv e-prints, art. arXiv:1905.02249, May 2019\n\n\n3. The overall technical contribution is somewhat incremental based on previous work on GNN and manifold mixup based regularizations. The model design is heuristic, and lack theoretic analysis, for example, on the parameter sharing based regularization over other choices of regularizations, if any. \n\nMuch of the recent work in Graph Neural Networks has been focused on designing complex architectures, with little attention to regularization techniques. The main contribution of this paper is to show that the state-of-the-art performance for the semi-supervised node classification can be achieved by using even simple regularization techniques proposed in this paper. This paves the way for attracting more attention to designing better regularization techniques which are tailor-made for the Graph Neural Networks. Furthermore, due to its simplicity, no-additional computation cost and state-of-the-art performance, this method can serve as a strong baseline for future research into designing novel architectures for the Graph Neural Networks. \n\n\n4. From the experimental results, the proposed model improves GCN and GAT, but the improvements on other baseline methods are a little bit subtle. The authors may want to further evaluate their framework on those GNNs such as GMNN and Graph U-Net to make the experiments more comprehensive, or justify on why they cannot be implemented using the proposed framework. \n\nWe evaluated GraphMix framework on Graph-U-Net. The results are included in Table 1 ,which  demonstrate that GraphMix can improve the accuracy of Graph-U-Net (Note that we were not able to reproduce the numbers reported in the Graph-U-Net paper, nevertheless GraphMix improved over the vanilla Graph-U-Net across all the datasets) . \n\nApplying GraphMix to GMNN is not straight forward. The reason is that GMNN is indeed a training framework for existing GNN models instead of being a GNN architecture (e.g., GCN, GAT, Graph U-Net). In this sense, GMNN is similar to GraphMix, which are used for enhancing existing GNN architectures, and it is nontrivial to apply GraphMix to GMNN. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "B1g6Dm86FH", "original": null, "number": 1, "cdate": 1571804005038, "ddate": null, "tcdate": 1571804005038, "tmdate": 1572972530826, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors developed GNN by integrating an interpolation based regularization. In particular, the proposed GraphMix model has two components, one GNN and one MLP. For the MLP, the manifold mixup loss is used for training. The two components share parameters so that the GCN training can be regularized by the manifold mixup loss. Moreover, the predicted samples of GCN are used to augment the data of MLP for training in a self-supervised manner. In the experiments, GraphMix was compared with several recent GNN models on several benchmark datasets and demonstrates effectiveness for node classification and link classification.\n\nThere are several concerns on this paper.\n1. The motivation to integrate interpolation based regularization for GNN is not intuitively clear. The authors may want to further clarify on how can Eq. (2) help improve the classification power of GNN in an intuitive manner.\n2. Some algorithm design choices, such as sharpening for entropy minimization, are not well justified. It is not clear why sharpening is selected over psudolabels.\n3. The overall technical contribution is somewhat incremental based on previous work on GNN and manifold mixup based regularizations. The model design is heuristic, and lack theoretic analysis, for example, on the parameter sharing based regularization over other choices of regularizations, if any.\n4. From the experimental results, the proposed model improves GCN and GAT, but the improvements on other baseline methods are a little bit subtle. The authors may want to further evaluate their framework on those GNNs such as GMNN and Graph U-Net to make the experiments more comprehensive, or justify on why they cannot be implemented using the proposed framework.\n5. In Table 2, 3, and 4, it is not clear on why GAT and GraphMix (GAT) are missing. This inconsistency with regard to Table 1 should be justified.\n6. In the ablation analysis, if the reason for the suboptimal performance when using EMA is the lack of hyperparameter search, then it is suggested to perform comprehensive hyperparameter search so that the results are more solid. Otherwise, the design choice cannot be well justified.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576140399445, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper956/Reviewers"], "noninvitees": [], "tcdate": 1570237744516, "tmdate": 1576140399456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Review"}}}, {"id": "HyemZINEcr", "original": null, "number": 2, "cdate": 1572255226869, "ddate": null, "tcdate": 1572255226869, "tmdate": 1572972530779, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a regularization method for graph neural networks that exploit unlabeled examples. The authors claim that the proposed method for semi-supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures, that is an architecture-agnostic regularization technique is considered in the paper.\n\nStrength\n\u2022\tSimple yet effective regularization technique to neural networks for graph-structured data\n\nWeaknesses\n\u2022\tWeak technical contribution to the problem\n\u2022\tThe performance gain is from mostly the use of a semi-supervised learning approach based on entropy minimization, which has been developed without consideration of graph-structured data.\n\n\nThe paper does not compare the computational complexity of the proposed approach to the baseline approaches. Although it seems the additional computation cost for the proposed approach compared to the baseline such as GCN does not increase dramatically, it would be better if the authors could provide its computational cost as well as performance given the number of perturbed samples.\n\nAccording to Table 1, self-learning approaches, i.e., GCN (with predicted targets), works comparable to GraphMix. I do not find the benefit of using such a more complicated approach other than simple approaches such as self-learning in the experiment section.\n\nManifold Mixup is not explained properly. For example, the authors may need to explain two random samples, i.e., (x, y) and (x\u2019, y\u2019) are drawn from the data distribution D, and how it is used in GraphMix more clearly.\n\nAdjacency matrix A is used without its definition."}, "signatures": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576140399445, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper956/Reviewers"], "noninvitees": [], "tcdate": 1570237744516, "tmdate": 1576140399456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Review"}}}, {"id": "SyxexpRpqH", "original": null, "number": 3, "cdate": 1572887783755, "ddate": null, "tcdate": 1572887783755, "tmdate": 1572972530731, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper presents a data augmentation procedure for semi-supervised learning on graph structured data. In general it is not clear how augmented observations can be incorporated into an existing graph while still preserving the graph structure. Instead of trying to incorporate an augmented dataset into the graph, this paper uses a seperate fully-connected network (FCN) that shares weights with a graph neural net (GNN). The final method also incorporates various components from the literature, such as Mixup and Sharpening. While the literature appears to have been adequately cited, the position of GraphMix relative to previous work could be better dilineated. In my opinion, the main contribution of this paper is the suggestion of using weight-sharing to sidestep the issue of incorporating augmented datapoints into graph-structured data. I am not sufficiently versed in the literature to assess whether the idea is sufficiently novel to warrant acceptance. The idea is simple and appears to improve the performance of Graph Convolutional Nets (GCNs), so I am leaning towards acceptance.\n\nRating Justification:\nEfficient data-augmentation procedures are an important area of research. The relative simplicity and generality of GraphMix is appealing. I would give the paper a higher score if the authors showed that GraphMix(Graph U-net) was an improvement over Graph U-net, or if it was made more clear that some substantial benefit is derived from using Mixup features.\n\nAdditional Comments:\n1. Based on the ablation study it seems that Mixup actually plays a very minor role in the overall success of the procedure. I would be curious to see the t-SNE visualization of the GraphMix(GCN) \\ Mixup features in order to determine how much of the cluster separation is due to Mixup specifically. I understand that previous work has suggested that Mixup features are superior for discrimination than normal features, but in this work specifically the evidence for this assertion is fairly weak. \n\n2. A clearer distinction between GraphMix(GCN) and GCN (with predicted targets) would be very helpful, especially since GCN (with predicted targets) actually performs the best on the standard PubMed splits. Why were the results for GCN (with predicted targets) not included in Table 2?"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576140399445, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper956/Reviewers"], "noninvitees": [], "tcdate": 1570237744516, "tmdate": 1576140399456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Review"}}}, {"id": "BJxCIUVfqH", "original": null, "number": 2, "cdate": 1572124245872, "ddate": null, "tcdate": 1572124245872, "tmdate": 1572124245872, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "rkxxSiNitr", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Public_Comment", "content": {"title": "Thanks.", "comment": "We thank the authors' attention in these papers."}, "signatures": ["~Bao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Bao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201700, "tmdate": 1576860595318, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Public_Comment"}}}, {"id": "rkxxSiNitr", "original": null, "number": 1, "cdate": 1571666743955, "ddate": null, "tcdate": 1571666743955, "tmdate": 1571666743955, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "BJx7VX0dYr", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment", "content": {"comment": "Hello,\n\nThanks for pointing out the references. We will refer to them adequately in the revised version of the paper.", "title": "Thanks for the references"}, "signatures": ["ICLR.cc/2020/Conference/Paper956/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper956/Authors|ICLR.cc/2020/Conference/Paper956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163507, "tmdate": 1576860562235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Official_Comment"}}}, {"id": "BJx7VX0dYr", "original": null, "number": 1, "cdate": 1571509034969, "ddate": null, "tcdate": 1571509034969, "tmdate": 1571509034969, "tddate": null, "forum": "SkxG-CVFDH", "replyto": "SkxG-CVFDH", "invitation": "ICLR.cc/2020/Conference/Paper956/-/Public_Comment", "content": {"comment": "Hi, it is a very cool idea to apply interpolation-based regularization, and I like it. Here I would like to point out a few related papers.\n\n1. B. Wang, et al. Deep Neural Nets with Interpolating Function as Output Activation, NeurIPS 2018.\n\n2. B. Wang, et al. Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization, arXiv:1809.08516 2018\n\n3. B. Wang, et al. Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning, arXiv:1907.06800 2019.\n\nThanks for your attention.\n", "title": "A related paper on graph interpolation function"}, "signatures": ["~Bao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Bao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "authorids": ["vikasverma.iitm@gmail.com", "meng.qu@umontreal.ca", "lambalex@iro.umontreal.ca", "yoshua.bengio@mila.quebec", "juho.kannala@aalto.fi", "jian.tang@hec.ca"], "keywords": ["Regularization", "Graph Neural Networks", "Mixup", "Manifold Mixup", "Semi-supervised Object Classification over graph Data"], "TL;DR": "Regularization techniques for training Graph Neural Networks. We show that with our  simple method, the state-of-the-art  results can be achieved even with simpler Graph Neural Network architectures, at virtullay no additional computation cost.", "abstract": "We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.", "pdf": "/pdf/e3044d341866274154b83b28dacbf86e48fb3903.pdf", "code": "https://github.com/anon777000/GraphMix", "paperhash": "verma|graphmix_regularized_training_of_graph_neural_networks_for_semisupervised_learning", "original_pdf": "/attachment/c4faa303b103a06b728ad96300506007654104d8.pdf", "_bibtex": "@misc{\nverma2020graphmix,\ntitle={GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning},\nauthor={Vikas Verma and Meng Qu and Alex Lamb and Yoshua Bengio and Juho Kannala and Jian Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxG-CVFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxG-CVFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201700, "tmdate": 1576860595318, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper956/Authors", "ICLR.cc/2020/Conference/Paper956/Reviewers", "ICLR.cc/2020/Conference/Paper956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper956/-/Public_Comment"}}}], "count": 13}