{"notes": [{"id": "Fn5wiAq2SR", "original": "rb-_kQpbfBD", "number": 720, "cdate": 1601308084671, "ddate": null, "tcdate": 1601308084671, "tmdate": 1614985726005, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tur68xw1lai", "original": null, "number": 1, "cdate": 1610040414277, "ddate": null, "tcdate": 1610040414277, "tmdate": 1610474012126, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose adversarial training using contrastive divergence based on ideas from Hybrid Monte Carlo methods.\nOn the positive side the shown experimental results are promising both in terms of robustness and efficiency. On the negative side the paper seems to be written in a hurry. At several places terms are not defined, not explained or important details (e.g. parameters of the attack algorithms) are missing. \n\nThus the paper is not ready for publication yet and below the bar for ICLR but I encourage the authors to submit a significantly revised version to another conference.\n\nDetails:\n- in (7) N(x) is nowhere explained or defined, here also several threat models are introduced but later on only l_infty seems\nto be used e.g in Algorithm 1 (should be clarified)\n- as noted in the reviews the definition in (13) is based on quantities nowhere introduced\n- the potential U is only defined in the Algorithm (but then the arguments do not match with the RHS)\n- the kinetic energy in the algorithm is different from what has been used before\n- the parameters for the attacks used are not reported\n- why is AutoAttack not used for all datasets? They report 28% robust accuracy for the model trained by the Madry group\n (https://github.com/MadryLab/robustness) whereas in the present paper it is 35%.\n- the scales of the plots should be chosen such that the curves can be distinguished"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040414262, "tmdate": 1610474012110, "id": "ICLR.cc/2021/Conference/Paper720/-/Decision"}}}, {"id": "5RuS1v53iuQ", "original": null, "number": 4, "cdate": 1606137645991, "ddate": null, "tcdate": 1606137645991, "tmdate": 1606137645991, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "iZc7pGkPuZJ", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment", "content": {"title": "Adding a critical ablation study, some relevant works, and clarifying some points", "comment": "Thank you so much for taking the time to review the paper and we appreciate the comments.\n\nQ1. Conduct ablation studies to find out what actually causes the performance improvement. \n\nA1. Thanks for your valuable suggestion. We have added the relevant ablation study in Appendix A.3.3. In short, both the generation objective and the M-H resampling bring performance improvement for DNN.\n\nQ2. \"In Eq (13) what is $Q_0$ and $Q_1$? And why using different $\\rho$ and $\\lambda$ helps?\"\n\nA2. Sorry for the confusion. $Q_0$ and $Q_1$ are the output vector of the learning model (with softmax operator in the top layer) given input images $x+v_0$ and $x+v_K$. As you said, since the equilibrium distribution and its output $Q^{\\infty}$ are unknown, we use the ground truth hard-label to approximate the output $Q^{\\infty}$ when $\\rho\\neq \\lambda$. The reason $\\rho$ and $\\lambda$ are introduced here is that we would like to make the connection with the traditional adversarial example generation objective. In fact, the original generation objective is equal to ours essentially when $\\rho \\ll \\lambda$ except the entropy terms of $Q_0$ and $Q_1$. Experimentally, the sensitive analyses of $\\rho$ and $\\lambda$ in Appendix A.3.5 show its stability. \n\nQ3. Evaluate model robustness via totally gradient-free methods, such as hard-label attacks like \"RayS\".\n\nA3. According to your valuable comments, we have added accuracy results under RayS attack in Table 2. The experimental results show the effectiveness of our method. \n\nQ4. Comment on some other recent adversarial training methods.\n\nA4. Thanks a lot for recommending us the reference. In the updated version of the text, we have added these two works in the related work section."}, "signatures": ["ICLR.cc/2021/Conference/Paper720/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Fn5wiAq2SR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper720/Authors|ICLR.cc/2021/Conference/Paper720/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867964, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment"}}}, {"id": "arX05CkXHiL", "original": null, "number": 3, "cdate": 1606137342298, "ddate": null, "tcdate": 1606137342298, "tmdate": 1606137342298, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "IuLdKSKM0eA", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment", "content": {"title": "Adding hyperparameter settings, sensitive analyses, and time complexity analysis", "comment": "Thanks for your positive and valuable comments.  \n\nQ1. Concern about Figure 3.\n\nA1. Thanks for pointing out the typo in Figure 3! Actually, Figure 3 has shown the whole process of advanced fast adversarial training methods since the total epoch of three advanced acceleration methods is 40. And they cannot directly compare with PGD whose total epoch is 105 because they speed up adversarial training by decreasing data access times or full forward / backward propagations. As for the configurations, all of them follow the official code of YOPO (Free can be considered as a special case of YOPO), where the initial learning rate is set to 0.2, reduced by 10 times at epoch 30 and 36. Equipped with the same initial learning rate and the learning schedule, it is fair to say that our method performs better than others.\n\nQ2. Clarify some hyperparameter settings and add sensitive analyses of $\\rho$ and $\\lambda$.\n\nA2. Thanks for your suggestion. We have included more setting details and added sensitive analyses of $\\rho$ and $\\lambda$ in Appendix A.3.5.\n\nQ3. Add time complexity analysis of ATCD.\n\nA3. Thank you for the constructive suggestion for us to consider. We have added the time complexity analysis of ATCD and other methods in Appendix A.4."}, "signatures": ["ICLR.cc/2021/Conference/Paper720/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Fn5wiAq2SR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper720/Authors|ICLR.cc/2021/Conference/Paper720/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867964, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment"}}}, {"id": "XGT1MRSEpTj", "original": null, "number": 2, "cdate": 1606137015027, "ddate": null, "tcdate": 1606137015027, "tmdate": 1606137015027, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "FNdhzvGM49o", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment", "content": {"title": "Analyses of experimental results, lengths of trajectory, and time complexity", "comment": "Thanks for your valuable feedback! We address them in detail as follows.\n\nQ1. Explain why PGD still performs better than the proposed method ATCD.\n\nA1. Thank you for pointing this out. Due to the word limits, here we just give some major reasons. First, ATCD is an anytime algorithm expected to find better and better solutions the longer it keeps running. Since samples from the proposal distribution are not accepted automatically as posterior samples, which is essentially different from PGD with random starts. Our ATCD reach our final judgments on the acceptance probability to stay in (and return large numbers of samples from) high-density regions of the candidate distribution, while only occasionally visiting low-density regions. If we take a longer time to collect more samples, ATCD can depict the underlying distribution of adversarial examples according to adversarial examples and their corresponding frequencies. But in this paper, we mainly focus on reaching a balance of performance and efficiency. Second, the selection of potential energy and kinetic energy function has a great influence on HMC to simulate the trajectories of adversarial examples, like $\\theta$ and $v$ should be not highly dependent. One potential solution is to reduce the relevance between $v_t$ and the current position $\\theta_{t-1}$. That makes the sequence of samples into an approximate Markov chain. Besides, a well-designed schedule like Robust-Overfitting may be also helpful.\n\nQ2. Is it possible to increase the iterations of ATCD for performance improvement?\n\nA2. Yes. We have investigated different lengths of trajectory in Appendix A.6 in the original paper, which proved using a long-run trajectory of HMC with M-H resampling can better approach equilibrium distribution. But in this paper we mainly focus on improving the efficiency of adversarial training, so we recommend the short-run trajectory increases in speed and maintains the suitable clean accuracy to be a worthwhile trade-off.\n\nQ3. Add time complexity analysis of ATCD.\n\nA3. Thanks for the reminder. The actual time cost has shown in the last columns of Table 1,2,3. As for the time complexity analysis, please refer to Appendix A.4 in the revised version."}, "signatures": ["ICLR.cc/2021/Conference/Paper720/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Fn5wiAq2SR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper720/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper720/Authors|ICLR.cc/2021/Conference/Paper720/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867964, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Comment"}}}, {"id": "FNdhzvGM49o", "original": null, "number": 1, "cdate": 1603619467632, "ddate": null, "tcdate": 1603619467632, "tmdate": 1605024623251, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Review", "content": {"title": "Contrastive divergence for adversarial sample generation. Theory formulation and results reporting", "review": "Adversarial examples are time-consuming to generate. In this paper, the adversarial training is reformulated as a combination of stationary distribution exploring, sampling, and training. A Hamiltonian system is proposed to model data samples from their initial states, and is shown as the general form of FGSM. The sample generation method is proposed via contrastive divergence with few training iterations. Experiments have been validated on datasets. \n\nThe formulation of HMC sounds interesting and its relationship towards FGSM/PGD is discussed. There are few issues towards the theoretical discovery and experimental validations:\n\n1. The FGSM/PGD based attacks are formulated as the degeneration of HMC, as explained in Sec. 4.2. However, the experiments on the benchmarks consistently indicate PGD performs better than the proposed method ATCD. As ATCD utilizes a more advanced HMC, there lacks explanations of why the results do not correspond to the theory.  \n\n2. The proposed ATCD is claimed to efficiently generate adversarial examples while the performance seems to suffer from limited iterations. Is it possible to increase the iterations of ATCD for performance improvement?\n\n3. As the efficiency is claimed as a major contribution of ATCD, the computational complexity and time cost compared to other methods (e.g., FGSM, PGD) shall be reported as well.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper720/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136675, "tmdate": 1606915776599, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper720/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Review"}}}, {"id": "iZc7pGkPuZJ", "original": null, "number": 3, "cdate": 1603947917067, "ddate": null, "tcdate": 1603947917067, "tmdate": 1605024623187, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "Summary: \nIn this paper, the authors reformulated the generation of adversarial examples as an MCMC process and present a new adversarial learning method called ATCD, which approaches the equilibrium distribution of adversarial examples with only a few iterations by building from small modifications of the standard Contrastive Divergence. Extensive results with comparisons on various datasets show that ATCD achieves a trade-off between efficiency and accuracy in adversarial training.\n\nComments:\n\n1 . The proposed algorithm ATCD views the generation of adversarial examples as a sampling procedure. Specifically, it can be seen as performing HMC sampling. Also, it modifies the adversarial example generation objectives using a modified contrastive divergence objective. Compared with free-adversarial-training, which also utilize mini-batch replay and use the last iterate\u2019s result as initialization, the difference only lies in the adversarial example generation objective and the extra noise brought by the HMC sampling procedure. However, it is still not clear to me what actually causes the performance improvement. The different objectives or the sampling noise? I would suggest the authors conduct ablation studies to find out this answer and it would be clearer for the readers to understand the true driving force for the proposed method.\n\n2 . In Eq (13) what is Q0 and Q1? It is better to formally define them. The intuition for this objective is a bit confusing to me. For standard contrastive divergence, it is the same as measuring the difference between the output probability between init point and K-step updated point. Here since the equilibrium distribution is unknown (fixed but unknown right?), how to compute the objective here with rho and lambda parameters? And why using different rho and lambda helps?\n\n3 . Notice that even adversarial training based algorithms could cause obfuscated gradient problem, therefore, it might be a good idea to further evaluate model robustness via totally gradient-free methods, such as hard-label attacks. I would suggest the authors to also evaluate using the following method: \n\n\u201cRayS: A Ray Searching Method for Hard-label Adversarial Attack\u201d KDD (2020)\n\nIn order to make the experimental results more convincing.\n\n4 . There are some other recent adversarial training methods that the authors might want to comment on\n\n\"Improving adversarial robustness requires revisiting misclassified examples.\" ICLR (2019).\n\"On the Convergence and Robustness of Adversarial Training.\" ICML (2019).\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper720/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136675, "tmdate": 1606915776599, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper720/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Review"}}}, {"id": "IuLdKSKM0eA", "original": null, "number": 2, "cdate": 1603844432615, "ddate": null, "tcdate": 1603844432615, "tmdate": 1605024623121, "tddate": null, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "invitation": "ICLR.cc/2021/Conference/Paper720/-/Official_Review", "content": {"title": "Interesting topic with weak experimental results", "review": "Summary:\nThis paper proposed a new adversarial attack method by using Markov chain Monte Carlo.  Based on this attack method, a new adversarial learning method called adversarial training by using Contrastive Divergence (ATCD) which approaches equilibrium distribution of adversarial examples with only a few iterations is performed. The experimental results demonstrated the effectiveness of ATCD.\n\nPros:\n1. The method of generating adversarial examples is new and efficient.\n2. Experimental results with comparisons on ImageNet, CIFAR and MNIST datasets show that ATCD achieves a good trade-off between efficiency and accuracy in adversarial training.\n\nCons:\n1. There is a notable natural accuracy gap between ATCD and Madry's PGD method and Free-m.\nMany papers were aware of overfitting in adversarial training such as 'Overfitting in adversarially robust deep learning'. Nature accuracy and robust accuracy are sometimes conflicted in the latter epochs. It seems like some plots like Figure 3 may answer the question, but Figure 3 only plots the first 40 iterations (I guess it means 'epochs' here), while the total epoch is 105.\nSo only observe the results of the last epoch may not fair since maybe different methods have different convergence rates.\n2. Some hyperparameter settings and sensitive analyses like $\\rho$ and $\\lambda$ are missing.\n3. Although, the experimental results show the efficiency of ATCD, the formal time complexity analysis of ATCD should be performed.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper720/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper720/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training using Contrastive Divergence", "authorids": ["~Hongjun_Wang2", "~Guanbin_Li2", "~Liang_Lin1"], "authors": ["Hongjun Wang", "Guanbin Li", "Liang Lin"], "keywords": ["Adversarial Training", "Contrastive Divergence"], "abstract": "To protect the security of machine learning models against adversarial examples, adversarial training becomes the most popular and powerful strategy against various adversarial attacks by injecting adversarial examples into training data. However, it is time-consuming and requires high computation complexity to generate suitable adversarial examples for ensuring the robustness of models, which impedes the spread and application of adversarial training. In this work, we reformulate adversarial training as a combination of stationary distribution exploring, sampling, and training. Each updating of parameters of DNN is based on several transitions from the data samples as the initial states in a Hamiltonian system. Inspired by our new paradigm, we design a new generative method for adversarial training by using Contrastive Divergence (ATCD), which approaches the equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD). Our adversarial training algorithm achieves much higher robustness than any other state-of-the-art adversarial training acceleration method on the ImageNet, CIFAR-10, and MNIST datasets and reaches a balance between performance and efficiency.", "one-sentence_summary": "We design a new generative method for adversarial training by using Contrastive Divergence to reaches a balance of performance and efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|adversarial_training_using_contrastive_divergence", "pdf": "/pdf/f2522082ad50cb9a45e34b29455d1d9d542d1188.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdmpIm9m7J", "_bibtex": "@misc{\nwang2021adversarial,\ntitle={Adversarial Training using Contrastive Divergence},\nauthor={Hongjun Wang and Guanbin Li and Liang Lin},\nyear={2021},\nurl={https://openreview.net/forum?id=Fn5wiAq2SR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Fn5wiAq2SR", "replyto": "Fn5wiAq2SR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper720/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136675, "tmdate": 1606915776599, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper720/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper720/-/Official_Review"}}}], "count": 8}