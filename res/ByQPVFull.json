{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396333625, "tcdate": 1486396333625, "number": 1, "id": "rJI3oGLOl", "invitation": "ICLR.cc/2017/conference/-/paper61/acceptance", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396334115, "id": "ICLR.cc/2017/conference/-/paper61/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396334115}}}, {"tddate": null, "tmdate": 1485753321772, "tcdate": 1485753321772, "number": 2, "id": "rkfx3rnPx", "invitation": "ICLR.cc/2017/conference/-/paper61/official/comment", "forum": "ByQPVFull", "replyto": "B1ag8aBVe", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "content": {"title": "update", "comment": "See edit at the bottom of review: I've upgraded my rating in light of the latest revisions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744787, "id": "ICLR.cc/2017/conference/-/paper61/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744787}}}, {"tddate": null, "tmdate": 1485753285922, "tcdate": 1482180085229, "number": 3, "id": "B1ag8aBVe", "invitation": "ICLR.cc/2017/conference/-/paper61/official/review", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.\n\n\nPros:\n\nProposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.\n\nThe use of foreground segmentation masks to improve image classification is also novel.\n\nThe method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.\n\n\nCons:\n\nThe evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.\n\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.\n\nThe presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).\n\nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d.\n\nMinor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).\n\n\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?\n\nThe ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.\n\n===============\n\nEdit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.\n\n(I'll reiterate a very minor point about Figure 1 though: I still think the \"0\" and \"1\" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text \"suppress foreground\", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711850, "id": "ICLR.cc/2017/conference/-/paper61/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper61/AnonReviewer1", "ICLR.cc/2017/conference/paper61/AnonReviewer3", "ICLR.cc/2017/conference/paper61/AnonReviewer2"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711850}}}, {"tddate": null, "tmdate": 1484960799228, "tcdate": 1484960799228, "number": 6, "id": "S1DX4VlDg", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Update", "comment": "We've already updated the paper. \n- The abstract and introduction have been rewritten with more explanation (on the motivation) and comparison. \n- The difference from ensemble models was highlighted in the related works.\n- We found that the Fig 1. is a bit confusing and have already updated it in the revised revision.\n- Eqn 3. has been corrected.\n- New results on ImageNet dataset."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484920343015, "tcdate": 1478165595473, "number": 61, "id": "ByQPVFull", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByQPVFull", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "content": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484919872509, "tcdate": 1484919872509, "number": 5, "id": "r1dS4qywe", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "B1ag8aBVe", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Re", "comment": "Thanks for your comments.\n\nQ: \n- The evaluation is lacking. There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term. \n- The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor. \n- Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG). \n\nA:\nWe found that the Fig 1. is a bit confusing and leads to misunderstanding for the reviewer. We have already updated it in the revised revision. In GoCNN, the feature extractor only serves for the suppression and the extracted features are not used by the image classifier. More specifically, since there is no such mask to separate foreground from background feature during the testing phase, the CNN has to learn to separate them by itself through pursuing group orthogonality in the training. Therefore, the classifier cannot directly use the masked features for training. \nTo guide the CNNs to learn group orthogonal features, we used the suppression term to penalize the contaminated parts if there is any. In this way, the suppression term guides the CNN to separate the foreground and background feature space. Therefore, it is not redundant. In contrary, it is a very important part.\n\nQ:\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available. This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data. \n\nA:\nThanks for the suggestion. We added a complementary experiment on the FULL ImageNet-1k dataset where only 10% privileged information is provided and we used 152-layer ResNet as the basic model. In the experiments, our proposed GoCNN model achieves 21.8% top-1 error while the top-1 error of the vanilla ResNet-152 is 23.0%. Such performance boost is consistent with the results shown in Table 4., which again confirms the effectiveness of the GoCNN.\n\nQ:\nThe presentation overall is a bit confusing and difficult to follow, for me. For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence). \n\nA:\nThanks for the suggestion. We change the title to \u201cArchitecture and Implementation Details of The GoCNN\u201d.\n\nQ: \nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d. \n\nA:\nEqn 3. has been corrected. Thank you for the suggestion.\n\nQ:\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2? Are these not the same setting? The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important. \n\nA: \nTable 1-2 shows validation result by using 10 crops in the testing, while Table 4 with 100% privileged information only shows the result using 1 centre crop in the testing. Since 10-crop testing is time-consuming, and the main target of the experiments is to verify the effectiveness of GoCNN compared with baseline method, results under the 1-crop setting are convincing enough to demonstrate it."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "tmdate": 1484919679048, "tcdate": 1484919679048, "number": 4, "id": "HJPKmcyvl", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "r1M1aeMEx", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Re", "comment": "Thanks for your comments.\n\nQ: \n(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation \n(2) No comparison with an ensemble \n\nA:\nWe feel the introduction section and the abstract are not well organized. We have rewritten these two sections and added more explanation (on the motivation) and comparison. We also highlight the difference from ensemble models in the related works. Please check our revised version.\n\nQ: \n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful.\n\nA:\nThanks for the suggestion. We added a complementary experiment on the FULL ImageNet-1k dataset where only 10% privileged information is provided and we used 152-layer ResNet as the basic model. In the experiments, our proposed GoCNN model achieves 21.8% top-1 error while the top-1 error of the vanilla ResNet-152 is 23.0%. Such performance boost is consistent with the results shown in Table 4., which again confirms the effectiveness of the GoCNN.\n\nQ: \nFigure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass\n\nA:\nWe found that the Fig 1. is a bit confusing and leads to misunderstanding for the reviewer. We have already updated it in the revised revision. In GoCNN, the feature extractor only serves for the suppression and the extracted features are not used by the image classifier. More specifically, since there is no such mask to separate foreground from background feature during the testing phase, the CNN has to learn to separate them by itself through pursuing group orthogonality in the training. Therefore, the classifier cannot directly use the masked features for training. \nTo guide the CNNs to learn group orthogonal features, we used the suppression term to penalize the contaminated parts if there is any. In this way, the suppression term guides the CNN to separate the foreground and background feature space. Therefore, it is not redundant. In contrary, it is a very important part.\n\nQ: \nThe equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2) \n\nA:\nYes, the normalization factor should be equal to the combinatorial number of the different groups multiplied by the number of paired features within two groups. The expression would be a bit complicated. Here we use the upper bound value c^{(k)}^2 that is sufficient to make sure the summation term to be smaller than 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "tmdate": 1484919503646, "tcdate": 1484919503646, "number": 3, "id": "B1dCGq1we", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "r1prcTW4g", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Re", "comment": "Thanks for your comments.\n\nQ: The evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of ImageNet for testing. They do get a substantial boost, but it is not clear if this will transfer to more data/layers. \n\nA: We have added new experiments on the full ImageNet-1k dataset where only 10% privileged information is provided and we used 152-layer ResNet as the basic model. In the experiments, our proposed GoCNN model achieves 21.8% top-1 error while the top-1 error of the vanilla ResNet-152 is 23.0%. Such performance boost is consistent with the results shown in Table 4., which again confirms the effectiveness of the GoCNN.\n\nQ: The authors could at least have also tried CIFAR-10/100. \n\nA: Because CIFAR dataset does not provide segmentation masks for the images and any other type of privileged information, we are not able to conduct such experiment. However, ImageNet-1k is much larger and more diverse than CIFAR-10/100. We believe the results on ImageNet-1k are convincing. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "tmdate": 1481931994141, "tcdate": 1481931994141, "number": 2, "id": "r1M1aeMEx", "invitation": "ICLR.cc/2017/conference/-/paper61/official/review", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer3"], "content": {"title": "Unclear focus", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.\n\nPros:\n- The paper is clear and easy to follow\n- The experimental results seem to show some benefit from the proposed approach\n\nCons:\n(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation\n(2) No comparison with an ensemble\n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful\n\nThis paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.\n\nFirst, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.\n\nSecond, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.\n\nMinor suggestions / comments:\n- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)\n- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711850, "id": "ICLR.cc/2017/conference/-/paper61/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper61/AnonReviewer1", "ICLR.cc/2017/conference/paper61/AnonReviewer3", "ICLR.cc/2017/conference/paper61/AnonReviewer2"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711850}}}, {"tddate": null, "tmdate": 1481919044775, "tcdate": 1481919044775, "number": 1, "id": "r1prcTW4g", "invitation": "ICLR.cc/2017/conference/-/paper61/official/review", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. https://arxiv.org/abs/1412.1283\nTo be fair, this is not truly the same thing as what the authors are doing, because in the reference above the masking is computed  during both training and testing, while here it is used as a method of decorrelating neurons at training time.\nBut I understand that to the broader iclr community this may seem as \"yet another vision-specific trick\", while to the vision community one would ask why not just use the mask during both training and testing, since one can compute it in the first place. \n\nMore importantly, the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet for testing. They do get a substantial boost, but it is not clear if this will transfer to more data/layers. \n\nThe authors could at least have also tried CIFAR-10/100. I would expect to see some more results during the rebuttal period. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711850, "id": "ICLR.cc/2017/conference/-/paper61/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper61/AnonReviewer1", "ICLR.cc/2017/conference/paper61/AnonReviewer3", "ICLR.cc/2017/conference/paper61/AnonReviewer2"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711850}}}, {"tddate": null, "tmdate": 1481001712791, "tcdate": 1481001712783, "number": 2, "id": "HJFxoTX7l", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "BkDz1qkXl", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Re: Suppressing background features", "comment": "Thanks for your comments.\n \nSec. 4.1 explains the way GoCNN learns group orthogonal features.\n \n- Both Eq.2 and Eq.3 are designed ONLY for the FOREGROUND group (not the whole model). They only penalize the FOREGROUND group from using background information.\n- GoCNN does utilize background information when it is informative, and the background features are extracted by the BACKGROUND group which also corresponds to two similar terms as Eq.2 and Eq.3, but with an inverted Mask (check the last paragraph in Sec.4.1 for more details).\n \nIn the experiment section, we visualized the learnt feature vectors on Val dataset and performed controlled experiments. As can be seen in Fig.2, the features within the foreground group are well orthogonal with features in the background group, Fig2(b) v.s. Fig2(c); Also, richer features are extracted in GoCNN compared with the baseline, Fig2(d) v.s. Fig2(e). The terms introduced in Sec.4.1 play a crucial role in achieving this. (Please check Sec 5.2 for more detailed analysis and deeper discussions.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "tmdate": 1480724239067, "tcdate": 1480724239063, "number": 2, "id": "BkDz1qkXl", "invitation": "ICLR.cc/2017/conference/-/paper61/pre-review/question", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer2"], "content": {"title": "Suppressing background features", "question": "The use of the background \"suppression term\" (eq 3) in the objective is not intuitive to me.   While I understand that the foreground object should be more informative than the background for the object classification task, why penalize the model for using the background when it is informative?  Do you have a baseline result that ablates this term and/or the group-wise correlation term? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959485243, "id": "ICLR.cc/2017/conference/-/paper61/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper61/AnonReviewer3", "ICLR.cc/2017/conference/paper61/AnonReviewer2"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959485243}}}, {"tddate": null, "tmdate": 1480685535172, "tcdate": 1480685535168, "number": 1, "id": "Hkvkug17l", "invitation": "ICLR.cc/2017/conference/-/paper61/public/comment", "forum": "ByQPVFull", "replyto": "BkHYnV5Ml", "signatures": ["~Yunpeng_Chen1"], "readers": ["everyone"], "writers": ["~Yunpeng_Chen1"], "content": {"title": "Re: Relationship to \"DeCov?", "comment": "\"DeCov\" and our \"GoCNN\" share similar motivation: more diverse learned features have stronger generalization ability; thus both of them emphasize and aim to explicitly pursuit such feature diversity. \n\nHowever, our \"GoCNN\" proposes a different approach to optimize diversity compared with \"DeCov\". Concretely, \"DeCov\" penalizes the covariance in an unsupervised fashion and cannot utilize extra available annotations, leading to insignificant performance improvement over vanilla models. For instance, the performance enhancement brought by \"DeCov\"(NiN) is quite marginal on ImageNet dataset compared with \"Dropout\"(NiN), see Table 5 (val top1/top5) in DeCov paper. When they intentionally increase the model complexity (i.e., NiN->AlexNet) with greater overfitting risk, the \"DeCov\" method performs even worse than a simple baseline method (\"Dropout\"), see Fig. 5 (left; val accuracy) in the DeCov paper.\n\nIn contrast, \"GoCNN\" regularizes the covariance (such that increases feature independency and diversity) in a supervised way. Thus \"GoCNN\" can fully utilize rich extra information (segmentation masks here). Benefitting from this, \"GoCNN\" provides significant performance boost on ImageNet dataset even with a more complex model (i.e. ResNet-50), compared with AlexNet. \n\nThanks for pointing out this related work. We would add it into the related works.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744912, "id": "ICLR.cc/2017/conference/-/paper61/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByQPVFull", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper61/reviewers", "ICLR.cc/2017/conference/paper61/areachairs"], "cdate": 1485287744912}}}, {"tddate": null, "tmdate": 1480375421292, "tcdate": 1480375421288, "number": 1, "id": "BkHYnV5Ml", "invitation": "ICLR.cc/2017/conference/-/paper61/pre-review/question", "forum": "ByQPVFull", "replyto": "ByQPVFull", "signatures": ["ICLR.cc/2017/conference/paper61/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper61/AnonReviewer3"], "content": {"title": "Relationship to \"DeCov?", "question": "Could you please discuss the relationship between your method and the \"DeCov\" method presented in https://arxiv.org/abs/1511.06068? How would your approach compare to this baseline? Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Group Orthogonal Neural Networks with Privileged Information", "abstract": "Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.", "pdf": "/pdf/d496e78bdacf7d3a3cda2b8a65cbe19b3717ef8b.pdf", "TL;DR": "A convolutional neural network for image classification which encourages learning more diverse feature representations by using image segmentations as privileged information.", "paperhash": "chen|training_group_orthogonal_neural_networks_with_privileged_information", "conflicts": ["u.nus.edu", "nus.edu.sg", "360.cn"], "keywords": ["Deep learning", "Computer vision", "Supervised Learning"], "authors": ["Yunpeng Chen", "Xiaojie Jin", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["chenyunpeng@u.nus.edu", "xiaojie.jin@u.nus.edu", "elefjia@nus.edu.sg", "yanshuicheng@360.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959485243, "id": "ICLR.cc/2017/conference/-/paper61/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper61/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper61/AnonReviewer3", "ICLR.cc/2017/conference/paper61/AnonReviewer2"], "reply": {"forum": "ByQPVFull", "replyto": "ByQPVFull", "writers": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper61/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959485243}}}], "count": 14}