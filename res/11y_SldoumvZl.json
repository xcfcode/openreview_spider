{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363623420000, "tcdate": 1363623420000, "number": 3, "id": "YYiHlnPjU5YVO", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "11y_SldoumvZl", "replyto": "11y_SldoumvZl", "signatures": ["Cheng Zhang, Carl Henrik Ek, Hedvig Kjellstrom"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewers, \r\nthe new version of the paper, addressing all the changes in our comments, is public visible now in arXiv. \r\nThanks  in advance for your time."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363382280000, "tcdate": 1363382280000, "number": 1, "id": "8nXtnZf5sU-bd", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "11y_SldoumvZl", "replyto": "eeCgjoYcgmDco", "signatures": ["Cheng Zhang, Carl Henrik Ek, Hedvig Kjellstrom"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Once again, thanks to reviewer c82a for very helpful comments. We agree that the statement regarding connection between the prior and F(k) was not correct. The parameter kappa should not be considered as a prior in the model, instead it is used as a implementation specific parameter. We have now reorganized the paper so that the model section contains a definition of the factorizing prior p(\theta) (Eq (5)), which we believe will make things a lot clearer. Furthermore, we have added an appendix B  which gives details about the training of the factorized LDA using Gibbs sampling explaining the role of kappa.  The definition of F(k) has been moved in appendix B, and its relation to the prior (Eq (5)) has been made clearer.\r\n\r\nThe definition of the measure H of class-specificity (Eq (3)) has been made clearer by improving the notation. \r\n\r\nThe new version of the paper has been uploaded in arXiv, which will be public visible at Mon 18, March, 2013, 00:00:00 GMT.   Thanks a lot for your time in advance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363139160000, "tcdate": 1363139160000, "number": 1, "id": "eeCgjoYcgmDco", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "11y_SldoumvZl", "replyto": "ADCLANJlZFDlw", "signatures": ["anonymous reviewer c82a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "The additional comparison with SLDA is a good step in the right direction and certainly improves my personal appreciation of this paper.\r\n\r\nUnfortunately, I still cant vouch for the validity of the learning algorithm. First, I'm now even more confused as to what the prior actually is. Indeed, the prior is stated to be proportional to F(k), which depends on a certain Delta A. Also, Delta A, as I understand it, is computed from a comparison between two topic assignments (sampled during Gibbs sampling). However, p(\theta | kappa) should not depend on topic assignments, since it is a *prior* over the parameters of the topic assignments distribution (p(z|\theta)). I'm under the impression that the authors are confusing the prior and the posterior over \theta here (the latter being involved in the process of Gibbs sampling, which would indeed involve comparisons between topic assignments).\r\n\r\nI also still don't find it obvious how the authors derived their learning algorithm from the proposed novel prior. What is the training objective function exactly? How is Gibbs sampling involved in the gradient descent optimization on that objective? How does one derive the specific sampling process described in this paper from the general procedure of Gibbs sampling? These might seem by the authors like questions with obvious answers, but their answer would help a lot for the reader to understand the learning algorithm and be able to reimplement it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362753660000, "tcdate": 1362753660000, "number": 4, "id": "LpoA5MF9bm520", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "11y_SldoumvZl", "replyto": "11y_SldoumvZl", "signatures": ["Cheng Zhang, Carl Henrik Ek, Hedvig Kjellstrom"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank the reviewers for their insightful comments about the paper. We will first provide general comments in response to issues raised by more than one reviewer, and then discuss each of the reviews in more detail.\r\n\r\nFrom reading the reviews, we realize that the main contribution of the paper seems to have been obscured in the presentation - for example, due to a formulation in the beginning of the abstract (which now is changed). We do not propose a new topic model, but rather introduce a method for latent factorization in topic models. The method that we propose is general and can be adopted to many different topic models. \r\n\r\nSeveral tasks benefit from a factorized topic space; classification - the one we use to exemplify with in the paper - is just one. Factorized models produce interpretable latent spaces, which has been exploited in continous models for synthesis, as in [A], or for ambiguity modelling or domain transfer, as in [5] (Ek et al.), [B]. We believe the benefits of this transfer to topic models as well.\r\n\r\nIt would be very interesting to evaluate the benefit of a factorized topic space for a much larger range of topic models than what we do in this paper - this is beyond the scope of this paper but will definitely be pursued in a future journal version. \r\n\r\nIn a revised version of the paper, which is now uploaded to ArXiv, we have however added results from the SLDA model of Blei and McAuliffe, as a second baseline in the experiments, as suggested by reviewers c82a and fda8. The factorized LDA consistently performs better than both the regular LDA and SLDA.\r\n\r\nTo stress the focus on factorization rather than a specific classification application, we have furthermore  added an experiment with video classification. Other changes, as described below, are also included in this new paper version.\r\n\r\nNew references (included in the new version):\r\n[A]\tA. C. Damianou, C. H. Ek, M. Titsias, and N. D. Lawrence, \u201cManifold Relevance Determination,\u201d International Conference on Machine Learning, 2012.\r\n[B]\tR. Navaratnam, A. W. Fitzgibbon, and R. Cipolla, \u201cThe joint manifold model for semi-supervised multi-valued regression,\u201d IEEE International Conference on Computer Vision, 2007.\r\n\r\n\r\nReviewer c82a:\r\n\r\nWe agree with reviewer c82a that we used the word entropy in a rather sloppy manner. We have strived to make the distinction clear in the revised version.\r\n\r\nIn Figure 1(b), theta in the main plate is connected with another theta outside, since we use all the topics in theta to compute the entropy-like information measure for each topic theta_m. In this, we adopt a graphical notation similar to [9] (Jia et al.).  This is explained more thoroughly in the revised version of the paper.\r\n\r\nMoreover, p(theta | kappa) is proportional to F(k) in Equation (8). In the revised version of the paper, we explicitly state the form of the proposed prior.\r\n\r\nFinally, as reviewer c82a clearly states, topic models do not produce state-of-the-art results for scene classification (however, they do produce state-of-the-art results in other domains, such as text). The motivation for using the current classification tasks is that we find that they provide a nice intuition into why one would want a factorized representation, which is able to model separately the 'important information' (class-dependent) and the 'unimportant information/noise' (class-independent). \r\n\r\n\r\nReviewer 232f:\r\n\r\nAs reviewer 232f correctly states, the class-dependent and the class-independent topics jointly encode the variations in the data. The argument is not, as reviewer 232f suggests, to throw the class-independent topics away - they are important for explaining parts of the data variation. This is not suggested anywhere in the paper.  There are many motivations for learning a factorization. In the example application used in the paper, classification, the class dependent topics are the important ones. However, in a transfer learning scenario, the class-independent information is highly relevant. The manner in which factorization is used is highly application and domain specific; in this paper we exemplify one use for classification.\r\n\r\nAs reviewer 232f points out, using a feature that has been created for discriminative methods in a generative framework might not be particularly sensible. Our motivation for still taking this approach is to make a fair comparison to other topic models, for example, [6] (Fei-Fei and Perona).\r\n\r\nWe have replaced the term 'view' with 'modality' in the revised version of the paper, and also clarified the relation of our factorization method to the multi-modality methods cited in Section 2. In the literature on factorized latent variable models the word 'view' is predominantly used, but we think that 'modality' is clearer here.\r\n\r\n\r\nReviewer fda8:\r\n\r\nAs reviewer fda8 points out, we could achieve the same effect by using a beta distribution instead of A in Equation (7). However, it would still require a entropy-like measurement to steer the beta distribution so as to achieve the desired factorization. \r\n\r\nAs described above, we have added results using SLDA, and show that the factorized LDA consistently performs better than both regular LDA and SLDA. However, we did not have time to implement other variants suggested by reviewer fda8 - this is definitely something which is interesting to do for a journal version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362753420000, "tcdate": 1362753420000, "number": 1, "id": "ADCLANJlZFDlw", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "11y_SldoumvZl", "replyto": "gD5ygpn3FZ9Tf", "signatures": ["Cheng Zhang, Carl Henrik Ek, Hedvig Kjellstrom"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We would like to thank the reviewers for their insightful comments about the paper. We will first provide general comments in response to issues raised by more than one reviewer, and then discuss each of the reviews in more detail.\r\n\r\nFrom reading the reviews, we realize that the main contribution of the paper seems to have been obscured in the presentation - for example, due to a formulation in the beginning of the abstract (which now is changed). We do not propose a new topic model, but rather introduce a method for latent factorization in topic models. The method that we propose is general and can be adopted to many different topic models. \r\n\r\nSeveral tasks benefit from a factorized topic space; classification - the one we use to exemplify with in the paper - is just one. Factorized models produce interpretable latent spaces, which has been exploited in continous models for synthesis, as in [A], or for ambiguity modelling or domain transfer, as in [5] (Ek et al.), [B]. We believe the benefits of this transfer to topic models as well.\r\n\r\nIt would be very interesting to evaluate the benefit of a factorized topic space for a much larger range of topic models than what we do in this paper - this is beyond the scope of this paper but will definitely be pursued in a future journal version. \r\n\r\nIn a revised version of the paper, which is now uploaded to ArXiv, we have however added results from the SLDA model of Blei and McAuliffe, as a second baseline in the experiments, as suggested by reviewers c82a and fda8. The factorized LDA consistently performs better than both the regular LDA and SLDA.\r\n\r\nTo stress the focus on factorization rather than a specific classification application, we have furthermore  added an experiment with video classification. Other changes, as described below, are also included in this new paper version.\r\n\r\nNew references (included in the new version):\r\n[A]\tA. C. Damianou, C. H. Ek, M. Titsias, and N. D. Lawrence, \u201cManifold Relevance Determination,\u201d International Conference on Machine Learning, 2012.\r\n[B]\tR. Navaratnam, A. W. Fitzgibbon, and R. Cipolla, \u201cThe joint manifold model for semi-supervised multi-valued regression,\u201d IEEE International Conference on Computer Vision, 2007.\r\n\r\n\r\nReviewer c82a:\r\n\r\nWe agree with reviewer c82a that we used the word entropy in a rather sloppy manner. We have strived to make the distinction clear in the revised version.\r\n\r\nIn Figure 1(b), theta in the main plate is connected with another theta outside, since we use all the topics in theta to compute the entropy-like information measure for each topic theta_m. In this, we adopt a graphical notation similar to [9] (Jia et al.).  This is explained more thoroughly in the revised version of the paper.\r\n\r\nMoreover, p(theta | kappa) is proportional to F(k) in Equation (8). In the revised version of the paper, we explicitly state the form of the proposed prior.\r\n\r\nFinally, as reviewer c82a clearly states, topic models do not produce state-of-the-art results for scene classification (however, they do produce state-of-the-art results in other domains, such as text). The motivation for using the current classification tasks is that we find that they provide a nice intuition into why one would want a factorized representation, which is able to model separately the 'important information' (class-dependent) and the 'unimportant information/noise' (class-independent). \r\n\r\n\r\nReviewer 232f:\r\n\r\nAs reviewer 232f correctly states, the class-dependent and the class-independent topics jointly encode the variations in the data. The argument is not, as reviewer 232f suggests, to throw the class-independent topics away - they are important for explaining parts of the data variation. This is not suggested anywhere in the paper.  There are many motivations for learning a factorization. In the example application used in the paper, classification, the class dependent topics are the important ones. However, in a transfer learning scenario, the class-independent information is highly relevant. The manner in which factorization is used is highly application and domain specific; in this paper we exemplify one use for classification.\r\n\r\nAs reviewer 232f points out, using a feature that has been created for discriminative methods in a generative framework might not be particularly sensible. Our motivation for still taking this approach is to make a fair comparison to other topic models, for example, [6] (Fei-Fei and Perona).\r\n\r\nWe have replaced the term 'view' with 'modality' in the revised version of the paper, and also clarified the relation of our factorization method to the multi-modality methods cited in Section 2. In the literature on factorized latent variable models the word 'view' is predominantly used, but we think that 'modality' is clearer here.\r\n\r\n\r\nReviewer fda8:\r\n\r\nAs reviewer fda8 points out, we could achieve the same effect by using a beta distribution instead of A in Equation (7). However, it would still require a entropy-like measurement to steer the beta distribution so as to achieve the desired factorization. \r\n\r\nAs described above, we have added results using SLDA, and show that the factorized LDA consistently performs better than both regular LDA and SLDA. However, we did not have time to implement other variants suggested by reviewer fda8 - this is definitely something which is interesting to do for a journal version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362457800000, "tcdate": 1362457800000, "number": 2, "id": "rr6RmiA9Hhs9i", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "11y_SldoumvZl", "replyto": "11y_SldoumvZl", "signatures": ["anonymous reviewer fda8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorized Topic Models", "review": "This paper introduces a new prior for topics in LDA to disentangle general variance and class specific variance.\r\n\r\nThe other reviews already mentioned the lack of novelty and some missing descriptions. Concretely, the definition of p(\theta | kappa), which is central to this paper, is not clear. Instead of defining these A(k) functions in figure 7, couldn't you just use a beta distribution as a prior?\r\n\r\nIn order to publish yet another variant of LDA, more comparisons are needed to the many other LDA-based models already published.\r\n\r\nIn particular, this paper tackles common issues that have been addresses many times by other authors. In order to have a convincing argument for introducing another LDA-like model, some form of comparison would be needed to a subset of these:\r\n- the supervised topic models of Blei et al., \r\n- DiscLDA from Lacoste, \r\n- partially labeled LDA from Ramage et al., \r\n- Factorial LDA: Sparse Multi-Dimensional Text Models by Paul and Dredze,\r\n- Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model by Chemudugunta et al.\r\n- sparsity inducing topic models of  Chong Wang et al.\r\n\r\n\r\nUnless the current model outperforms at least a couple of the above related models it is hard to argue for acceptance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362214440000, "tcdate": 1362214440000, "number": 5, "id": "InujBpA-6qILy", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "11y_SldoumvZl", "replyto": "11y_SldoumvZl", "signatures": ["anonymous reviewer 232f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorized Topic Models", "review": "This paper presents an extension of Latent Dirichlet Allocation (LDA) which explicitly factors data into a structured noise part (varation shared among classes) and a signal part (variation within a specific class). The model is shown to outperform a baseline of LDA with class labels (Fei-Fei and Perona). The authors also show that the model can extract class-specific and class-shared variability by examining the learned topics.\r\n\r\nThe authors show that the new model can outperform standard LDA on classification tasks, however, it's not clear to me why one would necessarily use an LDA-based topic model (or topic models in general) if they're just interested in classification. In the introduction, the paper motivates the use of generative models (all well known reasons - learning from sparse data, handling missing observations, and providing estimates of data uncertainty, etc.) But none of these situations are explored in the experiments. So in the end, the paper shows that a model that it not really necessarily good at classification being improved but not to the point where it's better than discriminative models and not in a context of where a generative model would really be helpful. \r\n\r\nPositive points:\r\n  * The method seems sound in its motivation and construction\r\n  * The model is shown to work on different modalities (text and images)\r\n  * The model outperforms classical LDA for classification\r\n\r\nNegative points\r\n  * As per my comments above, there may be situations in which one would want to use this type of model for classification, but they haven't been explored in this paper\r\n  * The argument that the model produces sparser topic representations could be more convincing: in 4.3, the paper claims that the class-specific topic space effectively used for classification consists of 8 topics, where 12 topics are devoted to modeling structured noise; however the 12 noise topics still form part of the representation. Is the argument that the non-class topics would be thrown away after learning while the class-specific topics are retained and/or stored?\r\n\r\nSpecific comments:\r\n\r\nIn the third paragraph of the introduction, I'm not sure about choosing SIFT as the feature extraction step of this example of inference in generative models. I can't think of examples where SIFT has been used as part of a generative model -- it seems to be a classical feature extraction step for discriminative methods. Therefore, why not use an example of features that are learned generatively for this example?\r\n\r\nIn Section 2, the paper begins to talk about 'views' without really defining what is meant by a 'view'. Earlier, the paper discussed 'class-dependent' and 'class-independent' variance, and now 'view-dependent' and 'view-independent' variance - but they are not the same thing (though connected, as in the next paragraph the paper describes providing class labels as an additional 'view'). Perhaps it's best just to define up-front what's meant by a 'view'. If 'views' are just being used as a generalization of classes here in describing related work, just state that. Maybe the generalization for purposes of this discussion is not even necessary and the concept of 'view' just adds confusion.\r\n\r\nSection 2: 'only the private class topics contain the relevant _____ for class inference'. \r\n\r\nEnd of section 3: 'we associate *low*-entropy topics as class-dependent while *low*-entropy topics are considered as independent' ? (change second low to high)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362079980000, "tcdate": 1362079980000, "number": 1, "id": "gD5ygpn3FZ9Tf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "11y_SldoumvZl", "replyto": "11y_SldoumvZl", "signatures": ["anonymous reviewer c82a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorized Topic Models", "review": "* A brief summary of the paper's contributions, in the context of prior work.\r\n\r\nThis paper suggests an improvement over the LDA topic model with class labels of Fei-Fei and Perona [6], which consists in the incorporation of a prior that encourages the class conditional topic distributions to either be specific to a particular class or to be 'shared' across classes. Experiments suggest that this change to the original LDA model of [6] yields topics that are sharply divided into class-specified or shared topics and that are jointly more useful as a discriminative latent representation.\r\n\r\n* An assessment of novelty and quality.\r\n\r\nI like the motivation behind this work: designing models that explicitly try to separate the class-specific and class-invariant factors of variation is certainly an important goal and makes for a particularly appropriate topic at a conference on learning representations.\r\n\r\nThe novelty behind this paper is not great, since it adds a small component to a known model. But I wouldn't see this as a strong reason for not accepting this paper. There are, however, other issues which are more serious.\r\n\r\nFirst, I find the mathematical description of the model to be imprecise. The main contribution of this work lies in the specification of a class-dependent prior over topics. It corresponds to the product of the regular prior from [6] and a new prior factor p(\theta | kappa), which as far as I know is not explicitly defined anywhere. The authors only describe how this prior affect learning, but since no explicit definition of p(\theta | kappa) is given, we can't verify that the learning algorithm is consistent with the definition of the prior. Given that the learning algorithm is somewhat complicated, involving some annealing process, I think a proper, explicit definition of the model is important, since it can't be derived easily from the learning algorithm.\r\n\r\nI also find confusing that the authors refer to h(k) (Eq. 3) as an entropy. To be an entropy, it would need to involve a sum over k, not over c. Even the plate graphic representation of the new model is hard to understand, since \theta is present in two separate plates (over M).\r\n\r\nFinally, since there are other alternatives than [6] to supervised training of an LDA topic model, I think a comparison with these other alternatives would be in order. In particular, I'm thinking of the two following alternatives:\r\n\r\nSupervised Topic Models, by Blei and McAuliffe, 2007\r\nDiscLDA: Discriminative Learning for Dimensionality Reduction and Classi\ufb01cation, by Lacoste-Julien, Sha and Jordan, 2008\r\n\r\nI think these alternatives should at least be discussed, and one should probably be added as a baseline in the experiments. \r\n\r\nAs a side comment (and not as a strong criticism of this paper), I'd like to add that I don't think the state of the art for scene classification (or object recognition in general) is actually based on LDA. My understanding is that approaches based on sparse coding + max pooling + linear SVM are better. I still think it's OK for some work to focus on improving a particular class of models. But at one point, perhaps a comparison to these other approaches should be considered.\r\n\r\n* A list of pros and cons (reasons to accept/reject).\r\n\r\n|Pros|\r\n- attacks an important problem, that of discovering and separating the factors of variation of a data distribution that are either class-dependent or class-shared, in the context of a topic model\r\n\r\n|Cons|\r\n- somewhat incremental work\r\n- description of the model is not enough detailed\r\n- no comparison with alternative supervised LDA models"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358353800000, "tcdate": 1358353800000, "number": 57, "id": "11y_SldoumvZl", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "11y_SldoumvZl", "signatures": ["metalgeek.cz@gmail.com"], "readers": ["everyone"], "content": {"title": "Factorized Topic Models", "decision": "conferencePoster-iclr2013-workshop", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "pdf": "https://arxiv.org/abs/1301.3461", "paperhash": "zhang|factorized_topic_models", "keywords": [], "conflicts": [], "authors": ["Cheng Zhang", "Carl Henrik Ek", "Hedvig Kjellstrom"], "authorids": ["metalgeek.cz@gmail.com", "chek@kth.se", "hedvig@kth.se"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 9}