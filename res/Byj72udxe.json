{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488572003865, "tcdate": 1478163491514, "number": 59, "id": "Byj72udxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Byj72udxe", "signatures": ["~Stephen_Merity1"], "readers": ["everyone"], "content": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396332127, "tcdate": 1486396332127, "number": 1, "id": "By4hjz8_x", "invitation": "ICLR.cc/2017/conference/-/paper59/acceptance", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396332612, "id": "ICLR.cc/2017/conference/-/paper59/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396332612}}}, {"tddate": null, "tmdate": 1484934660172, "tcdate": 1482835161200, "number": 3, "id": "BJZyrpJBg", "invitation": "ICLR.cc/2017/conference/-/paper59/official/review", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.\n\nThe reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.\n\nWhile the paper describes the differences between the proposed approach and Gulcehre et al.\u2019s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:\n\u201cRather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.\u201d\nAs far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?\n\nIn addition, quoting from section 3 which describes the model of Gulcehre et al.:\n\u201cRather than constructing a mixture model as in our work, they use a switching network to decide which component to use\u201d\nThis is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.\n\nFinally, in the following quote, also from section 3: \n\u201cThe pointer network is not used as a source of information for the switching network as in our model.\u201d \nIt is not clear what the authors mean by \u201csource of information\u201d here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.\n\nWith regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?\n\nI would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482835161892, "id": "ICLR.cc/2017/conference/-/paper59/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper59/AnonReviewer2", "ICLR.cc/2017/conference/paper59/AnonReviewer3", "ICLR.cc/2017/conference/paper59/AnonReviewer1"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482835161892}}}, {"tddate": null, "tmdate": 1484934639435, "tcdate": 1484934639435, "number": 2, "id": "ryDl06kwx", "invitation": "ICLR.cc/2017/conference/-/paper59/official/comment", "forum": "Byj72udxe", "replyto": "S1PEjF8Lx", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "content": {"title": "Thanks for addressing my comments", "comment": "Regarding the mixture model difference, I agree with you. The main difference is that your model has the switching variable is latent, while in Gulcehre it is observed. This indeed changes the training and testing procedure, but the practical implications are yet to be confirmed.\n\nRegarding using the pointer network as a source of information, I think that Gulcehre et al. is also using it as input to the switching network, since it computes the switching probability based on both the decoder state and the context (which uses the softmax of the pointer network). The fact that the context is fixed in Gulcehre et al. is a consequence of having a different application, which has a fixed context (source sentence in MT).\n\nAnyways, I think the dataset is interesting and is a good contribution by itself."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745749, "id": "ICLR.cc/2017/conference/-/paper59/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers", "ICLR.cc/2017/conference/paper59/areachairs"], "cdate": 1485287745749}}}, {"tddate": null, "tmdate": 1484602603763, "tcdate": 1484327993042, "number": 4, "id": "SkWBnKLUe", "invitation": "ICLR.cc/2017/conference/-/paper59/public/comment", "forum": "Byj72udxe", "replyto": "S1PEjF8Lx", "signatures": ["~Stephen_Merity1"], "readers": ["everyone"], "writers": ["~Stephen_Merity1"], "content": {"title": "Clarifying authorship of \"Response to the review\"", "comment": "Apologies, the above post is mine, though I accidentally posted it while not logged in, resulting in an anonymously authored comment.\n\nAs noted, we hope the above discussion has answered your queries. If you have any further questions, feel free to post them and we will attend to them as quickly and clearly as possible!\n\nP.S. I forgot to explicitly note that the Hutter Prize Wikipedia (enwik8) and Text8 datasets are only ever used at the character level, not the word level, as in our current task."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745876, "id": "ICLR.cc/2017/conference/-/paper59/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byj72udxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper59/reviewers", "ICLR.cc/2017/conference/paper59/areachairs"], "cdate": 1485287745876}}}, {"tddate": null, "tmdate": 1484327727861, "tcdate": 1484327727861, "number": 3, "id": "S1PEjF8Lx", "invitation": "ICLR.cc/2017/conference/-/paper59/public/comment", "forum": "Byj72udxe", "replyto": "BJZyrpJBg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Response to the review, clarifying using the pointer as source of information, mixture model definitions, and WikiText compared to Text8 / Hutter", "comment": "Thank you for your feedback and for the chance to clarify our model and any discussions arising from it.\n\nFirst, we do not feel that this is at all a direct application of the previous work from Gulcehre et al. Both approaches investigate how to use and integrate pointer networks within various problem domains but have many differences in both implementation, theory, and experimentation. In answering some of your queries, we hope to further highlight these differences.\n\n= The Pointer Network as a Source of Information =\n\nRegarding the pointer network being used as a source of information, this is a key difference between the Gulcehre et al. method and the pointer sentinel mixture model.\n\nAs an extreme example, imagine if the pointer window changed with each timestep. In the Gulcehre et al. switching network, the switching decision must be made without any understanding of what the new composition of the pointer window is. This is as the RNN hidden state is the only source of information as to whether the vocabulary softmax or pointer network should be used.\n\nIn the pointer sentinel mixture model, the pointer window is \u201cqueried\u201d by the attentional softmax operation. If nothing of interest is found there, the probability mass falls upon the pointer sentinel. As the pointer sentinel acts in a similar capacity to the switching network, it can use the contents of the pointer window to influence whether to use the pointer network or not.\n\nAs such, the pointer sentinel mixture model actively uses the contents of the pointer window to make the switching decision while the Gulcehre et al. switching network is not able to.\n\nWhile the composition of the pointer window changes simply in language modeling, it is still a limitation. Words fall out of the pointer window over time, resulting in either mistakes when the switching network defers to the pointer network at the wrong time, or a switching network that needs to record additional \u201cbook keeping\u201d (i.e. position of words over time) and/or is overly cautious as words approach the end of the pointer window.\n\nThe switching network approach would become even more limited if the pointer window were more dynamic, such as if you removed words from the pointer window depending on an additional filtering decision. In Python code prediction, the pointer window may have external knowledge and filter to only variables or Python keywords depending on the context for example. The pointer sentinel method would easily extend to this whilst the switching network would likely become highly problematic.\n\nThis is further detailed in Section 2.5 of the paper regarding the motivation of the pointer sentinel, especially regarding long sequences of text and the limited capacity of the RNN hidden state.\n\n= Regarding the Mixture Model =\n\n\u201cThe model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.\u201d\n\nAt training time, the Gulcehre et al. switching network decides to always use the pointer network if the target word is in the pointer window. As such, they consider these switching network variables as fully observed. For the pointer sentinel mixture model, we consider the decision of when to use the pointer network or the softmax as a latent variable, decided as part of the training of the network. This has the advantage that a target word within the pointer window may not have the correct context for the pointer network to correctly select it, while the softmax generator might.\n\nGulcehre et al. also doesn\u2019t have all the features of a mixture model. While the Gulcehre et al. switching network method produces a value d_t similar to a mixing proportion, it does not marginalize out (sum up) the final probability even though a target word may appear in both mixture components, the vocabulary softmax and the positional softmax. The target word may even appear in the positional softmax (pointer window) multiple times, with only one of these occurrences being selected.\n\nFrom Gulcehre et al., \u201c[a]t test time, we compute [the probabilities] for all shortlist word w_t and all location l_t , and pick the word or location of the highest probability\u201d.\n\nAs such, producing a prediction in the pointer sentinel method is equivalent at both train time and test time, but is instead quite different in the Gulcehre et al. method, with the training time forcing a value on what could be a latent variable and the test time taking the max value of the disparate components instead of forming a proper mixture probability density function. With the pointer sentinel method summing up the probability given to all occurrences of a target word, it also ensures that if there are multiple sources of information, all sources of information can contribute to the final decision. This ensures the final test time probability density function is a convex combination of the mixture components, which is not true of the Gulcehre et al. model due to the max operation.\n\n= WikiText compared to Text8 / Hutter =\n\nBoth the Hutter Prize and Text8 use the first 10^8 bytes of English Wikipedia. The Hutter Prize is simply the first 10^8 bytes of English Wikipedia unchanged, meaning it contains the XML of the Wikipedia pages, timestamps, contributing author usernames, Wikipedia text mark-up, Wikipedia categories, links, etc. This is not a clean dataset for use in our tasks. Text8 is a highly limited preprocessed version of the first 10^8 bytes. It only contains lowercase ASCII a-z and the space symbol, for a total vocabulary of 27 characters, converting numbers to their word equivalents. There is also no punctuation that maintains human readability of articles or many of the other characteristics of real world text.\n\nThe articles found in WikiText are specifically selected (Good and Featured articles) rather than simply those found in the first 10^8 bytes of Wikipedia. The text was normalized and tokenized using the Moses tokenizer in a manner very similar to that of text used for machine translation, with hopes that results on the WikiText dataset should be more directly applicable to such tasks. Neither of the datasets above perform similar levels of normalization or tokenization. For language modelling tasks, <unk> tags are required for many models as a standard way of handling infrequent or unseen tokens. WikiText provides the data with <unk> tags preprocessed such that results are directly comparable between different papers. WikiText-2 is approximately a tenth the size of Text8 while WikiText-103 is five times larger than Text8. Both have training, validation, and testing splits which neither of the datasets above feature. WikiText-2 and WikiText-103 are also formatted in the same manner as the Mikolov Penn Treebank dataset such that using it for experiments requires expending minimal effort, as opposed to both other datasets which are in quite raw forms.\n\n---\n\nHopefully the above discussion will have answered some of your queries and changed your view on the contributions of our paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745876, "id": "ICLR.cc/2017/conference/-/paper59/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byj72udxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper59/reviewers", "ICLR.cc/2017/conference/paper59/areachairs"], "cdate": 1485287745876}}}, {"tddate": null, "tmdate": 1481931136708, "tcdate": 1481931111323, "number": 2, "id": "ryk_YxzEg", "invitation": "ICLR.cc/2017/conference/-/paper59/official/review", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer3"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work is basically a combined pointer network applied on language modelling. \nThe smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. \nHence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. \n\nGenerally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. \nThis paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. \nI would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. \n\nIn addition, the new WikiText language modelling dataset is very interesting. \nIt probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset. \n\nOverall, this is a well-written paper. I recommend it to be accepted.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482835161892, "id": "ICLR.cc/2017/conference/-/paper59/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper59/AnonReviewer2", "ICLR.cc/2017/conference/paper59/AnonReviewer3", "ICLR.cc/2017/conference/paper59/AnonReviewer1"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482835161892}}}, {"tddate": null, "tmdate": 1481921283654, "tcdate": 1481921283654, "number": 1, "id": "BJhWQCWVl", "invitation": "ICLR.cc/2017/conference/-/paper59/official/review", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer2"], "content": {"title": "A nice approach for rare words / context biasing for LM", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. \nThe idea is appealing in general for context biasing and the specific approach appears quite simple.\n\nThe idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,\nbut not as a mixture model, as in this paper.\n\nThe paper is clearly written and the results seem promising.\nThe new dataset the authors created (WikiText) also seems of high interest. \n\nA comment regarding notation:\nThe symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) \nThis is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.\nIt would be helpful to use different symbol for the two objects.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482835161892, "id": "ICLR.cc/2017/conference/-/paper59/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper59/AnonReviewer2", "ICLR.cc/2017/conference/paper59/AnonReviewer3", "ICLR.cc/2017/conference/paper59/AnonReviewer1"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482835161892}}}, {"tddate": null, "tmdate": 1481632154209, "tcdate": 1481632154142, "number": 2, "id": "S1GotwT7x", "invitation": "ICLR.cc/2017/conference/-/paper59/public/comment", "forum": "Byj72udxe", "replyto": "B16IsOJQe", "signatures": ["~Stephen_Merity1"], "readers": ["everyone"], "writers": ["~Stephen_Merity1"], "content": {"title": "Dynamic versus static sentinel", "comment": "A dynamic sentinel, generated potentially from the hidden state itself, may indeed be useful and an interesting variation. I'd expect it to perhaps fit and then overfit more rapidly, as there is a more direct way to make the mixture value fit the desired value. During the course of our work, we did not try any dynamic variations of the sentinel however.\n\nWhile I would expect it to be biased for particular contexts, I would note that the pointer sentinel-LSTM surprised me regarding specific word biases before - specifically, I didn't expect the pointer to be used for frequent words such as \"said\" or \"billions\"."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745876, "id": "ICLR.cc/2017/conference/-/paper59/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byj72udxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper59/reviewers", "ICLR.cc/2017/conference/paper59/areachairs"], "cdate": 1485287745876}}}, {"tddate": null, "tmdate": 1481630683339, "tcdate": 1481630683327, "number": 1, "id": "HkQJVD6Qx", "invitation": "ICLR.cc/2017/conference/-/paper59/public/comment", "forum": "Byj72udxe", "replyto": "SJgmRDAfg", "signatures": ["~Stephen_Merity1"], "readers": ["everyone"], "writers": ["~Stephen_Merity1"], "content": {"title": "Initial experiment with the Gulcehre et al. method and the search for appropriate regularization", "comment": "I agree that, whilst the pointer sentinel method works quite well, it'd be interesting to provide a direct comparison to the Gulcehre et al. method.\nWhen you first posted this, we began to modify the code to add an implementation of the Gulcehre et al. method and run experiments.\nUnfortunately we've been able to run relatively few experiments due to various complications and the speed of the larger experiments.\n\nIt appears the hyperparameters and regularization required for a strong Gulcehre et al. baseline are quite different to that of the standard LSTM or pointer sentinel LSTM.\nThe most interesting issue we have is that the Gulcehre et al. method overfits in quite a different way to the pointer sentinel method, likely requiring different regularization (specifically on or around the switching network itself) and a grid search of regularization values.\n\nhttp://i.imgur.com/V9zd5Nt.jpg\n\nAbove is the link to an experiment on a two layer 200 hidden unit LSTM with no dropout.\nThis is similar to the \"small\" model in Zaremba et al. except two layers instead of one.\nThe LSTM achieves 99.77 perplexity while the pointer sentinel LSTM achieves 86.09 - about the same perplexity a standard medium LSTM (650 hidden units) would achieve with Zaremba et al. dropout of 0.5.\nWhen run with the Gulcehre et al. method, the model fits far more quickly than the LSTM or pointer sentinel LSTM, but then overfits and requires early stopping, achieving a best perplexity of 105.9.\nApplying dropout on the hidden state before entering the switch doesn't seem to help as much as expected, with the model having a highly similar overfitting profile without achieving the original lower perplexity of the model without dropout.\n\nWe hypothesize that the pointer sentinel may act as a natural regularizer.\nThe Gulcehre et al. switching network produces the mixing value and can easily and directly nudge the mixing value the direction it would like by modifying the switching network's weights.\nThis may help to fit the training data more quickly but may also overfit more easily too.\nThe pointer sentinel can't overfit as easily as the values of the RNN outputs in the pointer window are constantly shifting during training.\n\nGiven the Gulcehre et al. method should beat the LSTM, finding the right regularization seems an important pre-requisite for providing a fair comparison with the pointer sentinel method."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745876, "id": "ICLR.cc/2017/conference/-/paper59/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byj72udxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper59/reviewers", "ICLR.cc/2017/conference/paper59/areachairs"], "cdate": 1485287745876}}}, {"tddate": null, "tmdate": 1480719188571, "tcdate": 1480719188567, "number": 2, "id": "B16IsOJQe", "invitation": "ICLR.cc/2017/conference/-/paper59/pre-review/question", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer3"], "content": {"title": "design of the sentinel vector", "question": "In this work, you proposed a sentinel vector for deciding the switch of the gate. Have you tried other options instead of a vector shared across all the predictions? Would it bias towards some specific words?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959486461, "id": "ICLR.cc/2017/conference/-/paper59/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper59/AnonReviewer1", "ICLR.cc/2017/conference/paper59/AnonReviewer3"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959486461}}}, {"tddate": null, "tmdate": 1480650264358, "tcdate": 1480650264353, "number": 1, "id": "SJgmRDAfg", "invitation": "ICLR.cc/2017/conference/-/paper59/pre-review/question", "forum": "Byj72udxe", "replyto": "Byj72udxe", "signatures": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper59/AnonReviewer1"], "content": {"title": "empirical comparison between Gulcehre et al. and your model", "question": "Why don't you compare your model's results with a baseline based on the approach proposed by Gulcehre et al.? It would be nice to see if the differences between the two models are significant in practice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus.", "pdf": "/pdf/e3a966047da4a7a7a01edf68cdfcd0157bdd4f70.pdf", "TL;DR": "Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly introduced WikiText with few extra parameters.", "paperhash": "merity|pointer_sentinel_mixture_models", "conflicts": ["salesforce.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["smerity@salesforce.com", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959486461, "id": "ICLR.cc/2017/conference/-/paper59/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper59/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper59/AnonReviewer1", "ICLR.cc/2017/conference/paper59/AnonReviewer3"], "reply": {"forum": "Byj72udxe", "replyto": "Byj72udxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper59/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959486461}}}], "count": 12}