{"notes": [{"id": "PvVbsAmxdlZ", "original": "umGQatPCnzQ", "number": 2721, "cdate": 1601308301643, "ddate": null, "tcdate": 1601308301643, "tmdate": 1614985723371, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zUaPxSxPAPC", "original": null, "number": 1, "cdate": 1610040417330, "ddate": null, "tcdate": 1610040417330, "tmdate": 1610474015613, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a deep reinforcement learning method that aims at ensuring resilience to observational interference. During training labels that indicate presence or absence of interference are available to the algorithm. The training objective is augmented to learn the prediction of interference that is used at test time to infer the interference label. The experimental results show superior performance in comparison to other baseline RL methods.\n\nThe main objection raised by the reviewers was on the confusing and possibly unsound causal formulation. The authors' clarifications during the discussion did not eliminate bur rather exacerbated the reviewers' doubts. I read the paper in full myself to understand whether the reviewers' confusion was justified, and whether it could be easily resolved by an improved explanation, or it is a more serious issue.  I did not succeed in clearly understanding the causal formulation nor its relevance, and also have  soundness concerns. Figure 2a does not seem to be a correct explanation of the causal mechanism. It is also not clear from this figure why z is called confounder. More generally, I was not able to reach a coherent and sound causal formulation from the authors' explanation. My conclusion is that the framing of the paper as causal inference based is not well justified. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040417316, "tmdate": 1610474015597, "id": "ICLR.cc/2021/Conference/Paper2721/-/Decision"}}}, {"id": "T5Fv2YXrq0P", "original": null, "number": 3, "cdate": 1603896978797, "ddate": null, "tcdate": 1603896978797, "tmdate": 1606792124086, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review", "content": {"title": "Interesting framework with promising experimental results", "review": "##########################################################################\n\nSummary:\n\nThe paper presents a framework for deep reinforcement learning that is motivated by causal inference and with the central objective of being resilient to observational interferences. The key idea is to use interference labels in the training phase to learn a causal model including a hidden confounding state, and then use this model in the testing to make safer decisions and improve resilience. The authors also propose a new robustness measure, CLEVER-Q, which estimates a noise bound of an RL model below which the model's greedy decision would not be altered. The framework is tested extensively over multiple applications and under different types of observational interferences. The results show a clear advantage of the proposed framework over baseline RL methods in terms of resilience to interference.\n\n##########################################################################\n\nReasons for Score:\n\nThe paper addresses an important problem in AI relating to the robustness of the algorithms and paradigms to noisy interference. The proposed framework appears to be sound and the experimental results show superior performance in comparison to other baseline RL methods. That being said, I am not very familiar with the literature on RL and Deep learning, so my decision is more of an educated guess. However, I do have a concern about the causal inference component of the paper (explained below) and this is reflected by the score.\n\n##########################################################################\n\nCons:\n\nThe causal component of the proposed framework is not well-explained. More specifically, the causal graphical model in Figure 2 is introduced at the beginning of Subsection 3.3 very briefly, and the authors don't explain the intuition behind constructing this graph.\nThe authors say, \"We use z_t to denote the latent state which can be viewed as a confounder in causal inference\", but there is no explanation for why this makes sense. Is it just an assumption that happens to work?\n\nMoreover, the following phrase appears to be inaccurate \"knowing the interference labels it or not corresponds to different levels in Pearl\u2019s causal hierarchy...: the intervention level with the interference labels and the association level without the information\". Knowing vs not knowing the interference labels does not correspond to interventional vs associational levels in the causal hierarchy, but simply switching a variable/node in the CGM between observed and latent/unobserved. Such knowledge of a variable in the CGM does not account for an intervention.\n\n##########################################################################\n\nQuestions during the rebuttal period:\n\nIt would help if the authors can clarify the issue raised in the \"Cons\" above regarding the clarity of the causal component and its central role, as claimed, in the proposed framework.\n\n##########################################################################\n\nTypos:\n\n- p.1, \"the RL agent is asked to learn a binary causation label and *embedded* a latent state into its model\": embedded -> embed.\n- p.3, \"design an end-to-end structure ... and *evaluated* by treatment effects on rewards\": The statement does not parse.\n-p.3, \"where M is a *fix* number for the history\": fix -> fixed.\n-p.3, \"We assume that interference labels i_t *follows* an i.i.d. Bernoulli process\": follows -> follow.\n\n##########################################################################\n\nComments after Discussion:\n\nI appreciate the effort made by the authors to elaborate on the causal formulation behind CIQ. However, the additional discussions in the paper are still confusing and raise soundness concerns. Some of the issues are discussed below.\n\n1- Rubin's Causal Model: The authors reference RCM in Subsection 3.1 for the causal formulation yet the rest of the work does not seem to use the potential outcome notation. Instead, Subsection 3.3 uses graphical models and the do-operator which follows the causal framework by Pearl. Then, in Subsection 3.4, the authors go back to reference RCM. It is not clear why this alternation between the two approaches is employed.\n\n2- If $z_t$ is defined as a function of $x_t$ and $i_t$, shouldn't the CGM reflect that with an arrow from $i_t$ to $z_t$ instead of it being the other way around in Fig.2(a)? Despite the attempt by the authors to elaborate on the causal formulation, I'm unable to map the structural equations such as Eq. (1) and the function of $z_t$ to the given CGM in Fig.2(a).\n\n3- The discussion in Subsection 3.3 leading to Eq. (3) sounds flawed to me. Quoting the authors, \"the interference model of Eq. (1) can be viewed as the intervention logic with the interference label it being the treatment information\". This statement is elaborating on the formulation of $x_t'$ where $x_t$ is intervened on and replaced by an interfered state when $i_t=1$. Alternatively, $x_t$ is kept intact when $i_t=0$. This intervention on the mechanism of $x_t$ happens whether we obtain $i_t$ and train the DQN with it or not. In this sense, the intervention is not happening under the CIQ framework only, but also when we simply train based on $x_t'$. Accordingly, it is not clear to me how \"the learning problem is elevated to Level II of the causal hierarchy\" due to the presence of the interference labels. To be clear, I'm not questioning the significance of using the interference labels in the training, but rather the causal story and formulation behind CIQ.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090027, "tmdate": 1606915777451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2721/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review"}}}, {"id": "DYMOtcuxkK", "original": null, "number": 6, "cdate": 1605861030727, "ddate": null, "tcdate": 1605861030727, "tmdate": 1606259833888, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "7yZFJp9HmuZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (1/2)", "comment": "We thank that the reviewer thinks our approach is novel and acknowledge our efforts on extensive experiment results. We also appreciate the reviewer for pointing out some presentation issues for us to improve the paper. \n\n****\n\n**R3Q1** - \u201cI believe that the proposed framework has the potential to enhance a single agent with multiple types of attacks. Is it possible to have multiple interference types during training?\u201d\n\n**Ans** - We agree the proposed method can be extended to the setting of multiple types of attacks. Following the reviewer\u2019s suggestion, we conduct an extension of CIQ dealing with multiple interferences (MI), CIQ-MI. The results are shown in Table G5 and Appendix E.6.\n\nTable G5. CIQ-MI: CIQ agent with an extended multi-interference (MI) architecture.\n\n| Train \\ Test           | Gaussian | Adversarial | Gaussian + Adversarial |\n|------------------------|----------|-------------|------------------------|\n| Gaussian               | 195.1    | 154.2       | 96.3                   |\n| Adversarial            | 153.9    | 195.0       | 105.1                  |\n| Gaussian + Adversarial | 195.0    | 195.0       | 195.0                  |\n\nBelow we show how the proposed CIQ model could be extended from the architecture shown in Figure 2. to the multi-interference (MI) setting. The design intuition is based on two-step inference by a **common encoder**, to infer a clean or noisy observation, followed by an **individual decoder** tied to an interference type, to infer noisy types and activate the corresponding Q-network (named $\\theta_4$). Note that the two-step inference mechanism follows the Rubin's Causal Model (RCM) (Imbens & Rubin, 2010) as two sequential potential outcome estimation models (Rubin, 1974; Imbens & Rubin, 2010), where interfered observation $x^\\prime_t$ is determined by two labels $i_{1,t}$ andi $i_{2,t}$  and $x^\\prime_{t}= i_{1,t} (i_{2, t} \\mathcal I_{1}(x_{t}) + (1 - i_{2, t})\\mathcal I_{2}(x_{t}) )  + (1 - i_{1, t}) x_{t}$ extended from Eq. 1. \n\nAs proof of concept, we consider two interference types together, Gaussian noise and adversarial perturbation. From the results shown in Table. G5, we find that the extended version of CIQ, CIQ-MI, is capable of making correct action to solve (over 195.0) the environment when training with mixed interference types (last row). Another finding is that robustness transferability (153.9/154.2) in CIQ-MI is slightly degraded compared to the transfer learning results (162.8/165.2) in Table G3. with the same training episodes ($500$) and runs ($20$), which could be caused by the increased requirement of model capacity (Ammanabrolu et al. 2019) of CIQ-MI.\n\n****\n\n**R3Q2** - \u201cThe resulting agent would then have the potential to be resilient to all types of interferences during evaluation. Furthermore, it would then be possible to combine different types of adversarial attacks into adversarial training.\u201d\n\n**Ans** - We agree that the adversarial training would be an interesting direction to enhance CIQ performance. Meanwhile, from the recent studies, applying adversarial training to deep models also needs careful designs as it may undermine model generalization (A. Raghunathan 2019; Su 2018). From our robustness transferability results in Table G2&G3, CIQ with adversarial training shows a competitive performance transferring to Gaussian noise but with a degraded performance transferring to the Blackout and Frozen conditions. We also add the results of CIQ trained with multiple interference types in Table G5 and Appendix E.6.\n\n****\n\n**R3Q3** - \u201cCausal Insight DQN other than an additional interference type output. As such, I am not very convinced by the causal inference insight\u201d\n\n**Ans** - Predicting the interference type is indeed a key feature of CIQ, but the CIQ architecture is different from the regular DQN by incorporating causal inference insight from Rubin's Causal Model (RCM) (Rubin, 1974; Imbens & Rubin, 2010). We also motivate the design mechanism from both recent works from causal representation learning models and theoretical foundations in the updated section 3.3. and 3.4 (see the general response (1) and (2)). Intuitively, the switching mechanism (counterfactual inference) from RCM could be considered as a method to disentangle a single deep network into two non-parameter-sharing networks to improve model generalization under uncertainty. It has shown many advantages for representation learning in regression tasks (Shalit et al., 2017; Louizos et al., 2017). We show in our ablation study (Appendix E.3) that this mechanism using the predicted label is important for performance. We appreciate the reviewer\u2019s suggestion for our presentation. \n\n****"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "eyLgxTthSH", "original": null, "number": 18, "cdate": 1606259668193, "ddate": null, "tcdate": 1606259668193, "tmdate": 1606259668193, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "6UJtn-kpn7R", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Follow-up clarification ", "comment": "We understand the reviewers' confusion, and we are happy to clarify with more details. \n\n****\n\nThe reviewer is correct that the $do$ intervention on $x^\\prime_t$ does not affect the current reward $r_t$ according to Fig. 2a. However, we note that, once the observation $x^\\prime_t$ is intervened, the agent's action will be affected according to the policy. Therefore, through the agent's action, the subsequent confounders ($z_{t+1}, z_{t+2}$, .... etc) will be affected by $x^\\prime_t$ and the do-operation at time $t$, which will in turn affect future rewards ($r_{t+1}, r_{t+2}$, .... etc) according to Fig. 2a. \n\nNote that the training objective is to maximize the Q-function, which is given by $\\sum_{k=0}^\\infty \\gamma^k r_{t + k}$ for the agent at time $t$. As a result, the current intervention at time $t$ would affect the Q-function, which is exactly the value the CIQ agent is predicting with the help of the causal knowledge.\n\n****\n\nWe appreciate the reviewer for the in-depth discussion again. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "6UJtn-kpn7R", "original": null, "number": 17, "cdate": 1606251952385, "ddate": null, "tcdate": 1606251952385, "tmdate": 1606251952385, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "rbllYObyog", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Yes but still a little confused", "comment": "Dear authors, thanks for your clarification - I think I understand a bit better now but I am confused how to interpret Equation 3 in context of the causal graph in Figure 2a. It seems to me that the do intervention on $x'_t$ will have no effect on $r_t$ since it is blocked by $z_t$. Or does Equation 3 refer to the causal graph before $z$ is introduced?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "rbllYObyog", "original": null, "number": 16, "cdate": 1606242416968, "ddate": null, "tcdate": 1606242416968, "tmdate": 1606242416968, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "ARt_zij2ihu", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Does the reviewer find our clarification useful?", "comment": "Dear Reviewer 1,\n\nGiven the fact that the author rebuttal phase will be ending today, we would like to know does our reply address your question on the causal graphical model? We are happy to have more discussion on this matter."}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "7yZFJp9HmuZ", "original": null, "number": 2, "cdate": 1603875987233, "ddate": null, "tcdate": 1603875987233, "tmdate": 1606207853654, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review", "content": {"title": "Novel framework but much to do", "review": "Paper summary:\nThe paper makes two main contribution: 1). A metric for evaluating RL agent's relience 2). A framework that uses inteference type as label to enhance relience.\n\nReasons for score:\nOverall, I am towards accepting the paper but I believe there are much improvements to make. The idea of adding interference type as label is quite novel, and the authors provide extensive experiment results to show that CIQ achieves better resilience against interferences. I believe that the proposed framework has the potential to enhance a single agent with multiple types of attcks and I suggest the authors to look into this direction.\n\nComments:\n- The proposed network architecture does not seem to differ from a normal DQN other than an additional interference type output. As such, I am not very convinced by the causal inference insight.\n- Is it possible to have multiple interference type during training? The resulting agent would then have the potential to be resilient to all types of interferences during evaluation. Furthermore, it would then be possible to combine different types of adversarial attacks into adversarial training. For real life application, I believe that it is possible that different types of interference could happen simultaneously.\n- Computation cost of CLEVER-Q is expensive, and there is no guarantee of the estimated CLEVER-Q score. Also, the advantage of CLEVER-Q over AC-rate is not discussed.\n\n\nMinor issues:\n- Appendix C.1, this is just huber loss instead of quantile huber loss.\n- Appendix D, 'Refer to We'.\n\n\nQuestions:\n1. If I understand correctly, the difference between DQN-CF and CIQ is just that the inteference loss does not propogate to the Q-network parameters in DQN-CF?\n2. Depending on whether there are interference, how different are the outputs of f_2 and f_3? If f_2 output is similar to f_3 even when there is no interference, it would suggest that we only need f_2 and the switching mechanism can be removed.\n\nPost rebuttal:\nThe authors have addressed most of my main concerns and provided detailed experiment results, hence I increase my score to 7.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090027, "tmdate": 1606915777451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2721/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review"}}}, {"id": "e_INLgGEjf", "original": null, "number": 15, "cdate": 1606196298105, "ddate": null, "tcdate": 1606196298105, "tmdate": 1606196845030, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "oXl_yJTKqa", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "The reviewer's comments are correct", "comment": "**AQ1**: \"Gaussian + Adversarial\" in the newly added Table 19 refers to either of the two attack types, instead of applying both of them (i.e. $I_2$($I_1$(x))), is that correct?\n\n**Ans**:\u3000Yes. Gaussian + Adversarial means every observation (state) can possibly undergo an interference with either Gaussian or Adversarial, but not their composite form. We also updated our paper to better clarify this setting.\n\n****\n\n**AQ2**: What are the experiment settings of the architecture ablation studies?\n\n**Ans**: Appreciate you again for letting us know this missing description. The setting used for Table 16 is the same as the setting for the third column (noise level = 20%) in Table 5 and the third column (noise level = 20%) in Table 15, tested in Env$_1$ (Cartpole). We have updated the paper accordingly. \n\n****\n\n\uff37e thank the reviewer for careful reading, and we are happy to answer any other question you may have. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "oXl_yJTKqa", "original": null, "number": 14, "cdate": 1606191247829, "ddate": null, "tcdate": 1606191247829, "tmdate": 1606191247829, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "-FPYFt4xqtm", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Thanks for the authors' response", "comment": "I thank the authors' for addressing most of my main concerns. Just as a quick verification, \"Gaussian + Adversarial\" in the newly added table 19. refers to either of the two attack types, instead of applying both of them (i.e. I_2(I_1(x))), is that correct? Also, what are the experiment settings of the architecture ablation studies? I am trying to compare it to one of the columns in other experiments to find out raw DQN and DQN-CF performances using the same settings."}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "FwtoERNSRhU", "original": null, "number": 12, "cdate": 1606162999280, "ddate": null, "tcdate": 1606162999280, "tmdate": 1606162999280, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Looking forward to reviewers' reply", "comment": "Dear reviewers,\n\nAs Discussion Stage 2 is about to end, we are looking forward to your reply. \n\nPlease kindly let us know if our response has addressed your initial questions. We appreciate your input and are happy to discuss any follow-up questions. Thank you again!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "-FPYFt4xqtm", "original": null, "number": 7, "cdate": 1605861168004, "ddate": null, "tcdate": 1605861168004, "tmdate": 1605899753854, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "DYMOtcuxkK", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (2/2) ", "comment": "**R3Q4** - \u201cComputation cost of CLEVER-Q is expensive, and there is no guarantee of the estimated CLEVER-Q score. Also, the advantage of CLEVER-Q over AC-rate is not discussed.\u201d\n\n**Ans** - Thank you for pointing this out. We now realized that we did not fully explain the different roles CLEVER-Q and AC-rate play in robustness assessment, which could cause some confusion. In fact, their roles are actually complementary rather than conflicting. CLEVER-Q measures \u201csensitivity\u201d in terms of the margin (minimum perturbation) required for a given state to change the original action. AC-rate measures \u201cutility\u201d in terms of the consistency (matching rate) between actions taken under interference v.s. without interference. However, unlike CLEVER-Q, AC-rate does not provide an assessment of how prone a state is to change in the presence of interference. On the other hand, unlike AC-rate, CLEVER-Q does not provide an assessment of how consistent the current policy (under interference) is when compared to the gold standard policy (no interference). Therefore, we believe both metrics are necessary in order to provide a comprehensive robustness assessment. We have updated our paper to better clarify their roles as complementary robustness metrics. In addition, we understand the reviewers\u2019 concerns about the computation cost and the estimation quality of the CLEVER-Q score. However, we are not aware of any other interference-agnostic metric to measure the associated robustness in our setting, and that is what motivates us to delve into designing such a robustness metric. If there are any other metrics that the reviewer thinks will be useful to include, please let us know and we are happy to include it in our revised version.\n\n****\n\n**R3Q5** - If I understand correctly, the difference between DQN-CF and CIQ is just that the interference loss does not propagate to the Q-network parameters in DQN-CF?\n\n**Ans** - Not propagating the interference loss to Q-network is indeed one key difference between CIQ and DQN-CF, but CIQ also has other architectural innovation as described in the answer to one of your previous questions (R3Q3).\n\n****\n\n**R3Q6** - Depending on whether there is interference, how different are the outputs of f_2 and f_3? If f_2 output is similar to f_3 even when there is no interference, it would suggest that we only need f_2 and the switching mechanism can be removed.\n\n**Ans** - The difference in the outputs of f_2 and f_3 are actually large enough to affect the performance of the agent. We conducted ablation studies which show that the model variant without using f_3 (B2 in Appendix E.3) performs significantly worse than the complete CIQ model.\n\nThank you again. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "ARt_zij2ihu", "original": null, "number": 11, "cdate": 1605899218724, "ddate": null, "tcdate": 1605899218724, "tmdate": 1605899462429, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "4w9yCCyZ9Gc", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Clarification about CGM in Figure 2a", "comment": "Thank you for the question.\n\n- If there is no the confounder $z_t$, there should indeed be an arrow from $i_t$ to $x'_t$ due to their causal relation. In our model, however, we explicitly introduce a confounding variable $z_t$ to capture the causal relation among the variables. When $z_t = (x_t, i_t)$, we can write $x'_t = F^I(z_t)$, and $i_t = g(z_t)$ where $g(z_t)$ outputs the second component of $z_t$. Then, it's clear that both $x'_t$ and $i_t$ are affected by $z_t$, and $x'_t$ and $i_t$ are independent conditioned on $z_t$, i.e., $P(x'_t, i_t | z_t) = P(x'_t | z_t) P(i_t | z_t)$.\n\n-  When $z_t = h(x_t, i_t)$, our assumption means that $x'_t = F^I(z_t)$ and $i_t = g(z_t)$ hold for some appropriate functions $F^I$ and $g$, so the conditional independence between $x'_t$ and $i_t$ is still true, which leads to the CGM in Figure 2a. We do make use of the conditional independence relation of $x'_t$ and $i_t$ conditioned on $z_t$ in our CIQ model so that we first predict $\\tilde z_t$ from $x'_t$, and then use $\\tilde z_t$ to predict $\\tilde i_t$.\n\nHope the above explanation answers your question. Thank you again. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "4w9yCCyZ9Gc", "original": null, "number": 10, "cdate": 1605882350792, "ddate": null, "tcdate": 1605882350792, "tmdate": 1605882350792, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "nYyx5_VqLeq", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Still confused about CGM in Figure 2a", "comment": "Dear authors, thanks very much for your response. I am still confused why there is no arrow or indirect path going from i_t to x'_t in the CGM in Figure 2a given that x'_t is defined to be a *function* of i_t in Equation 1. The CGM would imply that if you were to intervene on i_t it would have no effect on x'_t, directly contradicting Equation 1, which describes the causal mechanism via which i_t influences x'_t. Where have I gone wrong here?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "zSPyYQWBZTm", "original": null, "number": 9, "cdate": 1605861525208, "ddate": null, "tcdate": 1605861525208, "tmdate": 1605861525208, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "nYyx5_VqLeq", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (2/2) ", "comment": "**R1Q4** - \u201cSection 3.3 describes a toy example that is used to show how being provided interference labels during training can lead to better sample efficiency in learning the reward distributions for each of two states (one of which is occasionally subject to interference that switches the observation to the other state). In this example, however, it is not possible to infer the latent state from the potentially interfered observation at test time, while in the DQN agents the advantage of the CIQ agent is stated to come from the fact that it can do just that. It\u2019s not clear here whether the message is that the CIQ agent performs better due to better sample efficiency or due to its ability to infer the latent state at test time.\u201d\n\n**Ans.** - It\u2019s correct that it\u2019s not possible to precisely know the actual latent state $z_t$ at test time. However, the CIQ agent can infer an estimated $\\tilde{z}_t$ for $z_t$ at test time. Using $\\tilde{z}_t$ with the training labels allow the agent to have better learning efficiency. To verify the learning efficiency with training labels, we conduct an additional ablation study to compare results using the same CIQ architecture but trained either without or without training labels (baseline model B3 in Table G4.) in appendix E.3. The ablation study shows the benefits and importance of using training labels. Note that CIQ utilizes the benefits of training labels for causal inference as well as the architectural innovation discussed in our response to your previous question (R1Q2).\n\nTable. G4. A new baseline in architecture ablation study of CIQ  \n\n| Model                                             | Return | CLEVER-Q | AC-Rate |\n|---------------------------------------------------|--------|----------|---------|\n| CIQ                                               | 195.1  | 0.241    | 97.3    |\n| B3: CIQ w/o providing grounded $i_t$ for training | 135.1  | 0.142    | 53.6    |\n\n****\n\n**R1Q5** - As far as I understood, the interference at test time always corresponds to the interference provided during training in the experiments. Were any experiments run to test generalization to unseen types of interferences at test time? In a real-world setting, this could be useful.\n\n**Ans.** - Following the reviewer\u2019s suggestion, we conduct additional experiments to study the robustness transferability of  DQN  and  CIQ  when training and testing under different kinds of interference types in Env1. Please see  Table G2&G3 and the discussion in appendix E.5. Note that both architectures would solve a clean environment successfully (over 195.0).  The reported numbers are averaged over 20 independent runs for each condition.  As shown in Table G2&G3, CIQ agents consistently attain significantly improved performance when compared the averaged returns with  DQN agents, especially between Gaussian (train) and adversarial perturbation (test). In particular, for CIQ, 12 times out of 20 independent runs are successfully transferred from Gaussian to Adversarial perturbation. Interestingly, augmenting adversarial perturbation does not always guarantee the best policy transfer when testing in the Blackout and Frozen conditions, which shows a slightly lower performance compared with training on Gaussian interference. The reason could be attributed to the recent findings that adversarial training can undermine model generalization (A. Raghunathan 2019; Su 2018).\n\nTable. G2. DQN adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian | Adversarial| Blackout| Frozen |\n|:-----------------------|:-------------------:|:----------------------:|:-------------------:|-----------------:|\n| Gaussian              | 67.4              | 38.4                 | 43.7              | 52.1            |\n| Adversarial           | 53.2              | 42.5                 | 35.3              | 44.2            |\n| Blackout              | 46.2              | 27.4                 | 85.7              | 50.3            |\n| Frozen                | 62.3              | 26.2                 | 45.9              | 62.1            |\n\nTable G3. CIQ adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian| Adversarial| Blackout | Frozen|\n|-----------------------|-------------------|----------------------|-------------------|-----------------|\n| Gaussian              | 195.1    | 165.2                | 158.2             | 167.8           |\n| Adversarial           | 162.8             | 195.0       | 152.4             | 162.5           |\n| Blackout              | 131.3             | 121.1                |195.3    | 145.7           |\n| Frozen                | 161.6             | 135.8                | 147.1             | 195.2  |"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "nYyx5_VqLeq", "original": null, "number": 8, "cdate": 1605861340572, "ddate": null, "tcdate": 1605861340572, "tmdate": 1605861340572, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "qT8kzxTEc2", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (1/2)", "comment": "Thank you for recognizing the defined problem in this work is important, the acknowledgment of experiment results and the performance discussion of CIQ. We apologize for the challenges you experienced when reading our initial version. As described in our general response, we have incorporated the review comments and improved our presentation accordingly. Below we provide detailed responses according to the reviewer\u2019s comments.\n\n****\n\n**R1Q1** - \u201cThere is little to no motivation given for the architectural choice of having the interference label modulate a switching mechanism between Q-networks\u201d\n\n**Ans** -  Following your comment, we include a paragraph in Section 3.4 of the revised paper to elaborate on the motivation for the architectural choice (see general response).\n\n****\n\n**R1Q2** - \u201c  Both the CIQ and DQN-CF agent learn to predict the interference probability and use it for predicting Q-values, with the difference that the CIQ uses this predicted probability to switch between two Q-networks, but DQN-CF performs significantly worse on average - what is the intuition for this?\u201d\n\n**Ans** - Not propagating the interference loss to A-network is one key difference between CIQ and DQN-CF, but CIQ also has other architectural innovations as following explanations. Predicting the interference type is indeed a key feature of CIQ, but the CIQ architecture is different from the regular DQN by incorporating causal inference insight from Rubin's Causal Model (RCM) (Rubin, 1974; Imbens & Rubin, 2010). We also motivate the design mechanism from both recent works from causal representation learning models and theoretical foundations in the updated section 3.3. and 3.4 (see the general response (1) and (2)).  Intuitively, the switching mechanism (counterfactual inference) from RCM could be considered as a method to disentangle a single deep network into two non-parameter-sharing networks to improve model generalization under uncertainty. It has shown many advantages for representation learning in regression tasks (Shalit et al., 2017; Louizos et al., 2017). We show in our ablation study (Appendix E.3) that this mechanism using the predicted label is important for performance. \n\n****\n\n**R1Q3** - \u201dThe causal graph drawn in Figure 2a seems to contradict the experimental setup described in the text:  (i) The interference label i_t is stated to be sampled from a Bernoulli process but in the causal graph drawn in Figure 2a, there is an arrow from the latent state z_t towards i_t; in what way does the latent state affect the probability of interference?\u2028 \u25e6 (ii) In the graph, there is no arrow (and no indirect path) from the interference label to the observed state x\u2019_t; this implies that intervening on the label, which according to Equation 1 directly affects the value of x\u2019_t, would not actually affect x\u2019_t. This is directly contradictory.\u2028 \u25e6 (iii) In the graph, there is an arrow from i_t to r_t but there is no mention in the paper for how the interference label can directly affect the reward in a given time step.\u2028 It\u2019s possible I have badly misunderstood the translation of the setup into the causal graph provided - in which case, could the authors please explain where I have gone wrong? \u201c\n\n**Ans** - To clarify details of our causal model, we add a new paragraph in Section 3.3 (see the general response (1)). The latent state is defined by $z_t = h(x_t, i_t)$, so when $h$ is the identity function, the arrows discussed in points (i) and (ii) are valid because $z_t = (x_t, i_t)$ clearly affects the interference label $i_t$ and the observed state $x\u2019_t$. We assume that there exists $h$ which compresses $z_t$ to a low-dimensional confounder such that the CGM holds, and we aim to learn to predict the confounder via a neural network. For point (iii), we originally draw an arrow from interference to reward because one could consider the causal relation of a more general resilient setting where the reward is also subjected to interference. But since the interference only affects observation in the considered setting as the reviewer correctly pointed out, we remove the arrow from interference to reward in the revised version to avoid confusion. Note that having this arrow or not doesn\u2019t affect the causal inference process described in Section 3.3."}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "Bqy8CCP8qY", "original": null, "number": 5, "cdate": 1605860379060, "ddate": null, "tcdate": 1605860379060, "tmdate": 1605861071455, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "T5Fv2YXrq0P", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for acknowledging the importance and soundness of our work. We also appreciate the reviewer for pointing out some presentation issues for us to improve the paper.\n\n****\n\n**R4Q1** - \u201cthe causal graphical model in Figure 2 is introduced at the beginning of Subsection 3.3 very briefly\u201c\n\n**Ans** - Following your comment, we add more sections to clarify the connection between causal inference, design motivation, explanation of theoretical foundation from references, as summarized in the general response (1) and (2) in section 3.3 and 3.4.\n\n****\n\n**R4Q2** - \u201cexplanation for confounder z_t\u201d \n\n**Ans** - Using the causal graphical model defined in Figure 2(a), the confounder can be formally defined by $z_t = h(x_t, i_t)$ where $h$ is a compression function such that $z_t$ is a hidden confounder in the CGM. Note that such function $h$ exists by simply choosing $h$ to be the identity function. What we assume is that there is a compression function $h$ such that $z_t$ is low-dimensional, and similar to (Louizos et al., 2017), we aim to learn to predict this low-dimensional hidden confounder by a neural network. We add this discussion in a new paragraph in Section 3.3 (see general response).\n\n****\n\n**R4Q3** - \u201cKnowing vs not knowing the interference labels does not correspond to interventional vs associational levels in the causal hierarchy, but simply switching a variable/node in the CGM between observed and latent/unobserved. Such knowledge of a variable in the CGM does not account for an intervention.\u201d\n\n**Ans** - To further clarify our descriptions and make connections to causal hierarchy, we add a new paragraph in Section 3.3 (see general response) with more details on the causal hierarchy (see Table G1) and what information is available for each level. We hope the explanation addresses the reviewer\u2019s concern.\n\nTable. G1 Causal Hierarchy in our resilient DRL settings. \n\n| Level                         | Activity    | Symbol                   | Example    |\n|-------------------------------|-------------|--------------------------|------------|\n| ($\\mathbb{I}$)  Association   | Observing   | $P(r_t \\| x'_t) $         | DQN        |\n| ($\\mathbb{II}$)  Intervention | Intervening | $P(r_t\\| do(x'_t), i_t)$ | CIQ (ours) |\n\n****\n\n**R4Q4** -Typos Issue.\n\n**Ans** - We have fixed the typos issues mentioned by the reviewer. Many thanks!\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "XrkBV-e__O", "original": null, "number": 4, "cdate": 1605860143711, "ddate": null, "tcdate": 1605860143711, "tmdate": 1605860143711, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "Rx_p9MiDGdL", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for acknowledging our work to be novel and the efforts on the experiments, and we also appreciate the useful suggestions on the presentation.\n\n****\n\n**R2Q1** - \u201ccausal terminology and the assumptions behind the causal graph in Figure 2a.\u201d\n\n**Ans** - We add a new paragraph in Section 3.3 (see general response) in the revised paper to clarify the causal terminology and the assumptions behind the causal graph.\n\n****\n\n**R2Q2** - \u201c\u2026 needs to be trained with the type of intervention that the agent will be resilient to. It would be interesting to create an intervention detector that is fully unsupervised and that is based more on a state anomaly detection \u2026\u201d\n\n**Ans** - We agree with the comment of extending CIQ to much complex and unsupervised conditions. Our original experiment design is based on careful condition control and intervened observations in order to clearly demonstrate the gain in our model design. Based on R2\u2019s comments, we conduct two additional experiments to study (1) robustness transferability among different interference types (see Table G2&G3); and (2) the performance of CIQ against two different interference types during training and testing (see Table G5). More Details are given in the general summary and Appendix E.5. \n\nTable. G2. DQN adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian | Adversarial| Blackout| Frozen |\n|:-----------------------|:-------------------:|:----------------------:|:-------------------:|-----------------:|\n| Gaussian              | 67.4              | 38.4                 | 43.7              | 52.1            |\n| Adversarial           | 53.2              | 42.5                 | 35.3              | 44.2            |\n| Blackout              | 46.2              | 27.4                 | 85.7              | 50.3            |\n| Frozen                | 62.3              | 26.2                 | 45.9              | 62.1            |\n\nTable G3. CIQ adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian| Adversarial| Blackout | Frozen|\n|-----------------------|-------------------|----------------------|-------------------|-----------------|\n| Gaussian              | 195.1    | 165.2                | 158.2             | 167.8           |\n| Adversarial           | 162.8             | 195.0       | 152.4             | 162.5           |\n| Blackout              | 131.3             | 121.1                |195.3    | 145.7           |\n| Frozen                | 161.6             | 135.8                | 147.1             | 195.2  |\n\nTable G5. CIQ-MI: CIQ agent with an extended multi-interference (MI) architecture.\n\n| Train \\ Test           | Gaussian | Adversarial | Gaussian + Adversarial |\n|------------------------|----------|-------------|------------------------|\n| Gaussian               | 195.1    | 154.2       | 96.3                   |\n| Adversarial            | 153.9    | 195.0       | 105.1                  |\n| Gaussian + Adversarial | 195.0    | 195.0       | 195.0                  |\n\n****\n\n**R2Q3** - \u201cOn the downside, the novelty of the paper does not seem major and the predefined nature of interventions might make it unrealistic in a lot of RW scenarios.\u201d\n\n**Ans** - We agree that knowing all types of interventions that could happen in RW can be challenging. However, this challenge should not prevent us from exploiting known intervention types to train resilience machine learning models, in order to reduce the gap between simulation and practical deployment. To further illustrate our point, we currently consider the same interference type in training and testing, but they can be different in distribution and dynamics (see Ans. to R2Q2 for discussion). \n\n****\n\n**R2Q4** - Minor Clarification: \u201ctwo types of interventions (attacks) \u2026 \u201d\n\n**Ans** - We would like to point out that \u201cfour\u201d types of interventions have been evaluated in this work as discussed in section 3.1. More experiment results are shown in Appendix C. \n\nThank you for the suggestion again. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "B7lqOZsXKdr", "original": null, "number": 3, "cdate": 1605859704003, "ddate": null, "tcdate": 1605859704003, "tmdate": 1605859856102, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "7XT48LBEokX", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "General Response and Summary of Main Updates (2/2) ", "comment": "**(3)**  Following reviewers\u2019 suggestions, we add three additional experiments:\n\n**I.** Robustness transferability among different interference types (Appendix E.5)\n\nTable. G2. DQN adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian | Adversarial| Blackout| Frozen |\n|:-----------------------|:-------------------:|:----------------------:|:-------------------:|-----------------:|\n| Gaussian              | 67.4              | 38.4                 | 43.7              | 52.1            |\n| Adversarial           | 53.2              | 42.5                 | 35.3              | 44.2            |\n| Blackout              | 46.2              | 27.4                 | 85.7              | 50.3            |\n| Frozen                | 62.3              | 26.2                 | 45.9              | 62.1            |\n\nTable G3. CIQ adaptation: train and test on different interference (noise) in Env$_1$.\n\n| Train\\Test | Gaussian| Adversarial| Blackout | Frozen|\n|-----------------------|-------------------|----------------------|-------------------|-----------------|\n| Gaussian              | 195.1    | 165.2                | 158.2             | 167.8           |\n| Adversarial           | 162.8             | 195.0       | 152.4             | 162.5           |\n| Blackout              | 131.3             | 121.1                |195.3    | 145.7           |\n| Frozen                | 161.6             | 135.8                | 147.1             | 195.2  |\n\n****\n\n**II** A new baseline in architecture ablation study of CIQ  (Appendix E.3)\n\nTable G4. Structure-wise ablation studies.\n\n| Model                                             | Return | CLEVER-Q | AC-Rate |\n|---------------------------------------------------|--------|----------|---------|\n| CIQ                                               | 195.1  | 0.241    | 97.3    |\n| B3: CIQ w/o providing grounded $i_t$ for training | 135.1  | 0.142    | 53.6    |\n\n****\n\n**III** Extension of CIQ model to a multiple-interference setting (Appendix E.6)\n\nTable G5. CIQ-MI: CIQ agent with an extended multi-interference (MI) architecture.\n\n| Train \\ Test           | Gaussian | Adversarial | Gaussian + Adversarial |\n|------------------------|----------|-------------|------------------------|\n| Gaussian               | 195.1    | 154.2       | 96.3                   |\n| Adversarial            | 153.9    | 195.0       | 105.1                  |\n| Gaussian + Adversarial | 195.0    | 195.0       | 195.0                  |\n\nWe then make separate responses to address each reviewer\u2019s comments.   \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "7XT48LBEokX", "original": null, "number": 2, "cdate": 1605859500433, "ddate": null, "tcdate": 1605859500433, "tmdate": 1605859735576, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment", "content": {"title": "General Response and Summary of Main Updates (1/2)", "comment": "We would like to thank all reviewers for spending time and effort to read our submission and provide valuable suggestions and comments on this work.\n\nWith this revised version and the additional page in the main content, we make updates from three main perspectives and highlight them in red color: \n\n****\n\n**(1)** In Sections 3.3, we provide the following connection of causal learning terminology to our model design intuition and theoretical foundations.  \n\nTable. G1 Causal Hierarchy in our resilient DRL settings. \n\n\n| Level                         |    Activity    | Symbol                   | Example    |\n|:-------------------------------|:-------------:|:--------------------------:|------------:|\n| ($\\mathbb{I}$) Association   |     (Observing)   | $P(r_t \\| x'_t) $         | DQN        |\n| ($\\mathbb{II}$) Intervention |     (Intervening) | $P(r_t\\| do(x'_t), i_t)$ | CIQ (ours) |\n\nFormally, we define $z_t = h(x_t, i_t)$ to be the hidden confounder. Here $h$ is a function that compresses $(x_t, i_t)$ into a confounder such that the causal graphical model (CGM) holds. It is clear from Eq. (1) and the MDP definition that the CGM holds with $h$ being the identity function, i.e., $z_t = (x_t, i_t)$. We assume that there exists some unknown compression function $h$ such that $z_t$ is low-dimensional. Similar to TARNets (Shalit et al., 2017; Louizos et al., 2017), we aim to learn to predict this low-dimensional hidden confounder by a neural network.\n\nAccording to the CGM, different training settings correspond to different levels of Pearl's **causal hierarchy** (Pearl 2009; Pearl 2019; E Bareinboim et al. 2020) as shown in Table 1. If only the observations are available, the training process corresponds to Level $\\mathbb{I}$ of the causal hierarchy, which associates the outcome $r_t$ to the input observation $x'_t$ directly by $P(r_t|x'_t)$.On the other hand, when interference type $\\mathcal I$ and the interference labels $i_t$ are available during training, the learning problem is elevated to Level $\\mathbb{II}$ of the causal hierarchy. In particular, the interference model of Eq. (1) can be viewed as the intervention logic with the interference label $i_t$ being the treatment information. With this information, we can describe the causal inference problem by\n$P(r_t| do(x'_t), i_t)=P(r_t| F^{\\mathcal I}(x_t, i_t)=x'_t, i_t)$\nwith the do-operator (Pearl 2019) in the intervention level of the causal hierarchy. \n\n\n****\n\n**(2)** In Section 3.4, we provide the following design intuition from Rubin\u2019s Causal Model\n\nThe design intuition of our inference mechanism is based on the potential outcome estimation theory in Rubin\u2019s Causal Model (RCM) (Rubin, 1974; Imbens & Rubin, 2010) and modeling of the interference scenario as described in Eq. 1. Intuitively, the switching mechanism (counterfactual inference (Imbens & Rubin, 2010; Shalit et al., 2017; Louizos et al., 2017) from RCM could be considered as a method to disentangle a single deep network into two non-parameter-sharing networks to improve model generalization under uncertainty. It has shown many advantages for representation learning in regression tasks (Shalit et al., 2017; Louizos et al., 2017). We also provide more implementation details in appendix C.1.\n\n\n****\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PvVbsAmxdlZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2721/Authors|ICLR.cc/2021/Conference/Paper2721/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845164, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Comment"}}}, {"id": "qT8kzxTEc2", "original": null, "number": 1, "cdate": 1603702400645, "ddate": null, "tcdate": 1603702400645, "tmdate": 1605024146959, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review", "content": {"title": "Good experimental results but approach lacking in motivation and explanation", "review": "This paper proposes a method, the Causal Inference Q-Network (CIQ), for training deep RL agents that are robust to abrupt interferences in observations, such as frame blackouts, Gaussian noise or adversarial perturbations. During training time, a binary interference label is provided to the agent at each time step indicating whether an interference has been applied to the observation; the interference label acts as a switch between two neural networks that process the observation to predict the Q-values. The CIQ agent learns to predict the interference label in a supervised fashion, and at test time it uses the predicted label to switch between networks, rather than the true label. The CIQ agent is shown to learn faster and more effectively when compared to a number of baselines on a selection of OpenAI Gym tasks that are modified to include various types of observational interferences.\n\nWhile this paper presents a method that is shown to perform well empirically in the setting is aimed to tackle, I cannot recommend it for acceptance because (i) almost no motivation or intuition is given for the architectural choices that seem to be key to the performance of the agent (in particular the switching mechanism between Q-networks), and (ii) the characterisation of the agent as performing causal inference, which is the key message of the paper, is confusing in a number of ways. As a result, it\u2019s not clear what we are supposed to learn conceptually from the paper and how it can guide future research.\n\nPositives:\n\t\u2022\tIn order to be able to apply deep RL agents in real-world settings, they need to be robust to noisy observations, and so the paper is tackling an important problem.\u2028\n\t\u2022\tThe chosen baselines seem fairly chosen, such as the safe-action DQN and the DQN with concatenated interference label, and the agents are evaluated across a variety of different interference types and levels. A number of different metrics are used to evaluate the performance of each agent.\u2028\n\t\u2022\tThe CIQ agent performs the best when compared to the baselines in the interfered observation setting.\u2028\n\nConcerns:\n\t\u2022\tThere is little to no motivation given for the architectural choice of having the interference label modulate a switching mechanism between Q-networks, which based on the comparison to baselines and the ablation studies in Appendix E.3 seem to be key to the agent\u2019s performance. The only justifying claim I could identify was the following: \u201cSuch a switching mechanism prevents our network from over-generalizing the causal inference state.\u201d Could the authors clarify what exactly they mean by over-generalizing the causal inference state and how can this be inferred from the empirical results?\u2028\n\t\u2022\tBoth the CIQ and DQN-CF agent learn to predict the interference probability and use it for predicting Q-values, with the difference that the CIQ uses this predicted probability to switch between two Q-networks, but DQN-CF performs significantly worse on average - what is the intuition for this?\u2028\n\t\u2022\tThe causal graph drawn in Figure 2a seems to contradict the experimental setup described in the text:\u2028\n\t\u25e6\t(i) The interference label i_t is stated to be sampled from a Bernoulli process but in the causal graph drawn in Figure 2a, there is an arrow from the latent state z_t towards i_t; in what way does the latent state affect the probability of interference?\u2028\n\t\u25e6\t(ii) In the graph, there is no arrow (and no indirect path) from the interference label to the observed state x\u2019_t; this implies that intervening on the label, which according to Equation 1 directly affects the value of x\u2019_t, would not actually affect x\u2019_t. This is directly contradictory.\u2028\n\t\u25e6\t(iii) In the graph, there is an arrow from i_t to r_t but there is no mention in the paper for how the interference label can directly affect the reward in a given time step.\u2028\nIt\u2019s possible I have badly misunderstood the translation of the setup into the causal graph provided - in which case, could the authors please explain where I have gone wrong?\u00a0\n\t\u2022\tSection 3.3 describes a toy example that is used to show how being provided interference labels during training can lead to better sample efficiency in learning the reward distributions for each of two states (one of which is occasionally subject to interference that switches the observation to the other state). In this example, however, it is not possible to infer the latent state from the potentially interfered observation at test time, while in the DQN agents the advantage of the CIQ agent is stated to come from the fact that it can do just that. It\u2019s not clear here whether the message is that the CIQ agent performs better due to better sample efficiency or due to its ability to infer the latent state at test time.\u2028\n\nOther questions / comments\n\t\u2022\t\u00a0As far as I understood, the interference at test time always corresponds to the interference provided during training in the experiments. Were any experiments run to test generalisation to unseen types of interferences at test time? In a real world setting, this could be useful.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090027, "tmdate": 1606915777451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2721/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review"}}}, {"id": "Rx_p9MiDGdL", "original": null, "number": 4, "cdate": 1604066162793, "ddate": null, "tcdate": 1604066162793, "tmdate": 1605024146762, "tddate": null, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "invitation": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review", "content": {"title": "Interesting paper and relevant approach, supported by rigorous experimentation. On the downside, the novelty of the paper does not seem major and the predefined nature of interventions might make it unrealistic in a lot of RW scenarios.", "review": "Overview: The paper introduces a causal mechanism that both creates and explains away noise interventions into observational data fed into RL agents. The authors propose a form of resilient agent, that based on training data containing labeled interventions, learns both Q function and the causal impact of interventions on the Q function. The model architecture consists of a intervention predictor and a split parametrization of the Q function estimation as a function of intervention as shown in L^{CIQ} presented as eq.3. Finally, the authors show the performance of their proposed method on 4 visual based RL agents with two types of interventions (attacks): namely adversarial and blackout against classical baselines such as DQN and DQN with safe actions.\n\nPros: \n- Clarity: Overall I find the paper well-written and reasonably easy to follow. The problem is well motivated and the related work relevant. \n- Significance/Impact: I think the problem that the paper is trying to solve is relevant and with potentially big impact\n- Experimental design: I think the experiments section is relevant and makes a strong case for the method\n\nCons:\n- Though clear at most times, the paper should spend more time explaining the basic causal terminology and the assumptions behind the causal graph introduced in Figure 2a. I find that the authors introduce the causal coneepts in an informal, intuitive way, but that should be followed-up by a clear formalism. \n- I find that the biggest downside of the method is that it needs to be trained with the type of invervention that the agent will be resilient to. It would be interesting to create an intervention detector that is fully unsupervised and that is based more on a state anomaly detection. \n\nBordeline:\n- Novelty: In terms of novelty, the paper is a relatively straight-forward application of do-calculus to Q-value learning.\n\nFinal comments:\n\tOverall i found the paper interesting and the approach relevant, supported by rigurous experimentation. On the downside, the novelty of the paper does not seem major and the predefined nature of interventions might make it unrealistic in a lot of RW scenarios.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2721/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2721/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["~Chao-Han_Huck_Yang1", "ih2320@columbia.edu", "~Yi_Ouyang1", "~Pin-Yu_Chen1"], "authors": ["Chao-Han Huck Yang", "Danny I-Te Hung", "Yi Ouyang", "Pin-Yu Chen"], "keywords": ["Deep Reinforcement Learning", "Causal Inference", "Robust Reinforcement Learning", "Adversarial Robustness"], "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a resilient DRL framework with observational interferences. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "one-sentence_summary": "We propose a causal inference based DRL algorithm called causal inference Q-network (CIQ) under interferences toward resilient learning. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yang|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "supplementary_material": "/attachment/d600966faa231b9c60864d090e29079b13a461f0.zip", "pdf": "/pdf/fb3f6e301a9b5d162d26242ad28646ac78706517.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tZb-TuF63", "_bibtex": "@misc{\nyang2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Chao-Han Huck Yang and Danny I-Te Hung and Yi Ouyang and Pin-Yu Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=PvVbsAmxdlZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PvVbsAmxdlZ", "replyto": "PvVbsAmxdlZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2721/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090027, "tmdate": 1606915777451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2721/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2721/-/Official_Review"}}}], "count": 22}