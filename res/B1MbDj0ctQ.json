{"notes": [{"id": "B1MbDj0ctQ", "original": "ryezOoM9t7", "number": 239, "cdate": 1538087769188, "ddate": null, "tcdate": 1538087769188, "tmdate": 1545355384395, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeMK1HWg4", "original": null, "number": 1, "cdate": 1544798074210, "ddate": null, "tcdate": 1544798074210, "tmdate": 1545354525691, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Meta_Review", "content": {"metareview": "The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Not quite not enough for acceptance"}, "signatures": ["ICLR.cc/2019/Conference/Paper239/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper239/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353285995, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353285995}}}, {"id": "SJgmbwx9A7", "original": null, "number": 8, "cdate": 1543272187055, "ddate": null, "tcdate": 1543272187055, "tmdate": 1543272187055, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "S1eLPJbgAX", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "content": {"title": "Updated score and review", "comment": "I have read your reply and have updated my score and comments up above."}, "signatures": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609932, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper239/Authors|ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609932}}}, {"id": "H1ljKxccnX", "original": null, "number": 3, "cdate": 1541214338868, "ddate": null, "tcdate": 1541214338868, "tmdate": 1543272135950, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "content": {"title": "Interesting ideas, but more justifications and comparisons necessary", "review": "Thank you for the detailed reply and for updating the draft \n\nThe authors have added in a sentence about the SLDS-VAE from Johnson et al and I agree that reproducing their results from the open source code is difficult. I think my concerns about similarities have been sufficiently addressed.\n\nMy main concerns about the paper still stem from the complexity of the inference procedure. Although the inference section is still a bit dense, I think the restructuring helped quite a bit. I am changing my score to a 6 to reflect the authors' efforts to improve the clarity of the paper. The discussion in the comments has been helpful in better understanding the paper but there is still room for improvement in the paper itself.\n=============\n\nSummary: The authors present an SLDS + neural network observation model for the purpose of fitting complex dynamical systems. They introduce an RNN-based inference procedure and evaluate how well this model fits various systems. (I\u2019ll refer to the paper as SLDVBF for the rest of the review.)\n\nWriting: The paper is well-written and explains its ideas clearly\n\nMajor Comments:\nThere are many similarities between SLDVBF and the SLDS-VAE model in Johnson et al [1] and I think the authors need to address them, or at least properly compare the models and justify their choices:\n\n- The first is that the proposed latent SLDS generative models are very similar: both papers connect an SLDS with a neural network observation model. Johnson et al [1] present a slightly simpler SLDS (with no edges from z_t -> s_{t + 1} or s_t -> x_t) whereas LDVBF uses the \u201caugmented SLDS\u201d from Barber et al. It is unclear what exactly  z_t -> s_{t + 1} is in the LDVBF model, as there is no stated form for p(s_t | s_{t -1}, z_{t - 1}).\n\n- When performing inference, Johnson et al use a recognition network that outputs potentials used for Kalman filtering for z_t and then do conjugate message passing for s_t. I see this as a simpler alternative to the inference algorithm proposed in SLDVBF. SLDVBF proposes relaxing the discrete random variables using Concrete distributions and using LSTMs to output potentials used in computing variational posteriors. There are few additional tricks used, such as having these networks output parameters that gate potentials from other sources. The authors state that this strategy allows reconstruction signal to backpropagate through transitions, but Johnson et al accomplish this (in theory) by backpropagating through the message passing fixed-point iteration itself. I think the authors need to better motivate the use of RNNs over the message-passing ideas presented in Johnson et al.\n\n- Although SLDVBF provides more experiments evaluating the SLDS than Johnson, there is an overlap. Johnson et al successfully simulates dynamics in toy image systems in an image-based ball-bouncing task (in 1d, not 2d). I find that the results from SLDVBF, on their own, are not quite convincing enough to distinguish their methods from those from Johnson et al and a direct comparison is necessary.\n\nDespite these similarities, I think this paper is a step in the right direction, though it needs to far more to differentiate it from Johnson et al. The paper draws on many ideas from recent literature for inference, and incorporating these ideas is a good start. \n\nMinor Comments:\n\n- Structurally, I found it odd that the authors present the inference algorithm before fully defining the generative model. I think it would be clearer if the authors provided a clear description of the model before describing variational approximations and inference strategies. \n- The authors do not justify setting $\\beta = 0.1$ when training the model. Is there a particular reason you need to downweight the KL term as opposed to annealing?\n\n[1] Johnson, Matthew, et al. \"Composing graphical models with neural networks for structured representations and fast inference.\" Advances in neural information processing systems. 2016.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "cdate": 1542234507328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335679665, "tmdate": 1552335679665, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1xjQg-l07", "original": null, "number": 2, "cdate": 1542619171380, "ddate": null, "tcdate": 1542619171380, "tmdate": 1543257594989, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "S1eYvPS0om", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "content": {"title": "Thanks for the feedback", "comment": "- The first line of section 4.1.1 shows that q_phi depends on x{1:T}. However from figure 2a it seems that it only directly depends on x{t:T}, and that the dependence on x{1:t-1} is modelled through the dependence on z{t-1}. \n\nThanks, this is indeed a bit confusing. The latter description is our chosen parameterization. We will correct this.\n\n- In the first line of equation 7 it seems that the variational approximation q_phi for z_t only depends on x_t, but it is actually dependent also on the future x through s_t and q_meas\n- Is there a missing s_t in q_trans in the first line of (7)?\n\nIndeed, there are multiple conflicting specifications in (7) which we will address. q_trans is certainly conditioned on s_t as that is of course the entire point of s_t - influencing the transition of z.\n\n- One may even wonder if it is worth the effort: could you have used instead a deterministic s_t parameterized for example as a bidirectional LSTM with softmax output?\n\nWith regards to deterministic switching variables s_t, this is exactly the approach the Deep Variational Bayes Filter took (although not parameterized by an RNN). We argue and try to demonstrate that the performance gains stem from the probabilistic treatment of the switching variable, be it Concrete or Gaussian.\n\n- To be able to detect walls the z variables basically need to learn to represent the position of the agent and encoding the information on the position of the walls in the connection to s_t.  Would you then need to train the model from scratch for any new environment?\n\nGiven that the model was trained on the agents' global position, a retraining would certainly be required in any case. If trained on local measurements, e.g. every agent equipped with some distance sensors, one could imagine that s_t is learned based on locally encoded information only, supposing a clean split between local and global information in z_t. This, however, is mere speculation, what is shown in our experiment is mainly that a single switching variable generalizes over the entire maze, e.g. encoding all vertical walls. This is in contrast to the deterministic treatment where we found a single switching variable only covering parts of the maze as shown in figure 3.\n\nAs multiple reviewers have raised concern about the chosen structure (inference treated before generative model), we will address this part in the hope to make the paper more coherent and readily understandable."}, "signatures": ["ICLR.cc/2019/Conference/Paper239/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609932, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper239/Authors|ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609932}}}, {"id": "H1e8dgblCm", "original": null, "number": 3, "cdate": 1542619246336, "ddate": null, "tcdate": 1542619246336, "tmdate": 1543257588989, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "ryla3ZvP3X", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "content": {"title": "Addressing some of the confusion and justifications", "comment": "We'd like to cast away at least some of the confusion caused by our paper in order to improve it.\n\n- For instance (7) is quite problematic [...] Third, the variable s_t seems to be in and out at the authors discretion\n\nIndeed, there are multiple conflicting specifications in (7) which we will address. q_trans is certainly conditioned on s_t.\n\n- First, this split is neither well motivated nor justified: does it come from smartly using the Bayes and other probability rules?\n\nThis split will become clearer when we restructure the paper to describe the generative model first. The motivation is the following. It has been noted in prior work that in this kind of recurrent VAE, getting the reconstruction gradient through the generative transition is paramount for performance. In many works, the transition is only forced by the KL term, which is not enough. We'd like to achieve that by sharing parameters of the transition between generative and inference model. Therefore, we require one part of the inference model to be identical to the generative transition model so that it can be reused. This is q_trans. The other part needs to integrate the observations and then adjust whatever the prior/transition model predicted. Together, they constitute basic essence of a filter. Another motivation is: if our goal is to learn transition dynamics, why not reuse it in the inference model, where it may also be useful?\n\nNow, there is a loose parallel we can draw to the update step of a Kalman filter and the reason why this type of integration (Gaussian fusion) between q_trans and q_meas was chosen. This could be viewed as the update step of a (extended) Kalman filter if q_meas gave us an observation in the latent space. We say this with a big asterisk, as this is not to be understood as a principled argument, but just as way to think about.\n\n- Finally, if the posterior q_phi is conditioned to s_t (and I am sure it must), then the measurement model also has to be conditioned on s_t, which poses perhaps another inference problem.  \n\nYou are right, it is. However, only q_trans is conditioned on s_t, not q_meas which is only conditioned on observations (and possibly control inputs). Only together they constitute our variational approximation. Inference remains unproblematic as we factorize over time and can infer z_t and s_t in an alternating fashion.\n\n- In particular, I do not understand how come, given that q_phi is not conditioned on s_t, the past measurements and control inputs can be discarded.\n\nNote that q_meas alone constitutes only one part of the inference model, only in combination with q_trans may it infer the approximate posterior. We make the assumption of a Markovian state space system (z_t \\indep x_1, .., x{t-1} | z{t-1}), meaning all information from observations/controls up to time t will be encoded in z_t and can come through q_trans. Therefore, we do not need to condition on past observations x_{<t} explicitly. \n\n- Second, do the authors impose that this a posteriori probability is a Gaussian? \n\nYes. We multiply the resulting Gaussian pdfs of q_meas and q_trans, which results in another Gaussian pdf. This is the parameterization of our approximate posterior z_t.\n\n- Equation (10) has the same problem, in the sense that we do not understand where does it derive from, why is the chosen split justified and why the convex sum of the two distributions is the appropriate way to merge the information of the inverse measurements and the transition model.\n\nThe split is again motivated as above, one part is the prior (q_trans) and shared between inference/generative model, the other takes into account external inputs (q_meas). As to the convex sum, this is less justified than the Gaussian multiplication, but an intuition is given in the paper. This allows the model to regulate whether external information is required, and it pays an increasing penalty when more information from the outside is required. \n\n- s_t(i) is not defined and the transition model for the switching variable is not defined. This part is difficult to understand and confusing. \n\nAs another reviewer also remarked this, we recognize that this part was insufficiently detailed. We'll rework this part  and make the parameterization and sharing of weights more clear. Right now, we have a list of all parameterizations listed in Appendix B.3. This is simply implemented as a feed forward neural net.\n\nAs multiple reviewers have raised concern about the chosen structure (inference treated before generative model), we will address this part in the hope to make the paper more coherent and readily understandable."}, "signatures": ["ICLR.cc/2019/Conference/Paper239/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609932, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper239/Authors|ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609932}}}, {"id": "S1eLPJbgAX", "original": null, "number": 1, "cdate": 1542618974466, "ddate": null, "tcdate": 1542618974466, "tmdate": 1543257580166, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "H1ljKxccnX", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "content": {"title": "Comparison to Johnson et al a valid concern", "comment": "SLDS-VAE by Johnson et al. is certainly worth comparing to.  We considered a direct comparison to SLDS-VAE, however Johnson et al. restrict their evaluation of experiments to qualitative analysis, with the exception of one plot comparing the convergence of natural to standard gradient updates. We considered running the publicly available code on our own data, but our success has been limited (with regards to both training speed and convergence; the speed may be attributed to the underlying software packages like numpy/autograd and not the method in itself). More realistically, a comparison on the theoretical approach might be more fruitful (and warranted). It will certainly be referred to in our related work section where it should have been all along.\n\n- as there is no stated form for p(s_t | s{t -1}, z{t - 1})\n\np(s_t | s{t -1}, z{t - 1}) is entirely shared with q_trans(s_t | s{t -1}, z{t - 1}). It's parameterized by a simple feed forward neural network and weights are shared. Furthermore, we have a list of all parameterizations listed in Appendix B.3.\n\n- The authors state that this strategy allows reconstruction signal to backpropagate through transitions, but Johnson et al accomplish this (in theory) by backpropagating through the message passing fixed-point iteration itself. I think the authors need to better motivate the use of RNNs over the message-passing ideas presented in Johnson et al.\n\nThe key point is not just the gating but that parameters are shared between inference and generative model for p(s_t | s{t -1}, z{t - 1}). If we did neither of those things (parameter sharing + simple combination with a gate or the like), the learning signal for the generative transition would indeed be restricted to the KL term which has been shown to be insufficient. Now, in theory, this should also work, but in practice, results of those parameterizations have been underwhelming. With regards to Johnson et al, it is hard for us tell how good the learned transition actually is, as no quantitative evidence has been provided in the paper.\n\nReasons for RNNs over message-passing are probably the same as for many others choosing to go for neural VI methods. Learning by backpropagation, being in principle more flexible with the chosen distributions, convergence issues of message-passing, in particular when combined with other model components. Even though you seem at least somewhat unconvinced, we think our empirical results are certainly an argument for our chosen approach/parameterization.\n\n- The authors do not justify setting \\beta = 0.1 when training the model. Is there a particular reason you need to downweight the KL term as opposed to annealing?\n\nThe chosen \\beta has led to significantly better results. In general, the effect of s_t on the loss is small compared to z_t. What can be encoded depends a lot on the chosen time discretization, whether the variance of the decoder is learned and so forth. Downscaling the KL seemed a robust solution for our experiments which is highlighted by the fact that we chose the same \\beta for all of them. There has been a lot of recent work[1,2] about the limitations of the ELBO and about justifying different scalings of KL or likelihood. At its core, the ELBO doesn't provide strong guidance on where information has to reside (latent space or in parameters of the generative model). Practical remedies have been put forward, the most trivial being a different scaling of the KL. This requires theoretical justification, but is out of scope for our paper. We just make use of empirically founded \"tricks\" for our empirical section. Our model, in general, is not affected.\n\nAs multiple reviewers have raised concern about the chosen structure (inference treated before generative model), we will address this part in the hope to make the paper more coherent and readily understandable.\n\n[1] Danilo Jimenez Rezende and Fabio Viola. Taming VAEs.  https://arxiv.org/pdf/1810.00597.pdf\n\n[2] Alexander A. Alemi et al. Fixing a Broken ELBO. https://arxiv.org/abs/1711.00464\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper239/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609932, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper239/Authors|ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609932}}}, {"id": "Bkx8EQ3BCQ", "original": null, "number": 7, "cdate": 1542992685571, "ddate": null, "tcdate": 1542992685571, "tmdate": 1542992685571, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "S1eLPJbgAX", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "content": {"title": "Comparison to Johnson et al", "comment": "I agree with the reviewer that a comparison with the SLDS-VAE from Johnson et al would be very interesting.\nHowever, I understand the authors' challenges with running experiments modifying their the publicly available code, as I have also tried it in the past with no success.\n\nPerhaps a simpler comparison could be with against the model of [1], which is an extension of the work from Johnson et al (code available at https://github.com/emtiyaz/vmp-for-svae/).\n\n[1] Variational Message Passing with Structured Inference Networks. Wu Lin, Nicolas Hubacher, Mohammad Emtiyaz Khan, ICLR2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609932, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1MbDj0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper239/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper239/Authors|ICLR.cc/2019/Conference/Paper239/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers", "ICLR.cc/2019/Conference/Paper239/Authors", "ICLR.cc/2019/Conference/Paper239/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609932}}}, {"id": "ryla3ZvP3X", "original": null, "number": 2, "cdate": 1541005749285, "ddate": null, "tcdate": 1541005749285, "tmdate": 1541534166854, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "content": {"title": "The proposed approach is not clearly presented.", "review": "This paper proposes a new model for switching linear dynamical systems. The standard model and the proposed model are presented. Together with the inference procedure associated to the new model. This inference procedure is based on variational auto-encoders, which model the transition and measurement posterior distributions, which is exactly the methodological contribution of the manuscript. Experiments on three different tasks are reported, and qualitative and quantitative results (comparing with different state-of-the-art methods) are reported.\n\nThe standard model is very well described, formally and graphically, except for the dynamic model of the switching variable, and its dependence on z_t-1. The proposed model has a clear graphical representation, but its formal counterpart is a bit  more difficult to grasp, we need to reach 4.2 (after the inference procedure is discussed) to understand the main difference (the switching variable does not influence the observation model). Still, the dependency of the dynamics of s_t on z_t is not discussed.\n\nIn my opinion, another issue is the discussion of the variational inference procedure, mainly because it is unclear what additional assumptions are made. This is because the procedure does not seem to derive from the a posteriori distribution (at least it is not presented like this). Sometimes we do not know if the authors are assuming further hypothesis or if there are typos in the equations. \n\nFor instance (7) is quite problematic. Indeed, the starting point of (7) is the approximation of the a posteriori distribution q_phi(z_t|z_t-1,x_1:T,u_1:T), that is split into two parts, a transition model and an inverse measurement model. First, this split is neither well motivated nor justified: does it come from smartly using the Bayes and other probability rules? In particular, I do not understand how come, given that q_phi is not conditioned on s_t, the past measurements and control inputs can be discarded. Second, do the authors impose that this a posteriori probability is a Gaussian? Third, the variable s_t seems to be in and out at the authors discretion, which is not correct from a mathematical point of view, and critical since the interesting part of the model is exactly the existence of a switching variable and its relationship with the other latent/observed variables. Finally, if the posterior q_phi is conditioned to s_t (and I am sure it must), then the measurement model also has to be conditioned on s_t, which poses perhaps another inference problem.\n\nEquation (10) has the same problem, in the sense that we do not understand where does it derive from, why is the chosen split justified and why the convex sum of the two distributions is the appropriate way to merge the information of the inverse measurements and the transition model.\n\nAnother difficulty is found in the generative model, when it is announced that the model uses M base matrices (but there are S possibilities for the switching variable). s_t(i) is not defined and the transition model for the switching variable is not defined. This part is difficult to understand and confusing. At the end, since we do not understand the basic assumptions of the model, it is very hard to grasp the contribution of the paper. In addition, the interpretation of the results is much harder, since we are missing an overall understanding of the proposed approach.\n\nThe numerical and quantitative results demonstrate the ability of the approach to outperform the state-of-the-art (at least for the normal distribution and on the first two tasks).\n\nDue to the lack of discussion, motivation, justification and details of the proposed approach, I recommend this paper to be rejected and resubmitted when all these concerns will be addressed.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "cdate": 1542234507328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335679665, "tmdate": 1552335679665, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eYvPS0om", "original": null, "number": 1, "cdate": 1540409185144, "ddate": null, "tcdate": 1540409185144, "tmdate": 1541534166498, "tddate": null, "forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "content": {"title": "Interesting paper showing how to use switching variables in deep probabilistic temporal models", "review": "This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The probability distributions are parameterized by deep neural networks and learning is performed end-to-end with amortized variational inference using inference networks.\n\nThere has been a lot of recent research trying to combine probabilistic models and deep learning to define powerful transition models that can be learned in an unsupervised way, to be used for model-based RL. This paper fits in this research area, and presents a nice combination of several interesting ideas presented in related works (switching variables, structured inference networks, merging updates as in the Kalman filter). The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches.\n\nThe paper reads well, but I found the explanation and notation in section 4 quite confusing (although easy to improve). The authors propose a structured variational approximation, but the factorization assumptions are not clear from the notation (I had to rely on Figure 2a to fully understand them). \n- In the first line of equation 7 it seems that the variational approximation q_phi for z_t only depends on x_t, but it is actually dependent also on the future x through s_t and q_meas\n- The first line of section 4.1.1 shows that q_phi depends on x_{1:T}. However from figure 2a it seems that it only directly depends on x_{t:T}, and that the dependence on x_{1:t-1} is modelled through the dependence on z_{t-1}. \n- Is there a missing s_t in q_trans in the first line of (7)?\n- why do you keep the dependence on future outputs in q_meas if it is not used in the experiments and not shown in figure 2a? It only makes the notation more confusing.\n- You use f_phi to denote all the function in 4.1.1 (with different inputs). It would be clearer to use a different letter or for example add numbers (e.g. f^1_\\phi) \n- Despite being often done in VAE papers, it feels strange to me to introduce the inference model (4.1) before the generative model (4.2), as the inference model defines an approximation to the true posterior which is derived from the generative model. One could in principle use other type of approximate inference techniques while keeping the generative model unchanged. \n\nIt is difficult for me to understand how useful are in practice the switching variables. Reading the first part of the paper it seems that the authors will use discrete random variables, but they actually use for s_t continuous relaxiations of discrete variables (concrete distribution), or gaussian variables. As described in appendix B2 by the authors, training models with such continuous relaxations is often challenging in terms of hyper-parameter tuning. One may even wonder if it is worth the effort: could you have used instead a deterministic s_t parameterized for example as a bidirectional LSTM with softmax output? This may give equivalent results and remove a lot of complexity. Also, the fact that the gaussian switching variables perform better in the experiments is an indication that this may be the case.\n\nTo be able to detect walls the z variables basically need to learn to represent the position of the agent and encoding the information on the position of the walls in the connection to s_t.  Would you then need to train the model from scratch for any new environment?\n\nMinor comment:\n- in the softmax equation (6) there are missing brackets: lambda is at the denominator both for g and the log\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper239/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching Linear Dynamics for Variational Bayes Filtering", "abstract": "System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.", "keywords": ["sequence model", "switching linear dynamical systems", "variational bayes", "filter", "variational inference", "stochastic recurrent neural network"], "authorids": ["philip.becker-ehmck@volkswagen.de", "peters@ias.tu-darmstadt.de", "smagt@volkswagen.de"], "authors": ["Philip Becker-Ehmck", "Jan Peters", "Patrick van der Smagt"], "TL;DR": "A recurrent variational autoencoder with a latent transition function modeled by switching linear dynamical systems.", "pdf": "/pdf/6f302e0241598e27d16dd94b6bcbf1dfaa7b750e.pdf", "paperhash": "beckerehmck|switching_linear_dynamics_for_variational_bayes_filtering", "_bibtex": "@misc{\nbecker-ehmck2019switching,\ntitle={Switching Linear Dynamics for Variational Bayes Filtering},\nauthor={Philip Becker-Ehmck and Jan Peters and Patrick van der Smagt},\nyear={2019},\nurl={https://openreview.net/forum?id=B1MbDj0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper239/Official_Review", "cdate": 1542234507328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1MbDj0ctQ", "replyto": "B1MbDj0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper239/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335679665, "tmdate": 1552335679665, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper239/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}