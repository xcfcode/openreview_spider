{"notes": [{"id": "HylLq2EKwS", "original": "rJeITY4JwB", "number": 116, "cdate": 1569438861873, "ddate": null, "tcdate": 1569438861873, "tmdate": 1577168284022, "tddate": null, "forum": "HylLq2EKwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Collaborative Filtering With A Synthetic Feedback Loop", "authors": ["Wenlin Wang", "Hongteng Xu", "Ruiyi Zhang", "Wenqi Wang", "Lawrence Carin"], "authorids": ["wlwang616@gmail.com", "hongtengxu313@gmail.com", "ryzhang@cs.duke.edu", "wenqiwang@fb.com"], "keywords": [], "abstract": "We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender'' and a ``virtual user.'' The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior.  The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior.  The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results.  Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets. ", "pdf": "/pdf/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "paperhash": "wang|collaborative_filtering_with_a_synthetic_feedback_loop", "original_pdf": "/attachment/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "_bibtex": "@misc{\nwang2020collaborative,\ntitle={Collaborative Filtering With A Synthetic Feedback Loop},\nauthor={Wenlin Wang and Hongteng Xu and Ruiyi Zhang and Wenqi Wang and Lawrence Carin},\nyear={2020},\nurl={https://openreview.net/forum?id=HylLq2EKwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qt8rN439b8", "original": null, "number": 1, "cdate": 1576798687849, "ddate": null, "tcdate": 1576798687849, "tmdate": 1576800947239, "tddate": null, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "invitation": "ICLR.cc/2020/Conference/Paper116/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes to learn a \"virtual user\" while learning a \"recommender\" model, to improve the performance of the recommender system. A reinforcement learning algorithm is used for address the problem the authors defined. Multiple reviewers raised several concerns regarding its technical details including the feedback signal F, but the authors have not responded to any of the concerns raised by the reviewers. The lack of authors involvement in the discussion suggest that this paper is not at the stage to be published.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Collaborative Filtering With A Synthetic Feedback Loop", "authors": ["Wenlin Wang", "Hongteng Xu", "Ruiyi Zhang", "Wenqi Wang", "Lawrence Carin"], "authorids": ["wlwang616@gmail.com", "hongtengxu313@gmail.com", "ryzhang@cs.duke.edu", "wenqiwang@fb.com"], "keywords": [], "abstract": "We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender'' and a ``virtual user.'' The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior.  The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior.  The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results.  Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets. ", "pdf": "/pdf/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "paperhash": "wang|collaborative_filtering_with_a_synthetic_feedback_loop", "original_pdf": "/attachment/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "_bibtex": "@misc{\nwang2020collaborative,\ntitle={Collaborative Filtering With A Synthetic Feedback Loop},\nauthor={Wenlin Wang and Hongteng Xu and Ruiyi Zhang and Wenqi Wang and Lawrence Carin},\nyear={2020},\nurl={https://openreview.net/forum?id=HylLq2EKwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730411, "tmdate": 1576800283199, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper116/-/Decision"}}}, {"id": "SkeAqcyO_H", "original": null, "number": 1, "cdate": 1570400918304, "ddate": null, "tcdate": 1570400918304, "tmdate": 1572972636775, "tddate": null, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "invitation": "ICLR.cc/2020/Conference/Paper116/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper is essentially an attempt to incorporate a form of reinforcement learning into recommender systems, via the use of a synthetic feedback loop and a \"virtual user\".\n\nOverall this seems like a nice attempt to combine Inverse Reinforcement Learning frameworks with collaborative filtering algorithms.\n\nWithout a background in reinforcement learning, it was difficult for me to assess exactly how this compares to similar reinforcement learning work, or exactly how \"obvious\" the technical contribution is.\n\nNevertheless combining reinforcement learning with recommender systems is a topic of growing interest, and it is nice to see a paper in this space that makes a contribution with strong quantitative experiments, i.e., it is able to compete with (reasonably) strong baselines.\n\nThe experiments mostly look good, with a few representative, and quite large, datasets. The baselines are perhaps not state-of-the-art but represent reasonable points of comparison and are enough for a proof-of-concept. The paper is quite thorough in terms of experimental comparisons.\n\nOverall this seems like an interesting approach and a reasonably timely paper, making what seems like a compelling contribution to a current hot topic, and passes the bar in terms of experiments. Regarding the merits of the technical contribution, I'll perhaps have to defer to other reviewers, but overall the contribution seems above the bar."}, "signatures": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Collaborative Filtering With A Synthetic Feedback Loop", "authors": ["Wenlin Wang", "Hongteng Xu", "Ruiyi Zhang", "Wenqi Wang", "Lawrence Carin"], "authorids": ["wlwang616@gmail.com", "hongtengxu313@gmail.com", "ryzhang@cs.duke.edu", "wenqiwang@fb.com"], "keywords": [], "abstract": "We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender'' and a ``virtual user.'' The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior.  The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior.  The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results.  Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets. ", "pdf": "/pdf/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "paperhash": "wang|collaborative_filtering_with_a_synthetic_feedback_loop", "original_pdf": "/attachment/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "_bibtex": "@misc{\nwang2020collaborative,\ntitle={Collaborative Filtering With A Synthetic Feedback Loop},\nauthor={Wenlin Wang and Hongteng Xu and Ruiyi Zhang and Wenqi Wang and Lawrence Carin},\nyear={2020},\nurl={https://openreview.net/forum?id=HylLq2EKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859161646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper116/Reviewers"], "noninvitees": [], "tcdate": 1570237756838, "tmdate": 1575859161664, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper116/-/Official_Review"}}}, {"id": "Hkg4-Ixptr", "original": null, "number": 2, "cdate": 1571780091524, "ddate": null, "tcdate": 1571780091524, "tmdate": 1572972636739, "tddate": null, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "invitation": "ICLR.cc/2020/Conference/Paper116/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to learn a \"virtual user\" while learning a \"recommender\" model, to improve the performance of the recommender system. The \"virtual user\" is used to produce the \"reward\" signal for training the recommendation system (which is trained using RL). Jointly learning the recommender and the \"virtual user\" provides a synthetic feedback loop that helps to improve the performance of the recommendation system. The paper formulates the training of the \"virtual user\" as an inverse RL problem and uses adversarial regularizer.\n\nThe paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work. In general, the writing needs to be polished as well.\n\n===============\n\nFollowing are the items that need to be improved:\n\n## Significance of the results\n\n* The results in Tables 3 and 4 provide only marginal improvements over the baseline. These improvements do not appear to be statistically significant. It would help if the authors comment on why these results appear significant and also provide variance values/curves for the results.\n\n## Role of the feedback general F\n\n* Is there any separate loss for training F or is it always trained along with \\pi?\n\n* Is the feedback capturing some sort of \"memory\" or \"past preferences/behaviors\" of the user? If yes, wouldn't using a recurrent recommender also capture these effects without needing the feedback model F? Note that I am not criticizing the choice of F. I am trying to understand the role played by F (in addition to the recommender pi).\n\n* If there is no separate loss for F, I wonder how would the performance change if the F network was to be removed and the reward value was to be fed into a recurrent recommended. (The paper seems to have considered a special case where a non-recurrent recommender is used with the reward value). The reason I am stressing on this is that one of the key distinctions of the authors' work is the use of feedback generator and it would be useful to quantify the benefits on this modification.\n\n\n## Others\n\n* How is the static representation, x, computed? From eq 16 (appendix), it appears that x is a binary vector that captures what items have been purchased/reviewed. Is that correct? If yes, wouldn't x have an enormous dimensionality?\n\n* The recommendation system is operating in a sequential decision-making setup. The formulation of R and F do no consider any sequential information. For example, let us say that the recommender recommends the items a1, a2 and a3 in 3 timesteps. The reward at time 3 is a function of x and a3 and the information of a1 and a2 is not used.\n\n* The loss function has many components and I want to make sure I understand what gradients flow where. So please correct me if I missed something;\n\n    * supervised learning loss (from the real data) trains pi and F (equation 6).\n    * pi and F collaborate to get a high reward from R (since we do not have the ground truth corresponding to R). No gradient flows to R.\n    * Adversarial game between pi and R - which is used to update R.\n\n* I understand that the loss is defined according to the output of the last step but do the gradients flow through all the intermediate steps?\n\n* The training/inference procedure seems to be doing something strange. Let us assume that T = 5. So the 5 items are recommended to the \"virtual user\" and only the 5th item is recommended to the real user. Now the recommendation of the 5th item depends on the first 4 items that the real user has not seen.\n\n* In equation 15, what does the subscript F stand for?\n\n* What algorithm is used to train the policy pi?\n\n=================\n\n\nThe following are the items that should be corrected in the future iterations of the paper. Please note that these things did not impact the score.\n\n* It seems that the irrelevance of an item (for a user) is treated the same way as missing information about the relevance of the item. Could the authors discuss this more in the paper?\n\n* Is there any reason why this approach is only used with CF methods?\n\n* Some writing choices seem to make the problem statement more complex than it is. For example, the authors discuss how their work is different than traditional RL instead of simply stating that their work is set up as a POMDP.\n\n* Articles are missing (or mixed) up at many places.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Collaborative Filtering With A Synthetic Feedback Loop", "authors": ["Wenlin Wang", "Hongteng Xu", "Ruiyi Zhang", "Wenqi Wang", "Lawrence Carin"], "authorids": ["wlwang616@gmail.com", "hongtengxu313@gmail.com", "ryzhang@cs.duke.edu", "wenqiwang@fb.com"], "keywords": [], "abstract": "We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender'' and a ``virtual user.'' The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior.  The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior.  The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results.  Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets. ", "pdf": "/pdf/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "paperhash": "wang|collaborative_filtering_with_a_synthetic_feedback_loop", "original_pdf": "/attachment/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "_bibtex": "@misc{\nwang2020collaborative,\ntitle={Collaborative Filtering With A Synthetic Feedback Loop},\nauthor={Wenlin Wang and Hongteng Xu and Ruiyi Zhang and Wenqi Wang and Lawrence Carin},\nyear={2020},\nurl={https://openreview.net/forum?id=HylLq2EKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859161646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper116/Reviewers"], "noninvitees": [], "tcdate": 1570237756838, "tmdate": 1575859161664, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper116/-/Official_Review"}}}, {"id": "rJx5gsqRKB", "original": null, "number": 3, "cdate": 1571887858148, "ddate": null, "tcdate": 1571887858148, "tmdate": 1572972636698, "tddate": null, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "invitation": "ICLR.cc/2020/Conference/Paper116/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Although I assume somebody well-versed in the recent collaborative filtering literature would not have trouble, I had too much difficulty understanding the setup and the model to be able to recommend the paper for acceptance.  Perhaps if you clarify the following questions in your rebuttal and the final version of the paper, I can view the paper more favorably:\n\n- In the Problem Statement section, it says that we assume that there is an unknown user preference vector a_i for each user with elements in reals.  How should we think about what a_i is?  Is it a binary vector? A probability distribution over items?  An arbitrary vector of reals? Nonnegative reals?  \n- In the problem statement, we have that \"recommender predicts preferred items . . . and the user generates feedback to the recommender.\"  Can you make this concrete?  Would feedback be like a binary label for whether each of the recommended items was of interest?  Or whether any were of interest? \n- In general, I had a very difficult time figuring out the difference between the information in x_i and \"feedback\".  Is it that x_i only has information about what user i selected, but no information about what the user was presented and did not select?  Is the feedback then the combination of what was presented and whether or not it was selected?\n- In Section 2.2, we discuss a_i^t as the recommended items provided by the recommender, sampled from the action space A -- this seems like a different a_i from the one in the Problem Statement section?  If so, this is an unfortunate notation clash.\n- If we're producing sets of items at a time, as your wording frequently indicates, should we think of the action space A as a set of sets of items?  \n- When describing the \"recommendation-feedback loop as a Markov Decision Process\", you introduce a transition probability over user preferences given actions.  Are you understanding a user's preferences to be changing over time?  Or is it just our estimate of the user's preferences that are changing over time?  \n- In Figure 2, does the Recommender have state that can accumulate all the virtual feedbacks v_i in each step?\n- After equation (1), it says that v_i is \"the embedding of user feedback to historical recommended items\", but in Figure 2 it seems like v_i can only have information about the preceding recommendation and estimated reward, rather than cumulative.  Can you clarify?\n- Is the reward estimator estimating something that could actually be observed?  Suppose we had a user in the loop -- can you give an example of what a reward would be for a particular set of actions?  Would it be 0/1 for whether the user clicked on one of the actions?   \n- In the setup for the learning task, Section 3.1 says that x_i is the \"historical behaviors of user i derived from the user-item matrix X and \\tilde{a}_i is the ground truth of the recommended item for the user based on his/her behavior x_i.\" I don't understand this.  x_i has all the items the user has clicked.  What new information is in \\tilde{a}_i?  Is it referring to the a_i in the problem statement, some vector representing preferences?  Is such a vector ever known during training?  Please clarify.\n- In the paragraph before equation 7, you say \"given the recommended policy and the feedback generator\" -- should that be \"given the recommender policy\"?\n- In Figure 2, we see the recommender producing what seems to be a vector a_i.  Is this a set of recommendations?  Or a distribution over items from which we could sample recommendations? Or a single recommendation, in which case the graphic should change?"}, "signatures": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper116/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Collaborative Filtering With A Synthetic Feedback Loop", "authors": ["Wenlin Wang", "Hongteng Xu", "Ruiyi Zhang", "Wenqi Wang", "Lawrence Carin"], "authorids": ["wlwang616@gmail.com", "hongtengxu313@gmail.com", "ryzhang@cs.duke.edu", "wenqiwang@fb.com"], "keywords": [], "abstract": "We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender'' and a ``virtual user.'' The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior.  The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior.  The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results.  Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets. ", "pdf": "/pdf/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "paperhash": "wang|collaborative_filtering_with_a_synthetic_feedback_loop", "original_pdf": "/attachment/e9d4060569b446c4911e42c0aa90496a501249dd.pdf", "_bibtex": "@misc{\nwang2020collaborative,\ntitle={Collaborative Filtering With A Synthetic Feedback Loop},\nauthor={Wenlin Wang and Hongteng Xu and Ruiyi Zhang and Wenqi Wang and Lawrence Carin},\nyear={2020},\nurl={https://openreview.net/forum?id=HylLq2EKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylLq2EKwS", "replyto": "HylLq2EKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859161646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper116/Reviewers"], "noninvitees": [], "tcdate": 1570237756838, "tmdate": 1575859161664, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper116/-/Official_Review"}}}], "count": 5}