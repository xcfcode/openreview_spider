{"notes": [{"id": "rJlk71rYvH", "original": "BJxQ_EhuPr", "number": 1601, "cdate": 1569439510854, "ddate": null, "tcdate": 1569439510854, "tmdate": 1577168230379, "tddate": null, "forum": "rJlk71rYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Aqpw4UFnyx", "original": null, "number": 1, "cdate": 1576798727546, "ddate": null, "tcdate": 1576798727546, "tmdate": 1576800908951, "tddate": null, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "invitation": "ICLR.cc/2020/Conference/Paper1601/-/Decision", "content": {"decision": "Reject", "comment": "I agree with the reviewers that this paper has serious limitations in the experimental evaluation.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709546, "tmdate": 1576800258332, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1601/-/Decision"}}}, {"id": "rkgu8hhqYr", "original": null, "number": 1, "cdate": 1571634255683, "ddate": null, "tcdate": 1571634255683, "tmdate": 1574792542375, "tddate": null, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "invitation": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper presents regularization techniques for model based reinforcement learning which attempt to build counterfactual reasoning into the model. In particular, they present auxiliary loss terms which can be used in \"what if\" scenarios where the actual state is unknown. Given certain assumptions, they show that this added regularization can improve generalization to unseen problem settings. Specifically they propose two forms of regularization: (1) enforcing that for different actions the predicted next state should be different (action-control) and (2) enforcing that when certain parts of the low dimensional state are perturbed, over a model rollout the perturbation should only affect the perturbed parts of the state, essentially encouraging the latent space features to be independent (disentanglement). \n\nOverall the idea is well motivated - incorporating counterfactual reasoning into model based RL has potential to to improve generalization. Also, while the assumptions needed for the regularization to be correct are not always true, they do seem to hold in many cases. Lastly, the results do seem to indicate that generalization is slightly improved when using the proposed forms of regularization.\n\nMy criticisms are:\n\n(1) As mentioned in the paper Action-Control assumes that at every single timestep the agent has potential to change the state. However there may be settings where the agent can always change state, but only a small component of the state. In these cases the states should be quite similar. For example a robot only moving a single object when the state consists of many objects. Also as mentioned in the paper Disentanglement will not work in stochastic environments. One concern I have is that since different environments can violate the assumptions to varying degrees, it seems like actually using the regularization and picking the correct hyperparameter to weight it will be very challenging. \n\n(2) The current results are only demonstrated in a single, custom environment. Additionally performance is shown on only 2 test tasks, and in all cases in Table 2 it is unclear how to interpret the reward. Does this performance constitute completing the task? What is the best possible cumulative reward in this case? The performance improvement seems small, but it is difficult to judge without knowing the details of the task.\n\nI think the paper would be significantly improved by (1) adding experiments in more environments, especially standard model based RL environments where the performance of many existing methods is known and (2) adding comparisons to other forms of model regularization, for example using an ensemble of models. My current rating is Weak Accept.\n\nSome other questions:\n- In Table 2 does MPC amount to PlaNet?\n- How sensitive are the current numbers to planning parameters (horizon, num samples)?\n- Can you provide error bars for the numbers in the tables?\n\n______________________________________________\n\nAfter author responses and closer examination of the paper I have some additional concerns about experimental details.  Changing my score from 'Weak Accept' to 'Weak Reject'", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829975658, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1601/Reviewers"], "noninvitees": [], "tcdate": 1570237735007, "tmdate": 1575829975670, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review"}}}, {"id": "ryeNrBz3jB", "original": null, "number": 3, "cdate": 1573819708084, "ddate": null, "tcdate": 1573819708084, "tmdate": 1573819708084, "tddate": null, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "invitation": "ICLR.cc/2020/Conference/Paper1601/-/Official_Comment", "content": {"title": "General Response", "comment": "\nWe would like to thank the reviewers for taking the time to review the paper and for their insightful feedback.\n\nRegarding environment dependency, we agree with the view that our proposed regularizations are environment-dependent; state and action spaces as well as an agent's ability to control the environment may vary.\nWe do find some sensitivity to training hyperparameters, as well as planning horizon, and consistent with the reviewers' intuitions we would expect some level of task-specific hyperparameter tuning to be required to achieve optimal results for most tasks.\n\nRegarding the case of action-control regularization in an environment where an agent's action may not always have an impact on the observed state, we would note that the regularization enforces a difference between learned latent states, not necessarily between observations.\nIt is possible in principle for a transition model to learn to produce different latent states given different agent actions, even if the latent states produce the same observations.\n\nRegarding the relationship between the planning algorithm used in our experiments and PlaNet, MPC in table 2 is not exactly equivalent to PlaNet. Although our model is similar in structure to Hafner et al.[1], their planning approach is based on the Cross Entropy Method [2] while we use a simpler deterministic search (select the action leading to maximum predicted reward over a fixed horizon).\n\nRegarding the details of the generalization test environment, the maximum score in every version of the task is 8.0, and this has been clarified in section 4.\n\nRegarding some of the related work mentioned by reviewer #3, we agree on the relevance of the literature on auxiliary tasks and have updated the related work section accordingly.\n\n[1] Hafner, Danijar, et al. \"Learning latent dynamics for planning from pixels.\" arXiv preprint arXiv:1811.04551 (2018).\n[2] Chua, Kurtland, et al. \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models.\" Advances in Neural Information Processing Systems. 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1601/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1601/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1601/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlk71rYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1601/Authors", "ICLR.cc/2020/Conference/Paper1601/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1601/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1601/Reviewers", "ICLR.cc/2020/Conference/Paper1601/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1601/Authors|ICLR.cc/2020/Conference/Paper1601/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153603, "tmdate": 1576860555055, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1601/Authors", "ICLR.cc/2020/Conference/Paper1601/Reviewers", "ICLR.cc/2020/Conference/Paper1601/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1601/-/Official_Comment"}}}, {"id": "rylvNBMCFS", "original": null, "number": 2, "cdate": 1571853615400, "ddate": null, "tcdate": 1571853615400, "tmdate": 1572972447442, "tddate": null, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "invitation": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers regularization based on \"counterfactual\" trajectories.\nNamely, it suggests two losses, action-control and disentanglement regularization.\nIt experimentally evaluates the benefits of such regularization in the StarIntruders environment.\n\nThe paper is well written and explained.\n\nIssues:\n\n1) Authors evaluated the two suggested regularizations in separate. \nI would like to also see numbers from a combination of these.\n\n2) I think the related work is missing a large line of work on \"auxiliary tasks\".\nIt seems to me that this paper would exactly fit within that scope?\n\n3) My main issue is the evaluation.\nThe evaluation is done on a in-house game and compares to very few methods.\nFor a paper that has very little theory and thus most of the value is in the empirical evaluation, I think that is a problem.\nIf authors opted for example for Space Invaders (they do say it is similar) or simply more games, one would have many more existing numbers to compare against.\n\nMinor issues:\n\n1) The first regularization - action control regularization is motivated by the idea that there is always an action that changes the state. While true for most environments, this does not hold in general.\n\nSummary:\n\nOverall, this paper has potential but I don not believe is good enough - I suggest a reject.\nThe main problem is that the idea is relatively simple, there is no theory and thus the crucial piece of the paper has to be the empirical evaluation.\nAnd the evaluation only compares to a single method with no regularization, no auxiliary tasks and reports only experiments on a single game."}, "signatures": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829975658, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1601/Reviewers"], "noninvitees": [], "tcdate": 1570237735007, "tmdate": 1575829975670, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review"}}}, {"id": "Hylr1WGlcH", "original": null, "number": 3, "cdate": 1571983580704, "ddate": null, "tcdate": 1571983580704, "tmdate": 1572972447399, "tddate": null, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "invitation": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method called \"counterfactual regularization\" whereby the dynamics/transition model is encourage to not have degeneracies where the actions don't influence the state transitions.  Concretely, this is done by, for every state, computing the maximum deviation of Transition model under a different action than the action taken in the history, and encourage that deviation to be as large as possible.  If that maximum deviation is 0, then all actions lead to the same next state. Empirical results show reasonable improvements in the StarIntruders task.\n\nMy biggest complaint (and the only one barring me from supporting acceptance) is that I don't see the body of results as scientifically solid.  Example of additional results that I would find much more convincing are:\n\n-- Experiments on more than one environment.  Currently, this paper should be judged solely for its empirical improvements, because there is little formal analysis or rigorous derivations.  But it's hard to judge that based on only one experiment.\n\n-- Deeper investigation into the effects of counterfactual regularization, including its interaction with learning disentangled representations.  Right now, there is no investigation, just a numerical score of reward attained.  This does not lead to much scientific insight.\n\n-- Exploration of the limitations of the approach.  Personally, I think this approach is reasonable for video games with a few discrete actions, but quickly runs into problems for more complex action spaces."}, "signatures": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1601/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nealla@oregonstate.edu", "fuxin.li@oregonstate.edu", "xiaoli.fern@oregonstate.edu"], "title": "Counterfactual Regularization for Model-Based Reinforcement Learning", "authors": ["Lawrence Neal", "Li Fuxin", "Xiaoli Fern"], "pdf": "/pdf/972823653278bfd6af8bff5300815117b60627c3.pdf", "TL;DR": "When training a world model, you can encode useful assumptions into the loss by using training-time counterfactuals.", "abstract": "In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.\n", "code": "https://drive.google.com/file/d/1aD-5x28ZuDo62i8cfAD7GnliK_fDXhfD/view", "keywords": ["Counterfactual", "Model-Based Reinforcement Learning"], "paperhash": "neal|counterfactual_regularization_for_modelbased_reinforcement_learning", "original_pdf": "/attachment/12186339467594a5e7188fa257638c7d2d034d2f.pdf", "_bibtex": "@misc{\nneal2020counterfactual,\ntitle={Counterfactual Regularization for Model-Based Reinforcement Learning},\nauthor={Lawrence Neal and Li Fuxin and Xiaoli Fern},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlk71rYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlk71rYvH", "replyto": "rJlk71rYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1601/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829975658, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1601/Reviewers"], "noninvitees": [], "tcdate": 1570237735007, "tmdate": 1575829975670, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1601/-/Official_Review"}}}], "count": 6}