{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573623011, "tcdate": 1521573623011, "number": 334, "cdate": 1521573622661, "id": "Byy-yJycz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkElQvkvz", "replyto": "HkElQvkvz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies", "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.", "pdf": "/pdf/10c27c4b74af7cd4a418d7b67a0afc93b62ff4c7.pdf", "TL;DR": "We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.", "paperhash": "dipietro|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies", "_bibtex": "@misc{\ndipietro2018analyzing,\ntitle={Analyzing and Exploiting {NARX} Recurrent Neural Networks for Long-Term Dependencies},\nauthor={Robert DiPietro, Christian Rupprecht, Nassir Navab, Gregory D. Hager},\nyear={2018},\nurl={https://openreview.net/forum?id=r1pW0WZAW},\n}", "keywords": ["recurrent neural networks", "long-term dependencies", "long short-term memory", "LSTM"], "authors": ["Robert DiPietro", "Christian Rupprecht", "Nassir Navab", "Gregory D. Hager"], "authorids": ["rdipietro@gmail.com", "christian.rupprecht@in.tum.de", "nassir.navab@tum.de", "hager@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730166726, "tcdate": 1518461677008, "number": 206, "cdate": 1518461677008, "id": "HkElQvkvz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkElQvkvz", "original": "r1pW0WZAW", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies", "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.", "pdf": "/pdf/10c27c4b74af7cd4a418d7b67a0afc93b62ff4c7.pdf", "TL;DR": "We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.", "paperhash": "dipietro|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies", "_bibtex": "@misc{\ndipietro2018analyzing,\ntitle={Analyzing and Exploiting {NARX} Recurrent Neural Networks for Long-Term Dependencies},\nauthor={Robert DiPietro, Christian Rupprecht, Nassir Navab, Gregory D. Hager},\nyear={2018},\nurl={https://openreview.net/forum?id=r1pW0WZAW},\n}", "keywords": ["recurrent neural networks", "long-term dependencies", "long short-term memory", "LSTM"], "authors": ["Robert DiPietro", "Christian Rupprecht", "Nassir Navab", "Gregory D. Hager"], "authorids": ["rdipietro@gmail.com", "christian.rupprecht@in.tum.de", "nassir.navab@tum.de", "hager@cs.jhu.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730166726, "tcdate": 1509133828993, "number": 739, "cdate": 1518730166716, "id": "r1pW0WZAW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1pW0WZAW", "original": "ryBWA-WA-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies", "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.", "pdf": "/pdf/f80021adf2fd7c8ee5a1a22b404ee18ab77bff66.pdf", "TL;DR": "We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.", "paperhash": "dipietro|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies", "_bibtex": "@misc{\ndipietro2018analyzing,\ntitle={Analyzing and Exploiting {NARX} Recurrent Neural Networks for Long-Term Dependencies},\nauthor={Robert DiPietro and Christian Rupprecht and Nassir Navab and Gregory D. Hager},\nyear={2018},\nurl={https://openreview.net/forum?id=r1pW0WZAW},\n}", "keywords": ["recurrent neural networks", "long-term dependencies", "long short-term memory", "LSTM"], "authors": ["Robert DiPietro", "Christian Rupprecht", "Nassir Navab", "Gregory D. Hager"], "authorids": ["rdipietro@gmail.com", "christian.rupprecht@in.tum.de", "nassir.navab@tum.de", "hager@cs.jhu.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}