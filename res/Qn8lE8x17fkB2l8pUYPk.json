{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458194214299, "tcdate": 1458194214299, "id": "ZY9l6rZ2Qh5Pk8ELfEXL", "invitation": "ICLR.cc/2016/workshop/-/paper/134/review/10", "forum": "Qn8lE8x17fkB2l8pUYPk", "replyto": "Qn8lE8x17fkB2l8pUYPk", "signatures": ["~Yangqing_Jia1"], "readers": ["everyone"], "writers": ["~Yangqing_Jia1"], "content": {"title": "An empirical way to dynamically change the network architecture via a weighted average", "rating": "6: Marginally above acceptance threshold", "review": "A common setting in deep networks is to design the network first, \"freeze\" the network architecture, and then train the parameters. The paper pointed out a potential dilemma of that, in the sense that complex networks may have better representation power but may be hard to train. To address this issue the paper proposed to train the network in a hybrid fashion where simpler components and more complex components are combined via a weight average, and the weight is updated over the training procedure to introduce the more complex components, while utilizing the fast training capability of simpler ones.\n\nThe paper is mainly presented in an empirical way, showing the performance improvement one can obtain from that. The theory is a bit lacking: for example, a proper decay schedule between the simple and complex components may be critical for convergence, and right now it is mostly setting by hand via hyperparameter \\tau. However, the paper does a proper claim of its contributions and does not exaggerate it.\n\nI think this would be an interesting empirical paper to be presented as a workshop publication.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "abstract": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "pdf": "/pdf/Qn8lE8x17fkB2l8pUYPk.pdf", "paperhash": "almeida|gradnets_dynamic_interpolation_between_neural_architectures", "conflicts": ["enlitic.com"], "authors": ["Diogo Almeida", "Nate Sauder"], "authorids": ["diogo@enlitic.com", "nate@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580040723, "ddate": null, "super": null, "final": null, "duedate": 1460326500000, "tcdate": 1456580040723, "id": "ICLR.cc/2016/workshop/-/paper/134/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "Qn8lE8x17fkB2l8pUYPk", "replyto": "Qn8lE8x17fkB2l8pUYPk", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/134/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468102500000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457616518142, "tcdate": 1457616518142, "id": "4QyA39PRPhBYD9yOFqWy", "invitation": "ICLR.cc/2016/workshop/-/paper/134/review/12", "forum": "Qn8lE8x17fkB2l8pUYPk", "replyto": "Qn8lE8x17fkB2l8pUYPk", "signatures": ["ICLR.cc/2016/workshop/paper/134/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/134/reviewer/12"], "content": {"title": "Blending two architectures results in an unknown closed-form loss.", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose to blend any two architectural components as the time of optimisation progresses. As the time progresses, the initial approach, e.g. employed rectifier, is gradually switched off in place of another rectifier. The authors claim that this strategy is good for a fast convergence and they present some experimental results.\n\nPros:\n- recognition of the convergence problem e.g. with drop-out\n- idea of evolving objective\n\nCons:\n- as the network switches between two approaches, it is unclear what is the closed form loss that the network optimizes\n- not clear what are theoretical guarantees of such optimization or the landscape of the local minima\n- the results indeed show some improvement, however, is this amount of improvement statistically significant and justified at a cost of even more obscure optimisation process?\n- lack of clear timing analysis - as the authors propose an approach which supposedly helps fast convergence, why not provide detailed plots of objective/accuracy vs. epochs?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "abstract": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "pdf": "/pdf/Qn8lE8x17fkB2l8pUYPk.pdf", "paperhash": "almeida|gradnets_dynamic_interpolation_between_neural_architectures", "conflicts": ["enlitic.com"], "authors": ["Diogo Almeida", "Nate Sauder"], "authorids": ["diogo@enlitic.com", "nate@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580039969, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580039969, "id": "ICLR.cc/2016/workshop/-/paper/134/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "Qn8lE8x17fkB2l8pUYPk", "replyto": "Qn8lE8x17fkB2l8pUYPk", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/134/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455826298880, "tcdate": 1455826298880, "id": "Qn8lE8x17fkB2l8pUYPk", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "Qn8lE8x17fkB2l8pUYPk", "signatures": ["~Diogo_Almeida1"], "readers": ["everyone"], "writers": ["~Diogo_Almeida1"], "content": {"CMT_id": "", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "abstract": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "pdf": "/pdf/Qn8lE8x17fkB2l8pUYPk.pdf", "paperhash": "almeida|gradnets_dynamic_interpolation_between_neural_architectures", "conflicts": ["enlitic.com"], "authors": ["Diogo Almeida", "Nate Sauder"], "authorids": ["diogo@enlitic.com", "nate@enlitic.com"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}