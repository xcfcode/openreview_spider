{"notes": [{"id": "mZLhA0xFGmR", "original": "7aYFFMdX-8Y", "number": 3389, "cdate": 1601308376091, "ddate": null, "tcdate": 1601308376091, "tmdate": 1614985648308, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7OItmzpyghr", "original": null, "number": 1, "cdate": 1610040514268, "ddate": null, "tcdate": 1610040514268, "tmdate": 1610474122381, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "invitation": "ICLR.cc/2021/Conference/Paper3389/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes an approach to sparse CCA with deep neural nets, performing simultaneous feature selection with stochastic gating and canonical correlation maximization.  The reviewers think that there is merit in defining an objective function that optimizes the goals jointly throughout the networks. However, the paper has not clearly presented the novelty in methodology. In particular, the reviewers agree that the paper needs to clearly distinguish itself from the two building blocks (Andrew et al. 2013 and Louizos et al. 2017), and demonstrate the significance of combining the two techniques theoretically and/or experimentally. Also, there is a large literature in sparsifying classical method. Sufficient discussions and comparisons with prior work can better position the current work in the literature."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040514255, "tmdate": 1610474122365, "id": "ICLR.cc/2021/Conference/Paper3389/-/Decision"}}}, {"id": "KFwh2klSf6J", "original": null, "number": 1, "cdate": 1603881615561, "ddate": null, "tcdate": 1603881615561, "tmdate": 1605024009779, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "invitation": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review", "content": {"title": "Sparse Deep CCA variant building on existing components with good but somewhat partial empirical evaluation", "review": "Summary: The authors propose a new non-linear CCA variant that learns mappings that are sparse with respect to the input variables, using approximate $l_0$ regularisation, to improve performance for applications with large number of features but few samples.\n\nReasons for score: I am leaning towards rejection due to the straightforward nature of the work. The method combines two existing techniques in fairly obvious way and despite good empirical comparisons has also issues in evaluation since more recent comparison methods are missing.\n\nDetailed feedback: The related work and importance of the application are well covered, and the technical solution is sound. The conceptual novelty is, however, fairly limited; several sparse CCA variants have been proposed in the past and switching to proper sparsity ($l_0$ vs more common $l_1$) is a natural thing to do. Furthermore, in recent years technical solutions building on stronger sparsity priors have been proposed for closely related models (e.g. Boyveyron et al. \"Bayesian variable selection of globally sparse probabilistic PCA\" (2018) discusses this in detail for PCA, and many algorithmic details for PCA generalise easily for CCA by interpreting CCA as group-sparse PCA).\n\nThe specific technical solution presented here appears to be new, but builds directly on existing and relatively obvious choices: The loss matches Andrew et al. (2013) and the $l_0$ approximation is from Yamada et al. (2020). Even though the specific formulation in the latter is recent, the underlying auxiliary variable construct has been used for similar purposes before. The simplicity of the technical approach is highlighted by the fact that the whole model description takes only slightly more than one page of the paper. In summary, the paper does not make fundamental conceptual or technical contributions. It certainly has potential for being a useful practical tool for the task, but the required scientific insight is limited.\n\nThe empirical demonstrations are nice and illustrative, but carried out on somewhat simplified benchmark data. They do show that the method works well in comparison against reasonably chosen competing methods, but do not clearly indicate qualitative change in CCA applications. The advantage over ordinary DCCA, published already 7 years ago, is not particularly striking in Table 2, and in recent years quite a few deep CCA variants have been proposed but are not compared against (or cited). Consequently, we cannot really evaluate whether this advances the field in practice; there is potential, but as it is the empirical comparisons do not seem sufficient to overcome the lack of technical and conceptual contribution.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3389/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3389/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3389/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076749, "tmdate": 1606915804119, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3389/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review"}}}, {"id": "rWE5HoiDBew", "original": null, "number": 2, "cdate": 1603998175173, "ddate": null, "tcdate": 1603998175173, "tmdate": 1605024009717, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "invitation": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review", "content": {"title": "Gated CCA", "review": "This paper combines an approximate $L_0$ regularization on the canonical vectors with CCA to encourage the CCA for getting sparse vectors. In addition, the CCA is computed on embeddings from a neural network, which make it possible to capture non-linear correlations.\n\nOverall, the paper is well written and easy to follow. The paper seems to be a combination of deep CCA (Andrew et al. 2013) and Louizos et al. 2017. In particular, the $L_0$ regularization approximation is very similar to that proposed in Louizos et al. 2017. It would be great if the authors could be more clear on illustrating the differences (if any). Therefore, the novelty of this paper is unclear. \n\nThe experiments could be improved. Since most of the experiments were carried out on relatively small datasets with reasonable sized model, it would be great to have multiple runs that illustrate the stability/variance of the method. In addition, the major benefit of using neural networks as embedding function is the ability to capture non-linear relationships. It would be great to add a synthetic example to illustrate this benefit. The authors mentioned the use of early stopping and hyper-parameter selection, however, it is not clear based what criteria those actions were carried out. My guess is that it is based on the objective in eq. 4 on the validation set. It would be great if the authors could make this clear, because from the synthetic experiments, $\\lambda$ plays a quite important role for the final performance.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3389/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3389/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3389/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076749, "tmdate": 1606915804119, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3389/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review"}}}, {"id": "InP7y7fCC8W", "original": null, "number": 3, "cdate": 1604215253347, "ddate": null, "tcdate": 1604215253347, "tmdate": 1605024009650, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "invitation": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review", "content": {"title": "The paper presents a new deep CCA method that applies gating to input variables using a latent clipped Gaussian random variable to avoid overfitting. The total novelty and technical contributions seems limited.", "review": "This paper presents a new deep CCA method to learn non-linear relationships between two modalities. It trains two neural networks each for a modality to maximize the total correlations of their output representations. Gating is applied to input variables by associating each with a latent Bernoulli variables which is then relaxed with the clipped Gaussian random variable. Experiments on one synthetic and two real datasets demonstrate the superiority of the proposed method.\n\nBelow are specific comments.\n\n1. In the last sentence of Section 2.2, it is unclear to me how the stochastic part of the gates is removed to determine whether $z_x[i_x]$ is equal to or larger than zero. Is $z_x[i_x]$ determined based on the estimated $\\mu_x[i_x]$: $z_x[i_x] > 0$ if $\\mu_x[i_x] > 0$ and $z_x[i_x] = 0$ otherwise?\n\n2. Minor comments (notation inconsistencies/abuse, typos, etc.):\n\nThe sentence \"For example, in biology ... and engineering (Chen et al., 2017)\" is not complete (sentence fragment). Please rephrase it or join it to the preceding sentence.\nIs \"the degeneracy inherit to $N < D_x,D_y$\" supposed to be \"the degeneracy inherent to $N < D_x,D_y$\"?\nThe word \"interpetability\" is misspelled.\nIn the middle subfigure of Figure 1, it is clearer if the label $\\epsilon$ and tick values $\\{-0.5,0,0.5\\}$ are added along the horizontal axis.\n\"straight forward\" should be spelled as \"straightforward\" (no space).\n\nEq. (4): It seems $\\boldsymbol{z}_x^T \\boldsymbol{X}, \\boldsymbol{z}_y^T \\boldsymbol{Y}$ should be written as $\\mathop{{\\rm diag}}\\left(\\boldsymbol{z}_x\\right) \\boldsymbol{X}, \\mathop{{\\rm diag}}\\left(\\boldsymbol{z}_y\\right) \\boldsymbol{Y}$. Note that the $\\boldsymbol{X}, \\boldsymbol{Y}$ here represent the observed data matrices of dimensions $D_x \\times N, D_y \\times N$ [rather than random vectors based on which the data are observed].\n\nIn Section 2.2, third line, the expression of the regularization:\n- $\\mathbb{P}(\\boldsymbol{z}_x[i] \\geq 0)$ should be $\\mathbb{P}(\\boldsymbol{z}_x[i] > 0)$ or $\\mathbb{P}(0 < \\boldsymbol{z}_x[i] \\leq 1)$;\n- For consistency, it should write $\\|\\boldsymbol{z}\\|_0$ as $\\|\\boldsymbol{z}_x\\|_0$ and the index $i$ as $i_x$.\n\n\"a similar notations\" should be \"similar notations\".\n\nIn Section 2.2, first paragraph, last sentence \"The total correlation in Eq. 4 can be expressed using the trace of ...\"\n- \"total correlation\" should be \"total squared correlation\".\n\nIn Section 3.1, second paragraph, $\\hat{\\rho}=\\bm{\\hat{\\phi}}\\boldsymbol{X}\\boldsymbol{Y}^T\\bm{\\hat{\\eta}}^T$ should be $\\hat{\\rho}=\\bm{\\hat{\\phi}}^T\\boldsymbol{X}\\boldsymbol{Y}^T\\bm{\\hat{\\eta}}$.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3389/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3389/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3389/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076749, "tmdate": 1606915804119, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3389/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review"}}}, {"id": "YEyAeZBsEtu", "original": null, "number": 4, "cdate": 1604271907126, "ddate": null, "tcdate": 1604271907126, "tmdate": 1605024009590, "tddate": null, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "invitation": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review", "content": {"title": "A method for deep sparse canonical correlation analysis", "review": "1. Paper summary:\n\nThis paper proposes a DL method for learning sparse non-linear transformations that maximize correlations between two views. In particular, each view is passed through a separate network. Stochastic Gating is applied to the input layer of each network. The two networks are jointly trained by maximising the correlation between their outputs. Sparsity is obtained by imposing L0 regularization terms on the Stochastic Gating variables.\n\n2. Strong points of the paper:\n\nStochastic Gating gives way to an objective function that can be optimized through Stochastic Gradient Descent.\n\nThe method can detect correlation between two views even when data size is less than the number of dimensions, as demonstrated by the experimental results.\n\n3. Weak points of the paper:\n\nThe proposed method is very similar to DCCA paper of Andrew et al. The only difference is Andrew et al. use L2 regularization, while the authors use L0 regularization.\n\nSimilarly to DCCA, the method suffers from the two issues.\n\nFirst, the method learns non-linear transformations that however are hard to interpret. Non-linear CCA can be achieved by learning linear transformations through non-linear correlation measure, such as HSIC. HSIC-CCA [1] can also learn sparse representations. Given the rising importance of explainable AI, non-linear transformations seem to be a drawback.\n\nSecond, the method relies on Stochastic Gradient Descent. However, the loss function is not decomposable into batches. This makes batch training somewhat random.\n\n4. Conclusion:\n\nFor the above reasons, I find the contributions of the paper to be marginal.\n\n[1] Billy Chang, Uwe Kr\u00fcger, Rafal Kustra, Junping Zhang: Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment. ICML (2) 2013: 316-324", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3389/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3389/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Gated Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "moshebar-s@013.net", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindenbaum|deep_gated_canonical_correlation_analysis", "supplementary_material": "/attachment/38e8a4c3c734b1dd3b5ceb0a86bd2b5f64063d0b.zip", "pdf": "/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nV-t9KbfvB", "_bibtex": "@misc{\nlindenbaum2021deep,\ntitle={Deep Gated Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nyear={2021},\nurl={https://openreview.net/forum?id=mZLhA0xFGmR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mZLhA0xFGmR", "replyto": "mZLhA0xFGmR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3389/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076749, "tmdate": 1606915804119, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3389/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3389/-/Official_Review"}}}], "count": 6}