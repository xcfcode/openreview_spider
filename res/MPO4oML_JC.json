{"notes": [{"id": "MPO4oML_JC", "original": "Bv2HrslO67E", "number": 2204, "cdate": 1601308242710, "ddate": null, "tcdate": 1601308242710, "tmdate": 1614985750828, "tddate": null, "forum": "MPO4oML_JC", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "N17NVxCAoeP", "original": null, "number": 1, "cdate": 1610040384740, "ddate": null, "tcdate": 1610040384740, "tmdate": 1610473978174, "tddate": null, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper presents an approach to multi-agent coordination using goal-driven exploration on subspaces of the observation space.\n\nThe results of the paper show that the authors' approach performs baselines on grid worlds and two tasks from the StartCraft Multi-agent Challenge. While the rebuttal clarified many points raised by the reviewers, there was an agreement that the paper should be more convincing regarding the applicability of the approach. The reviewers were concerned with the scalability of the approach to larger environment, as well as the amount of hand-crafting/domain knowledge required to apply the approach. Overall, while the paper contributes interesting results showing that such domain knowledge can help when properly leveraged, it feels like the approach needs be validated on more challenging environments before acceptance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040384726, "tmdate": 1610473978156, "id": "ICLR.cc/2021/Conference/Paper2204/-/Decision"}}}, {"id": "4O1aLIyii_R", "original": null, "number": 4, "cdate": 1604072997821, "ddate": null, "tcdate": 1604072997821, "tmdate": 1607313146708, "tddate": null, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review", "content": {"title": "The proposed method outperforms prior work, but does not seem scalable to larger environments", "review": "---Post rebuttal---\n\nThank you for the detailed response. My main concern was regarding the scalability of the method to larger environments, e.g. w/ visual state space. I agree with the other reviewers regarding limited applicability of the method, and maintain my original score (Weak Reject).\n\n---\n\nThe paper introduces a coordinated multi-agent exploration method that selects goals based on normalized entropy from multiple projected state spaces. The method greatly outperforms other algorithms on sparse-reward environments.\n\nWeaknesses:\n\n1. The process of selecting subspaces in the CMAE algorithm (Section 3.2) is not scalable to larger environments. I\u2019d be willing to raise my score if, in more complex environments, using level 0 subspace still results in improved performance over existing work.\n\n2. The related works section seems incomplete. It discusses single-agent exploration, concurrent RL, and multi-agent noise-based exploration (p.8), but these categories do not cover the following papers:\n[1] Concurrent Meta Reinforcement Learning (Parisotto et al., 2019): Parallel multi-agent exploration in the same environment using shared memory and communication.\n[2] Efficient Exploration via State Marginal Matching (Lee et al., 2019): Multi-task/multi-agent exploration that learns multiple \u201cskills\u201d (or \u201cagents\u201d) in the same environment, that together match some target state density.\n\nMinor typo:\n- \u201cThe resulting experience tuple is then stored* in a replay memory\u201d (p.3)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101691, "tmdate": 1606915768085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2204/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review"}}}, {"id": "2aaWYWg1sLF", "original": null, "number": 2, "cdate": 1603916116621, "ddate": null, "tcdate": 1603916116621, "tmdate": 1606764916945, "tddate": null, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review", "content": {"title": "Unconvinced that single-agent exploration bonuses applied to the group aren't enough in the CTDE setting", "review": "Summary of paper:\nThe paper introduces an exploration bonus tailored to multi-agent learning in the CTDE (centralized training, decentralizing execution) setting. The bonus works by: 1) dividing up the observation space into subspaces (in this case, corresponding to the entities, pairs of entities, triplets of entities, etc), 2) maintaining running counters within each subspace of every possible configuration within that subspace, 3) identifying the lowest entropy subspace, 4) sampling the replay buffer to find a rarely occurring (but importantly, reachable) \u201cgoal\u201d state within the lowest entropy subspace, and finally, 5) rewarding the exploration policy for visiting the goal state. The authors test their method in two domains: 1) a series of coordinated multi-agent exploration challenges in grid worlds, and 2) the Starcraft Multi-Agent Challenge (SMAC). They demonstrate much faster learning over a series of baselines.\n\nPros:\nThe paper is generally well-written. The paper uses interesting tasks, and the experiments are generally well-structured. The experiments include modern baselines, and the sample efficiency over the baselines is impressive (at least at first glance).\n\nCons:\n1) I don\u2019t think the authors adequately convinced me that there is a new problem to solve here that couldn\u2019t be tackled with straightforward applications of single-agent exploration methods in the multi-agent setting. I assume their IDQ + RND baseline applies the RND bonus independently to each agent (i.e. each agent has a separate target network to predict)? That is a useful baseline for sure, but its not exactly very strong. Why not use a single RND bonus for the *group* observations to induce correlated exploration? Since this is the CTDE setting, single-agent exploration bonuses can be applied somewhat straightforwardly to the group. Even simpler, why not include a simple count-based bonus on the group observations (i.e. r(s)=1/sqrt(c(s)), where c(s) is the count of the group observation s)? The authors\u2019 method is more or less a group count-based exploration bonus that also takes advantage of structured observations, so this baseline is both possible and relevant.\n2) The authors\u2019 method seems to me to build in a lot more hand-crafted knowledge than they let on (and more than any of their baselines, as far as I can tell). Their method requires structured knowledge of the agent\u2019s observations (e.g. these variables correspond to these entities), which is not always available (e.g. from pixels). Can the authors comment on the fairness of comparing their method to the baselines they use, as well as the scalability to less structured observation spaces?\n3) Additionally, it looks to me like their method would be fooled by a large number of unreachable states for one entity - in this case, Hmax would be large, so the normalized entropy would be small, and the agent would keep exploring a particular entity set of variables, without progress. For example, imagine a world with two boxes, one which is movable, and one which is not, but imagine the agent does not know in advance that one is unmovable. CMAE would focus on the unmovable box forever (granting a fixed useless exploration bonus for the single state the box has ever and will ever occupy). Can the authors comment on this?\n\nQuestions:\n1) I understand how counting observations works in the grid worlds, but isn\u2019t SMAC a continuous state space? How does counting work there? More generally, can you comment on how the method would work in continuous state spaces (e.g. binning, neural density models, etc)?\n2) Maybe I missed it, but how many subspaces (K) are there in these experiments? The authors state that 2^M is \u201cclearly intractable\u201d (where M is the number of component subspaces) but seem to imply that 2^N is tractable (where N is the number of entity subspaces). However, M and N are not obviously very different numbers, depending on the number of components per entity.\n3) Deterministically focussing exploration on the lowest entropy subspace seems potentially unstable (e.g. in presence of many unreachable states - see comment above). Did the authors consider probabilistic subspace selection, in which the subspace is chosen from, say, a softmax distribution over the subspace entropies?\n4) In the discussion of \u201cExploration for Reinforcement Learning\u201d, the authors state \u201cIn contrast to our approach, all the techniques mentioned above target single-agent deep reinforcement learning.\u201d Strictly speaking, this is true, but can\u2019t most (if not all) single-agent exploration techniques be applied to multi-agent in the CTDE setting by treating the group as the agent?\n\nTypos/formatting:\n1) Figure 3 legends are nearly impossible to read. Find space to increase their size.\n2) Figure 2 legends are a) too small and b) the same across all 4 subplots, so just use one large legend on the right.\n3) In paragraph under Figure 3: \u201cSMAC achieves similar performance\u2026\u201d -> \u201cCMAE achieves similar performance\u2026\u201d\n4) In the Multi-Agent RL part of Related Work, there are some citep<->citet mishaps.\n5) Conclusion: \u201cincrease\u201d -> \u201cincreases\u201d\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101691, "tmdate": 1606915768085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2204/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review"}}}, {"id": "a0Ra2P0kp3", "original": null, "number": 7, "cdate": 1606282523887, "ddate": null, "tcdate": 1606282523887, "tmdate": 1606282523887, "tddate": null, "forum": "MPO4oML_JC", "replyto": "yQGX5U3GYMT", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for the comments.\n\n\n>Although these environments are used as test beds to experiment with multi-agent algorithms, the eventual goal of multi-agent RL research should be to provide value on real world problems. The issue with the algorithm proposed in this paper is that it depends on an environment which provides component sets, which is an unrealistic expectation, and prevents it from scaling to the real world.\n\n\nEven in a real-world setting component sets can be available. For instance, in autonomous driving we may use object detectors to identify other agents and participants that are part of an environment. We remain convinced that this is a reasonable assumption to get research started. Should it be generalized? Absolutely. But do we have to solve all problems at once? We think the answer is no. The paper shows that an available decomposition can significantly improve algorithmic effectiveness. If the community is as excited about this result as we are, together, we can leverage component sets even in real-world settings.\n\n\n>This is a good point. However, there is no reason, in principle, why algorithms like QMIX or MADDPG cannot be applied to problems with a high dimensional state space. Whereas the proposed algorithm suffers from this limitation.\n\n\nThe algorithm may not be directly scalable to a pure visual state-space but vanilla QMIX and MADDPG cannot be effectively applied in this setting either. Scalability challenges are due to different reasons, but this shouldn\u2019t, according to our opinion, preclude a discussion about algorithms, their benefits and disadvantages.\n\n\n>I have made note of that experiment. One suggestion for future work would be to combine your approach with something like a neural density model. This could help show how it can be extended beyond simulated games.\n\n\nThanks for sharing this thought which we agree is an interesting direction for future work. We believe that the careful analysis of the approach without this component (as presented in the current draft) still remains useful as it demonstrates why your thought of a neural density model should work. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "yQGX5U3GYMT", "original": null, "number": 6, "cdate": 1606279699220, "ddate": null, "tcdate": 1606279699220, "tmdate": 1606279732480, "tddate": null, "forum": "MPO4oML_JC", "replyto": "Ozd8isoj1gd", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Thank you for the response.", "comment": "Thank you for the response, and for providing the results of the QMIX+RND comparison. I think that strengthens the paper. \n\n>Information about component sets of a state space is provided by most of the existing multi-agent RL environments, such as the StarCraft multi-agent challenge (SMAC) [a], the multiple-particle environment (MPE) [b], the multi-agent emergence environments [c], Google Research football (GFootball) [d], and DeepMind MuJoCo soccer [e]. We think the proposed technique is hence widely applicable.\n\nAlthough these environments are used as test beds to experiment with multi-agent algorithms, the eventual goal of multi-agent RL research should be to provide value on real world problems. The issue with the algorithm proposed in this paper is that it depends on an environment which provides component sets, which is an unrealistic expectation, and prevents it from scaling to the real world.\n\n> Scaling MARL approaches to pure visual state spaces is an open problem. To our best knowledge, the state-of-the-art MARL approaches, such as Weighted QMIX [f], MAVEN [g], QMIX [h], MADDPG [b], do not provide any experimental results on environments with pure visual state spaces.\n\nThis is a good point. However, there is no reason, in principle, why algorithms like QMIX or MADDPG cannot be applied to problems with a high dimensional state space. Whereas the proposed algorithm suffers from this limitation. \n\n> In continuous spaces, counting could be performed by either binning or any off-the-shelf continuous counting methods such as the one proposed by Tang et al. [i] or the neural density model [j]. Empirically, we found our approach with binning achieves good results on SMAC 3m-sparse and 8m-sparse, where the state space is continuous. Please see Fig. 2 of the paper for more details.\n\nI have made note of that experiment. One suggestion for future work would be to combine your approach with something like a neural density model. This could help show how it can be extended beyond simulated games. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "VnjYjbx61rS", "original": null, "number": 5, "cdate": 1606267887395, "ddate": null, "tcdate": 1606267887395, "tmdate": 1606267887395, "tddate": null, "forum": "MPO4oML_JC", "replyto": "Za0xr7LSuO3", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for the detailed comments. We think there are several misunderstandings that we point out below.\n\nRe 1: Value of $l$ (level).\\\nFor all the experiments, we consider level 0 to level 3 subspaces. We clarified in Sec 4. \n\nRe 2: No level 0 results?\\\nPlease see Fig. 3 (right), the level 0 subspace (wolf_health) in the *island* task has the lowest entropy throughout the training, and is selected as the subspace to explore. Thanks to the selected level 0 subspace, our approach is much more data efficient than baseline approaches on *island* (Tab. 1, Fig. 1). \n\nRe 3: Level 0 reduced to standard single-agent count-based exploration?\\\nCMAE at level 0 differs from single-agent count-based exploration in two aspects. First, CMAE decouples the exploration and target policy, and uses the level 0 variable as a shared goal to guide coordinated exploration. In contrast, single-agent count-based exploration does neither consider any form of coordination between agents nor does it decouple exploration and target policies. Second, in level 0, we only consider one variable of the state space. In contrast, single-agent count-based exploration considers an agent\u2019s local observation or the state, which contains more than one variable. \n\nRe 4: Usage of Q-learning and DQN.\\\nWe use Q-learning for *pass*, *secret room*, and *push box* simply because the number of states is small and tabular Q-learning is applicable. DQN is used for *island* because the number of states is larger. Therefore, we use a deep net to approximate the Q table. The fact that we use both demonstrates the generality of the proposed approach. \n\nRe 5: Comparison with [1].\\\n[1] reports the number of model updates, which is not a standard metric for evaluating RL algorithms. The metric doesn\u2019t reflect an RL approach\u2019s data efficiency. In contrast, the number of environment steps is a standard metric which reflects the data efficiency and is adopted by most RL works [a, b, c, d, e], particularly works on RL exploration [f, g, h, i, j]. \nTherefore, following most existing works, we report the number of environment steps, which we believe is the right metric for evaluating RL algorithms.  \n\nRe 6: Small adjustments would make EDTI and EITI much more competitive in terms of environment frames. \\\nFollowing the reviewer\u2019s suggestion, we run the official code of EITI with a return length of 128 and 256 on *pass*. At 3M environment steps, EITI with 128 and 256 return length achieves 0%$\\pm$ 0% success rate while our approach achieves  100% $\\pm$ 0% success rate.\nAfter 590M environment steps of training, EITI with 128 and 256 return length achieves 0.0% $\\pm$ 0% and 54.8% $\\pm$ 12% success rate which is much lower than the success rate of 80% $\\pm$ 2% achieved by using the default setting.  \nThis shows that decreasing the return length does not make EITI more competitive in terms of environment steps. Very much in contrast, the adjustment suggested by the reviewer undermines EITI\u2019s performance. \n\nRe 7: Confidence interval of Table 1. \\\nWe added confidence intervals to Table 1. \n\nRe 8: Not comparing with EDTI and EITI on SMAC. \\\nWe compare our approach to the state-of-the-art methods, including MAVEN, QMIX and QTRAN, on SMAC. EDTI and EITI [1] only report results on 2D grid world tasks, and do not report results on any SMAC tasks. It is unclear why the reviewer claims that EDTI and EITI are the strongest baselines to date on SMAC. To the best of our knowledge there is no evidence for this claim.\n\nRe 9: Harder SMAC environment with dense reward. \\\nWe run our approach and MAVEN on *6h_vs_8z super hard* with dense reward. Following the MAVEN paper, both approaches are trained with 8M environment steps. Our approach and MAVEN achieves 60.9% $\\pm$ 1.3% and 61.2% $\\pm$ 2.3% success rate respectively. This demonstrates that in dense reward environments, our approach has similar performance to the state-of-the-art approach. The results are added to Sec. 4 and training curves are presented in Fig. 6. \n\n[a] Continuous control with deep reinforcement learning. Lillicrap et al. ICLR16\\\n[b] Human-level control through deep reinforcement learning. Mnih et al. Nature17\\\n[c] Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation, Wu et al. NeurIPS17\\\n[d] Hindsight Experience Replay. Andrychowicz et al. NeurIPS17\\\n[e] Model-based Policy Optimization with Unsupervised Model Adaptation. Shen et al. NeurIPS20\\\n[f] MAVEN: Multi-Agent Variational Exploration. Mahajan et al. NeurIPS19\\\n[g] Optimistic Exploration even with a Pessimistic Initialisation. Rashid et al. ICLR20\\\n[h] On Bonus Based Exploration Methods In The Arcade Learning Environment. Taiga et al. ICLR20\\\n[i] Curiosity-driven Exploration by Self-supervised Prediction. Pathak et al. ICML17\\\n[j] #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. Tang et al. NeurIPS17\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "riZodbOe1IQ", "original": null, "number": 4, "cdate": 1606267416950, "ddate": null, "tcdate": 1606267416950, "tmdate": 1606267416950, "tddate": null, "forum": "MPO4oML_JC", "replyto": "2aaWYWg1sLF", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for the detailed comments. \n\nRe 1: Why not include a single-agent count-based bonus on the group observations. \\\nFollowing the reviewer\u2019s suggestion, we run Q-learning with a shared count-based bonus on the group observations for the *secret room* and *push box* tasks. The shared count method achieves 5.1% $\\pm$ 1.3% and 2.2% $\\pm$ 1.1% success rate on *secret room* and *push box* respectively. In contrast, our approach can achieve a 100% $\\pm$ 0.1% success rate in both tasks. The count-based bonus method is suboptimal because group observation is not necessarily the most efficient subspace to explore. This demonstrates the effectiveness of our subspace selection mechanism. The results are added to Sec. 4 and training curves are shown in Fig. 5.\n\nRe 2: Use a lot of domain knowledge of handcrafted features. \\\nThe only information we need is the component set of the state space. This information is provided by most of the existing multi-agent RL environments, such as the StarCraft multi-agent challenge (SMAC) [a], the multiple-particle environment (MPE) [b], the Multiagent emergence environments [c], Google Research Football (GFootball) [d], and MuJoCo Soccer [e]. The proposed approach leverages the component set information to build a useful inductive bias into the algorithm and thus achieves more efficient exploration. In contrast, prior MARL approaches for exploration don\u2019t leverage the valuable information provided by the environment. \n \nRe 3: CMAE would be fooled by a large number of unreachable states for one entity. \\\nThanks for pointing this out. Indeed, if a variable remains constant in the state space, CMAE may stick with this variable. However, adding a constant variable in a state representation doesn\u2019t seem to be a reasonable MARL environment design. Empirically, we don\u2019t observe any of these constant variables in multi-agent RL environments such as SMAC [a] and MPE [b]. \n\nRe 4: How would the method work in continuous state spaces. \\\nIn continuous spaces, counting could be performed by either binning or any off-the-shelf continuous counting methods such as work by Tang et al. [f] or the neural density model [g]. Empirically, we found our approach with binning achieves good results on SMAC 3m-sparse and 8m sparse tasks, where the state space is continuous. Please see Fig. 2 of the paper for more details. \n\nRe 5: How many subspaces (K) are there in these experiments. \\\nThe subspaces and their normalized entropies on *push box*, *pass*, and *island* are presented in Fig. 3. We consider level 0 to level 3 subspaces, which results in 9, 7, and 11 subspaces on *push box*, *pass*, and *island* tasks.\nFor SMAC 3m and 8m tasks, we consider level 0 to level 3 subspaces, which results in 11 subspaces in both tasks. \n  \nRe 6: Probabilistic subspace selection. \\\nThanks for the suggestion. We think probabilistic subspace selection is a good strategy when faced with environments with constant variables. We added a discussion on probabilistic subspace selection in Sec. 3.1. \n\nRe 7: Can\u2019t most single-agent exploration techniques be applied to multi-agent in the CTDE setting by treating the group as the agent? \\\nPlease see Re 1. \n\n[a] The StarCraft Multi-Agent Challenge. Samvelyan et al. arxiv.19\\\n[b] Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. Lowe et al. NeurIPS17\\\n[c] Emergent Tool Use From Multi-Agent Autocurricula. Baker et al. ICLR20\\\n[d] Google Research Football: A Novel Reinforcement Learning Environment. Kurach et al. AAAI20\\\n[e] Emergent Coordination through Competition. Liu et al. ICLR19\\\n[f] #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. Tang et al. NeurIPS17\\\n[g] Count-Based Exploration with Neural Density Models. Ostrovski et al. ICML17\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "Ozd8isoj1gd", "original": null, "number": 3, "cdate": 1606266657569, "ddate": null, "tcdate": 1606266657569, "tmdate": 1606266657569, "tddate": null, "forum": "MPO4oML_JC", "replyto": "AFPeknCI_oT", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for the detailed comments. \n\nRe 1: Identify entity (component set) in the state space.\\\nInformation about component sets of a state space is provided by most of the existing multi-agent RL environments, such as the StarCraft multi-agent challenge (SMAC) [a], the multiple-particle environment (MPE) [b], the multi-agent emergence environments [c], Google Research football (GFootball) [d], and DeepMind MuJoCo soccer [e]. We think the proposed technique is hence widely applicable. \nCMAE leverages the component set information to build useful inductive biases into our algorithm and thus achieves more efficient exploration. In contrast, the existing MARL approaches for exploration don\u2019t use this valuable information provided by the environment. \n\nRe 2: Cannot scale to visual state space. \\\nScaling MARL approaches to pure visual state spaces is an open problem. To our best knowledge, the state-of-the-art MARL approaches, such as Weighted QMIX [f], MAVEN [g], QMIX [h], MADDPG [b], do not provide any experimental results on environments with pure visual state spaces.  \n\nRe 3: Counting in continuous state space.\\\nIn continuous spaces, counting could be performed by either binning or any off-the-shelf continuous counting methods such as the one proposed by Tang et al. [i] or the neural density model [j]. Empirically, we found our approach with binning achieves good results on SMAC 3m-sparse and 8m-sparse, where the state space is continuous. Please see Fig. 2 of the paper for more details. \n\nRe 4: Selecting subspaces requires domain knowledge.  \\\nAs discussed in Re 1, the only information we need is the component set of the state space. This information is provided by most of the MARL environments. \n\nRe 5: Benchmark against QMIX + RND. \\\nWe run QMIX with RND on the SMAC 8m-sparse task. Both our approach and QMIX + RND are trained for 2M environment steps. Our approach achieves 80.1% $\\pm$ 1.3% win rate while QMIX + RND achieves only 1.5% $\\pm$ 0.4% win rate. We added the results to Sec. B of the appendix. \n\n\n[a] The StarCraft Multi-Agent Challenge. Samvelyan et al. arxiv.19\\\n[b] Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. Lowe et al. NeurIPS17\\\n[c] Emergent Tool Use From Multi-Agent Autocurricula. Baker et al. ICLR20\\\n[d] Google Research Football: A Novel Reinforcement Learning Environment. Kurach et al. AAAI20\\\n[e] Emergent Coordination through Competition. Liu et al. ICLR19\\\n[f] Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. Rashid et al. NeurIPS20\\\n[g] MAVEN: Multi-Agent Variational Exploration. Mahajan et al. NeurIPS19\\\n[h] QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. Rashid et al. ICML18\\\n[i] #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. Tang et al. NeurIPS17\\\n[j] Count-Based Exploration with Neural Density Models. Ostrovski et al. ICML17\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "CptaCupegeq", "original": null, "number": 2, "cdate": 1606266282946, "ddate": null, "tcdate": 1606266282946, "tmdate": 1606266282946, "tddate": null, "forum": "MPO4oML_JC", "replyto": "4O1aLIyii_R", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We thank the reviewer for the detailed comments. \n\nRe 1: More complex environment.\\\nWe conduct experiments on a sparse reward version of SMAC 6h_vs_8z (super hard) task. Agents can only see non-zero reward when an opponent is eliminated or a teammate is eliminated. We compare our approach to MAVEN. All approaches are trained for 8M steps. Our approach with level 0 exploration achieves 45.6% $\\pm$3.2%  win rate while MAVEN achieves 4.3%$\\pm$0.9% win rate. The results are added to Sec. 4 and training curves are shown in Fig. 6. \n\nRe 2: Related work section is incomplete. \\\nWe added discussions about the papers suggested by the reviewer. See Sec. 5 for details.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MPO4oML_JC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2204/Authors|ICLR.cc/2021/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851092, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Comment"}}}, {"id": "Za0xr7LSuO3", "original": null, "number": 1, "cdate": 1603856339171, "ddate": null, "tcdate": 1603856339171, "tmdate": 1605024264837, "tddate": null, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review", "content": {"title": "Lack of clarity", "review": "## Overview\n\nThis paper proposes an exploration method for collaborative multi agent RL similar to count-based approaches in single-agent RL. It then provide some experimental analysis of the method in two kinds of environments (Starcraft and multi-agent particle-based environments)\n\nOverall, the exposition of the method lacks clarity, and the experimental results suffer some limitations. Therefore, I am not recommending this paper for acceptation at ICLR 2021.\n\n## Method\n\nThe algorithm considers all the variables of the problem $V_1,\\cdots,V_M$, which can sometimes be grouped when they correspond to the same agents (eg there are two variables for the x,y coordinates of a given agent).\nAt the core, a counter is incremented when a given subset $\\mathcal{S}_k$ (\"a low dimensional subspace\") of the variables reach a given \"configuration\", which is then used to identify subsets of variables that have rarely been attained. An intrinsic reward is then used to train a policy to reach these rare configurations.\nSeveral things are unclear to me:\n* The exploration are trained at the end of episode, and since the counts have changed since the last episode the target configuration (thus the reward) will change too. However it is not clear if the exploration policy is retrained from scratch or simply fine-tuned from the previous iteration\n* It is not clear what a \"configuration $s_k\\in\\mathcal{S_k}$\" is. According to section 2.1, the variables $V_i$ are assumed to be in $\\mathbb{R}$, thus finding the exact same configuration several times over the course of training seems unlikely. I assume there is some kind of discretization applied, but more details are required here. In particular, the exact way a configuraton is defined determines how many configurations are being tracked and thus the complexity of the algorithm (both in space and time), which is not discussed.\n\n\nSection 3.2 describes how the subset of variables are being selected. Essentially, the sets are designed such that the variables of a given entity are always in the same subset, and then only subsets of entities of a limited size (l) are being considered.\nIn practice, the exact value of $l$ doesn't seem to be specified in the paper. Also lacking is the actual number of subspaces that the choice of $l$ incurs in each of the considered environments, and how the choice of $l$ influences the performance the performance of the algorithm (both in terms of final reward and runtime/memory)\n\nTowards the end of section 3.2, one can read:\n\n> In addition, we also consider level 0, where the component set of each subspace has only one element. Empirically, we found that level 0 subspace leads to very efficient exploration in some tasks\n\nI don't quite understand what this level 0 corresponds to. Is it simply a subspace per variable (as opposed to a subspace per entity set, which in my understanding would be level 1) ?\nAs for the empirical claim, it doesn't seem to be backed by any experiments of the experimental section. We note that at one variable per variable, the algorithm would reduce to a standard count-based (single agent) exploration scheme. If such scheme are found to be efficient, they should be benchmarked in the experiments, which they currently aren't.\n\n\n## Experiments\n\nThe authors provide experiments on two domains: some discrete multi-agent particle world environments and some starcraft scenarios (from SMAC). Overall, the experimental results suffer some limitations that undermine the strength of the claim.\n\n### Particle environments\n\nThe environments are introduced in [1], and the authors directly compare to the results of the corresponding paper.\nThe proposed exploration method is applied on top of different base learning algorithms (sometimes vanilla tabular Q-learning, sometimes DQN). This inconsistency isn't discussed and make comparison between algorithms quite challenging since it adds a moving part.\n\nThe authors provide comparison with the methods proposed from [1], EDTI and EITI. However the comparison doesn't seem entirely fair. In [1], the authors reported (and presumably optimized for) the number of environment steps, while this papers reports the number of environment steps. These are two distinct metrics, and it is reasonable to believe that small adjustments would make EDTI and EITI much more competitive in terms of environment frames. For example, in [1] they used returns of length 2048 (at it is the default in PPO2), but based on ppo2 performance in environments like Atari, a much more conservative return length (say 128 or 256) could have sufficed, thereby drastically improving the sample complexity. Similarly, reducing the batch-size could improve the sample complexity as well. As it stands, the claim \" Specifically, EITI and EDTI need 64,000 environment steps between each update, which makes them less sample efficient\" isn't backed by any experimental evidence.\nIt is not clear how the number of environment-steps that are reported in table 1 are obtained. In particular, despite the fact that the experiments were run multiple times, the table doesn't reflect any kind of confidence interval. This is problematic because the environment \"Island\", for example, exhibits quite a high variance in Figure 1. While table 1 claims that 50% success rate is achieved at 13.9M steps, Figure1 shows that, on average, the success rates dips below 50% after 17M, which raises question on the stability of the learnt policy.\n\n### Starcraft\n\nThe paper presents the results on the two easiest Starcraft environments of the SMAC set, either with sparse rewards (+1 for victory, -1 for defeat) or with dense rewards.\nContrary to the other environments, the authors don't provide a comparison with EDTI and EITI, which is unfortunate as they appear to be the strongest available baseline to date.\nAmongst the reported baselines, CMAE is the only methods achieving decent win-rate on the sparse environments, and it appears to match the performance of the baselines on the dense counter-part, which is encouraging.\nIt has to be noted that, according to the training curves in appendix, it appears that the baselines learnt a fleeing strategy (ie never engaging the enemy, thus never \"solving\" the task), which, while clearly sub-optimal, is a local optimum. Perhaps a table reporting the average returns of each method could paint a more accurate picture.\n\nFinally, it's unfortunate that the paper sticks to these two easy environments. Understanding the failure modes in the easiest environment that can't be solved by this method would yield significantly more insights on the methods than near-perfect scores on easy tasks. In particular, in the MAVEN paper [2], results on harder (dense) starcraft tasks were reported, and it would be valuable to see how the proposed method performs on those.\n\n\n[1] Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent explo-ration. InProc. ICLR, 2020\n[2] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-agentvariational exploration. InProc. NeurIPS, 2019\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101691, "tmdate": 1606915768085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2204/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review"}}}, {"id": "AFPeknCI_oT", "original": null, "number": 3, "cdate": 1603950636047, "ddate": null, "tcdate": 1603950636047, "tmdate": 1605024264712, "tddate": null, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "invitation": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review", "content": {"title": "Proposed technique has limited applicability ", "review": "The paper proposes to improve the exponential sample complexity of finding a coordinated multi-agent strategy by learning an exploration policy for each agent that conditions on a shared goal. The exploration policy is mixed with the normal RL policy according to a parameter alpha, which is scaled down over time. The shared goal that agents pursue is selected by using an explicit counter mechanism over objects in the environment. \n\nStrengths:\n- The paper is well written.\n- The reduction in sample complexity due to this technique is very large.\n- Algorithms 1 and 2 are clear.\n\nWeaknesses:\n- The proposed counter mechanism relies on being able to manually identify entities in the environment, such as the box in the push-box environment. This has limited applicability to real-world problems with large-dimensional or visual state spaces, in which entities are not obvious a priori. Being able to explicitly count the number of times an agent has experienced an entity in a specific configuration is not a realistic expectation for interesting, real-world problems. Therefore, it is unclear how this method can be applied beyond simple tabular settings and video games.\n- Similarly, it seems that deciding which subspaces are equivalent requires a significant amount of domain knowledge into each problem, and does not seem to be generally applicable. \n- Why not benchmark against QMIX + RND, since both are tested independently? \n\nOther suggestions:\n- Typo on p. 3 \"tuple is then store in a replay memory\" -> stored\n- Why was the number of steps between updates for EITI and EDTI held constant at 64,000? How many steps between updates were used for the proposed technique? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2204/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2204/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coordinated Multi-Agent Exploration Using Shared Goals", "authorids": ["~Iou-Jen_Liu1", "~Unnat_Jain1", "~Alex_Schwing1"], "authors": ["Iou-Jen Liu", "Unnat Jain", "Alex Schwing"], "keywords": ["Multi-agent RL", "Deep RL", "Exploration"], "abstract": "Exploration is critical for good results of deep reinforcement learning algorithms and has drawn much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. It was recognized recently that noise-based exploration is suboptimal in multi-agent settings, and exploration methods that consider agents' cooperation have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and don't coordinate their exploration efforts toward those states. To address this shortcoming, in this paper, we proposed coordinated multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected by a normalized entropy-based technique from multiple projected state spaces. Then, agents are trained to reach the goal in a coordinated manner. We demonstrated that our approach needs only $1\\%-5\\%$ of the environment steps to achieve similar or better returns than state-of-the-art baselines on various sparse-reward  tasks, including a sparse-reward version of the Starcraft multi-agent challenge (SMAC).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|coordinated_multiagent_exploration_using_shared_goals", "one-sentence_summary": "The proposed coordinated multi-agent exploration (CMAE) leverages shared goals to coordinate agents' exploration and achieves good performance in various sparse reward environments. ", "supplementary_material": "/attachment/94dcabe1390297a58ba77a94f073eb3310f6d06c.zip", "pdf": "/pdf/fbff006e7d4362796b3dcd4c54fc0e144734708e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OO7L2KEpaO", "_bibtex": "@misc{\nliu2021coordinated,\ntitle={Coordinated Multi-Agent Exploration Using Shared Goals},\nauthor={Iou-Jen Liu and Unnat Jain and Alex Schwing},\nyear={2021},\nurl={https://openreview.net/forum?id=MPO4oML_JC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MPO4oML_JC", "replyto": "MPO4oML_JC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101691, "tmdate": 1606915768085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2204/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2204/-/Official_Review"}}}], "count": 12}