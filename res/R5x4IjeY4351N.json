{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392743700000, "tcdate": 1392743700000, "number": 1, "id": "KnM8tXXIeyngG", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "R5x4IjeY4351N", "replyto": "O5Q3nYYA3cn-B", "signatures": ["Jun Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "\textbf{Response:} to Anonymous 090b. Please put the responses into the NIPS 2013 latex and generate PDF because there are some tables and equations. \r\n\r\nThanks for your detailed comments. The typos will be corrected in the coming version and the following are our responses to Anonymous 090b' comments.\r\n\r\n\textbf{Question 1:} emph{I have found the beginning of Section 3.1 very confusing to read -- it requires some rephrasing (what is $\frac{1}{R}$?) especially since it seems relatively crucial in terms of understanding the authors' main point. The forward-references at the end of that Section are also confusing the reader.}\r\n\r\n\textbf{Response 1:} We assume that the number of classes (or object categories) is $R$ in a database. $\frac{1}{R}$ is an ideal sparseness measure. The more details and our main point are in the \textbf{Response 1} of Anonymous ae2f.\r\n\r\n\textbf{Question 2:} emph{In Section 3.2 -- where does the 0.15 threshold come from? Why is there a jump in Figure 1? What happened at epoch 25?}\r\n\r\n\textbf{Response 2:} To facilitate the AOM and AIM, we firstly calculate the mean center features of every class. Then we use the mean center features to calculate the AOM and AIM. The hyperparameter $\tau$ is used to distinguish the activation unit of the mean center feature. The $\tau$ are set to $0.05$ as we consider a unit of the mean center feature non-activated if the absolute value of its activation is below $0.05 $. The different hyperparameters $\tau$ ($\tau=0.01$ or $\tau=0.05$) could lead to similar result that compared to DNNs with sigmoid activation function, the features have low AOM and high AIM. The more details are in the \textbf{Response 2} of Anonymous 230f. \r\n\r\nThe reason of the jump in Figure 1 is the different momentums. Generally, the momentum is 0.5 in the first 25 epochs and is 0.9 in the rest 25 epochs.\r\n\r\n\textbf{Question 3:} emph{It is unclear how the authors' implementation of relus + fully-connected nets differs from the one initially proposed by Hinton and collaborators (as well as the others cited)? Why are the authors' results better?}\r\n\r\n\textbf{Response 3:} In really, we also want to know the reason. There are some different details: 1) On MNIST we choose masking noise as the corruption process: each pixel has a probability of 0.2 of being artificially set to 0 and we do not choose masking noise on CIFAR10 and JC-NORB. But on all database Glorot et al. (2009) choose a a probability of 0.25; 2) We do use an $L2$ penalty on the activations with a coefficient of 0.00001, but Glorot et al. (2009) uses an $L1$ penalty on the activations with a coefficient of 0.001. 3) We do mix up the momentum with the learning rate.\r\nWe will report our codes of DNNs with ReLU and you can see the details that are in the \textbf{Response 2} of Anonymous ae2f. \r\n\r\n\r\n\textbf{Question 4:} emph{I'd say the paper is strangely structured/motivated: the claim that ReLUs produce sparser activations is believable and they measure it. The authors also measure that rectified units achieve better test errors (well-known before that).\r\nBut I don't see any evidence that sparsity of rectified units causes better generalization -- rectified units have other properties (such as the absence of saturation) that could be hypothesized to make them generalize better.}\r\n\r\n\textbf{Response 4:} The rectified linear units (ReLU) and pretraining give us the sparseness benefits. Based on experiments we conjecture that to some extent the ReLU can capture the sparseness benefits of the pretraining. Later, we will visualize the filters of DNNs with ReLU. If ReLU and pretraining have some similar filters, then the ReLU can capture the sparseness benefits of the pretraining.\r\n\r\n\textbf{Question 5:} emph{I would say that the authors should work on finding a mechanism for why sparsity specifically is beneficial in supervised-backprop nets (the fact that unsupervised pre-training *also* leads to sparse representations, according to their metrics, is not a mechanism in and of itself). Generally the paper has a number of typos, awkward phrasings (e.g., 'this paper does not intend to obliterate the contribution')}\r\n\r\n\textbf{Response 5:} The mechanism is based on the feature learning. Beyond the feature learning, the activation units of features are sparse (You can see the details that are in the \textbf{Response 1} of Anonymous ae2f). The typos and awkward phrasings will be corrected in the coming version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392743580000, "tcdate": 1392743580000, "number": 1, "id": "ddASypuiWRb9E", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "R5x4IjeY4351N", "replyto": "ZTe2rdw5jcTGU", "signatures": ["Jun Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "\textbf{Response} to Anonymous ae2f. Please put the responses into the NIPS 2013 latex and generate PDF because there are some tables and equations. \r\n\r\nThanks for your detailed comments. The typos will be corrected in the coming version and the following are our responses to Anonymous ae2f' comments.\r\n\r\n\textbf{Question 1:} emph{Overall I see very little originality in this paper, the conclusions are not really novel, and the paper is very unclearly written. Contrary to its title it does not bring any insight as to '*WHY* unsupervised pretraining encourages moderate sparseness'. A thorough and clearly presented empirical study of the sparsity properties of representations learned by different algorithms could have been interesting. But unfortunately, in its current state this work is extremely unclearly written, with confusing terminology, vague reasoning, poor structure, and insufficiently described experimental setup.}\r\n\r\n\r\n\textbf{Response 1:} The reason is as following:\r\n\r\nIn DNNs with the unsupervised pretraining, the promoted procedure of the data can be described at many levels (Lee2009,Bengio2012): raw-level features (data samples), lower-level features (edges and object-parts) and higher-level features (objects). Correspondingly, the first, second, and third hidden layers of DNNs learn edge detectors, object parts, and objects respectively. Those detectors (filters) are visualized in Fig. 1 of citep{Erhan2010} (or Fig. 3 of citep{Lee2009}). This shows that the higher layer learned to combine the lower layer's feature into more complex, higher-level feature.\r\n\r\nWe assume that the number of classes (or object categories) is $R$ in a database, such as MNIST ($R=10$). Suppose that in DNNs the number of hidden units of first (lower) layer is $M$ and the number of hidden units of second (higher) layer is $N$. Correspondingly, the number of detectors of first and second layers is $M$ and $N$, respectively. The unsupervised pretraining averagely trains the detectors because every class has the similar number of simples. Ideally, the number of higher-level detectors of every class is a little more than $\frac{N}{R}$ since higher-level detectors of different objects share a few common lower-level feature detectors. The number of lower-level detectors of every class is above $\frac{M}{R}$ since lower-level detectors of different objects share a few common raw-level feature detectors.\r\n\r\nBeyond the feature learning, we study the activations of units of features. A simple belonged to a class transfers from the lower layer to the higher layer. So, detectors belonged to the class are detected in higher layer. The number of higher-level detectors belonged to the class is a little more than $\frac{N}{R}$. In other word, the units that correspond to the detectors are activated in higher layer. The number of activation units of higher-level feature is a little more than $\frac{N}{R}$. Thus, we hope that the ideal sparseness measure is a little more than the ratio between \r\nthe number of activation units of higher layer and the number of units of higher layer. The ratio is $\frac{\frac{N}{R}}{N} = \frac{1}{R}$.\r\n\r\nWe do a lot of experiments (you can see \textbf{Responses 1 and 2} of Anonymous 230f) and use Hoyer's sparseness measure (Hoyer 2004) and Rath's sparseness measure (Rath 2008) to calculate the number of low activation (or zero) units in a feature. This shows that unsupervised pretraining encourages sparseness.\r\n\r\n\textbf{Question 2:} emph{The classification performance obtained with 'their ReLU' appears rather good, but the paper does not investigate in depth what would explain the improvement over the other ReLU results from the literature they report. If their approach/implementation indeed has an advantage, its specificity should be explained and evaluated in detail. It is unclear from the writeup whether hyper-parameters were chosen/tuned using proper methodology.}\r\n\r\n\textbf{Response 2:} The rectified linear units (ReLU) and pretraining give us the sparseness benefits. Based on experiments we conjecture that to some extent, the rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Later, we will visualize the filters of DNNs with ReLU. If ReLU) and pretraining ReLU and pretraining have some similar filters, then the ReLU can capture the sparseness benefits of the pretraining.\r\n\r\nWe will report our codes of DNNs with ReLU and the details are as following:\r\n\r\nConsider an $L$-layer network.\r\nLet us denote by $a^ell$ the output vector of the $ell$th layer, starting with $a^1$ (the input), and finishing with a linear combination of the variables $a^{L-1}$.\r\nIn a fully connected neural network, we can recursively define for $ell=2$ to $L-1$:\r\n\begin{align}\r\na_{j}^{ell}=f_{j}^{ell}(z_{j}^{ell})qquad qquad z_{j}^{ell}=sum_{i=1} w_{ji}^{ell-1}a_{i}^{ell-1}+b_{j}^{ell-1} ,\r\nlabel{eq:nn}\r\nend{align}\r\nwhere $w_{ji}^{ell}$ denotes the weights from the $i$-th unit of the $ell$-th layer to the $j$-th unit of the $ell+1$th layer, $b_j^{ell}$ denotes the bias term for the $j$-th unit of the $ell+1$-th layer, and $f_{j}^{ell}(cdot)$ is an activation function (for example, sigmoid or hyperbolic tangent).\r\nFor traditional neural networks, the model parameters are ${w_{ji}^{ell},b_j^ell}$, and the activation functions are fixed.\r\nIn this paper, we the rectified linear units (ReLU).\r\n\r\nThe objective of learning is to find the optimal network parameters so that the network output $a^L$ matches the target closely. The output $a^{ell}$ can be compared to a target vector $t$ through a loss function $psi(a^{ell},t)$. There are two common loss functions in the neural network literature.\r\nThe first is squared loss $psi(a^{ell},t)=|a^{ell}-t|^2=sum_j(a_j^{ell}-t_j)^2$, where $a_j^L=z_j^L$. The second loss function is the negative log-likelihood loss $psi(a^{ell},t)=-logpsi_t(a^L)=-sum_jlog(a_j^L)$, where $a_j^L=\frac{e^{z_j^L}}{sum_ie^{z_i^L}}$. Given the loss function, the goal of neural network learning is to minimize the object function\r\n\begin{align}\r\nE(phi)=sum_{n=1}^NE_n(phi) , qquad E_n(phi)= psi(a_n^L(x_n,phi),t_n) \u951b?\r\nlabel{eq:opt}\r\nend{align}\r\nwhere $phi$ denote the set of all model parameters ${w_{ji}^ell, b_j^{ell}}$.\r\n\r\nIn order to train the model parameters, we employ the standard  {em back-propagation} algorithm that can be divided into three phases (cite{Bishop2006}): forward propagation, error back-propagation, and parameter update.\r\nGiven a training set of $N$ input sample ${x_n}$, where $n =1,cdots,N$, together with a set of target vectors ${t_n}$, the three phases can be described as follows, and the resulting algorithm is in Figure~\ref{fig:alg}.\r\n\r\n\textbf{Forward propagation}: We construct the forward propagation of information through the network: that is, we compute the hidden unit outputs ${z_j^ell;a_j^ell}$ using\r\neqref{eq:nn}.\r\n\r\n\textbf{Error back-propagation}:\r\nThe back-propagation phase computes the gradient $partial E_n(phi)/partial phi$.\r\nThe gradients with respect to the weights ${w_{ji}^ell}$ and biases ${b_j^ell}$ can be described as follows.\r\nStarting at the output node $delta_j^L=\rho_j^L=2(a_{j}^{ell}(x_n)-t_{nj})$ (least squares loss) or $delta_j^L=a_{j}^{ell}(x_n)-1_{t_n=j}, \rho_j^L=\frac{1}{a_{j}^{ell}(x_n)}$ (negative log-likelihood loss); we compute the partial derivatives from $L-1$ to $2$ using the following chain-rule formulae:\r\n\begin{align}\r\n\frac{partial E_n}{partial w_{ji}^{ell-1}}=delta_j^{ell}z_i^{ell-1} qquad \frac{partial E_n}{partial b_{j}^{ell-1}}=delta_j^{ell} \nonumber\\\r\ndelta_j^{ell}=\rho_j^{ell}\frac{partial f_{j}^{ell}}{partial z_{j}^{ell}} qquad \rho_j^{ell}=sum_{h} delta_h^{l+1}w_{hj}^{ell}\r\nlabel{eq:grad}\r\nend{align}\r\nwhere $\frac{partial f_{j}^{ell}}{partial z_{j}^{ell}}$ easily calculates.\r\n\r\n\textbf{Parameters update}: We employ (mini-batch) stochastic gradient descent (SGD) with both $L_2$ regularization for the weights $w_{ji}^ell$  and the momentum update rule (cite{Jacobs1988}). The update for all parameters $phi$ based on one data point at a time $t$ is\r\n\begin{align}\r\nphi^{(t+1)}&=phi^{(t)} +\trianglephi^{(t)} \nonumber\\\r\n \trianglephi^{(t)}& =omega \trianglephi^{(t-1)}-eta(1-omega)(\nabla E_n(phi^{(t)})+gamma phi^{(t)}) ,\r\nlabel{eq:param}\r\nend{align}\r\nwhere the parameter $eta$ is known as the learning rate, $omega$ is the momentum parameter and $gamma$ is the $L_2$ regularization parameter.\r\nFor mini-batch, the gradient is replaced by the average gradient over the mini-batch.\r\n\r\n\begin{figure}\r\ncaption{Back-Propagation}\r\nlabel{fig:alg}\r\n\begin{tabular}{l}\r\n hline\r\n\textbf{initialize} all parameters (weights and biases) of the DNNs with $L$ layers\\\r\n\textbf{do}\\\r\nhspace{0.5cm}\textbf{for} each example $x_n$ in the training set\\\r\nhspace{0.9cm}  $a_n^L$ = DNNs-output(DNNs, $x_n$) using eqref{eq:nn}. $\backslash *$ forward pass\\\r\nhspace{0.9cm}  $t_n$  = target output for $x_n$ \\\r\nhspace{0.9cm}   compute error $psi(a_n^L,t_n)$ using eqref{eq:opt} \\\r\nhspace{0.9cm}   compute gradients of all parameters in DNNs using eqref{eq:grad}. $\backslash*$ backward pass\\\r\nhspace{0.9cm}   update all parameters in the DNNs using eqref{eq:param}\\\r\nhspace{0cm}   {\bf until} a stopping criterion is satisfied\\\r\n\textbf{return} the network\r\n\\ hline \\\r\nend{tabular}\r\nend{figure}"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392743460000, "tcdate": 1392743460000, "number": 1, "id": "99dtL-WuDn5xP", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "R5x4IjeY4351N", "replyto": "4CLOCIvoAizst", "signatures": ["Jun Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "\textbf{Response} to Anonymous 230f. Please put the responses into the NIPS 2013 latex and generate PDF because there are some tables and equations. \r\n\r\nThanks for your detailed comments. The typos will be corrected in the coming version and the following are our responses to Anonymous 230f' comments.\r\n\r\n\textbf{Question 1:} emph{-- I am not convinced that the unsupervised pretraining itself indeed encourage sparseness in higher layers of networks. As mentioned in your paper, those pretraining algorithms try to decrease the reconstruction errors under some constrains. If those models were trained longer enough, the resulting networks could try their best to exploit their capacity to reconstruct data (in other word, the networks could become less and less sparse). -- the sparseness measurement used in this paper is improper. The activations of sigmoid and tanh are unlike ReLU, at a certain scale. You could consider Hoyer's sparseness measure (Hoyer 2004) which is on a normalized scale and avoids the hyperparameter $epsilon$ in the SPM.}\r\n\r\n\textbf{Response 1:}\r\n\r\n1). If we could consider Hoyer's sparseness measure (HSPM), compared to DNNs with sigmoid activation function (Dsigmoid) the unsupervised pretraining also encourages sparseness. Table~\ref{tab:hspm552} shows that HSPM of DBNs, RBMs and DNNs with ReLU (DReLU) is better than one of Dsigmoid.\r\n\r\n2). When the pretraining models were trained longer enough, HSPM of the RBMs could have an upper bound. Table~\ref{tab:hspmrbm1} shows that when training epochs change from 100 to 1000, the upper bounds of HSPM of RBMs with 500 and 1000 hidden units are 0.451 and 0.528 respectively.\r\n\r\n3). When the number of hidden unites increases, the networks will become more sparse and also have an upper bound. Table~\ref{tab:hspmrbm2} shows that when the number of hidden units changes from 500 to 10000, an upper bound of HSPM is 0.719 after 1000 training epochs.\r\n\r\n4). The unsupervised pretraining itself indeed encourage sparseness that HSPM of the networks has an upper bound in higher layers of networks. Table~\ref{tab:deephspm} shows that HSPM of DNNs with five hidden (500) layers changes from 0.45 to 0.582 after 1000 training epochs and HSPM of DNNs with five hidden (1000) layers changes from 0.528 to 0.665.\r\n\r\n\textbf{Question 2:} emph{-- the author did not explain how the hyperparameters (of SPM, AIM and AOM) were chosen. The different hyperparameters could lead to very different conclusions. -- AOM should also consider the activation overlapping between samples from any number of classes'.}\r\n\r\n\textbf{Response 2:} The sparse parameters $epsilon$ are set to $0.05$ as we consider a unit activated if the absolute value of its activation is below $0.05 $. Compared to DNNs with sigmoid activation function, the different hyperparameters $epsilon$ also lead to sparseness.\r\n\r\nTo facilitate the AOM and AIM, we firstly calculate the mean center features of every class. Then we use the mean center features to calculate the AOM and AIM. The hyperparameter $\tau$ is used to distinguish the activation unit of the mean center feature. The $\tau$ are set to $0.05$ as we consider a unit of the mean center feature activated if the absolute value of its activation is below $0.05$.\r\n\r\nThe different hyperparameters $\tau$ could lead to similar conclusions. For classification tasks, it is always desirable to extract features that have low AOM and high AIM because the features are most effective for preserving class separability. Compared to DNNs with sigmoid activation function, we have following result that the features have low AOM and high AIM under $\tau=0.01$ (in Tables~\ref{tab:aom1} and ~\ref{tab:aim1}) and $\tau=0.05$ (in Tables~\ref{tab:aom5} and ~\ref{tab:aim5}). Moreover, the AOM of higher-level features is lower than one of lower-level features in Tables~\ref{tab:aom1} and ~\ref{tab:aom5}.\r\n\r\n\textbf{Question 3:} emph{Paragraph 4, page 2: The results of your DNNs are not state of the art for those data set.}\r\n\r\n\textbf{Response 3:} Yes, the results of your DNNs are not state of the art for those data set as other papers use the convolution and dropout. We only do train DNNs with the standard back-propagation algorithm from randomly initialized parameters and the results are state of the art for those data set.\r\n\r\n\textbf{Question 4:} emph{Section 3: I cannot understand assumption 2.}\r\n\r\n\textbf{Response 4:} Since simples of different objects have a certain amount of common raw-level features, lower-level features of different objects share the common raw-level features and also have a certain amount of common lower-level features. Similarly, higher-level features share the common lower-level features.\r\n\r\n\textbf{Question 5:}emph{In equation 2, what $x_{ij}$ represent?\r\nIn equation 3, $m$?}\r\n\r\n\textbf{Response 5:} Sorry, $x_{ij}$ should be replaced $h_{ij}$ that is the $j$th unit of the mean center features of $i$th class.\r\n$m$ should be replaced $n$ that is the number of units of the mean center features.\r\n\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{Hoyer's sparseness measure (HSPM) of DNNs (500-500-2000) on MNIST. The momentum is 0.5 in the first 25 epochs and is 0.9 in the rest 25 epochs.}\r\nlabel{tab:hspm552}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c}\r\nhlinehline\r\nModels   & data & 1st & 2nd & 3rd & error\\\r\nhlinehline\r\nDReLU &   0.634  & 0.584  &  0.492  &  0.482  &  1.17$%$\\\r\nDBNs   &  0.634  & 0.388   & 0.525 &   0.634   &  1.17$%$\\\r\nRBMs   &  0.634  & 0.398   & 0.576  &  0.665   &  1.80$%$\\\r\nDsigmoid & 0.634 &  0.109  &  0.023 &   0.077  &  2.01$%$\\\r\nhlinehline\r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{Hoyer's sparseness measure (HSPM) of RBMs with different training epochs. The momentum is 0.5 in the first 100 epochs and 0.9 in the rest 900 epochs.}\r\nlabel{tab:hspmrbm1}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c}\r\nhlinehline\r\nTraining epochs   & 100 & 200 & 500 & 800 & 1000\\\r\nhlinehline\r\n784-500       &   0.261 & 0.425 & 0.441 & 0.448 &  0.451\\\r\n784-1000      &   0.327 & 0.508 & 0.525 & 0.527 & 0.528\\\r\nhlinehline\r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{Hoyer's sparseness measure (HSPM) of RBMs with different number of hidden unites after training for 1000 epochs. The momentum is 0.5 in the first 100 epochs and 0.9 in the rest 900 epochs.}\r\nlabel{tab:hspmrbm2}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c}\r\nhlinehline\r\nThe number   & 500 &  1000  & 2000  & 5000 &  10000\\\r\nhlinehline\r\n             & 0.451 &  0.528 & 0.598  & 0.676 & 0.719\\\r\nhlinehline\r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{Hoyer's sparseness measure (HSPM) of DNNs (500-500-500-500-500 and 1000-1000- 1000-1000-1000) on MNIST. The momentum is 0.5 in the first 100 epochs and 0.9 in the rest 900 epochs.}\r\nlabel{tab:deephspm}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c}\r\nhlinehline\r\nLayers                  &   1st  &    2nd  &    3rd   &    4th   &    5th\\\r\nhlinehline\r\n500-500-500-500-500      &  0.45  &   0.582 &  0.509  &   0.560  &   0.520\\\r\n1000-1000-1000-1000-1000 & 0.528  &  0.665  &  0.589  &   0.654  &   0.606\\\r\nhlinehline\r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{AOM of DNNs (500-500-2000) under different number of activation overlapping classes on MNIST. $\tau$ is set to $0.01$.}\r\nlabel{tab:aom1}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c||c||c||c||c||c}\r\nhlinehline\r\n The number   &   & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\r\nhlinehline \r\n     & data  & 0.409  &  0.368  &  0.338  &  0.314  &  0.292  &  0.274 &   0.256  &  0.240  &  0.226 \\\r\nhlinehline \r\nDsigmoid     & 1st   & 1.000  &  1.000  &  1.000  &  1.000  &  1.000  &  1.000 &   1.000  &  1.000  &  1.000\\\r\n     & 2nd   & 0.994  &  0.993  &  0.992  &  0.991  &  0.991  &  0.990  &  0.989  &  0.989  &  0.988\\\r\n     & 3rd   & 1.000  &  1.000  &  1.000  &  1.000  &  1.000  &  1.000 &   1.000  &  1.000  &  1.000\\\r\nhlinehline \r\nRBMs & 1st   & 0.978  &  0.968  &  0.959 &   0.950  &  0.941  &  0.933  &  0.925  &  0.917  &  0.910\\\r\n     & 2nd   & 0.686  &  0.602  &  0.537 &   0.484  &  0.439  &  0.400  &  0.366  &  0.337  &  0.312\\\r\n     & 3rd   & 0.815  &  0.734  &  0.658  &  0.586  &  0.518  &  0.452  &  0.388  &  0.326  &  0.267\\\r\nhlinehline \r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{AIM of DNNs (500-500-2000) under different number of activation overlapping classes on MNIST. $\tau$ is set to $0.01$.}\r\nlabel{tab:aim1}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c||c||c||c||c||c}\r\nhlinehline\r\n  the number   &   & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\r\nhlinehline \r\n     & data  & 0.069  &  0.032  &  0.018  &  0.010   & 0.005  &  0.002  &  0.001  &  0.000 &   0.000 \\\r\nhlinehline \r\nDsigmoid     & 1st   &  0.000  &  0.000  &  0.000  &  0.000  &  0.000 &   0.000 &   0.000 &   0.000  &  0.000 \\\r\n     & 2nd   & 0.001  &  0.000 &   0.000 &   0.000 &   0.000  &  0.000 &   0.000 &   0.000 &   0.000\\\r\n     & 3rd   & 0.000 &   0.000  &  0.000 &   0.000 &   0.000 &   0.000 &   0.000 &   0.000 &   0.000\\\r\nhlinehline RBMs & 1st   & 0.006 &   0.001 &   0.000 &   0.000 &   0.000  &  0.000  &  0.000 &   0.000 &   0.000\\\r\n     & 2nd   & 0.101  &  0.033  &  0.014 &   0.007 &   0.003  &  0.001  &  0.001  &  0.000  &  0.000\\\r\n     & 3rd   & 0.033 &   0.005  &  0.002 &   0.001 &   0.000  &  0.000  &  0.000  &  0.000 &   0.000\\\r\nhlinehline\r\nend{tabular}\r\nend{table}\r\n\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{AOM of DNNs (500-500-2000) under different number of activation overlapping classes on MNIST. $\tau$ is set to $0.05$.}\r\nlabel{tab:aom5}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c||c||c||c||c||c}\r\nhlinehline\r\nthe number  &  & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\r\nhlinehline & data  & 0.304  &  0.260  &  0.227  &  0.201  &  0.178  &  0.158 &   0.141  &  0.125  &  0.110\\\r\nhlinehline Dsigmoid & 1st &   0.987  &  0.981  &  0.975  &  0.970  &  0.965  &  0.960  &  0.955  &  0.950  &  0.946\\\r\n     & 2nd   &     0.961  &  0.951  &  0.942  &  0.934  &  0.927  &  0.920 &   0.914 &   0.909  &  0.904\\\r\n     & 3rd   &     0.994  &  0.993  &  0.992  &  0.991  &  0.991  &  0.990 &   0.990 &   0.989  &  0.989\\\r\nhlinehline \r\nRBMs & 1st   & 0.863  &  0.812  &  0.770  &  0.734  &  0.704  &  0.678 &   0.655  &  0.636  &  0.618\\\r\n     & 2nd   & 0.421  &  0.329  &  0.271  &  0.231  &  0.202  &  0.179  &  0.161  &  0.145  &  0.132\\\r\n     & 3rd   & 0.164  &  0.096  &  0.059  &  0.038  &  0.026  &  0.019  &  0.014  &  0.011  &  0.009\\\r\nhlinehline \r\nend{tabular}\r\nend{table}\r\n\r\n\begin{table}[!t]\r\n\renewcommand{arraystretch}{1.3}\r\ncaption{AIM of DNNs (500-500-2000) under different number of activation overlapping classes on MNIST. $\tau$ is set to $0.05$.}\r\nlabel{tab:aim5}\r\ncentering\r\n\begin{tabular}{c||c||c||c||c||c||c||c||c||c||c}\r\nhlinehline\r\nthe number &   & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\r\nhlinehline \r\n     & data  & 0.077  &  0.038 &   0.024  &  0.017 &   0.013 &   0.010  &  0.007 &   0.006  &  0.005 \\\r\nhlinehline \r\nDsigmoid     & 1st   &  0.004   & 0.000   & 0.000  &  0.000  &  0.000  &  0.000  &  0.000  &  0.000  &  0.000\\\r\n     & 2nd   & 0.008  &  0.001 &   0.000  &  0.000  &  0.000  &  0.000 &   0.000 &   0.000 &   0.000\\\r\n     & 3rd   & 0.002  &  0.001  &  0.000  &  0.000  &  0.000  &  0.000  &  0.000 &   0.000  &  0.000\\\r\nhlinehline \r\nRBMs & 1st   & 0.055  &  0.010  &  0.002  &  0.001  &  0.000  &  0.000  &  0.000  &  0.000  &  0.000\\\r\n     & 2nd   & 0.168 &   0.082  &  0.046  &  0.029  &  0.019  &  0.013  &  0.009  &  0.007  &  0.006\\\r\n     & 3rd   & 0.127  &  0.062  &  0.033  &  0.020  &  0.013  &  0.009  &  0.006  &  0.003  &  0.000\\\r\nhlinehline\r\nend{tabular}\r\nend{table}"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391945040000, "tcdate": 1391945040000, "number": 3, "id": "O5Q3nYYA3cn-B", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "R5x4IjeY4351N", "replyto": "R5x4IjeY4351N", "signatures": ["anonymous reviewer 090b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Why does the unsupervised pretraining encourage moderate-sparseness?", "review": "The authors study the effect of unsupervised pre-training on the levels of sparsity that hidden nodes have. The claimed contributions are that unsupervised pre-training encourages what they call \u201cmoderate-sparseness\u201d and that their implementation of relus + fully-connected nets achieves state of the art performance on a number benchmarks such as MNIST, CIFAR-10 and JC-NORB.\r\n\r\nI have found the beginning of Section 3.1 very confusing to read -- it requires some rephrasing (what is 1/R?) especially since it seems relatively crucial in terms of understanding the authors\u2019 main point. The forward-references at the end of that Section are also confusing the reader. In Section 3.2 -- where does the 0.15 threshold come from? Why is there a jump in Figure 1? What happened at epoch 25?\r\n\r\nIt would be best if the authors put the best comparable numbers in the tables along with their *citations*, rather than just presenting their own results or other results without references attached.\r\n\r\nIt is unclear how the authors\u2019 implementation of relus + fully-connected nets differs from the one initially proposed by Hinton and collaborators (as well as the others cited)? Why are the authors\u2019 results better?\r\n\r\nI\u2019d say the paper is strangely structured/motivated: the claim that ReLUs produce sparser activations is believable and they measure it. The authors also measure that rectified units achieve better test errors (well-known before that). But I don\u2019t see any evidence that sparsity of rectified units causes better generalization -- rectified units have other properties (such as the absence of saturation) that could be hypothesized to make them generalize better. I would say that the authors should work on finding a mechanism for why sparsity specifically is beneficial in supervised-backprop nets (the fact that unsupervised pre-training *also* leads to sparse representations, according to their metrics, is not a mechanism in and of itself). \r\n\r\nGenerally the paper has a number of typos, awkward phrasings (e.g., \u201cthis paper does not intend to obliterate the contribution\u201d)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391857620000, "tcdate": 1391857620000, "number": 2, "id": "ZTe2rdw5jcTGU", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "R5x4IjeY4351N", "replyto": "R5x4IjeY4351N", "signatures": ["anonymous reviewer ae2f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Why does the unsupervised pretraining encourage moderate-sparseness?", "review": "Brief summary of paper: \r\nThe paper is an experimental evaluation of the sparsity levels of representations in deep networks learned through several approaches. These include 3 types of non-linearity (sigmoid, tanh and ReLU), with and without pretraining, and dropout. The main findings/conclusions are that pretraining yields a moderate level of representation sparsity, and that using their brand of purely supervised ReLU with dropout yields even sparser solutions and better classification on MNIST/CIFAR10/NORB.\r\n\r\nAssessment:\r\nOverall I see very little originality in this paper, the conclusions are not really novel, and the paper is very unclearly written. Contrary to its title it does not bring any insight as to '*WHY*  unsupervised pretraining encourages moderate sparseness'. A thorough and clearly presented empirical study of the sparsity properties of representations learned by different algorithms could have been interesting. But unfortunately, in its current state this work is extremely unclearly written, with confusing terminology, vague reasoning, poor structure, and insufficiently described experimental setup. The classification performance obtained with 'their ReLU' appears rather good, but the paper does not investigate in depth what would explain the improvement over the other ReLU results from the literature they report. If their approach/implementation indeed has an advantage, its specificity should be explained and evaluated in detail. It is unclear from the writeup whether hyper-parameters were chosen/tuned using proper methodology.\r\n\r\nPros & Cons:\r\n- lack of originality\r\n- very unclear, confusing, writeup"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391471580000, "tcdate": 1391471580000, "number": 1, "id": "4CLOCIvoAizst", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "R5x4IjeY4351N", "replyto": "R5x4IjeY4351N", "signatures": ["anonymous reviewer 230f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Why does the unsupervised pretraining encourage moderate-sparseness?", "review": "Review Summary\r\nThis work empirically show the unsupervised pretraining encourage moderate-sparseness in a deep neural network, which could lead better classification performance. The author also proposed that ReLU DNNs without pretraining could also lead to moderate-sparseness.  \r\nThough empirically studying sparseness in DNNs is interesting and important, this submission could potentially be improved. \r\n\r\nPros\r\n-- well-written and organized\r\n-- an interesting idea for exploring the role of unsupervised pretraining in deep learning\r\nCons\r\n-- I am not convinced that the unsupervised pretraining itself indeed encourage sparseness in higher layers of networks. As mentioned in your paper, those pretraining algorithms try to decrease the reconstruction errors under some constrains. If those models were trained longer enough, the resulting networks could try their best to exploit their capacity to reconstruct data (in other word, the networks could become less and less sparse). \r\n-- the author did not explain how the hyperparameters (of SPM, AIM and AOM) were chosen. The different hyperparameters could lead to very different conclusions. \r\n-- the sparseness measurement used in this paper is improper. The activations of sigmoid and tanh are unlike ReLU, at a certain scale. You could consider Hoyer\u2019s sparseness measure (Hoyer 2004) which is on a normalized scale and avoids the hyperparameter epsilon in the SPM.\r\n-- AOM should also consider the activation overlapping between samples from any number of classes'. \r\n\r\nMinor comments\r\nParagraph 4, page 2: The results of your DNNs are not state of the art for those data set.\r\nSection 3: I cannot understand assumption 2.\r\nIn equation 2, what $x_{ij}$ represent?\r\nIn equation 3, m?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387779540000, "tcdate": 1387779540000, "number": 24, "id": "R5x4IjeY4351N", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "R5x4IjeY4351N", "signatures": ["junl.njust@gmail.com"], "readers": ["everyone"], "content": {"title": "Why does the unsupervised pretraining encourage moderate-sparseness?", "decision": "submitted, no decision", "abstract": "It is well known that direct training of deep multi-layer neural networks (DNNs) will generally lead to poor results. A major progress in recent years is the invention of various unsupervised pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of the pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of the pretraining, where we empirically show the pretraining leads to a higher level of sparseness of hidden unit activation in the resulting neural networks, and the higher sparseness is positively correlated to faster training speed and better prediction accuracy. Moreover, we also show that rectified linear units (ReLU) can capture the sparseness benefits of the pretraining. Our implementation of DNNs with ReLU does not require the pretraining, but achieves comparable or better prediction performance than traditional DNNs with pretraining on standard benchmark datasets.", "pdf": "https://arxiv.org/abs/1312.5813", "paperhash": "li|why_does_the_unsupervised_pretraining_encourage_moderatesparseness", "keywords": [], "conflicts": [], "authors": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "authorids": ["junl.njust@gmail.com", "lw860123@gmail.com", "csjyang@njust.edu.cn", "xtyuan1980@gmail.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}