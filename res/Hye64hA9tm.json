{"notes": [{"id": "Hye64hA9tm", "original": "HJlL7-RqtQ", "number": 1490, "cdate": 1538087988612, "ddate": null, "tcdate": 1538087988612, "tmdate": 1545355394589, "tddate": null, "forum": "Hye64hA9tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1l0xiKlxE", "original": null, "number": 1, "cdate": 1544751861627, "ddate": null, "tcdate": 1544751861627, "tmdate": 1545354516743, "tddate": null, "forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Meta_Review", "content": {"metareview": "This paper addresses important general questions about how linear classifiers use features, and about the transferability of those features across tasks. The paper presents a specific new analysis method, and demonstrates it on a family of NLP tasks. \n\nAll four reviewers (counting the emergency fourth review) found the general direction of research to be interesting and worthwhile, but all four shared several serious concerns about the impact and soundness of the proposed method. \n\nThe impact concerns mostly dealt with the observation that the method is specific to linear classifiers, and that it's only applicable to tasks for which a substantial amount of training data is available. \n\nAs the AC, I'm willing to accept that it should still be possible to conduct an informative analysis under these conditions, but I'm more concerned about the soundness issues: The reviewers were not convinced that a method based on the counting of specific features was appropriate for the proposed setting (due to rotation sensitivity, among other issues), and did not find that the experiments were sufficiently extensive to overcome these doubts.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting direction, but no compelling new method yet"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1490/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352820485, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352820485}}}, {"id": "SyxZ_mXK0m", "original": null, "number": 7, "cdate": 1543218024583, "ddate": null, "tcdate": 1543218024583, "tmdate": 1543218024583, "tddate": null, "forum": "Hye64hA9tm", "replyto": "BJgm9FTSCm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thank you for elaborating and clarifying", "comment": "I appreciate the time you took to explain your reasoning about simpler methods, and I look forward to the comparisons you mentioned.\n\nIt also does sound like you you've already thought about how to adapt these ideas to other settings, which I think will be a good next test for these methods. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "r1gQHcar0X", "original": null, "number": 6, "cdate": 1542998586817, "ddate": null, "tcdate": 1542998586817, "tmdate": 1542998586817, "tddate": null, "forum": "Hye64hA9tm", "replyto": "SyefBu5O3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thank you for your review. Please find a few clarifications below.", "comment": "Thank you for your feedback. We are glad to know that you find the problem inherently interesting and important. \n\nRe: no exploration of encoder architectures is performed\n> We are not sure if we understand this completely. Just to clarify, we do compare 4 different sentence encoders [1][2][3][4] which display a fair amount of variety in ways which sentence representations can be computed. For instance, SkipThought vectors [1] use bi-GRU based encoder-decoder model to reconstruct the surrounding sentences. ParaNMT [2] and InferSent [3] use different LSTM based architectures to perform back-translation and textual entailment respectively. Lastly, SIF [4] is a tf-idf based weighted average of individual GloVe word representations.\n\nOne of the key findings of our work is that task-specific information is captured succinctly for a majority of 13 different NLP tasks across 4 different choices of encoder architectures.\n\n1. Skip-Thought Vectors (https://arxiv.org/pdf/1506.06726.pdf)\n2. PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (https://arxiv.org/pdf/1711.05732.pdf)\n3. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (https://arxiv.org/pdf/1705.02364.pdf)\n4. A Simple but Tough-to-Beat Baseline for Sentence Embeddings (https://openreview.net/forum?id=SyK00v5xx)\n\n\nRe: Utility of the methods is a bit unclear\n> We agree that our approach to estimate transfer potential reaps true benefits only when n is large. However, this is not uncommon in scenarios like machine translation, where there are hundreds of potential language pairs that could be used as candidate tasks.\n\nFurthermore, we believe (although acknowledge that this is subjective) that curiosity-driven questions about how the information is encoded are interesting: while they might not be useful in a way that is easily measurable by quantifiable metrics, they provide insights that can help guide future work.\n \n\nRe: CFS metric depends on a hyperparameter (the \"retention ratio\")\n> Sorry about the lack of clarity! To clarify, we used the elbow method (used to find an appropriate number of clusters for clustering) and observed that the \u2018elbow\u2019 in the accuracy vs dimensions plot was around the 80% accuracy mark for most tasks, and hence, we used 80% as the retention ratio. We will discuss this process and test with different retention ratios in the final version.\n\n\nRe: motivation for the restriction to linear models?\n> Our motivation to use linear models is to keep the setup simple and fast. As the classifiers are able to extract task-specific information and reliably estimate transfer potential; changing to a different classifier like MLP, we believe, shouldn\u2019t affect our results in a significant way. However, we will empirically verify this, and discuss this in the camera-ready/future versions of the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "BJgm9FTSCm", "original": null, "number": 5, "cdate": 1542998410743, "ddate": null, "tcdate": 1542998410743, "tmdate": 1542998410743, "tddate": null, "forum": "Hye64hA9tm", "replyto": "rklOwd45hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thanks for the review! A few clarifications below", "comment": "We thank you for your thoughtful review. We are happy to learn that you believe it is an interesting direction that holds potential for high impact. \n\nRe: simpler methods (like clustering, BoW etc.) might work equally well\n> To assess transfer learning potential reliably, we require both the X and y for the target task (i.e supervision). Consider the case where the target task is sentiment analysis, and one of the candidate tasks is finding sentence length (SentLen). For the sake of the argument, let us assume that the X for both sentiment analysis and sentence length is exactly the same set of movie reviews. In such a case, unsupervised metrics like clustering, BoW etc. would indicate maximum transfer potential, whereas the actual transfer potential would be close to zero (assuming the lengths of reviews aren\u2019t correlated with the sentiment). This is a fundamental problem of measures that look directly at the input data X without considering the nature of the labels y. \nFor the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.\n\n\nRe: not clear if the classifier weight difference is well defined\n> You are right in noting that the classifier weights might capture dissimilar yet useful features for two similar tasks, and hence the classifier weight difference might under-predict the transfer potential. We discuss this issue in the paper (section 4.1), which is why we avoid the set overlap metric. Owing to similar concerns, we recommend using CFS information transfer metric over classifier weight difference (which is also supported by results in Table 2 and Figure 3).\n\n\nRe: thoughts on how this could be applied outside the context of sentence representations and classification\n> It is easy to adopt our approach to study the information encoded in the encoders for other problems involving structured prediction (say POS Tagging). Instead of using a decoder that takes in all the dimensions of the encoded input token, one could iteratively select dimensions that provide the highest gains in decoding the right target sequence (say POS tags). Our formulation is very general, and it could potentially also be applied to other modalities like images for tasks like image classification and captioning.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "BkgH9OTSAQ", "original": null, "number": 4, "cdate": 1542998157038, "ddate": null, "tcdate": 1542998157038, "tmdate": 1542998157038, "tddate": null, "forum": "Hye64hA9tm", "replyto": "BklIRmpxa7", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thanks for the review! A few clarifications below", "comment": "We thank the reviewer for their insightful and constructive feedback.\n\nRe: (W1 & W2) Adversely affected by rotations\n> While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly. This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space. \n\nSome of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c\u2019}_{t}, which is free from matrix rotations. As previously noted, this empirically also results in single cells of the LSTM being interpretable. To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4]. \n\n\nRe: (W3) Baselines for transfer learning: \n> The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods. We can clearly see that the random ordering is much worse compared to informed metrics that use representations. Upon your suggestion, we would also add this random baseline in table 2 as well.\n\n\n\nRe: (W4) Metrics for ranking of transfer don't make sense (and some are missing). How is precision and NDCG calculated\n> To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task. The ranked list (in the decreasing utility of transfer learning gain) is then considered the \u2018gold\u2019 set. \n\nWe further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list. Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.\n\nThese metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task. Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.\n\nRegarding missing values:\nAs we explain in the paper, classifier weight difference metric is only applicable in cases\nwhere the number of features between the tasks are of the same size. Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.\n\n\n\nRe: (W5) Multi-task learning\n> Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained. We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.\n\n\n\nRe: (W6) Motivation for CFS\n> There is a rich literature concerning what information is captured in the representations. Further, there are a few initial works that show that certain characteristics like length [1][2], sentiment [3], presence and absence of tokens like brackets [4] are densely captured in a single dimension of the representation space. In a similar spirit, we wanted to quantitatively study this surprising phenomenon, and we were curious about how densely is information encoded in representations.\n\n\nRe: (W7) Alternatives to CFS / Computational concerns\n> We agree that LARS/LASSO could act as potential ways to attain reduced dimensions. For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions. We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions. \n\n\nRe: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels. They would have a very low weight difference score though they are ideal representations for each other\n> You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2). \n\nReferences:\n1.\u201cWhy Neural Translations are the Right Length\u201d :http://www.aclweb.org/anthology/D16-1248.pdf\n2. On the Practical Computational Power of Finite Precision RNNs for Language Recognition: https://arxiv.org/abs/1805.04908\n3. Learning to Generate Reviews and Discovering Sentiment: https://arxiv.org/abs/1704.01444\n4. Visualizing and Understanding Recurrent Networks : https://arxiv.org/abs/1506.02078\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "Skgy6Qpr07", "original": null, "number": 3, "cdate": 1542996919064, "ddate": null, "tcdate": 1542996919064, "tmdate": 1542996919064, "tddate": null, "forum": "Hye64hA9tm", "replyto": "SJecygkZp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thanks for the review! A few clarifications below.", "comment": "We thank the reviewer for the detailed and thorough reviews (that too, likely, on a short notice). We wish to clarify the following: \n\nRe: The method is only applicable to linear classifiers. \n> While the motivation for using linear classifiers was to keep our approach simple and computationally fast, our method can easily be extended to non-linear classifiers, by extending the set of functions (\\mathcal{F}) in section 3 to non-linear functions. We will also, thus, verify the application of this method to the intermediate layers of the networks and will include our results in the camera-ready/future versions of this paper.\n\nRe: more quick experiments to sanity check the method for predicting transfer learning\n> This is a very good idea! We would further experiment on different splits of the data to sanity check our observations and key findings, and include the analysis in the final version of the paper.\n\nRe: description about the \u201cgold\u201d rankings for transfer learning\n> Sorry for the lack of clarity! To compute the \u201cgold\u201d transfer gains, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task. The candidate tasks are then ranked based on the improvements from this pretraining to compute the \u201cgold\u201d rankings.\n\nRe: explanation of how hyperparameters were chosen, especially the \\alpha parameter\n\n> We discuss the motivation for the selection of \\alpha parameter in section 7.1; sorry for not mentioning it clearly. To determine the parameter, we used the elbow method (used to find an appropriate number of clusters for clustering) and observed that the \u2018elbow\u2019 in the relative accuracy vs dimensions plot was around the 80% accuracy mark for most tasks (which can be inferred from in Figure 2).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "rklXOqkaaQ", "original": null, "number": 2, "cdate": 1542417002752, "ddate": null, "tcdate": 1542417002752, "tmdate": 1542417002752, "tddate": null, "forum": "Hye64hA9tm", "replyto": "SJecygkZp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "content": {"title": "Thanks for posting this anyway!", "comment": "- Your AC"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1490/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626506, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye64hA9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1490/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1490/Authors|ICLR.cc/2019/Conference/Paper1490/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626506}}}, {"id": "SJecygkZp7", "original": null, "number": 1, "cdate": 1541627873937, "ddate": null, "tcdate": 1541627873937, "tmdate": 1541627873937, "tddate": null, "forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Public_Comment", "content": {"comment": "This is an emergency fill-in review that was originally asked for but now is unnecessary as the missing review was posted. Here it is anyways.\n\nReview:\n\nThis paper attempts to answer two questions: how densely is information included in sentence representations and how similar are encodings from encoders learned from different tasks?\n\nPros:\n\t1) This paper analyzes representation from a perspective that seems distinct from previous work. It is somewhat in-line with work stating that NNs are heavily overparameterized, and this work might be considered how overparameterized the representations are for NLP tasks.\n\t2) They present a fairly new method for trying to predict what tasks might be useful pretraining for other tasks.\n\t3) Their motivation, thought process, and formalism for their method is well-written and very clear, if almost too long.\n\nCons: \nViewing this paper as making a methods contribution, I think the proposed approach is somewhat limited: \n\t1) the method is only applicable to linear classifiers. I understand practically the decision to use only linear classifiers, but this decision limits the set of representations that can be fairly studied with this method to only representations just before the final linear layer, as using other parts of the model's internal representation are confounded by the fact that they are optimized for use in non-linear models.\n\t2) the method is not comparable across tasks with the different input/output format (slightly mitigated by the fact that you can recast tasks, but it's hard to overcome the fundamental limitation of one input vs two input tasks without introducing some weirdness)\n\t3) the method seems limited to sentence-to-vector models\nAlso, it'd be nice to give the upper bound on the quality of the approximation for the proposed greedy algorithm (I imagine it's something like (1 - 1/e) and the runtime.\n\t\nAs an analysis paper, which I think is more compelling than as a methods paper, the results are fairly interesting, but there isn't enough discussion of the results and I have some concerns regarding the experiments:\n\t4) it would have been nice to have more quick experiments to sanity check the method for predicting transfer learning. Using the predicted transfer between SST2 and SST5 is a good starting point, but there could, and I think should, have been more, e.g.: between random subsets of the same task, between different genres within MNLI, between MNLI and SNLI. \n\t5) without a description of how the transfer learning is done, it's really hard to say how accurate these \"gold\" rankings of transfer learning are or what confounders are potentially introduced in their transfer learning approach\n\t6) I think there needed to be some explanation, even just a one sentence explanation of how hyperparameters were chosen, especially the \\alpha parameter. How quickly does the algorithm pick the entire set as \\alpha approach 1?\nThe discussion of results is very short relative to the density of the experiments and plots. The early exposition explaining everything mathematically and intuitively is nice, but I think the notation was somewhat superfluous and could have been condensed to include more analysis/discussion of the results.\n\n\nStyle / Presentation\n\t1) It'd be nice if each task was the same color across plots in Figure 2\n\t2) typos: section 3, p2: \"...we first define accuracy score of the best classifier...\"; section 1, last p: \"...transferring the knowledge acquired therefrom to improve performance...\"\n\t3) There's some related work analyzing contextual representations (outside sentence-to-vector) that would be worthwhile to mention, e.g. http://aclweb.org/anthology/D18-1179\n\n\nRating: 5\nConfidence: 4\n", "title": "Now unneeded emergency fill-in review"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311585177, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hye64hA9tm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1490/Authors", "ICLR.cc/2019/Conference/Paper1490/Reviewers", "ICLR.cc/2019/Conference/Paper1490/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311585177}}}, {"id": "BklIRmpxa7", "original": null, "number": 3, "cdate": 1541620685961, "ddate": null, "tcdate": 1541620685961, "tmdate": 1541620685961, "tddate": null, "forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "content": {"title": "Some nice pieces and ideas but have concerns about methodology and shallow analysis", "review": "This paper tries to quantify how \"dense\" representations we need for a specific task -- more specifically, how many dimensions are needed from a given representation (for a given task) to achieve a percentage of the performance of the entire representation. The second thing the paper tries to quantify is how well representations learned for one task can be fine tuned for another. Experiments are conducted with 4 different representation technique on a dozen or so tasks.\n\nQuick summary: While I liked aspects of this -- including the motivation of having a lightweight way of understanding how well representations transfer across tasks, overall my concerns surrounding the methodology and some missing analysis leads me to believe this needs more work before it is ready for publication.\n\nQuality: Below average\nI believe the proposed techniques have some flaws which hurt the eventual method. There are also concerns about the motivations behind parts of the technique.\n\nClarity: Fair\nThere were some experimental details that were poorly explained but in general the paper was readable.\n\nOriginality: Fair\nThere were some nice ideas in the work but I remain concerned about aspects of it.\n\nSignificance: Below average\nMy concern is that the flaws in the method do not make it conducive to use as is.\n\n\nStrengths / Things I liked:\n\n+ I really liked the motivating problem of being able to (hopefully cheaply / efficiently) estimate transfer potential to understand how well representations will perform on a different task.\n\n+ Multiple representations and tasks experimented with\n\nWeaknesses / Things that concerned me:\n(In no specific order)\n\n- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed. While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations. This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)\n\nLet's take an example: Say there is a single dimension of the representation that is a perfect predictor of a task. Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.\n\nTo me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.\n\n- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments. This again greatly concerned me as I am not certain how stable these metrics are.\n\n- (W3) Baselines for transfer learning: I felt this was another notable oversight. I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations. This latter baseline is a zero-cost baseline as it is not even dependent on the method.\n\n- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how \"precision\" and NDCG are used as metrics. Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the \"gold\" set. How is precision and NDCG calculated from this?\n\nMore importantly I don't believe looking at rank alone is sufficient since that completely obscures the actual performance numbers obtained via transfer. In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.\n\n- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks. (This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)\n\n- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful\n\n- (W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.\n\nI find this striking because I can easily come up with cheaper alternatives to get at this \"density\". For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.\n\nIf I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?\n\nAlternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?\n\n- (W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets\n\n- (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels. They would have a very low weight difference score though they are ideal representations for each other. Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "cdate": 1542234218869, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335957490, "tmdate": 1552335957490, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklOwd45hm", "original": null, "number": 2, "cdate": 1541191776263, "ddate": null, "tcdate": 1541191776263, "tmdate": 1541533093073, "tddate": null, "forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "content": {"title": "An interesting approach to an important problem; but limited in scope and relevant comparisons", "review": "MEASURING DENSITY AND SIMILARITY OF TASK RELEVANT INFORMATION IN NEURAL REPRESENTATIONS\n\nSummary:\n\nThis work attempts to define two kinds of metrics (metrics for information density and for information similarity) for the sake of automatically detecting similarity between tasks so that transfer learning can be done more efficiently. The concepts are clearly explained, and the metric for information density seems to match up with intuitions coming out of forward selections approaches. The metric for information transfer seems to be the commonplace metric that other works default to when they show that pre-trained representations are effective on downstream tasks. It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions. The problem addressed (automatic similarity scoring of tasks) is important for transfer learning, and thus the results have potential to be very impactful if they generalize to other kinds of tasks; as is, they seem to apply only to classification tasks, but that is a good step.\n\nPros:\n\nClearly written; experiments on the datasets chosen do seem to suggest that the proposed methods have potential. Brings in nice intuition from forward feature selection. An important problem with potential for high impact.\n\n\nCons:\n\nIt is not clear to me that the classifier difference metric is well-defined. Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested? Is it not the case that classifier weights could come out quite different despite the tasks being quite similar if the linear classifiers learned to capitalize on dissimilar, yet equally fruitful patterns in the input features?\n\nDo you have thoughts on how this could be applied outside the context of sentence representations and further outside the context of classification? Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.\n\nThese classification datasets are often so close, that I do wonder whether even simpler methods would work just as well. For example, clustering on bags-of-words might also show that SST, SST-fine, and IMDb are close/similar/transferable. The same could be said for SICK and SNLI. It would be nice to see a comparison to such baselines in order to get a sense of how the proposed methods give insights that other unsupervised or supervised methods might give just as well. Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "cdate": 1542234218869, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335957490, "tmdate": 1552335957490, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyefBu5O3m", "original": null, "number": 1, "cdate": 1541085241728, "ddate": null, "tcdate": 1541085241728, "tmdate": 1541533092857, "tddate": null, "forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "content": {"title": "An interesting direction, but limited apparent utility", "review": "This paper proposes simple metrics for measuring the \"information density\" in learned representations. Overall, this is an interesting direction. However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain. And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared. \n\n+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important. \n\n+ The proposed metrics and simple and intuitive.\n\n+ It is interesting that a few units seem to capture most task specific information. \n\n- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here. As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task. Yet the metrics proposed depend on supervision in the target domain. If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set. It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks. I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue. \n\n- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed. The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.\n\n- The CFS metric depends on a hyperparameter (the \"retention ratio\"), which here is arbitrarily set to 80% without any justification.\n\n- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1490/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "abstract": "Neural models achieve state-of-the-art performance due to their ability to extract salient features useful to downstream tasks. However, our understanding of how this task-relevant information is included in these networks is still incomplete. In this paper, we examine two questions (1) how densely is information included in extracted representations, and (2) how similar is the encoding of relevant information between related tasks. We propose metrics to measure information density and cross-task similarity, and perform an extensive analysis in the domain of natural language processing, using four varieties of sentence representation and 13 tasks. We also demonstrate how the proposed analysis tools can find immediate use in choosing tasks for transfer learning.", "keywords": ["Neural Networks", "Representation", "Information density", "Transfer Learning"], "authorids": ["ddanish@cs.cmu.edu", "mansig1@cs.cmu.edu", "nitishkk@andrew.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cmu.edu"], "authors": ["Danish Pruthi", "Mansi Gupta", "Nitish Kumar Kulkarni", "Graham Neubig", "Eduard Hovy"], "TL;DR": "Measuring information density and cross-task similarity in neural models and its application in transfer learning.", "pdf": "/pdf/a09c333bba24c78483deee1ff093b4b63deb70f6.pdf", "paperhash": "pruthi|measuring_density_and_similarity_of_task_relevant_information_in_neural_representations", "_bibtex": "@misc{\npruthi2019measuring,\ntitle={Measuring Density and Similarity of Task Relevant Information in Neural Representations},\nauthor={Danish Pruthi and Mansi Gupta and Nitish Kumar Kulkarni and Graham Neubig and Eduard Hovy},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye64hA9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1490/Official_Review", "cdate": 1542234218869, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye64hA9tm", "replyto": "Hye64hA9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1490/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335957490, "tmdate": 1552335957490, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1490/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}