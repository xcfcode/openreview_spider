{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124453768, "tcdate": 1518469131852, "number": 265, "cdate": 1518469131852, "id": "rkNGgYkDG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rkNGgYkDG", "signatures": ["~Dylan_Malone_Stuart1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8\\% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks.", "paperhash": "stuart|quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training", "keywords": ["dynamic precision scaling", "acceleration", "training", "reduced precision"], "_bibtex": "@misc{\n  stuart2018quantization,\n  title={Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training},\n  author={Dylan Malone Stuart and Ian Taras},\n  year={2018},\n  url={https://openreview.net/forum?id=rkNGgYkDG}\n}", "authorids": ["dylan.stuart@mail.utoronto.ca", "tarasian@ece.utoronto.ca"], "authors": ["Dylan Malone Stuart", "Ian Taras"], "TL;DR": "We propose a dynamic precision scaling algorithm that uses information about the quantization error to reduce the bitwidth of parameters, gradients, and activations during training without hurting accuracy.", "pdf": "/pdf/e2239295bf29fb08cf35a405513578f25d24c452.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582889920, "tcdate": 1520525482029, "number": 1, "cdate": 1520525482029, "id": "Syzhekytf", "invitation": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "signatures": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer3"], "content": {"title": "good work with solid experiments", "rating": "7: Good paper, accept", "review": "The authors proposed a modified fixed point scheme for training neural networks with reduced precision. the integer and fractional part of the fixed point format can be independently adjustable during training.\n\nIt is very interesting (and convincing) that the authors showed that 13-bit fixed point explodes at some point of training, while the baseline and the model trained with DPS stay stable throughout the training.\n\nAlso, since the bit width for activations/gradients/weights is adaptive during training, the authors showed that gradients always need a high bit width, the activations need a relatively low bit width, while for weights the need for accuracy increases during training. \n\nOne caveat is its realization on a real hardware. If the precision of fixed point format always changes, is there an efficient enough way to utilize the benefit of the dynamically reduced bit width? But I agree that this question could be beyond the scope of a workshop paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8\\% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks.", "paperhash": "stuart|quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training", "keywords": ["dynamic precision scaling", "acceleration", "training", "reduced precision"], "_bibtex": "@misc{\n  stuart2018quantization,\n  title={Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training},\n  author={Dylan Malone Stuart and Ian Taras},\n  year={2018},\n  url={https://openreview.net/forum?id=rkNGgYkDG}\n}", "authorids": ["dylan.stuart@mail.utoronto.ca", "tarasian@ece.utoronto.ca"], "authors": ["Dylan Malone Stuart", "Ian Taras"], "TL;DR": "We propose a dynamic precision scaling algorithm that uses information about the quantization error to reduce the bitwidth of parameters, gradients, and activations during training without hurting accuracy.", "pdf": "/pdf/e2239295bf29fb08cf35a405513578f25d24c452.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582889742, "id": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper265/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer2"], "reply": {"forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582889742}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582839009, "tcdate": 1520595011111, "number": 2, "cdate": 1520595011111, "id": "rkiSxgxKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "signatures": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer1"], "content": {"title": "Incremental idea with a weak evaluation ", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an alternative way for dynamic precision scaling of the fixed point representations that depends on quantisation error statistics; the integer and fractional parts are adjusted according to the overflow rate and average quantisation error respectively. This extends the work of Essam et al.  to also include dynamic bit-widths. The authors then employ successfully the proposed scheme for the quantization of  the weights, activations and gradients of a simple LeNet-5 on MNIST. \n\nThe paper is reasonable well written and the idea is simple, easy to implement and well framed in the context of prior work. Nevertheless, I do believe that the contribution is incremental and that the experiment section needs more work. For example, comparisons against the relevant literature discussed at Table 1 are missing so it is hard to understand the significance of the contribution. Furthermore, it would be good if the authors could discuss the sensitivity of the method to the E_max and R_max hyper parameters. \n\nPros:\n- Simple idea that seems to work on the toy experiment\n\nCons:\n- Experiment section needs more work to be convincing", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8\\% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks.", "paperhash": "stuart|quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training", "keywords": ["dynamic precision scaling", "acceleration", "training", "reduced precision"], "_bibtex": "@misc{\n  stuart2018quantization,\n  title={Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training},\n  author={Dylan Malone Stuart and Ian Taras},\n  year={2018},\n  url={https://openreview.net/forum?id=rkNGgYkDG}\n}", "authorids": ["dylan.stuart@mail.utoronto.ca", "tarasian@ece.utoronto.ca"], "authors": ["Dylan Malone Stuart", "Ian Taras"], "TL;DR": "We propose a dynamic precision scaling algorithm that uses information about the quantization error to reduce the bitwidth of parameters, gradients, and activations during training without hurting accuracy.", "pdf": "/pdf/e2239295bf29fb08cf35a405513578f25d24c452.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582889742, "id": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper265/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer2"], "reply": {"forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582889742}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582822812, "tcdate": 1520611165080, "number": 3, "cdate": 1520611165080, "id": "S1SDJ4ltG", "invitation": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "signatures": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer2"], "content": {"title": "This paper proposes a dynamic precision scaling method for training quantized networks. The idea is interesting, however the achieved compression rate is very impressive.", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose quantization error as a metric for training networks with dynamic precision. This idea of dynamic precision is interesting, as it may lead to better compression rate. However, the experimental results of this paper is not that promising.\n\nIn distribute training, gradient sync requires large network bandwidth. And highly compressed gradient can help a lot. Only compressing weights or activations lightly is not that important, unless we can compress them to much less bits (such as 1-8 bits). Actually, there are already many related works about network quantization (fixed quantization level) that can achieve much better results.\n\nFor experiments, only evaluations on MNIST are conducted. The authors are recommended to try larger datasets (eg. imagenet) and more stoa networks (eg. residual nets).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8\\% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks.", "paperhash": "stuart|quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training", "keywords": ["dynamic precision scaling", "acceleration", "training", "reduced precision"], "_bibtex": "@misc{\n  stuart2018quantization,\n  title={Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training},\n  author={Dylan Malone Stuart and Ian Taras},\n  year={2018},\n  url={https://openreview.net/forum?id=rkNGgYkDG}\n}", "authorids": ["dylan.stuart@mail.utoronto.ca", "tarasian@ece.utoronto.ca"], "authors": ["Dylan Malone Stuart", "Ian Taras"], "TL;DR": "We propose a dynamic precision scaling algorithm that uses information about the quantization error to reduce the bitwidth of parameters, gradients, and activations during training without hurting accuracy.", "pdf": "/pdf/e2239295bf29fb08cf35a405513578f25d24c452.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582889742, "id": "ICLR.cc/2018/Workshop/-/Paper265/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper265/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper265/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper265/AnonReviewer2"], "reply": {"forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper265/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582889742}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573583334, "tcdate": 1521573583334, "number": 172, "cdate": 1521573583001, "id": "rkwA0R0Yf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rkNGgYkDG", "replyto": "rkNGgYkDG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8\\% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks.", "paperhash": "stuart|quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training", "keywords": ["dynamic precision scaling", "acceleration", "training", "reduced precision"], "_bibtex": "@misc{\n  stuart2018quantization,\n  title={Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training},\n  author={Dylan Malone Stuart and Ian Taras},\n  year={2018},\n  url={https://openreview.net/forum?id=rkNGgYkDG}\n}", "authorids": ["dylan.stuart@mail.utoronto.ca", "tarasian@ece.utoronto.ca"], "authors": ["Dylan Malone Stuart", "Ian Taras"], "TL;DR": "We propose a dynamic precision scaling algorithm that uses information about the quantization error to reduce the bitwidth of parameters, gradients, and activations during training without hurting accuracy.", "pdf": "/pdf/e2239295bf29fb08cf35a405513578f25d24c452.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}