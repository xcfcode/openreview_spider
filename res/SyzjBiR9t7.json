{"notes": [{"id": "SyzjBiR9t7", "original": "rkl7E8FtFQ", "number": 118, "cdate": 1538087747322, "ddate": null, "tcdate": 1538087747322, "tmdate": 1545355424359, "tddate": null, "forum": "SyzjBiR9t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJgJNMvXlN", "original": null, "number": 1, "cdate": 1544938023206, "ddate": null, "tcdate": 1544938023206, "tmdate": 1545354491272, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Meta_Review", "content": {"metareview": "This manuscript proposes an extension of convolution operations for manifold-valued data. The primary contributions include the development and description of the approach and implementation and evaluation on real data.\n\nThe reviewers and AC expressed concern about the clarity of the presentation, particularly for a general ICLR audience. Though the contributions are primarily conceptual/theoretical, reviewers expressed concern about the breadth and quality of the presented experimental results. Some additional concerns related to missing proofs and details were addressed in the rebuttal.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper118/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353331364, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353331364}}}, {"id": "SJlgBKulk4", "original": null, "number": 14, "cdate": 1543698743667, "ddate": null, "tcdate": 1543698743667, "tmdate": 1543698743667, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "HJg9flnACX", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Response to second round of questions from Reviewer-2", "comment": "(1) \"However, my overall view of this paper has not been changed as the writing of this paper was not clear enough for me. \"\n\nAns: We are sorry that you feel the writing was unclear. Can you please point out any further clarifications that you need which we can provide in order to change this opinion of yours?\n\nWe should point out that the content of our paper is primarily addressing non-Euclidean spaces and assumes readers with some background in non-Euclidean geometry as was expected from the readership of the \"Spherical CNNs\" paper in ICLR 2018 that was judged the best paper. \n\n(2) \"I still not understand how to obtain k-dimensional subspaces of feature vectors from video sequence for the input of wFM layer in the CNN model.\"\n\nAns: This was described in Section 3.2. the first paragraph in detail. We will now give a simplified explanation. Given a video, i.e., a time sequence of $N$ image frames, consider $k$ consecutive time frames and assemble them as column vectors of a matrix say $A$. The size of $A$ will then be $n\\times k$. Now take the span of this matrix $A$ which will be a $k$-dimensional subspace in $\\mathbf{R}^n$, hence will be a point on $Gr(k,n)$. \n\nHope we have clarified all the issues that you have raised. We will be more than happy to clarify anymore that you might have.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "SylACz_g14", "original": null, "number": 13, "cdate": 1543697110155, "ddate": null, "tcdate": 1543697110155, "tmdate": 1543697110155, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "rkxvysGcCX", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Proof of non-equivalence of single-layer and multi-layer ManifoldNets", "comment": "Here we present a counterexample to the claim by Reviewer-1 that\nour multi-layer ManifoldNet cannot be collapsed to a single layer\nManifoldNet. Although we agreed with the Reviewer in our earlier\nresponse, upon more careful thought about this interesting question,\nwe found that this is only true for constant curvature manifolds\nsuch as Euclidean space (zero curvature), the hypersphere and the\nhyperbolic space. For the Euclidean case, the proof is obvious and we will\nnot provide the proof for the latter two manifolds here. Instead, we\nprovide a proof via a counterexample that the claim by Reviewer-1 is untrue\nfor non-constant curvature manifolds such as SPD(n), which is a\nnegatively curved Riemannian manifold.\n\nLet us suppose we are given 4 symmetric positive definite (SPD)\nmatrices, \n$A=\n\\begin{bmatrix} \n0.9593 & 0.3429 \\\\ \n0.3429 & 0.1493\n\\end{bmatrix}$, \n\n$B=\n\\begin{bmatrix} \n1.2575 & 0.5475 \\\\ \n0.5475 & 1.8143\n\\end{bmatrix}$, \n\n$C=\n\\begin{bmatrix} \n1.2435 & 0.6396 \\\\ \n0.6396 & 1.1966\n\\end{bmatrix}$, \n\n$D=\n\\begin{bmatrix} \n1.2511 & 0.5446 \\\\ \n0.5447 & 1.3517\n\\end{bmatrix}$, \n\nwhose wFM we want to compute. Let us consider\ntwo sequences $S1 = \\left\\{A,B,C,D\\right\\}$ and $S2 =\n\\left\\{A,C,B,D\\right\\}$.  Consider a one layer ManifoldNet for\ncomputing the wFM of these four matrices. For simplicity of\nexposition, suppose this one layer network learns equal weights\n$(=0.25)$ for all matrices and hence yields the wFM \n$M=\n\\begin{bmatrix}\n1.1640 & 0.4667 \\\\ \n0.4667 & 0.6388\n\\end{bmatrix}$ \nas the solution for both sequences $S1$ and $S2$ respectively. To compute the wFM, we use a gradient descent applied to the weighted sum of square geodesic\ndistances between the unknown wFM and the sample points.\n\nNow, let us consider a two layer wFM. For $S1$, the first layer\ncomputes wFM of $\\left\\{A, B\\right\\}$ and $\\left\\{C, D\\right\\}$\nrespectively and returns $M1$ and $M2$ as the wFMs. Then, the second\nlayer takes $M1$ and $M2$ as inputs and returns their wFM say,\n$M3$. Analogously for the sequence $S2$, the first layer computes wFM\nof $\\left\\{A, C\\right\\}$ and $\\left\\{B, D\\right\\}$ and returns\n$\\bar{M1}$ and $\\bar{M2}$. Then the second layer takes as input,\n$\\bar{M1}$ and $\\bar{M2}$ and returns $\\bar{M3}$ as their wFM.\n\nIt can be verified that for the first layer if we use equal weights,\nwe need the weights for the second layer to be $0.4980$ and $0.5050$\nfor $S1$ and $S2$ respectively such that both $M3=M$ and\n$\\bar{M3}=M$. This counterexample shows that the weights are\ndependent on the data matrices, which are points on the SPD\nmanifold. Thus, for each data set, in order for one to collapse the\nmulti-layer ManifoldNet to a single layer ManifoldNet, one needs a\ndistinct set of weights that are dependent on the data. This is unlike\nin the Euclidean case, where, the weights interact in a known fixed\nway for which there is an obvious analytic expression. In the case of\nRiemannian manifolds with non-constant curvature, such as SPD(n) or\nthe Grassmanian, Stiefel, and others, this is not true.\n\nThis clearly shows that the two-layer ManifoldNet cannot be collapsed\nto a single layer.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "HJg9flnACX", "original": null, "number": 12, "cdate": 1543581714451, "ddate": null, "tcdate": 1543581714451, "tmdate": 1543581714451, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "BJgWKjEt67", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Thank you for the explanation. ", "comment": "Thank you for answering my questions. From the answers, several points of this paper become clear. However, my overall view of this paper has not been changed as the writing of this paper was not clear enough for me. \n\nAns. b): \u201dThe output of each convolution is equivalent (not invariant) \u2026\u201d \nThank you for pointing it out. There was a misunderstanding in this point; I was confusing equivalence and invariance. \n\nAns. f): \u201cWe take wFM of theses subspaces ..\u201d\nI understand the proposed model averages linear subspaces on Grassmannian manifold. However, I still not understand how to obtain k-dimensional subspaces of feature vectors from video sequence for the input of wFM layer in the CNN model.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "rkxvysGcCX", "original": null, "number": 9, "cdate": 1543281375000, "ddate": null, "tcdate": 1543281375000, "tmdate": 1543281375000, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "B1lwsi8KAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Response", "comment": "\nIt appears that the reviewer misunderstood our earlier response. Our response to \"h) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seems quite strong....\u201d was to clarify why the constraint is not very strong and not impractical in practice.  This constraint has to be satisfied in order to guarantee the uniqueness of the computed wFM and we are not the first to use this constraint in literature. All researchers that have used FM or wFM computation from data in published literature have and must use this assumption for the uniqueness of the computed estimates. \n\nIn our response regarding the inclusion of tangent-ReLU, we tried to explain why after the application of a simple extrinsic non-linearity such as a tangent-ReLU, the geodesic ball constraint is violated. This is primarily because, application of an arbitrary extrinsic nonlinearity to a manifold-valued data point cannot guarantee the result to lie inside the geodesic ball.  We also provided in our earlier response a possible way to achieve this without violating the geodesic ball containment constraint. However, we did not find the need to use it in our experiments since we proved that the wFM is already a nonlinear operation and possesses the required contraction property of a ReLU type unit. This however does not rule out our using an additional nonlinearity if deemed necessary in the future. \n\nThese two responses do not contradict each other at all. The former is explaining why the assumption is not impractical while the later explains why tangent-ReLU type operation can violate this constraint. We hope the reviewer finds the above elaborated responses to this issue of concern more palatable and request that he/she read it with care.\n\n\u201cSo far, I don't see this discussion changes my view of the paper, in that there are too many aspects that make me worry about the paper.\u201d\n\nAns: This is very unfortunate, and we request the reviewer to list out for us the \u201ctoo many aspects\u201d stated above that are yet unanswered to the reviewer\u2019s satisfaction. We would be happy to settle any and all issues of concern regarding this work to this reviewer. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "B1lwsi8KAQ", "original": null, "number": 8, "cdate": 1543232415073, "ddate": null, "tcdate": 1543232415073, "tmdate": 1543232415073, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "BkxpMQpdAm", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "response", "comment": "You write:\n\n\"In order achieve this, we have to ensure that after applying the non-linearity and mapping it back to the manifold using the Riemannian Exp map, the resultant manifold valued points (on which we are going to perform wFM in the next conv./ inv. layer) are inside a geodesic ball (as mentioned in Section 2 of the manuscript). A tangent-ReLU violates such a constraint.\"\n\nbut previously you wrote:\n\n\"h) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seems quite strong....\n\nAns: This assumption is not a hindrance in many applications that we are aware of and is commonly used by many researchers in existing literature. For example, in the case of a hypersphere, the injectivity radius defining this ball is \\pi/2 and this results in a fairly large region for consideration in the upper/lower hemisphere. The restriction is imposed to guarantee the uniqueness of the computed FM/wFM.\"\n\nThese two statements appear to be in direct conflict with each other as far as I can tell.\n\nSo far, I don't see this discussion changes my view of the paper, in that there are too many aspects that make me worry about the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "HkgL4XTO0Q", "original": null, "number": 7, "cdate": 1543193389627, "ddate": null, "tcdate": 1543193389627, "tmdate": 1543193564113, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "Hkxunv4HRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Response", "comment": "\"``I do not understand this. Does that mean that you work with images where each pixel is manifold-valued, such as what you would see in DTI?\u2019\u2019\n\nAns:  In these experiments, we do not have a DTI \u201ctype\u201d field. Instead we have a time ordered sequence of SPD matrices, which is a 1D field. For example, in the moving MNIST experiment, from one video we get a sequence of SPD matrices, which is treated as one data sample for the ManifoldNet. It should however be noted that our work is applicable to 2D and 3D SPD matrix-valued fields as well."}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "BkxpMQpdAm", "original": null, "number": 6, "cdate": 1543193364961, "ddate": null, "tcdate": 1543193364961, "tmdate": 1543193364961, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "Byx2rYEHRm", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Response", "comment": "\u201cMy concern regarding stacking layers is that if this is mathematically equivalent to a single layer, then perhaps you actually should be using a more traditional nonlinear activation. \u201c \n\nAns: Traditional nonlinear activations are not applicable here since they expect the input to be real-valued. For manifold-valued input, some researchers have applied the traditional nonlinear activation after Log-mapping the manifold-valued data into the tangent space anchored at some point on the manifold. We will call this the tangent-ReLU here in this discussion. The reason a tangent-ReLU is not applicable is that after the application of this kind of non-linearity we have to ensure that we can apply the wFM operation in the next layer (or FM  operation for final invariant layer). In order achieve this, we have to ensure that after applying the non-linearity and mapping it back to the manifold using the Riemannian Exp map, the resultant manifold valued points (on which we are going to perform wFM in the next conv./ inv. layer) are inside a geodesic ball (as mentioned in Section 2 of the manuscript). A tangent-ReLU violates such a constraint. \n\n\"``If the network depth is purely a computational trick, that does not aid in solving highly nonlinear problems, then I am not convinced about the point of \"going deep\u201d.\n\nAns: Note that, in standard CNN the reason for going deeper is two fold, (1) Use non-linearity in between layers to get a non-linear CNN model (2) Efficient usage of network parameters, i.e., equivalence of a deep network and a shallow network with larger number of parameters. Moreover, the reason we cannot collapse a standard CNN is because of the non-linearity in between. \n\nIn our ManifoldNet framework, note that since we defined convolution using wFM, one can easily show that a convolution operator defined in such a way is non-linear on any non-flat manifold. Hence, the need for a further non-linearity is not apparent. Furthermore, we have shown that wFM is not only a non-linear operator but also a contraction mapping, hence it shares some characteristics with standard ReLU like operators. Since wFM, a.k.a the defined convolution operator is non-linear and satisfies the contraction property, we do not have the need for additional nonlinearity. The only need for going deeper is to make the model parameter estimation efficient which is the reason we gave in our earlier response. This however does not rule out introduction of any further nonlinearity if deemed necessary in the future. One such possible non-linear map: $F: \\mathcal{M} \\rightarrow \\mathcal{M}$ is given as follows: $F(x) = g.x$ for some fixed $g \\in G$. It is easy to see that this operator $F$ has the following properties: (1) It is non-linear, (2) it can guarantee that, if ${x}$ is within a geodesic ball, so is ${g.x}$. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "Byx2rYEHRm", "original": null, "number": 5, "cdate": 1542961476297, "ddate": null, "tcdate": 1542961476297, "tmdate": 1542961476297, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "HJxVi6NK6X", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Stacking", "comment": "\"b) I am concerned about the stacking of multiple \"convolution\" (wFM) layers ...\n\nAns: This is an interesting point and indeed is true, i.e., two wFM layers can be collapsed into one. But the learning becomes much more computationally expensive if one resorts to collapsing. We illustrate this with a toy example here. Let us consider a 4*4 manifold valued field.  Let us say we used 2*2 wFM filters (with stride 2*2). If we use two such wFM layers to get a single manifold valued point, then the total number of parameters is 4+4=8. Now, instead if we compute the mean using a single layer wFM, the number of parameters increases (doubled) to 4*4=16. This indicates that the learning becomes computationally harder even in this simple toy problem. Though theoretically, the collapsing is a valid thing to do, but in practice, it makes the learning of the filter computationally intractable.\"\n\nI agree with this comment, but it misses my point (which perhaps was unclear). In the paper it is argued that the weighted mean also serves the role of a contraction mapping, i.e. that a nonlinear activation is not required. My concern regarding stacking layers is that if this is mathematically equivalent to a single layer, then perhaps you actually should be using a more traditional nonlinear activation. If the network depth is purely a computational trick, that does not aid in solving highly nonlinear problems, then I am not convinced about the point of \"going deep\"."}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "Hkxunv4HRQ", "original": null, "number": 4, "cdate": 1542961071670, "ddate": null, "tcdate": 1542961071670, "tmdate": 1542961071670, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "HJxVi6NK6X", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Follow-up", "comment": "\"a) A key point of CNNs is that you learn filters that are small ....\n \nAns: For a field of manifold valued data, our filter size will be exactly like standard CNN, i.e., one weight for one point inside the filter. So for a 5*5 filter, we have 25 weights to learn for wFM. Now, we share the filter across the entire field (analogous to the standard CNN).\"\n\nI do not understand this. Does that mean that you work with images where each pixel is manifold-valued, such as what you would see in DTI?"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "BJgWKjEt67", "original": null, "number": 1, "cdate": 1542175608942, "ddate": null, "tcdate": 1542175608942, "tmdate": 1542287310137, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "rkxR6zqqhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Thank the reviewer+answers to the comments", "comment": "a)\tIt is hard to understand how each theory presented in this paper helps to improve CNN. For example, the invariance to group operation. Some experimental results would help to understand the advantage of the group invariance. \n\nAns: Many classification problems should naturally exhibit group invariance, or more generally, group equivariance. For instance, an image of a cat remains an image of a cat regardless of any applied rigid or similarity transformation. More generally, whenever a NN must learn a function that exhibits group equivariance, it is highly desirable for the NN itself to exhibit group equivariance at each layer.   While the layers of our network are equivariant, we also design a special output layer that is invariant to group operations (just as in CNNs whose last layer is translation invariant) which is required for classification problems. This is precisely our contribution.\n\nIn our moving MNIST experiment, we classify different moving patterns. But note that in any two sample videos from a same class, the digits started moving from random location. So, within a class, two paths correspond to two videos, related to one another by some affine transformation. In our representation, we used SPD matrices and the group that acts on the space of SPD matrices is the affine group. So, our network can classify two moving patterns if they are in the same orientation modulo any affine transformation. This is one example of the power of a group invariant network.\n\nb) It is also unclear why the authors constructed the invariant last layer although the inputs of the last layer are invariant under group operations. \n\nAns:  The inputs to the last layer are not invariant to group operations admitted by the manifold on which the data reside, instead they are equivariant to these group operations. It seems like there is a misunderstanding in this context.  The output of each convolution is equivariant (not invariant) to the group action, and hence the input to the last layer is equivariant (not invariant). In order to make the output of the network invariant, we need a group invariant last layer. This last layer can be viewed as a substitute for a fully connected layer in traditional CNN\u2019s (which is translation invariant).\n\nc) In the introduction section, the authors raised the omnidirectional camera, diffusion magnetic resonance imaging, elastography as examples of manifold-valued data. However, experiments are limited to standard video sequences. \n\nAns: In the introduction section, we stated that there are two types of problems that can be consider here, (i) data that are manifold-valued and (ii) data that are samples of functions defined on manifolds. We stated example applications for both but clearly stated that the focus of this work is the problem stated in (i).   The example that this reviewer mentioned (such as, omnidirectional camera data) is a case that belongs to the problem in (ii) and is not the focus of our present paper. Other examples such as diffusion MRI, Elastography etc. do not have sufficient number of data sets that are publicly accessible. Further, they are specialized imaging techniques that are very expensive and public databases at most contain a few hundred scans, e.g., the Human Connectome Project data, which is insufficient amount of data for a deep network setting unless one uses a patch-based approach, which we will consider in the future.\n\nd) It is unclear how to obtain the weights {w_i} of wFM by backpropagation. \n\nAns: Note that wFM is on the manifold and we provided a closed form recursive expression for its computation.  The weights {w_i} are however real valued and satisfy the convexity constraint. So, we used standard backpropagation to learn the {w_i}s.\n\n\ne) Since the contribution of this paper is to to use wFM instead of a convolutional layer, it is more interesting to visualize the weights {w_i}. \n\nAns: For both the dimensionality reduction and the video classification experiments the weights reside in the temporal domain. Unlike for the regular CNN, where filter weights correspond to image features, our Temporal CNN structures are difficult to interpret by simple visualization since they capture temporal patterns. \n\n\nf) More explanation needs for the model used for experiments. Especially in dimensional reduction experiments, I could not understand how each subspace is obtained and averaged. If each frame is a subspace, by averaging frames, the reconstruction would be blurred. \n\nAns: The k-dimensional subspaces spanned by the feature vectors correspond to the points on the Grassmannian. We take the wFM of these subspaces (which is a linear subspace) and then project the data onto this subspace. The projection is then passed onto the decoder. This is computationally a better substitute for doing PCA of the feature vectors directly (as shown in  Chakraborty et al CVPR17); since, for long videos, doing PCA on feature vectors is computationally infeasible. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "BJxlJnVY6m", "original": null, "number": 2, "cdate": 1542175704172, "ddate": null, "tcdate": 1542175704172, "tmdate": 1542239632712, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "B1lTKrfq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Thank the reviewer+answers to the comments", "comment": "a) The paper\u2019s major contribution is using wFM to generalize the traditional convolution for manifold-valued data. Accordingly, the critical technical contribution is to estimate the wFM. The paper suggests a inductive/recursive way for the wFM approximation. To the best of my knowledge, there already exist some inductive/recursive wFM estimation methods like [1,2]. Unfortunately, the authors seem to overlook them and do not discuss them at all. Accordingly, I think the paper missed some important related works for sufficient study.\n\nAns: The major technical contribution of this paper is not the wFM estimator but is in showing that wFM on manifolds can be used in place of convolutions for manifold-valued data sets (not samples of functions on a manifold). Further, proving that wFM is equivariant to the group actions admitted by the manifold on which the data reside. Additionally, proving that wFM is equivalent to an unweighted FM in a different metric which then allows us to use existing results to prove consistency of the recursive wFM estimator.   Finally, all this was done for a general Riemanian manifold. The references [1], [2] are focused on the manifold of symmetric positive definite matrices (SPD) and hypersphere respectively and hence not valid for other manifolds such as the Grassman, etc. Thus, in this paper, we did not discuss recursive FM estimators in [1,2]. We will gladly include them and place them in the context of the work presented here, in the revision.\n\nb) Due to the inconsistent fonts, chaotic layout it is really hard for the readers to follow the content of the paper.\n\nAns: In the figure, we have used smaller fonts which we will revise. But can the reviewer please point out the \u201cchaotic layout\u201d that he/she is referring to as we were unable to find any chaotic layout in the paper.\n\nc) It is not easy to reimplement the wFM layers, which is the core contribution of the paper.\n\nAns: Since we used analytical expression for geodesics to compute wFM, for most of the manifolds encountered in the applications shown in this paper, the implementation of wFM is easy. \n\nd) Please see the anonymized link for the code:\n\nhttps://drive.google.com/open?id=1n18T7Ea-ides8NS-NEhnlG8PRK_DJM4e\n\ne) Anyway it fails to describe what kind of intrinsic metric they used for the specific manifold-valued data like SPD matrices and linear subspace.\n\nAns: We used the canonical metric in both cases, i.e., GL-invariant canonical metric for SPD and the canonical metric for Grassmannian (which is the L2 norm of the principal angles between two subspaces).\n\nf) Is it \\Gamma_{M_{n-1}}^{X_n} rather than \\Gamma_{M_{n-1}}^{X_M} for Eq.(2)? \n\nAns: M_n = \\Gamma_{M_{n-1}}^{X_n}(.), since M_n lies on the geodesic between M_{n-1} and X_n. We do not think there is any X_M in this paper, so the reviewer probably misunderstood.\n\ng) For video classification, the paper only uses the moving MNIST, which is not a challenging dataset while there are plenty of large scale video datasets. In addition, the paper is expected to compare SOTA video classification methods.\n\nAns: The primary goal of this paper is to introduce a theoretical framework that parallels CNNs but is suited for manifold-valued data sets. The paper introduces a novel technique to perform convolutions on general manifold valued data and also gave some proof-of-concept experiments.  Experiments with large scale video data sets will be one of our future aims and is currently not the focus of this paper. As mentioned above, the focus is to provide a novel intrinsic geometric framework for performing convolution operations on manifold-valued data sets and support the theory with proof of concept experiments.\n\nh) To learn the advantage of the proposed network over some related manifold networks like Huang et al., 2016, Huang & Van Gool 2017, it is necessary to evaluate them in the experiments. \n\nAns: We already did a comparative experiment using Huang et al., 2017 on Moving MNIST data and the performance is as follows:30-60: 99.6%, 10-15: 50.4%, 10-15-20: 38.8%. Furthermore, each epoch took ~30.2 seconds and the total number of parameters is 110,000. We have tried with larger or smaller sized network but the performance is worse.\n\ni) Furthemore, it is also expected to compare more SOTA auto-encoder based reconstruction models like VAE [3], AAE [4] and WAE [5].\n\nAns: In our auto encoder experiment, we did PCA in the encoding space, so if the encoding space follows a normal distribution (as in case of VAE), then doing PCA does not make sense as the encoding space is already decorrelated. This is the reason for not using a VAE type construction here. "}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "HJxVi6NK6X", "original": null, "number": 3, "cdate": 1542176156489, "ddate": null, "tcdate": 1542176156489, "tmdate": 1542176156489, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "Byg4qQrP2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "content": {"title": "Thank the reviewer+answers to the comments", "comment": "a) A key point of CNNs is that you learn filters that are small ....\n \nAns: For a field of manifold valued data, our filter size will be exactly like standard CNN, i.e., one weight for one point inside the filter. So for a 5*5 filter, we have 25 weights to learn for wFM. Now, we share the filter across the entire field (analogous to the standard CNN).\n\nb) I am concerned about the stacking of multiple \"convolution\" (wFM) layers ...\n\nAns: This is an interesting point and indeed is true, i.e., two wFM layers can be collapsed into one. But the learning becomes much more computationally expensive if one resorts to collapsing. We illustrate this with a toy example here. Let us consider a 4*4 manifold valued field.  Let us say we used 2*2 wFM filters (with stride 2*2). If we use two such wFM layers to get a single manifold valued point, then the total number of parameters is 4+4=8. Now, instead if we compute the mean using a single layer wFM, the number of parameters increases (doubled) to 4*4=16. This indicates that the learning becomes computationally harder even in this simple toy problem. Though theoretically, the collapsing is a valid thing to do, but in practice, it makes the learning of the filter computationally intractable.\n\n\nc) In section 2.2 it is argued that the weighted average (wFM) is a contraction mapping. ...\n\nAns: The definition of contraction mapping depends on the choice of metric on both sides of the inequality. So, this is the reason for the choice of \u201cmaximal distance\u201d instead of \u201cHausdroff distance\u201d. Nonetheless, this particular choice of distance is not an assumption but a choice to prove that the mapping is a contraction mapping.\n\n\nd)Large parts of section 2.1 is devoted to an efficient algorithm for computing weighted averages on manifolds .... \n\nAns: We will rewrite the part for efficient computation for weighted averaging computation to make it explicit that this part is not a contribution of this paper. One of the key contributions is to show that wFM is FM in a different metric and then show that we can use the existing incremental FM algorithm to compute wFM. We will make this clarification in the paper.\n\n\ne) Proposition 5 does not appear to come with a proof.\n\nAns: We included the proof in the appendix.\n\nf) Section 3.2 introduces a new dimensionality reducing layer based on the Grassmann average construction .... \n\nAns: This experiment is motivated by Chakraborty'17. The purpose of this experiment is to demonstrate that one can perform PCA (dimensionality reduction) inside a deep network. In Chakraborty'17, the authors used intrinsic Grassmann averages to compute PCs, so this experiment is an example of using a convolution layer to extract PCs, since we defined a convolution layer on a manifold (Grassmannian in this example) using wFM (Grassmann averages in this example). \n\ng) At times the paper is rather sloppy written, e.g. fonts are way too small in figures....\n\nAns: We thank the reviewer for pointing out the mistakes, we fixed these minor mistakes in the revision.\n\nh) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seems quite strong....\n\nAns: This assumption is not a hindrance in many applications that we are aware of and is commonly used by many researchers in existing literature. For example, in the case of a hypersphere, the injectivity radius defining this ball is \\pi/2 and this results in a fairly large region for consideration in the upper/lower hemisphere. The restriction is imposed to guarantee the uniqueness of the computed FM/wFM.\n\ni) It is not clear to me that the weighted average is a particularly good way to generalize convolution. Yes, I agree, it is *one way to generalize, but why should I pick this one in particular?\n\nAns: In standard CNN, correlation operator can be computed by using \\sum_i w_i x_i inside a filter\u2019s support, where {x_i} are intensities and {w_i} are filter weights. Now, if {w_i} satisfies convexity constraint \\sum_i w_i x_i is the minimizer of the weighted variance, i.e., wFM on Euclidean space. This motivates us to generalize the convolution on the manifold using the minimizer of weighted variance, i.e., wFM. Note that the convexity constraint though is not common in standard CNN, is a crucial constraint to ensure that the output of convolution is on the manifold. Also, this convexity constraint can be thought of as a regularizer on the filter. \n\nj) In the experiment in sec. 3.1 the manifold comes from a particular way to extract features from data .... \n\nAns: We disagree with this comment. In section 3.1., we learned the features inside the network and used the covariance of the extracted features as the input to our manifoldnet. The covariance of the learned features is a choice we made motivated by the work by Yu & Salzmann (2017). Note that, as we train the network in an end-to-end fashion, the features are not hand crafted but learned using the network.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620456, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzjBiR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper118/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper118/Authors|ICLR.cc/2019/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers", "ICLR.cc/2019/Conference/Paper118/Authors", "ICLR.cc/2019/Conference/Paper118/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620456}}}, {"id": "rkxR6zqqhQ", "original": null, "number": 3, "cdate": 1541214918026, "ddate": null, "tcdate": 1541214918026, "tmdate": 1541534266288, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "content": {"title": "Interesting idea, however, more explanations needed. ", "review": "This paper proposes a method to use weighted Frechet Mean (wFM) for the operation on Manifold valued data for CNN. The novel point is to view wFM as a convolutional layer. Overall, this paper is mathematically well written, however, how each theory improves CNN and the model used in experiments are not clear enough. \n\nPros\n+ The use of wFM instead of a convolutional layer is an interesting idea. \n+ This paper is mathematically well written. \n\nCos \n- It is hard to understand how each theory presented in this paper helps to improve CNN. For example, the invariance to group operation. Some experimental results would help to understand the advantage of the group invariance.\n\n- It is also unclear why the authors constructed the invariant last layer although the inputs of the last layer are invariant under group operations. \n\n- In the introduction section, the authors raised the omnidirectional camera, diffusion magnetic resonance imaging, elastography as examples of manifold-valued data. However, experiments are limited to standard video sequences. \n\n- It is unclear how to obtain the weights {w_i} of wFM by backpropagation. \n\n- Since the contribution of this paper is to to use wFM instead of a convolutional layer, it is more interesting to visualize the weights {w_i}. \n\n- More explanation needs for the model used for experiments. Especially in dimensional reduction experiments, I could not understand how each subspace is obtained and averaged. If each frame is a subspace, by averaging frames, the reconstruction would be blurred. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "cdate": 1542234533768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335653342, "tmdate": 1552335653342, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lTKrfq2m", "original": null, "number": 2, "cdate": 1541182852560, "ddate": null, "tcdate": 1541182852560, "tmdate": 1541534266075, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "content": {"title": "missing important related works, careless writing and insufficient evaluation", "review": "This paper introduces a generalization of convolution operations to manifold-valued data using  the computation of the weighted Frechet mean (wFM). Without applying any non-linear units and pooling layers, the paper proposes to merely stack suching generalized convolutional layers to construct a deep network for the data residing on Riemannian manifolds. The evaluations on video classification and reconstruction tasks show the advantages of the introduced network over some baselines. \n\nThe paper\u2019s major contribution is using wFM to generalize the traditional convolution for manifold-valued data.  Accordingly, the critical technical contribution is to estimate the wFM. The paper suggests a inductive/recursive way for the wFM approximation. To the best of my knowledge, there already exist some inductive/recursive wFM estimation methods like [1,2]. Unfortunately, the authors seem to overlook them and do not discuss them at all. Accordingly, I think the paper missed some important related works for sufficient study.\n\n[1] Y. Lim and M. Pa \u0301lfia. Weighted inductive means .LAA, 453:59\u201383, 2014.\n[2] Hesamoddin Salehian et al., An efficient recursive estimator of the Fre \u0301chet mean on a hypersphere with applications to Medical Image Analysis, Mathematical Foundations of Computational Anatomy. 2015.\n\nDue to the inconsistent fonts, chaotic layout it is really hard for the readers to follow the content of the paper. I feel like the paper seems to be completed in the last minute. This brings another critic problem, it is not easy to reimplement the wFM layers, which is the core contribution of the paper. For instance, the paper claims that they used intrinsic Riemannian metric when using wFM to convolve the manifold-valued data. I guess it is involved in \\Gamma_X^Y (Eq.2) which is explained as the shortest curve from X to Y. Anyway it fails to describe what kind of intrinsic metric they used for the specific manifold-valued data like SPD matrices and linear subspace. In addition, is it \\Gamma_{M_{n-1}}^{X_n} rather than \\Gamma_{M_{n-1}}^{X_M} for Eq.(2)? \n\nAnother problem is the evaluations are far from sufficient. For video classification, the paper only uses the moving MNIST, which is not a challenging dataset while there are plenty of large scale video datasets. In addition, the paper is expected to compare SOTA video classification methods. To learn the advantage of the proposed network over some related manifold networks like Huang et al., 2016, Huang & Van Gool 2017, it is necessary to evaluate them in the experiments.  For video reconstruction, using 1000 frame color sample of video is also not sufficient to study the effectiveness of the proposed ManifoldNet. Furthemore, it is also expected to compare more SOTA auto-encoder based reconstruction models like VAE [3], AAE [4]  and WAE [5].\n\n[3] Kingma et al., Auto-encoding variational bayes, 2013\n[4] Makhzani et al., Adversarial autoencoders, 2015\n[5] Wasserstein auto-encoders, 2017\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "cdate": 1542234533768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335653342, "tmdate": 1552335653342, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byg4qQrP2Q", "original": null, "number": 1, "cdate": 1540998027951, "ddate": null, "tcdate": 1540998027951, "tmdate": 1541534265829, "tddate": null, "forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "content": {"title": "Too many things bother me to recommend acceptance", "review": "Brief summary:\nThe paper considers a generalization of convolutional neural networks (CNNs) to data residing on Riemannian manifolds. The idea is to replace convolutions with weighted averages, which are implemented intrinsically on the manifold. It is shown that this operator is equivariant to isometric group actions. A related approach for dimensionality reduction is also proposed, but I think this only applies to Euclidean data, so I am a bit confused about that part. Empirical performance is reported on toy data with weak baselines.\n\nGood points about the paper:\n+ It is a relevant point that the intrinsic average in manifolds is a way to generalized convolutions to the Riemannian domain.\n+ The paper is generally fairly easy to read.\n\nConcerns with the paper:\n- A key point of CNNs is that you learn filters that are small, i.e. only have non-zero weight assigned to a few neighboring pixels. As far as I can tell, the here proposed \"filters\" would be one weight per data point. \n\n- I am concerned about the stacking of multiple \"convolution\" (wFM) layers: since each layer computes the average of a set of points, then doesn't stacking multiple \"convolution\" (wFM) layers on top of each other correspond to computing the average of a set of averages? And can this not be computed by a single average? In other words, is a cascade of \"convolution\" (wFM) layers equivalent to a single layer? Seems like a complicated way of doing shallow learning unless I misunderstand.\n\n- In section 2.2 it is argued that the weighted average (wFM) is a contraction mapping. While I think the proof is correct, I am concerned about the prerequisite assumption that that distance between a set of points X and Y is the *maximal* distance between points in the two sets. Usually, one would define this distance as the *minimal* distance (akin to the Hausdorff distance). It seems that under this more reasonable choice of distance, the proof no longer holds\n\n- Large parts of section 2.1 is devoted to an efficient algorithm for computing weighted averages on manifolds.  Here the text is written such as to indicate that this is a novel contribution of the present paper, even if these results are readily available in the literature. I strongly encourage a re-writing to emphasize that this is a repetition of previous knowledge.\n\n- Proposition 5 does not appear to come with a proof.\n\n- Section 3.2 introduce a new dimensionality reducing layer based on the Grassmann average construction for subspace learning. I was quite confused when reading this. From what I can tell this layer is only applicable when the input data is Euclidean, and as such appears to be unrelated to the rest of the paper.  \n\n- At times the paper is rather sloppy written, e.g. fonts are way too small in figures, the dot(.) notation is not defined (e.g. in def. 7), and the citation style is very difficult to read (please use \\citet and \\citep instead of \\cite).\n\nOther comments:\n*) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seem quite strong. It would be good to comment on this in the paper.\n\n*) It is not clear to me that the weighted average is a particularly good way to generalize convolution. Yes, I agree, it is *one\n way to generalize, but why should I pick this one in particular?\n\n*) In the experiment in sec. 3.1 the manifold comes from a particular way to extract features from data. In a deep learning context, we would usually learn such features instead of manually designing them. This seem like a general issue, that most often manifold come into play through a manual feature-design, which seem to be at odds with the end-to-end learning mindset. It might be good to have examples where this is not the case.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA", "abstract": "Developing deep neural networks (DNNs) for manifold-valued data sets\nhas gained much interest of late in the deep learning research\ncommunity.  Examples of manifold-valued data include data from\nomnidirectional cameras on automobiles, drones etc., diffusion\nmagnetic resonance imaging, elastography and others. In this paper, we\npresent a novel theoretical framework for DNNs to cope with\nmanifold-valued data inputs.  In doing this generalization, we draw\nparallels to the widely popular convolutional neural networks (CNNs).\nWe call our network the ManifoldNet.\n\nAs in vector spaces where convolutions are equivalent to computing the\nweighted mean of functions, an analogous definition for\nmanifold-valued data can be constructed involving the computation of\nthe weighted Fr\\'{e}chet Mean (wFM). To this end, we present a\nprovably convergent recursive computation of the wFM of the given\ndata, where the weights makeup the convolution mask, to be\nlearned. Further, we prove that the proposed wFM layer achieves a\ncontraction mapping and hence the ManifoldNet does not need the\nadditional non-linear ReLU unit used in standard CNNs. Operations such\nas pooling in traditional CNN are no longer necessary in this setting\nsince wFM is already a pooling type operation. Analogous to the\nequivariance of convolution in Euclidean space to translations, we\nprove that the wFM is equivariant to the action of the group of\nisometries admitted by the Riemannian manifold on which the data\nreside. This equivariance property facilitates weight sharing within\nthe network.  We present experiments, using the ManifoldNet framework,\nto achieve video classification and image reconstruction using an\nauto-encoder+decoder setting. Experimental results demonstrate the\nefficacy of ManifoldNet in the context of classification and\nreconstruction accuracy.", "keywords": [], "authorids": ["rudrasischa@gmail.com", "josebouza@ufl.edu", "jonathan.manton@ieee.org", "baba.vemuri@gmail.com"], "authors": ["Rudrasis Chakraborty", "Jose Bouza", "Jonathan Manton", "Baba C. Vemuri"], "pdf": "/pdf/380a4fe9969c914464b716162d8be299f93dc72b.pdf", "paperhash": "chakraborty|manifoldnet_a_deep_neural_network_for_manifoldvalued_data", "_bibtex": "@misc{\nchakraborty2019manifoldnet,\ntitle={{MANIFOLDNET}: A {DEEP} {NEURAL} {NETWORK} {FOR} {MANIFOLD}-{VALUED} {DATA}},\nauthor={Rudrasis Chakraborty and Jose Bouza and Jonathan Manton and Baba C. Vemuri},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzjBiR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper118/Official_Review", "cdate": 1542234533768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzjBiR9t7", "replyto": "SyzjBiR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper118/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335653342, "tmdate": 1552335653342, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper118/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 17}