{"notes": [{"id": "ByME42AqK7", "original": "S1xC3liqFQ", "number": 1442, "cdate": 1538087980100, "ddate": null, "tcdate": 1538087980100, "tmdate": 1551085801841, "tddate": null, "forum": "ByME42AqK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1eQLvrbgN", "original": null, "number": 1, "cdate": 1544800074583, "ddate": null, "tcdate": 1544800074583, "tmdate": 1545354526180, "tddate": null, "forum": "ByME42AqK7", "replyto": "ByME42AqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Meta_Review", "content": {"metareview": "The paper proposes an evolutionary architecture search method which uses weight inheritance through network morphism to avoid training candidate models from scratch.  The method can optimise multiple objectives (e.g. accuracy and inference time), which is relevant for practical applications, and the results are promising and competitive with the state of the art. All reviewers are generally positive about the paper. Reviewers\u2019 feedback on improving presentation and adding experiments with a larger number of objectives has been addressed in the new revision. \n\nI strongly encourage the authors to add experiments on the full ImageNet dataset (not just 64x64) and/or language modelling -- the two benchmarks widely used in neural architecture search field.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "interesting method, promising results"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1442/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352836632, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByME42AqK7", "replyto": "ByME42AqK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352836632}}}, {"id": "BygMkWst37", "original": null, "number": 1, "cdate": 1541152986008, "ddate": null, "tcdate": 1541152986008, "tmdate": 1543198501493, "tddate": null, "forum": "ByME42AqK7", "replyto": "ByME42AqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "content": {"title": "The proposed method is interesting, but the proposed method does not seem to provide a large contribution", "review": "\n- Summary\nThis paper proposes a multi-objective evolutionary algorithm for the neural architecture search. Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search. The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods. In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.\n\n- Pros\n  - The proposed method does not require to be initialized with well-performing architectures.\n  - This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.\n\n- Cons\n  - Judging from Table 1, the proposed method does not seem to provide a large contribution. For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.\n  - It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.\n  - In the case of the search space II, how many GPU days does the proposed method require? \n  - About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "cdate": 1542234228669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByME42AqK7", "replyto": "ByME42AqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335947883, "tmdate": 1552335947883, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgjDn7V07", "original": null, "number": 4, "cdate": 1542892643433, "ddate": null, "tcdate": 1542892643433, "tmdate": 1542892643433, "tddate": null, "forum": "ByME42AqK7", "replyto": "ByME42AqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "content": {"title": "Revised version of our paper online!", "comment": "Dear reviewers,\n\nthanks again for your valuable feedback. We just updated our paper. We mainly made two modifications, based on your feedback:\n1) We reorganized the paper according to your suggestions; some parts of the main paper were moved to the appendix, some parts of the appendix were moved to the main paper.\n2)As you were asking whether LEMONADE is applicable to more than 2 objectives, we ran an experiment with 5 objectives, namely 1) performance on Cifar 10 (expensive objective), 2) performance on Cifar 100 (expensive) , 3) number of parameters (cheap), 4) number of multiply-add operations (cheap), 5) inference time (cheap). We refer to Appendix 3, \u201cLEMONADE with 5 objectives\u201d, for details and results, but in a nutshell the results are very positive and qualitatively resemble those for two objectives. While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if you agree we would also be very happy to include this experiment in the main paper.\n\nWe hope the updated version and our answers to your reviews have cleared out all major concerns and we kindly ask you to update your rating if we clarified your concerns.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615769, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByME42AqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1442/Authors|ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615769}}}, {"id": "ryxN-nQVRX", "original": null, "number": 3, "cdate": 1542892539604, "ddate": null, "tcdate": 1542892539604, "tmdate": 1542892539604, "tddate": null, "forum": "ByME42AqK7", "replyto": "BygMkWst37", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "content": {"title": "Answering your questions", "comment": "Dear AnonReviewer2,\nthank you for your constructive feedback. Below we address your concerns and questions.\n\n\u201cJudging from Table 1, the proposed method does not seem to provide a large contribution. For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.\u201c\n\u2192 The authors of NASNet only provide results for two regimes of parameters (3.3M and  27M) as they do not perform multi-objective optimization but rather just vary two parameters for building NASNet models (number of cells stacked, number of filters). Their method might be optimized to yield good results in these regimes and, admittedly, LEMONADE does not outperform NASNet for models with ~4M parameters. However, from Figure 3 and Table 2 one can see that only varying these two parameters for NASNet models is not necessarily sufficient to generate good models across all parameter regimes. E.g., LEMONADE clearly outperforms NASNet for very small models (50k params, 200k params - Table 2). We also refer to Appendix 3 (\u201cLEMONADE with 5 objectives\u201d), Figure 6, in the updated version of our paper, where one can see that while NASNet has quite strong performance in terms of error, number of parameters and number of multiply-add operations, it performs poorly in terms of inference time. Hence, there is a benefit in doing multi-objective optimization if one is actually interested in multiple objectives and diverse models rather than a single model. This is the main contribution of our paper and different to, e.g., the NASNet paper. The same likely also applies for ENAS (as they use the same search space and conduct very similar experiments).  We also would like to highlight two things: 1) NASNet requires 40x computational resources than LEMONADE, so even if NASNet performs better for ~4M parameter models, LEMONADE achieves competitive performance in significantly less time. 2) Table 1 shows results for models trained with different training pipelines and hyperparameters, and hence it is hard to say architecture X performs better than architecture Y since differences could simply be due to e.g. different learning rates, batch sizes, etc.  In contrast, all other results in the paper (e.g., Figure 3 and Table 2) provide comparisons with exactly the same training pipeline and hyperparameters. .\n\n\u201cIt would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix. \u201c\n-> Thanks, we agree; we re-organized our paper accordingly.\n\n\n\u201c- In the case of the search space II, how many GPU days does the proposed method require? \n-> We also ran this experiments for 7*8 GPU days, however the method converged after roughly 3*8 GPU days (meaning that there were no significant differences afterwards).\n\n\u201cAbout line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.\u201d\n-> The population is updated to be all non-dominated points from the current population and the generated children, i.e. the Pareto frontier based on all current models. We clarified this in Algorithm 1. Thanks for pointing us towards this.\n\n\nWe hope this clarifies your questions. Thanks again for the review!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615769, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByME42AqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1442/Authors|ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615769}}}, {"id": "B1x65oXNRX", "original": null, "number": 2, "cdate": 1542892436800, "ddate": null, "tcdate": 1542892436800, "tmdate": 1542892491969, "tddate": null, "forum": "ByME42AqK7", "replyto": "rJeAPV55hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "content": {"title": "Answering the questions", "comment": "Dear AnonReviewer1,\nthank you for your positive and constructive feedback. Below we address your concerns and questions.\n\n\u201cWhat value of $\\epsilon$ in Eqn (1) is used? [...] how can they guarantee the \\epsilon-ANM condition?\u201d\n\u2192 Indeed, one can not guarantee the \\epsilon-ANM condition for an arbitrary epsilon. However, in our application one does not need to explicitly select $\\epsilon$ at all. We simply apply an approximate network morphism operator. Case 1, epsilon is small: the output is a network that is \u201csmaller\u201d than its parent and has a similar error, so the children will likely be non-dominated and it will be part of the pareto front in the next generation. Case 2, epsilon is large (hence likely also the error): the children will likely be dominated by some other network and it will be discarded when the Pareto front is updated. Thus, in both cases, the specific epsilon doesn\u2019t matter. The step of LEMONADE, where the Pareto front is updated, will automatically decide whether the morphing was successful or not based on the (non-)domination criterion. We updated (shortened) the section on approximate network morphism to not put a too strong emphasis on this. Hopefully it is now less confusing.\n\n\n\u201c[...] the method as currently presented does not show possible generalization beyond these two objectives, which is a weakness of the paper.\u201d\n-> We respectfully disagree. In principle, the proposed method is - as is - applicable to arbitrary objectives and arbitrary many objectives. It is neither restricted to these specific objectives nor to n=2 objectives. To demonstrate this, we carried out a new experiment with exactly the same method on 5 objectives (2 expensive ones, 3 cheap ones). We refer to the additional experiment, Appendix 3 (\u201cLEMONADE with 5 objectives\u201d), in the updated version of our paper.\n\n\u201cHow would LEMONADE handle situations when there are more than one $f_{cheap}$, especially when different $f_{cheap}$ may have different value ranges? Eqn (8) and Eqn (9) does not seem to handle these cases.\u201d\n-> Both equations are not restricted to 1D inputs. (Kernel) density estimators can, in general,  be applied to arbitrary dimensions and most packages allow multi-dimensional inputs by default (e.g. KDE in scipy or scikit-learn). Of course, density estimation becomes problematic with increasing number of dimensions, but we believe 4-6 objectives is a realistic dimensionality for NAS applications, and scaling to significantly more objectives will typically not be necessary.  \n\nNote that the output of a KDE is always 1D, independent of the input. Also, most packages provide methods for, e.g., automatic bandwidth computation (per input dimension) to handle different value ranges. To make the input and output spaces in equations 8,9 (equations 1,2 in the updated version) clearer, we provide them in detail here:\nf_cheap: <some neural network space>  \u2192 R^n, where n is the number of cheap objectives\np_kde: R^n \u2192 R\np_p: <some neural network space> \u2192 R\n\n\u201cSame question with $f_{exp}$.\u201d\n\u2192 The expensive objectives are only involved in the last two steps of LEMONADE (evaluate $f_{exp}$ on the subset of children, update the Pareto frontier). These steps can be applied to more than one expensive objective. E.g. instead of training the children only on CIFAR-10, we can also train them on some other data set as well (and in our new experiment with 5 objectives we indeed also train them on CIFAR-100 as a second expensive objective). Of course, the runtime of the method will increase linearly in the number of expensive objectives. \n\nSo, to summarize regarding having only 2 objectives:\n1) Our method can in principle handle more than 2 objectives (both cheap and expensive), there is no general restriction to n=2 objectives.\n2) From an implementation point of view, common packages for computing density estimators automatically deal with multi-dimensional inputs and different ranges, hence LEMONADE can be run  - as is -  with multi-dimensional objectives without any further user interaction or modifications.\n3) To confirm these statements, we ran an additional experiment with 5 objectives - 2 expensive ones (performances on Cifar-10, performance on Cifar-100) and 3 cheap ones (number of parameters, number of multiply-add operations, inference time). We refer to Appendix 3, \u201cLEMONADE with 5 objectives\u201d, in the updated version of our paper for details and results. \n\n\nWe hope this clarifies your questions. Thanks again for the review!\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615769, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByME42AqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1442/Authors|ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615769}}}, {"id": "BkgSVo7VRm", "original": null, "number": 1, "cdate": 1542892332841, "ddate": null, "tcdate": 1542892332841, "tmdate": 1542892332841, "tddate": null, "forum": "ByME42AqK7", "replyto": "SJgMKr5h3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "content": {"title": "Answering the questions", "comment": "Dear AnonReviewer3,\nthank you for your positive review and constructive feedback!\n\nWe agree that the structure of the paper was not optimal and reorganized it along the lines you suggested (thanks for the suggestion!). Below we address specific questions.\n\n\u201cI am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?\u201d\n-> The latter: we compared with the models with the closest match in # of parameters.\n\n\u201cWhy is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?\u201d \n-> We stated that defining a trade-off between objectives is not necessary (in case you are referring to this statement), which would, e.g., be necessary when one would scalarize objectives by using a weighted sum. Rescaling an objective, however, is different as it is independent from other objectives: it only depends on that specific objective and which scale is important to the user and the application. For the number of parameters, the log scale is natural to cover a large range of sizes: think of a plot of size vs. performance; in order to see anything for small sizes one would typically put the size on a log scale (and we indeed did, see, e.g., Figures 3 and 4). Therefore, it is most natural to also put the number of parameters on a log scale for LEMONADE.\n\n\u201cIt seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectives-1 dimensional surface with the population of parents. How could scaling be handled?\u201d\n-> We think having 4-6 objectives is a realistic dimensionality for NAS applications, and scaling to significantly more objectives (which would indeed be problematic for our method, but also for multi-objective optimization in general) is typically not necessary. In response to this question, to demonstrate this, wee conducted a new experiment with 5 objectives (performance on Cifar 10, performance on Cifar 100, number of parameters, number of multiply-add operations, inference time) to show that LEMONADE can handle these realistic scenarios natively. We refer to the updated version of our paper for the results (Appendix 3,\u201cLEMONADE with 5 objectives\u201d), but in a nutshell the results are very positive and qualitatively resemble those for two objectives.\nWhile we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.\n\nWe hope this clarifies your questions. Thanks again for the review!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615769, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByME42AqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1442/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1442/Authors|ICLR.cc/2019/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers", "ICLR.cc/2019/Conference/Paper1442/Authors", "ICLR.cc/2019/Conference/Paper1442/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615769}}}, {"id": "SJgMKr5h3X", "original": null, "number": 3, "cdate": 1541346681986, "ddate": null, "tcdate": 1541346681986, "tmdate": 1541533128706, "tddate": null, "forum": "ByME42AqK7", "replyto": "ByME42AqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "content": {"title": "An interesting method with a troubled presentation", "review": "This paper proposes LEMONADE, a random search procedure for neural network architectures (specifically neural networks, not general hyperparameter optimization) that handles multiple objectives.  Notably, this method is significantly more efficient more efficient than previous works on neural architecture search.\n\nThe emphasis in this paper is very strange.  It devotes a lot of space to things that are not important, while glossing over the details of its own core contribution.  For example, Section 3 spends nearly a full page building up to a definition of an epsilon-approximate network morphism, but this definition is never used.  I don't feel like my understanding of the paper would have suffered if all Section 3 had been replaced by its final paragraph.  Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.   Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.\n\nThat said, those complaints are just about presentation and not about the method, which seems quite good once you take the time to dig it out of the appendix.\n\nI am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?\n\nWhy is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?\n\nIt seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.  How could scaling be handled?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "cdate": 1542234228669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByME42AqK7", "replyto": "ByME42AqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335947883, "tmdate": 1552335947883, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeAPV55hQ", "original": null, "number": 2, "cdate": 1541215333808, "ddate": null, "tcdate": 1541215333808, "tmdate": 1541533128494, "tddate": null, "forum": "ByME42AqK7", "replyto": "ByME42AqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "content": {"title": "Official Review", "review": "Summary:\nThe paper proposes LEMONADE, an evolutionary-based algorithm the searches for neural network architectures under multiple constraints. I will say it first that experiments in the paper only actually address to constraints, namely: log(#params) and (accuracy on CIFAR-10), and the method as currently presented does not show possible generalization beyond these two objectives, which is a weakness of the paper.\n\nAnyhow, for the sake of summary, let\u2019s say the method can actually address multiple, i.e. more than 2, objectives. The method works as follows.\n\n1. Start with an architecture.\n\n2. Apply network morphisms, i.e. operators that change a network\u2019s architecture but also select some weights that do not strongly alter the function that the network represents. Which operations to apply are sampled according to log(#params). Details are in the paper.\n\n3. From those sampled networks, the good ones are kept, and the evolutionary process is repeated.\n\nThe authors propose to use operations such as \u201cNet2WiderNet\u201d and \u201cNet2DeeperNet\u201d from Chen et al (2015), which enlarge the network but also choose a set of appropriate weights that do not alter the function represented by the network. The authors also propose operations that reduce the network\u2019s size, whilst only slightly change the function that the network represented.\n\nExperiments in the paper show that LEMONADE finds architecture that are Pareto-optimal compared to existing model. While this seems like a big claim, in the context of this paper, this claim means that the networks found by LEMONADE are not both slower and more wrong than existing networks, hand-crafted or automatically designed.\n\nStrengths:\n1. The method solves a real and important problem: efficiently search for neural networks that satisfy multiple properties.\n\n2. Pareto optimality is a good indicator of whether a proposed algorithm works on this domain, and the experiments in the paper demonstrate that this is the case.\n\nWeaknesses:\n1. How would LEMONADE handle situations when there are more than one $f_{cheap}$, especially when different $f_{cheap}$ may have different value ranges? Eqn (8) and Eqn (9) does not seem to handle these cases.\n\n2. Same question with $f_{exp}$. In the paper the only $f_{exp}$ refers to the networks\u2019 accuracy on CIFAR-10. What happens if there are multiple objectives, such as (accuracy on CIFAR-10, accuracy on ImageNet) or (accuracy on CIFAR-10, accuracy on Flowers, image segmentation on VOC), etc.\n\nI thus think the \u201cMulti-Objective\u201d is a bit overclaimed, and I strongly recommend that the authors adjust their claim to be more specific to what their method is doing.\n\n3. What value of $\\epsilon$ in Eqn (1) is used? Frankly, I think that if the authors train their newly generated children networks using some gradient descent methods (SGD, Momentum, Adam, etc.), then how can they guarantee the \\epsilon-ANM condition? Can you clarify and/or change the presentation regarding to this part?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1442/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution", "abstract": "Architecture search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2)most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight GPUs, which is about 20-40x less compute power than previous architecture search methods that yield state-of-the-art performance.", "keywords": ["Neural Architecture Search", "AutoML", "AutoDL", "Deep Learning", "Evolutionary Algorithms", "Multi-Objective Optimization"], "authorids": ["thomas.elsken@de.bosch.com", "janhendrik.metzen@de.bosch.com", "fh@cs.uni-freiburg.de"], "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "TL;DR": "We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.", "pdf": "/pdf/5e81ec86474db6f04ea7dc4b6b0683a16c1f099a.pdf", "paperhash": "elsken|efficient_multiobjective_neural_architecture_search_via_lamarckian_evolution", "_bibtex": "@inproceedings{\nelsken2018efficient,\ntitle={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},\nauthor={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByME42AqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1442/Official_Review", "cdate": 1542234228669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByME42AqK7", "replyto": "ByME42AqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1442/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335947883, "tmdate": 1552335947883, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1442/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}