{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396302380, "tcdate": 1486396302380, "number": 1, "id": "SJIqoGI_x", "invitation": "ICLR.cc/2017/conference/-/paper2/acceptance", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Based on the feedback, I'm going to be rejecting the paper on the following grounds:\n 1. Results are not SOTA as reported.\n 2. No real experiments other than cursory experiments on Hutter prize data.\n 2. Writing is very poor.\n \n However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc.."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396302972, "id": "ICLR.cc/2017/conference/-/paper2/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396302972}}}, {"tddate": null, "tmdate": 1484163287881, "tcdate": 1484163287881, "number": 1, "id": "ryeJF-N8g", "invitation": "ICLR.cc/2017/conference/-/paper2/official/comment", "forum": "S1J0E-71l", "replyto": "Bka0xjVNx", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "content": {"title": "Not underestimating the online prediction task here but..", "comment": "The writing and the experimental setting of the paper shows that the main and original focus of this paper is not aimed on proposing a better solution for online prediction problem. Even though it did, the experiment still does not provide sufficient and useful information to the readers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767521, "id": "ICLR.cc/2017/conference/-/paper2/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767521}}}, {"tddate": null, "tmdate": 1483395466564, "tcdate": 1482105045223, "number": 6, "id": "Bka0xjVNx", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "r1pQ4-zNe", "signatures": ["~David_Krueger1"], "readers": ["everyone"], "writers": ["~David_Krueger1"], "content": {"title": "Online prediction will be useful for model-based RL", "comment": "Next-step prediction will likely be used more in the future if model-based RL will be more popular, as many (e.g. Lecun and Bengio) suspect.\n\nEDIT: Although, thinking about it more, since a model in RL is mostly useful for planning, I guess it might not be so useful there.  I think a better application might be compression via entropy encoding."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1482220280849, "tcdate": 1482220280849, "number": 3, "id": "SyWZXPLEg", "invitation": "ICLR.cc/2017/conference/-/paper2/official/review", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer2"], "content": {"title": "Badly Written", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\nThis paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.\n\nContributions:\nThe introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.\n\nQuestions:\nA point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.\n\nCriticisms:\nThe paper is really badly written, authors should rethink the organization of the paper.\nMost of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. \nThe justification is not convincing enough.\nExperimental results are lacking, only results on a single dataset are provided.\nAlthough the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong.\nThe model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).\n\nHigh-level Review:\n    Pros: \n        - A simple modification of the model that seems to improve the results and it is an interesting modification.\n\n    Cons:\n       - The authors need to use test-set labels.\n       - Writing of the paper is bad.\n       - The authors assume that they have access to the ground-truth labels during the test-set.\n       - Experimental results are lacking", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512729387, "id": "ICLR.cc/2017/conference/-/paper2/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer3", "ICLR.cc/2017/conference/paper2/AnonReviewer2"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512729387}}}, {"tddate": null, "tmdate": 1482210613580, "tcdate": 1481933860896, "number": 2, "id": "r1pQ4-zNe", "invitation": "ICLR.cc/2017/conference/-/paper2/official/review", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "content": {"title": "Misleading", "rating": "3: Clear rejection", "review": "This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. \n\n-This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.\n\n-The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.\n\n-The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512729387, "id": "ICLR.cc/2017/conference/-/paper2/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer3", "ICLR.cc/2017/conference/paper2/AnonReviewer2"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512729387}}}, {"tddate": null, "tmdate": 1481926799174, "tcdate": 1481926799174, "number": 1, "id": "SJvqO1GEl", "invitation": "ICLR.cc/2017/conference/-/paper2/official/review", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer1"], "content": {"title": "Need some revisions", "rating": "3: Clear rejection", "review": "\nThis paper proposes to leverage \"surprisal\" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.\n\nThe general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.\n\n- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.\n\n- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.\nAlso, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).\n\n- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. \n Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage \"extra\" supervised information through the prediction errors.\n\n\nIn summary:\nPros: \n- Interesting idea\n- Seems to improve performances\n\nCons:\n- Paper writing\n- Weak evaluation (only one dataset)\n- Compare only with approaches that does not use the last-timestep error signal", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512729387, "id": "ICLR.cc/2017/conference/-/paper2/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer3", "ICLR.cc/2017/conference/paper2/AnonReviewer2"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512729387}}}, {"tddate": null, "tmdate": 1481832553275, "tcdate": 1481832553271, "number": 5, "id": "H1-uddlNx", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "SkfVKMe7e", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "writers": ["~Kamil_M_Rocki1"], "content": {"title": "linux dataset", "comment": "In the 'adaptive zoneout' paper, we also tried the algorithm on 'linux source' data - http://olab.is.s.u-tokyo.ac.jp/~kamil.rocki/data/\nWe thought it would be more challenging than enwik8 because it requires more generalization and less memorization, the algorithm works remarkably well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1481831711761, "tcdate": 1481831711755, "number": 4, "id": "r1dQS_gEe", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "rJ9apDymx", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "writers": ["~Kamil_M_Rocki1"], "content": {"title": "Thank you", "comment": "Regarding points 1 and 2, please see my comments above. Please let me know if they answer your questions. Regarding 3) the notation is simply accumulation - that is instead of writing 'x' += y, we wrote x = x + y"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1481831396242, "tcdate": 1481831396236, "number": 3, "id": "Hyn1VulVx", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "SkfVKMe7e", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "writers": ["~Kamil_M_Rocki1"], "content": {"title": "Hello", "comment": "Hello,\n\n1. We tried SF with zoneout (https://arxiv.org/abs/1606.01305) which improved performance and recently we also implemented adaptive zoneout which relies on the feedback information (https://arxiv.org/pdf/1610.07675.pdf) which set new SOTA on enwik8 dataset. It shows that SF is very effective as a driving signal for regularizing network activations.\n\nRegarding batch or layer norm, we haven't tried it yet, but it might be possible to apply it and further improve the result. We tried to keep the changes to the algorithm as minimal as possible, so SF requires just a few lines of code changed. Batch and layer normalization also make inference problematic as far as I understand the algorithm.\n\n2. So far we only experimented with character-level text prediction currently we are implementing neural-turing like algorithm with surprisal-driven controller, but we don't have any publicly available results yet.\n\n3. In the adaptive-zoneout paper which I mentioned - https://arxiv.org/pdf/1610.07675.pdf, we visualized how surprisal can affect gate activations. We believe that the reason this works so well on language modeling is the fact that some portions of text are just unpredictable by nature (like dates or numbers, we know that it's a number, but it would be pointless to try to predict document id), so we learn this fact and simply don't 'pay attention' to those fragments. Without suprisal-feedback, the network would try to memorize everything and possibly could overwrite some more general rules, which would lead to overfitting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1481830639919, "tcdate": 1481830639883, "number": 2, "id": "ryuxbOg4e", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "r1GuklzXx", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "writers": ["~Kamil_M_Rocki1"], "content": {"title": "Surprisal-feedback is not comparable to 'dynamic evaluation'. ", "comment": "Surprisal-feedback is not comparable to 'dynamic evaluation'. \n\nThe concept might be familiar, however the way they operate is fundamentally different. \n\nSurprisal-feedback 'plugs in' previous prediction into the inputs. No changes to the original learning algorithm are needed. It is exactly the same as vanilla 'RNN'/'LSTM'. We don't even give it exact information about the mistake, just the sum of surprises (a single scalar). \n\nDynamic evaluation requires changes to the algorithm. The backpropagate errors suring the test phase which is conflict with the idea of splitting data into train/test set. We give allow the network to change during the test phase, which might lead to (a) overfitting, (b) lack of information how it actually works for other datasets (c) which network should we use at the end - the weights will change.\n\nThe main difference is that in surprisal-feedback LSTM, the network learns how to utilize the surprisal magnitude, whereas dynamic evaluation directly modifies the weights by backpropagation. Therefore, SF learns the 'algorithm' to use surprisal and can generalizes to the test set or any other unseen data. We never actually 'tell' the network how to use surprisal.\n\nPlease take a look at the source code provided and see that the algorithm is directly comparable to methods which do not use 'dynamic evaluation'. We do not believe that it is fair to put SF and 'dynamic evaluation' methods in the same category, because we keep strict separation of train/validation and test sets.\n\nNote that in order to implement SF for the test phase, no changes are needed for the algorithm. The only change is within the SF-layer. In order to show this, we have supplied the source code. This proves that we don't get any more information than the original plain RNN. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1481658925541, "tcdate": 1481658925535, "number": 1, "id": "BySEMCTXg", "invitation": "ICLR.cc/2017/conference/-/paper2/public/comment", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "writers": ["~Kamil_M_Rocki1"], "content": {"title": "Source Code (C++/CUDA) for reproducing the results", "comment": "Source Code (C++/CUDA) for reproducing the results\n\nhttps://github.com/krocki/Surprisal-Feedback-LSTM"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287767654, "id": "ICLR.cc/2017/conference/-/paper2/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1J0E-71l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper2/reviewers", "ICLR.cc/2017/conference/paper2/areachairs"], "cdate": 1485287767654}}}, {"tddate": null, "tmdate": 1480880253847, "tcdate": 1480879978286, "number": 3, "id": "r1GuklzXx", "invitation": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer3"], "content": {"title": "Model size?", "question": "Most of the papers that report test BPC scores on enwik8 also report the size of the models.\nThis is because, it is important to know how many parameters are required to obtain the performance that they are reporting.\n\"The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 10 days\" is vague information without knowing the size of the model. Could you please report the size of the model that was used in this paper?\n\nAccording to the statement: \"This method does not belong to the \u2019dynamic evaluation\u2019 group: 1. It never actually sees test data during\ntraining. 2. It does not adapt weights during testing\"\nIn fact, it consumes the test error signal to achieve the test BPC score that is being reported. Most of the models in the same table do not use any test error signal at inference time. I think dynamic evaluation is more proper baseline in this case."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959513190, "id": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer2", "ICLR.cc/2017/conference/paper2/AnonReviewer3"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959513190}}}, {"tddate": null, "tmdate": 1480759593582, "tcdate": 1480759593577, "number": 2, "id": "SkfVKMe7e", "invitation": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer2"], "content": {"title": "Few Questions", "question": "- It seems like for the recent language modelling experiments such as enwiki8, recurrent/variational dropout and batch/layer norm seems to be very effective. Have you tried those techniques with your model as well? \n- Do you have experiments on different datasets and tasks other than language modelling?\n- Do you have more theoretical justification or analysis for this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959513190, "id": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer2", "ICLR.cc/2017/conference/paper2/AnonReviewer3"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959513190}}}, {"tddate": null, "tmdate": 1480715713890, "tcdate": 1480715713886, "number": 1, "id": "rJ9apDymx", "invitation": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "forum": "S1J0E-71l", "replyto": "S1J0E-71l", "signatures": ["ICLR.cc/2017/conference/paper2/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper2/AnonReviewer1"], "content": {"title": "Pre reviews question", "question": "1) Have you tried suprisal LSTM on some other datasets?\n2) Suprisal LSTM uses the same error signal than dynamic evaluation (http://arxiv.org/abs/1308.0850). Did you compare with them?\n3) In some of the backprop equations, why is there the term on the left-hand-side and right-hand-side of the equation, for instance \\partial E_t/\\partial y_t in equation 15?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959513190, "id": "ICLR.cc/2017/conference/-/paper2/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper2/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper2/AnonReviewer1", "ICLR.cc/2017/conference/paper2/AnonReviewer2", "ICLR.cc/2017/conference/paper2/AnonReviewer3"], "reply": {"forum": "S1J0E-71l", "replyto": "S1J0E-71l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959513190}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1476851626267, "tcdate": 1476756679535, "number": 2, "id": "S1J0E-71l", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1J0E-71l", "signatures": ["~Kamil_M_Rocki1"], "readers": ["everyone"], "content": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "pdf": "/pdf/2c4a40726fd9c87df0bea025fc4b56729323c30b.pdf", "TL;DR": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "paperhash": "rocki|surprisaldriven_feedback_in_recurrent_networks", "conflicts": ["ibm.com"], "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 15}