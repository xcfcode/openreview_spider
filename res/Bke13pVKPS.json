{"notes": [{"id": "Bke13pVKPS", "original": "SJxCfj1dDr", "number": 766, "cdate": 1569439143004, "ddate": null, "tcdate": 1569439143004, "tmdate": 1577168254066, "tddate": null, "forum": "Bke13pVKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "authorids": ["slgonzalez@utexas.edu", "risto@cs.utexas.edu"], "keywords": ["metalearning", "evolutionary computation", "loss functions", "optimization", "genetic programming"], "TL;DR": "Using evolutionary computation, a system for loss function metalearning was built (GLO) that discovered a new loss function for classification that can train more accurate models in less time.", "abstract": "As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "pdf": "/pdf/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "paperhash": "gonzalez|improved_training_speed_accuracy_and_data_utilization_via_loss_function_optimization", "original_pdf": "/attachment/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "_bibtex": "@misc{\ngonzalez2020improved,\ntitle={Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke13pVKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KuLk5umrx", "original": null, "number": 1, "cdate": 1576798705456, "ddate": null, "tcdate": 1576798705456, "tmdate": 1576800930670, "tddate": null, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "invitation": "ICLR.cc/2020/Conference/Paper766/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a GA-based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance). The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive. It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance. My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "authorids": ["slgonzalez@utexas.edu", "risto@cs.utexas.edu"], "keywords": ["metalearning", "evolutionary computation", "loss functions", "optimization", "genetic programming"], "TL;DR": "Using evolutionary computation, a system for loss function metalearning was built (GLO) that discovered a new loss function for classification that can train more accurate models in less time.", "abstract": "As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "pdf": "/pdf/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "paperhash": "gonzalez|improved_training_speed_accuracy_and_data_utilization_via_loss_function_optimization", "original_pdf": "/attachment/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "_bibtex": "@misc{\ngonzalez2020improved,\ntitle={Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke13pVKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728592, "tmdate": 1576800281026, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper766/-/Decision"}}}, {"id": "rkgPrHM4tH", "original": null, "number": 1, "cdate": 1571198270876, "ddate": null, "tcdate": 1571198270876, "tmdate": 1572972554961, "tddate": null, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "invitation": "ICLR.cc/2020/Conference/Paper766/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors present a framework to perform meta-learning on the loss used for\ntraining. They introduce the Baikal loss, obtained using the MNIST dataset, and\nBaikalCMA where the coefficients have been tuned. The evaluation of these loss\nfunctions is performed on the MNIST and CIFAR-10, and according to the results\nthey converge faster, towards lower test error and need fewer samples to obtain\nresults similar to the cross-entropy loss.\n\nThe claims are clearly stated and the framework is detailed, the experiments\ncover all the potential benefits of the Baikal loss. However it seems that some\npotentially critical points have been omitted. The cross-entropy loss is well\nknown to be beneficial in dataset with severe class imbalance. The two datasets\nused for evaluation are perfectly balanced, it might beneficial to see how it\nperforms in the unbalanced case.\n\nI have a couple of concerns about the method. First about step \"(1) loss\nfunction discovery\": The initial population starts with trees of depth at most\n2, and the final solution(Baikal) has either 2 or 3 (depending on which\ndefinition of depth is chosen). It is unclear that the genetic optimization is\nsuperior to simply choosing random loss functions. I think it would be relevant\nto add a figure that shows how the fitness of the leader of each generation\nevolves over time.\n\nThe second step \"(2) coefficient optimization\", while objectively generating a\nloss function that was superior on the metrics evaluated, raised some\nquestions. In equation (2) the factor \"1.5352\" seems to be equivalent to adding\na constant to the loss, which should not impact optimization. Also the factor\n\"2.7279\" seems to be equivalent to a change in learning rate. This may be an\nindication that the learning rate search was not done thoroughly. It would be\nbeneficial to clarify when it is happening: a) For each individual of the\npopulation during step (1), b) before performing CMA, c) after CMA. Also: Was\nlearning rate search was performed on the network trained with Cross-Entropy? It\nwas not entirely clear from the experiment details in Appendix A.2.1.\n\nAbout the Baikal loss itself, I fear that it could produce models that have very\npoor calibration, it might be nice to evaluate that (even if it is only in\nthe appendix).\n\n\nWhile the paper does a great job at presenting the problem and its applications\nand propose a framework that generated a loss that can transfer to other\ndatasets without any tuning required. I think it lacks a more thorough\nevaluation and description of the dynamics observed during the genetic\nevolution, and the performance of the Baikal loss on other datasets (my quick\nexperients with it on ImageNet diverged I did not have the time necessary to\ntune the hyper-parameters).\n\nMinor remarks:\n\nThere might be a slight omission in section 3.1: according to Figure 1, exp(x)\nis one of the potential unary operators explored by the GLO framework. However\nit is not present it the list of operators. Could you clarify this?\n\nTo the best of my knowledge, in the machine learning literature, it seems that\nthe letter x is used to denote the prediction and y for the ground truth. The\nfact that this paper used the opposite convention confused me the first time I\nread it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "authorids": ["slgonzalez@utexas.edu", "risto@cs.utexas.edu"], "keywords": ["metalearning", "evolutionary computation", "loss functions", "optimization", "genetic programming"], "TL;DR": "Using evolutionary computation, a system for loss function metalearning was built (GLO) that discovered a new loss function for classification that can train more accurate models in less time.", "abstract": "As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "pdf": "/pdf/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "paperhash": "gonzalez|improved_training_speed_accuracy_and_data_utilization_via_loss_function_optimization", "original_pdf": "/attachment/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "_bibtex": "@misc{\ngonzalez2020improved,\ntitle={Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke13pVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650003786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper766/Reviewers"], "noninvitees": [], "tcdate": 1570237747403, "tmdate": 1575650003799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper766/-/Official_Review"}}}, {"id": "HJenQ5wnYS", "original": null, "number": 2, "cdate": 1571744292149, "ddate": null, "tcdate": 1571744292149, "tmdate": 1572972554922, "tddate": null, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "invitation": "ICLR.cc/2020/Conference/Paper766/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a very interesting idea of loss function optimization. At first sight, loss function is the goal of optimization and can not be optimized directly. However, the true goal of optimization is the final accuracy (for classification). So lots of loss functions can be designed and combined to form a large search space. In this paper, the authors adopt genetic programming to design loss functions hierarchically.  And experiments show that GLO (Genetic Loss-function Optimization) based loss function can achieve better results than cross entropy. \n\nThe paper is well written and easy to understand. I like the idea. Baikal loss is a form searched by GLO. Interestingly and counter intuitively , it is not a monotonically decreasing function. The authors explain it as a regularizer which can prevent the model to be too confident. \n\nExperiments on MNIST and Cifar10 are conducted to show the effectiveness of the proposed method. This part is very weak since MNIST and Cifar10 are very small datasets and the provided results are far from state-of-the-art results. Experiments on larger datasets such as ImageNet and more analysis about the optimization details are suggested to make this work more promising. Since the optimization is rather complex, it's better to show if it is stable enough to generalize to various datasets and models."}, "signatures": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "authorids": ["slgonzalez@utexas.edu", "risto@cs.utexas.edu"], "keywords": ["metalearning", "evolutionary computation", "loss functions", "optimization", "genetic programming"], "TL;DR": "Using evolutionary computation, a system for loss function metalearning was built (GLO) that discovered a new loss function for classification that can train more accurate models in less time.", "abstract": "As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "pdf": "/pdf/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "paperhash": "gonzalez|improved_training_speed_accuracy_and_data_utilization_via_loss_function_optimization", "original_pdf": "/attachment/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "_bibtex": "@misc{\ngonzalez2020improved,\ntitle={Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke13pVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650003786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper766/Reviewers"], "noninvitees": [], "tcdate": 1570237747403, "tmdate": 1575650003799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper766/-/Official_Review"}}}, {"id": "SJg3SzKCFB", "original": null, "number": 3, "cdate": 1571881539855, "ddate": null, "tcdate": 1571881539855, "tmdate": 1572972554866, "tddate": null, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "invitation": "ICLR.cc/2020/Conference/Paper766/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary*\nThe authors propose using evolutionary computation (EC) to perform meta learning over the set of symbolic expressions for loss functions. It's a compelling idea that is well-motivated. They find that applying their EC method to mnist yields an interesting loss function that they name the 'Baikal loss.' Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. \n\n*Overall Assessment*\nThe paper's idea is very interesting. However, there are some important drawbacks of this work. These should be fixed and the paper should be resubmitted to a different conference soon.\n1) The experiments focus almost entirely on the Baikal loss (a particular loss function found once when running EC on mnist), and do not analyze the overall behavior of EC for loss functions. Does EC consistently converge to the same loss, or do different ones emerge different times you run it? What happens if you optimize convergence speed vs. generalization accuracy with EC? How do these loss functions differ?\n2) The experiments are largely on mnist, with a small study showing that the Baikal loss can be applied to cifar-10. It would be good to show that loss functions meta-learned on mnist generalize to larger-scale problems than cifar. \n\n*Comments*\nI was surprised when you optimized in fig 3 for convergence speed, rather than final accuracy of something that runs for a while. Why should our goal be to find loss functions that lead to fast optimization, instead of loss functions that lead to models that generalize best? If these are two different goals, then you should have two sets of experiments analyzing how GLO can find interesting (and perhaps different) loss functions for each.\n\nMnist is possible to get basically 100% accuracy. This means that the loss will only be evaluated in certain regimes of its inputs. What happens when you transfer this to problems where the best achievable accuracy is something like 60% for binary classification?\n\nYou should cite the Focal loss as another alternative to the cross entropy loss. Is the focal loss achievable in your particular grammar over loss functions? You should also cite label smoothing as an additional way to achieve a very similar implicit regularization effect as the Baikal loss.\n\nYou only analyze one loss function that came from your EC. What if you run it multiple times? Do you find different formulas? How do these perform? The beginning of the paper is very focused on EC, but then you transition suddenly to only discussing the Baikal loss. Can you present experiments demonstrating, for example, how the EC performance varies with the number of steps, with different ways to define the search space, etc?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper766/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "authorids": ["slgonzalez@utexas.edu", "risto@cs.utexas.edu"], "keywords": ["metalearning", "evolutionary computation", "loss functions", "optimization", "genetic programming"], "TL;DR": "Using evolutionary computation, a system for loss function metalearning was built (GLO) that discovered a new loss function for classification that can train more accurate models in less time.", "abstract": "As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "pdf": "/pdf/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "paperhash": "gonzalez|improved_training_speed_accuracy_and_data_utilization_via_loss_function_optimization", "original_pdf": "/attachment/38d0f6f04d00eea14bc4bacd508182673bc2233d.pdf", "_bibtex": "@misc{\ngonzalez2020improved,\ntitle={Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke13pVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke13pVKPS", "replyto": "Bke13pVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper766/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650003786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper766/Reviewers"], "noninvitees": [], "tcdate": 1570237747403, "tmdate": 1575650003799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper766/-/Official_Review"}}}], "count": 5}