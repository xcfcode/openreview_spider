{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730156737, "tcdate": 1509139117572, "number": 1147, "cdate": 1518730156727, "id": "H1I3M7Z0b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "H1I3M7Z0b", "original": "SkAsMXZRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "abstract": "\tWe present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up to \\textbf{16$\\times$} faster than the well-established baselines, without noticeable performance drop.", "pdf": "/pdf/bec3a5d58827cadae79c5f9723e7e514ce07a5f4.pdf", "TL;DR": "We present a novel network architecture for learning compact and efficient deep neural networks", "paperhash": "jin|wsnet_learning_compact_and_efficient_networks_with_weight_sampling", "_bibtex": "@misc{\njin2018wsnet,\ntitle={{WSN}et: Learning Compact and Efficient Networks with Weight Sampling},\nauthor={Xiaojie Jin and Yingzhen Yang and Ning Xu and Jianchao Yang and Jiashi Feng and Shuicheng Yan},\nyear={2018},\nurl={https://openreview.net/forum?id=H1I3M7Z0b},\n}", "keywords": ["Deep learning", "model compression"], "authors": ["Xiaojie Jin", "Yingzhen Yang", "Ning Xu", "Jianchao Yang", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["xiaojie.jin@u.nus.edu", "superyyzg@gmail.com", "ning.xu@snap.com", "jiachao.yang@snap.com", "elefjia@nus.edu.sg", "yanshuicheng@360.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260090874, "tcdate": 1517249647075, "number": 386, "cdate": 1517249647056, "id": "S1vdEk6BM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper received generally positive reviews, but the reviewers also had some concerns about the evaluations.\n\nPros:\n-- An improvement over HashNet, the model ties weights more systematically, and gets better accuracy.\nCons:\n-- Tying weights to compress models already tried before.\n-- Tasks are all small and/or audio related.\n-- Unclear how well the results will generalize for 2D convolutions.\n-- HashNet results are preliminary; comparisons with HashNet missing for audio tasks.\n\nGiven the expert reviews, I am recommending the paper to the workshop track.\n", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "abstract": "\tWe present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up to \\textbf{16$\\times$} faster than the well-established baselines, without noticeable performance drop.", "pdf": "/pdf/bec3a5d58827cadae79c5f9723e7e514ce07a5f4.pdf", "TL;DR": "We present a novel network architecture for learning compact and efficient deep neural networks", "paperhash": "jin|wsnet_learning_compact_and_efficient_networks_with_weight_sampling", "_bibtex": "@misc{\njin2018wsnet,\ntitle={{WSN}et: Learning Compact and Efficient Networks with Weight Sampling},\nauthor={Xiaojie Jin and Yingzhen Yang and Ning Xu and Jianchao Yang and Jiashi Feng and Shuicheng Yan},\nyear={2018},\nurl={https://openreview.net/forum?id=H1I3M7Z0b},\n}", "keywords": ["Deep learning", "model compression"], "authors": ["Xiaojie Jin", "Yingzhen Yang", "Ning Xu", "Jianchao Yang", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["xiaojie.jin@u.nus.edu", "superyyzg@gmail.com", "ning.xu@snap.com", "jiachao.yang@snap.com", "elefjia@nus.edu.sg", "yanshuicheng@360.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515805617604, "tcdate": 1511746994380, "number": 1, "cdate": 1511746994380, "id": "Bkc2TkFlG", "invitation": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "signatures": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review of WSNet", "rating": "6: Marginally above acceptance threshold", "review": "In this work, the authors propose a technique to compress convolutional and fully-connected layers in a network by tying various weights in the convolutional filters: specifically within a single channel (weight sampling) and across channels (channel sampling). When combined with quantization, the proposed approach allows for large compression ratios with minimal loss in performance on various audio classification tasks. Although the results are interesting, I have a number of concerns about this work, which are listed below:\n\n1. The idea of tying weights in the neural network in order to compress the model is not entirely new. This has been proposed previously in the context of feed-forward networks [1], and convolutional networks [2] where the choice of parameter tying is based on hash functions which ensure a random (but deterministic) mapping from a small set of \u201ctrue\u201d weights to a larger set of \u201cvirtual\u201d weights. I think it would be more fair to compare against the HashedNet technique.\n\nReferences:\n[1] Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. 2015. Compressing neural networks with the hashing trick. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 (ICML'15), Francis Bach and David Blei (Eds.), Vol. 37. JMLR.org 2285-2294.\n[2] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. 2016. Compressing Convolutional Neural Networks in the Frequency Domain. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). ACM, New York, NY, USA, 1475-1484. DOI: https://doi.org/10.1145/2939672.2939839\n\n2. Given that the experiments are conducted on tasks where there isn\u2019t a large amount of training data, one concern is that the baseline model used by the authors might be overparameterized. It would be interesting to see how performance varies as a function of number of parameters for these tasks without any \u201ccompression\u201d, i.e., just by reducing filter sizes, for example.\n\n3. It seems somewhat surprising that repeating the filter weights across channels as is done in the channel sharing technique yields no loss in accuracy, especially for the deeper convolutional layers. Could this perhaps be a function of the tasks that the binary \u201cmusic detection\u201d task that these models are evaluated on? Do the authors have any comments on why this doesn't hurt performance?\n\n4. In citing relevant previous work, the authors should also include student-teacher approaches [1, 2] and distillation [3], and work by Denil et al. [4] on compression.\nReferences:\n[1] C. Bucilua, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM, 2006\n[2] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654\u20132662, 2014.\n[3] G. Hinton, O. Vinyals, J. Dean. Distilling the Knowledge in a Neural Network, NIPS 2014 Deep Learning Workshop. 2014.\n[4] M. Denil, B. Shakibi, L. Dinh, N. de Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156, 2013.\n\n5. Section 3, where the authors describe the proposed techniques is somewhat confusing to read, because of a lack of detailed mathematical explanations of the proposed techniques. This makes the paper harder to understand, in my view. Please re-write these sections in order to clearly express the parameter tying mechanism. In particular, I had the following questions:\n- Are weights tied across layers i.e., are the \u201cweight sharing\u201d matrices shared across layers?\n- There appears to be a typo in Equation 3: I believe it should be m = m* C.\n- Filter augmentation/Weight quantization are applicable to all methods, including the baseline. It would therefore be interesting to examine how they affect the baseline, not just the proposed system.\n- Section 3.5, on using the \u201cIntegral Image\u201d to speed up computation was not clear to me. In particular, could the authors re-write to explain how the computation is computed efficiently with \u201ctwo subtraction operations\u201d. Could the authors also clarify the savings achieved by this technique?\n\n6. Results are reported on the various test sets without any discussion of statistical significance. Could the authors describe whether the differences in performance on the various test sets are statistically significant?\n\n7. On the ESC-50, UrbanSound8K, and DCASE tasks, it is a bit odd to compare against previous baselines which use different input features, use different model configurations, etc. It would be much better to use one of the previously published configurations as the baseline, and apply the proposed techniques to that configuration to examine performance. In particular, could the authors also use log-Mel filterbank energies as input features similar to (Piczak, 2015) and (Salomon and Bello, 2015), and apply the proposed techniques starting from those input features? Also, it would be useful when comparing against previously published baselines to indicate total number of independent parameters in the system in addition to accuracy numbers.\n\n8. Minor Typographical Errors: There are a number of minor typographical/grammatical errors in the paper, some of which are listed below:\n- Abstract: \u201cCombining weight quantization ...\u201d \u2192 \u201cCombining with weight quantization ...\u201d\n- Sec 1: \u201c... without sacrificing the loss of accuracy\u201d \u2192 \u201c... without sacrificing accuracy\u201d\n- Sec 1: \u201cAbove experimental results strongly evident the capability of WSNet \u2026\u201d \u2192 \u201cAbove experimental results strongly evidence the capability of WSNet \u2026\u201d\n- Sec 2: \u201c... deep learning based approaches has been recently proven ...\u201d \u2192 \u201c... deep learning based approaches have been recently proven ...\u201d\n- The work by Aytar et al., 2016 is repeated twice in the references.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "abstract": "\tWe present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up to \\textbf{16$\\times$} faster than the well-established baselines, without noticeable performance drop.", "pdf": "/pdf/bec3a5d58827cadae79c5f9723e7e514ce07a5f4.pdf", "TL;DR": "We present a novel network architecture for learning compact and efficient deep neural networks", "paperhash": "jin|wsnet_learning_compact_and_efficient_networks_with_weight_sampling", "_bibtex": "@misc{\njin2018wsnet,\ntitle={{WSN}et: Learning Compact and Efficient Networks with Weight Sampling},\nauthor={Xiaojie Jin and Yingzhen Yang and Ning Xu and Jianchao Yang and Jiashi Feng and Shuicheng Yan},\nyear={2018},\nurl={https://openreview.net/forum?id=H1I3M7Z0b},\n}", "keywords": ["Deep learning", "model compression"], "authors": ["Xiaojie Jin", "Yingzhen Yang", "Ning Xu", "Jianchao Yang", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["xiaojie.jin@u.nus.edu", "superyyzg@gmail.com", "ning.xu@snap.com", "jiachao.yang@snap.com", "elefjia@nus.edu.sg", "yanshuicheng@360.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389838, "id": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1147/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer2"], "reply": {"forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389838}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642389914, "tcdate": 1511760439541, "number": 2, "cdate": 1511760439541, "id": "S1xBMQtgG", "invitation": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "signatures": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a method to compress deep network by weight sampling and channel sharing.  The method combined with weight quantization provides 180x compression with a very small accuracy drop. \n\nThe method is novel  and tested on multiple audio classification datasets and results show a good compression ratio with a negligible accuracy drop.  The organization of the paper is good. However, it is a bit difficult to understand the method. Figure 1 does not help much. Channel sharing part in Figure 1 is especially confusing; it looks like the whole filter has the same weights in each channel. Also it isn\u2019t stated in Figure and text that the weight sharing filters are learned by training.\n\nIt would be a nice addition to add number of operations that are needed by baseline method and compressed method with integral image.\n\nTable 5: Please add network size of other networks (SoundNet and Piczak ConvNet).  For setting, SoundNet has two settings, scratch init and unlabeled video, what is that setting for WSNet and baseline? \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "abstract": "\tWe present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up to \\textbf{16$\\times$} faster than the well-established baselines, without noticeable performance drop.", "pdf": "/pdf/bec3a5d58827cadae79c5f9723e7e514ce07a5f4.pdf", "TL;DR": "We present a novel network architecture for learning compact and efficient deep neural networks", "paperhash": "jin|wsnet_learning_compact_and_efficient_networks_with_weight_sampling", "_bibtex": "@misc{\njin2018wsnet,\ntitle={{WSN}et: Learning Compact and Efficient Networks with Weight Sampling},\nauthor={Xiaojie Jin and Yingzhen Yang and Ning Xu and Jianchao Yang and Jiashi Feng and Shuicheng Yan},\nyear={2018},\nurl={https://openreview.net/forum?id=H1I3M7Z0b},\n}", "keywords": ["Deep learning", "model compression"], "authors": ["Xiaojie Jin", "Yingzhen Yang", "Ning Xu", "Jianchao Yang", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["xiaojie.jin@u.nus.edu", "superyyzg@gmail.com", "ning.xu@snap.com", "jiachao.yang@snap.com", "elefjia@nus.edu.sg", "yanshuicheng@360.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389838, "id": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1147/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer2"], "reply": {"forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389838}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642389869, "tcdate": 1511886822374, "number": 3, "cdate": 1511886822374, "id": "rJRJeMoxz", "invitation": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "signatures": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a method for reducing the number of parameters of neural networks by sharing the set of weights in a sliding window manner, and replicating the channels, and finally by quantising weights. The paper is clearly written and results seem compelling but on a pretty restricted domain which is not well known. This could have significance if it applies more generally.\n\nWhy does it work so well? Is this just because it acts on audio and these filters are phase shifted?\nWhat happens with 2D convnets on more established datasets and with more established baselines?\nWould be interesting to get wall clock speed ups for this method?\n\nOverall I think this paper lacks the breadth of experiments, and to really understand the significance of this work more experiments in more established domains should be performed.\n\nOther points:\n- You are missing a related citation \"Speeding up Convolutional Neural Networks with Low Rank Expansions\" Jaderberg et al 2014\n- Eqn 2 should be m=m* x C\n- Use \\citep rather than \\cite", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WSNet: Learning Compact and Efficient Networks with Weight Sampling", "abstract": "\tWe present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up to \\textbf{16$\\times$} faster than the well-established baselines, without noticeable performance drop.", "pdf": "/pdf/bec3a5d58827cadae79c5f9723e7e514ce07a5f4.pdf", "TL;DR": "We present a novel network architecture for learning compact and efficient deep neural networks", "paperhash": "jin|wsnet_learning_compact_and_efficient_networks_with_weight_sampling", "_bibtex": "@misc{\njin2018wsnet,\ntitle={{WSN}et: Learning Compact and Efficient Networks with Weight Sampling},\nauthor={Xiaojie Jin and Yingzhen Yang and Ning Xu and Jianchao Yang and Jiashi Feng and Shuicheng Yan},\nyear={2018},\nurl={https://openreview.net/forum?id=H1I3M7Z0b},\n}", "keywords": ["Deep learning", "model compression"], "authors": ["Xiaojie Jin", "Yingzhen Yang", "Ning Xu", "Jianchao Yang", "Jiashi Feng", "Shuicheng Yan"], "authorids": ["xiaojie.jin@u.nus.edu", "superyyzg@gmail.com", "ning.xu@snap.com", "jiachao.yang@snap.com", "elefjia@nus.edu.sg", "yanshuicheng@360.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389838, "id": "ICLR.cc/2018/Conference/-/Paper1147/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1147/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1147/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1147/AnonReviewer2"], "reply": {"forum": "H1I3M7Z0b", "replyto": "H1I3M7Z0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389838}}}], "count": 5}