{"notes": [{"id": "BJxDNxSFDH", "original": "B1xopIeYDB", "number": 2251, "cdate": 1569439790891, "ddate": null, "tcdate": 1569439790891, "tmdate": 1577168287341, "tddate": null, "forum": "BJxDNxSFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_Exo7Gxf9", "original": null, "number": 1, "cdate": 1576798744368, "ddate": null, "tcdate": 1576798744368, "tmdate": 1576800891791, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Decision", "content": {"decision": "Reject", "comment": "All reviewers agree that this paper is not ready for publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725147, "tmdate": 1576800276925, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Decision"}}}, {"id": "r1x-IvpijH", "original": null, "number": 4, "cdate": 1573799752732, "ddate": null, "tcdate": 1573799752732, "tmdate": 1573799752732, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "r1ggNMGw5B", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment", "content": {"title": "Reply to Reviewer 3", "comment": "Thank you for your review and comments. We will do our best to address your comments/questions below.\n\nWe apologize if our method is not clearly explained enough. Yes indeed as you pointed in Eq (5). Both the weights of the Basis Function Learner, \\theta and Weights Generator \\psi are optimized jointly end-to-end. A description of the self-attention block was included in the supplementary section C of the paper.\n\nAs noted by you and other reviewers, we will include experiments with more realistic regression tasks in future versions/submissions of the paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxDNxSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2251/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2251/Authors|ICLR.cc/2020/Conference/Paper2251/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144120, "tmdate": 1576860532436, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment"}}}, {"id": "SkemNPaiiH", "original": null, "number": 3, "cdate": 1573799723412, "ddate": null, "tcdate": 1573799723412, "tmdate": 1573799723412, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "S1eefcvhtH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment", "content": {"title": "Reply to Reviewer 2", "comment": "Thank you for a thorough reading and review of our paper. We will try our best to address your comments/concerns below. \n\nWe thank you for the suggestion of an alternative experimental setting of the few-shot image completion task with disjoint classes in training and testing. We will include this experimental setup in future versions of the paper.\n\nWe will strive to include experiments of more realistic regression tasks in the future versions of the paper and we thank you for giving us a list of related works that we could look into and compare against.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxDNxSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2251/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2251/Authors|ICLR.cc/2020/Conference/Paper2251/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144120, "tmdate": 1576860532436, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment"}}}, {"id": "BJl_xPaojB", "original": null, "number": 2, "cdate": 1573799663742, "ddate": null, "tcdate": 1573799663742, "tmdate": 1573799663742, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "B1eXuCxRYS", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for a thorough reading and review of our paper. We will try our best to address your comments/concerns below. \n\nIt is true our method might be able to be extended to classical few-shot classification tasks. However the main idea of our paper is to learn an optimal combination basis that can be used to predict a regression function, we choose to limit our experiments and evaluations to just few-shot regression tasks.\n\nRegarding your comment on more diverse/realistic datasets, we will strive to include experiments with more realistic regression tasks in future versions/submissions of the paper.\n\nFor comparisons of ANP/NP/CNP against our method, we show the results below:\n\n                                          |            ANP                    |              NP                    |             CNP                 |\n--------------------------------------------------------------------------------------------------------------------------------\nAlt. Sinusoidal 10 shot   |  1.234 +- 0.075             |      3.240 +- 0.125         |     3.045 +- 0.120        |\nAlt Sinusoidal 5 shot      |  2.613 +- 0.109             |      3.829 +- 0.125         |     3.686 +- 0.125        |\n--------------------------------------------------------------------------------------------------------------------------------\n1D Heat Eqn 10 shot      | (6.02 +- 0.50)*10^-3    | (1.18 +- 0.06)*10^-2   | (1.04 +- 0.06)*10^-2 |\n1D Heat Eqn 5 shot        | (7.88 +- 0.47)*10^-3    | (1.41 +- 0.07)*10^-2   | (1.42 +- 0.08)*10^-2 |\n--------------------------------------------------------------------------------------------------------------------------------\n2D Gaussian 10 shot      |  (1.26 +- 0.26)*10^-3   |  (1.40 +- 0.29)*10^-3  | (1.30 +- 0.28)*10^-3 |\n2D Gaussian 20 shot      |  (5.67 +- 1.10)*10^-4   |  (7.05 +- 1.65)*10^-4  | (7.06 +- 1.45)*10^-4 |\n2D Gaussian 50 shot      |  (2.38 +- 1.22)*10^-4   |  (4.51 +- 0.85)*10^-4  | (4.39 +- 0.96)*10^-4 |\n--------------------------------------------------------------------------------------------------------------------------------\n\nWe note that our method outperforms the NP family of methods for the alternative sinusoidal task but is slightly worse in performance compared to ANP for the Heat Equation task and is worse than all NP methods for the Gaussian task. The gap in performance on the Gaussian task certainly indicates that there is room for improvement in our method and we will take that into account in our future submissions.\n\nThe Ensemble results of our method, as specified in Section 4.1 consist of 10 separately instances of our model (with randomly initialized weights) trained on the same set of regression tasks. The final prediction of the ensemble model is obtained by taking a mean of predictions of the 10 separate models. As for the ensemble results for the image completion task, we found that the ensemble version of our method does perform slightly better than ANP in the MNIST image completion task (2.12e-2 for 100-shot). Though we note It is not an equivalent comparison against the NP methods as they themselves are not ensemble methods.\n\t\nYou are correct to note that the outputs of the Basis Function Learner are passed through a ReLU activation function. We choose this particular design choose to emulate the structure of traditional neural networks as the output of the Basis Function Learner can be seen as the penultimate layer of a neural network whereas the linear combination of the weights vector and the learned basis functions can be seen as the final layer of the network.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxDNxSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2251/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2251/Authors|ICLR.cc/2020/Conference/Paper2251/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144120, "tmdate": 1576860532436, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment"}}}, {"id": "S1eefcvhtH", "original": null, "number": 1, "cdate": 1571744264378, "ddate": null, "tcdate": 1571744264378, "tmdate": 1572972363296, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a regression approach that, given a few training (support) samples of a regression task (input and desired output pairs), should be able to output the values of the target function on additional (query) inputs. The proposed method is to learn a set of basis functions (MLPs) and a weight generator that for a given support set predicts weights using which the basis functions are linearly combined to form the predicted regression function, which is later tested (using the MSE metric) w.r.t. the ground truth. The method is trained on a large collection of randomly sampled task from the target family and is tested on a separate set of random tasks. The experiments include: \n* sinusoidal wave prediction from a few samples\n* MNIST and CelebA inpainting from a set of known pixel values (in 28x28 and 32x32 resolution respectively)\n* additional experiments on heat equation and 2D Gaussian distribution task in Appendix\nThe experiments show that the proposed approach outperforms the other methods on the sinusoidal wave toy problem, and yet performs less good then than Kim et al. 2019 on MNIST and CelebA.\n\nI propose to reject the paper in its current form, and consider the following negative points for further improvement:\n\n1. The posed problem is not really few-shot learning, in (now classical) few-shot learning, such as few-shot classification on benchmarks such as miniImageNet, CUB, tieredImageNet, CIFAR-FS, FC100, etc. the meta-training is done on a disjoint set of categories and testing is done on a completely new set of categories unseen during training. The gap between disjoint visual categories is very large, and it does not come close to being tested on a different from training samples sinusoidal wave or different set of hidden pixels in inpainting on the same (seen during training) set of classes (where the basis function module could learn a set of basis functions for every class). In the proposed setting, I think a better definition would be \"learning a structured regression\" from a set of sample points to a function, and not few-shot regression.\nIf the authors would like to keep the \"few-shot flavour\", I would suggest re-formulating the experiments, and meta-train on some set of classes (e.g. inpainting over digits 0 to 4) and meta-test on a different set of classes (e.g. inpainting over digits 5 to 9). This partially holds for faces as they are all mostly different categories (different people), but in 32x32 resolution and MSE metric, I don't think they are sufficiently different.\n\n2. I would expect stronger results on the more realistic MNIST and CelebA experiments (although as suggested in 1. the setting there should be different), currently it does less well then existing method.\n\n3. An emerging important class of few-shot regression problems is few-shot object detection, where the bounding box coordinates of objects location need to be regressed. There are several papers and benchmarks in this space, and it will help the current paper to test on this challenging family of problems. Please see the following papers for benchmarks and settings:\n* LSTD: A Low-Shot Transfer Detector for Object Detection, Chen et al. 2018\n* RepMet: Representative-based metric learning for classification and one-shot object detection, Karlinsky et al. 2019\n* Few-shot Object Detection via Feature Reweighting, Kang et al. 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221453408, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Reviewers"], "noninvitees": [], "tcdate": 1570237725523, "tmdate": 1576221453421, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review"}}}, {"id": "B1eXuCxRYS", "original": null, "number": 2, "cdate": 1571847786741, "ddate": null, "tcdate": 1571847786741, "tmdate": 1572972363249, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose using sparse adaptive basis function models for few shot regression. The basis functions and the corresponding weights are generated via respective networks whose parameters are shared across all tasks. Elastic net regularization is used to encourage task specific sparsity in the weights,  the idea being  that with only a small number of available training examples, learning a sparse basis is  easier than learning a dense basis with many more parameters. The method is validated on both synthetic data and on image completion tasks. \n\nI am leaning towards rejecting the paper. 1) Although, the paper is well written and easy to follow the technical contributions of the paper are limited. Adaptive basis functions and their sparse combinations are decades old ideas.  While the application of these ideas to few shot regression does appear to be novel,  this combination don\u2019t seem to provide an obvious improvement over existing alternatives. 2) The empirical evidence presented is rather limited, the proposed approach only seems to outperform competitors on the synthetic sinusoidal regression experiments. Lack of strong empirical performance along with the limited novelty \n\nDetailed comments and questions: \n+ The approach naturally extends to few shot classification problems once the MSE loss in Equation 5 is replaced with an appropriate cross entropy loss. Was this considered and is the approach competitive on few shot classification problems.\n\n+ The empirical section could be significantly improved. \n\n -  Diverse synthetic data: I don\u2019t see the value in presenting two sets of synthetic sinusoid regression experiments.  It would be better to replace the alternative sinusoid task with qualitatively different tasks. This would help the  audience ascertain whether the favorable performance demonstrated in Table 1 generalizes beyond sinusoidal signals. \n\n - Comparisons:  1.  Why are comparisons to neural processes missing in the additional synthetic experiments presented in the supplement and from Table 2? This is a particularly egregious omission since on the real data (attentive) neural processes outperform the proposed method. \n2. The ensemble approach seems to improve on the individual model significantly in Table 2. Why was this not considered for the image completion experiments? The authors would also do well to more clearly describe how the ensembling was performed. \n\n+ I find it curious that the basis functions are restricted to be non-negative. The description in 3.3 suggests that the basis function network outputs are passed through a ReLU.  What was the rational behind this design choice?\n\nMinor:\n Why are both ANP and \u201cOurs\u201d highlighted in Table 3, when ANP clearly outperforms and does not appear to be within statistical noise of \u201cOurs\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221453408, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Reviewers"], "noninvitees": [], "tcdate": 1570237725523, "tmdate": 1576221453421, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review"}}}, {"id": "r1ggNMGw5B", "original": null, "number": 3, "cdate": 1572442664104, "ddate": null, "tcdate": 1572442664104, "tmdate": 1572972363205, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper describes an approach to few-shot regression based on\nlearning a sparse basis and a weight estimator network. The authors\nintroduce a sparsity inducing term in the loss to encourage sparse\nweight generation for tasks; these sparse coefficient vectors are then\nprojected onto the learned, task-dependent basis for\nregression. Experimental results are given on two synthetic regression\nproblems, with a comparison with the recent state-of-the-art.\n\nThis paper has some interesting ideas in it. However, it does have\nsome issues:\n\n 1. Clarity. There are several points of the proposed technique that\n    are not described clearly enough. For example, the diagram in\n    Figure 1 leads me to believe that the basis and weights generators\n    are independent (and thus not trained end-to-end). However, the\n    loss in eq. (5) seems (though it is not completely clear to me) to\n    depend on both networks (which is how I would expect things to\n    work). Also, the \"self attention blocks\" mentioned at several\n    points are never completely defines. And from the ablation study\n    is seems that the improvement form self-attention is the lion's\n    share of the overall improvement. I do not feel that it would be\n    easy to reproduce the results reported in this paper without\n    significant guesswork.\n\n 2. The experimental results are somewhat limited. The sinusoidal\n    regression problem is very artificial, as is the image regression\n    task. Focusing on regression is inherently limiting, but results\n    on more realistic regression problems would help establish more\n    clearly the significance of the contribution.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221453408, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Reviewers"], "noninvitees": [], "tcdate": 1570237725523, "tmdate": 1576221453421, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Review"}}}, {"id": "HklCW2U3Pr", "original": null, "number": 1, "cdate": 1569643526500, "ddate": null, "tcdate": 1569643526500, "tmdate": 1569643526500, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "HylDbJr2DS", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment", "content": {"comment": "Hi,\n\nThank you for the comment.\n\nThe link to the Github repo has been updated with our code for the paper.", "title": "Code uploaded"}, "signatures": ["ICLR.cc/2020/Conference/Paper2251/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxDNxSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2251/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2251/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2251/Authors|ICLR.cc/2020/Conference/Paper2251/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144120, "tmdate": 1576860532436, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Official_Comment"}}}, {"id": "HylDbJr2DS", "original": null, "number": 1, "cdate": 1569636095419, "ddate": null, "tcdate": 1569636095419, "tmdate": 1569636095419, "tddate": null, "forum": "BJxDNxSFDH", "replyto": "BJxDNxSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2251/-/Public_Comment", "content": {"comment": "Hi,\n \nNo code is present in the repo of the github link. It is not fair to provide a placeholder link for code submissions (which impact the review process) and submit code taking considerable buffer time after submission deadline.\n", "title": "No code in the repo of the provided github link "}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["loo_yi@sutd.edu.sg", "guoyl1990@outlook.com", "ngaiman_cheung@sutd.edu.sg"], "title": "Few-Shot Regression via Learning Sparsifying Basis Functions", "authors": ["Yi Loo", "Yiluan Guo", "Ngai-Man Cheung"], "pdf": "/pdf/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "TL;DR": "We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.", "abstract": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of sparsifying basis functions. This enables a few labeled samples to approximate the function. We design a Basis Function Learner network to encode basis functions for a task distribution, and a Weights Generator network to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "code": "https://github.com/fewshotreg/Few-Shot-Regression.git", "keywords": ["meta-learning", "few-shot learning", "regression", "learning basis functions", "self-attention"], "paperhash": "loo|fewshot_regression_via_learning_sparsifying_basis_functions", "original_pdf": "/attachment/48b259e15f3c0663cbf33ab632b12f529dc4aa24.pdf", "_bibtex": "@misc{\nloo2020fewshot,\ntitle={Few-Shot Regression via Learning Sparsifying Basis Functions},\nauthor={Yi Loo and Yiluan Guo and Ngai-Man Cheung},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxDNxSFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxDNxSFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182964, "tmdate": 1576860566065, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2251/Authors", "ICLR.cc/2020/Conference/Paper2251/Reviewers", "ICLR.cc/2020/Conference/Paper2251/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2251/-/Public_Comment"}}}], "count": 10}