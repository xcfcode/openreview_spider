{"notes": [{"id": "9G5MIc-goqB", "original": "fezewxx4WKL", "number": 1123, "cdate": 1601308126261, "ddate": null, "tcdate": 1601308126261, "tmdate": 1615777759945, "tddate": null, "forum": "9G5MIc-goqB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yIV93tIFVLj", "original": null, "number": 1, "cdate": 1610040408354, "ddate": null, "tcdate": 1610040408354, "tmdate": 1610474005360, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Dear authors,\n\nThe reviewers appreciated the insights provided by your paper and the strong results. Congratulations.\nI encourage you to address the other points raised to make your final submission as complete as possible."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040408341, "tmdate": 1610474005344, "id": "ICLR.cc/2021/Conference/Paper1123/-/Decision"}}}, {"id": "OD0GqcPoeOR", "original": null, "number": 1, "cdate": 1603820300681, "ddate": null, "tcdate": 1603820300681, "tmdate": 1606798823761, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review", "content": {"title": "REWEIGHTING AUGMENTED SAMPLES BY MINIMIZING THE MAXIMAL EXPECTED LOSS", "review": "##########################################################################\n\nSummary:\n\n \nThe paper investigate an interesting problem, improving generalization performance by leveraging augmented data.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, my vote lies on borderline and my main concern is the novelty of the work.\n\n \n##########################################################################\nPros: \n1. The paper studies an important problem and propose a method that is a mixture of weighted augmentation and adversarial loss.\n2. The paper is written well and easy to follow.\n- Figure 1 is a nice representation to help the reader to understand the loss function better.\n3. The paper investigates the performance of the proposed method on two domains: computer vision and natural language processing. The authors also provide ablation studies to evaluate the role of parameters.\n \n##########################################################################\n\nCons: \n\n \n1- I have two main concerns:\na) The novelty of the method is questionable for me as I have already seen a very close idea in the following papers:\n[1] https://ieeexplore.ieee.org/abstract/document/8658998\n[2] https://arxiv.org/pdf/1712.04621.pdf\n[3]https://arxiv.org/abs/1907.12934\nIn [1], the min-max (adversarial) framework used and the distribution over the data provides weighting augmentation.\nIn [2], the effectiveness of data augmentation in object recognition has been discussed.\nIn [3], the min max entropy has been leveraged for weakly supervised (pixel-level) localization.\n....\nIt is unfortunate that this papers were not even cited.\nb) Generalization aspect.\nThe authors of the paper presents their main objective to provide generalization while they have not provided comprehensive experiments or discussion to address this concern.\n\n2- Although the proposed method provides experiments on computer vision and natural language processing, I still suggest the authors to conduct more experiments considering more datasets from computer vision domain.\n\n3-It would be great if the author can what would be the advantages/ weaknesses of their approach with the references. As the proposed approach has high similarity with the previous works, the minimum requirement is reporting more experiments and compare their method with exist methods. This extra study would present how their approach affect the performance.\n \n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\nAfter rebuttal:\n\nDear Authors, Thanks for providing more details. I believe more discussion and experiments are required to present the difference of you method. As you have mentioned, one difference is in considering summation rather than maximization, so it would be required to know what would be the advantages/ weaknesses of this difference. How does it make any impact on the performance? I would increase my score considering the closed-form solution as a nice contribution and requiring more experiments and analysis on the discussed references.\n\nThanks!\n\n\n ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126325, "tmdate": 1606915774957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1123/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review"}}}, {"id": "sfDtyrPHl6_", "original": null, "number": 2, "cdate": 1603861057672, "ddate": null, "tcdate": 1603861057672, "tmdate": 1606797885186, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review", "content": {"title": "Proposes an efficient example reweighting scheme that in some experiments can improve the performance but not always.", "review": "Summary:\nThis paper proposes a simple scheme for training with multiple augmentations of training data in one iteration and reweighting the instances by their relative loss. As authors note in their related works, the idea of reweighting examples based on their relative loss has been widely studied in a variety of machine learning problems. In contrast, this work proposes the reweighting only within augmentations of a single sample. They derive their particular reweighting scheme by proposing an alternative risk (Eq. 3). The new objective is a function of both model parameters and the distribution of augmentations. They propose to find the model parameters that minimize the alternative risk for the hardest distribution of augmentations that maximizes their alternative risk (Eq. 4). Then they consider the distribution of augmentations that are a function of model parameters and input and show that for fixed model parameters, the optimal distribution on a fixed finite set of augmentations is determined by the softmax on the loss of the model for each augmented input. In section 3.2, they propose two variations of their loss using the ground-truth label to evaluate the loss (hard loss) versus using the prediction of the model for the original raw input (soft loss). In section 3.3, they propose specific considerations for augmenting text data. They provide experiments on image and text data with ablations studies.\n\nPros:\n- Proposed methods are particularly good on large models (resnet44, resnet56)\n- Figure 2 is particularly interesting because the curves for proposed methods seem less noisy in multiple datasets (CoLA, SST-2 mrpc (only soft), qnli).\n- The method might show more advantage if tested in low data settings or small mini-batches. Have authors tried any setting with small training data? Or smaller mini-batches?\n\nCons:\n- The method does not always beat DA+UNI that uses multiple augmentations but weights them uniformly. Although one could argue DA+UNI is also an interesting contribution as it can be more efficient than DA+Long for small mini-batches.\n- Tables: no standard deviation is given? In particular, in Table 1, 0.5 can easily be within the standard deviation.\n- Table 1: DA+UNI should have the same time as MMEL but for resnet56 it takes 7h while MMEL takes 4.5h to train\n- It is not clear how the hyperparameter lambda is tuned? Was it done using cross-validation? In the appendix the lambda is given as 1. Is that the best value?\n- Table 3: Results on CoLA are good but others not significant? Again, not clear based on missing standard deviation.\n\nAdditional Notes:\n- Can you clarify the following sentence: \u201cRemark 3: If we ignore the KL divergence term in equation (3), due to the equivalence of minimizing cross-entropy loss and MLE loss...\u201d\n- Is Eq 7. just Eq 5 but for a single example?\n- Figure 1 is not really descriptive enough. Maybe make it one figure and use color and line style to show the difference.\n- DA+UNI: does the implementation use bigger batch size to perform computations in parallel?\n\nTypos:\nSection 3.1: an uniform -> a uniform\nRemark 2: regularizedr -> regularizer\n\n==============\nAfter rebutall:\nI thank the authors and appreciate addressing all my concerns. I'm glad that the additional experiments with smaller mini-batches and smaller training set size provide more supporting evidence for the method. I encourage the authors to point to these results in the main body.\n\nOne more minor suggestion is regarding the sentence in Remark 3. The way I read the sentence the \"equivalence of minimizing CE loss and MLE loss\" is the reason we can remove the KL divergence term. But the authors' response seems to say it is for the 3rd part of the sentence. The authors' might want to rewrite that sentence to make it clear.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126325, "tmdate": 1606915774957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1123/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review"}}}, {"id": "nsNEh5C0Lj-", "original": null, "number": 3, "cdate": 1603899908886, "ddate": null, "tcdate": 1603899908886, "tmdate": 1606664338740, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review", "content": {"title": "Reweighting review", "review": "##########################################################################\n\nSummary:\n \nThe paper proposes a novel data augmentation method. In particular, it proposes a reweighted loss function that allows to find the optimal weighting of the augmented samples. The approach is tested on standard image and \nlanguage tasks and compared to mulitple alternative approaches.\n\n##########################################################################\n\nReasons for score: \n \n\nOverall, I vote for accepting. The approach is interesting, well motivated and clearly formalized. However the experiments raise some questions which I would like the authors to comment on in the rebuttal. \n \n\n##########################################################################\n\nPros: \n \n1. The paper has an interesting take on the important topic of data augmentation  \n\n2. It proposes a clever and theoretically well motivated way to incorporate augmented samples. \n\n3. The base line models are chosen reasonable. \n\n4. The paper is clearly structured and well written.\n\n##########################################################################\n\nCons: \n \n1. One major concern is the performance comparison of varying numbers of augmented samples. Table 2 shows that often a smaller amount of augmented samples perform better. However, if the reweighting is indeed optimal, adding augmented samples should not hurt the performance.  \n\n\n2. It is not clear when and why to use the hard or soft loss. In addition with the varying performance dependent on the number of augmentation samples this adds additional hyperparameters. The increase in performance might not be worth the increase in training time (see point 3).\n\n\n3. Are the results significant? \n \n\n##########################################################################\n\nQuestions during rebuttal period: \n \n\nPlease address and clarify the cons above \n \n\n#########################################################################\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126325, "tmdate": 1606915774957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1123/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Review"}}}, {"id": "Swaoka6PV8", "original": null, "number": 10, "cdate": 1606218646579, "ddate": null, "tcdate": 1606218646579, "tmdate": 1606218646579, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "FZvhHvcoVnt", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thanks very much for your quick response. We really appreciate your detailed comments and feedback. Below is the response to your further concerns.\n\nQ1. Comment (1) is not valid as you have ignored the optimization step and the constraints in [1]. Considering these points, you would find that $P(y|x)$ is dependent to the model output. Therefore, I still see your approach very close to [1].\n\nA1. (1) First of all, we would like to clarify that, the statement (in the former reply) of independence of the weights on $f(\\cdot)$ are based on the general objective in equation (2), rather than the detailed optimization step where an additional constraint for object detection (which restricts the expected feature statistic under adversarial annotation distribution $P(y|x)$ close to the original ground-truth one) is included\n\n(2) Moreover, the constraint is indeed another difference between our method and [1]. The dependence of $P(y|x)$ on the model $f(\u22c5)$ is introduced by an additional constraint (instead of the loss $\\ell(\\cdot)$), while we directly formulate this dependence on $f(\u22c5)$ in the loss $\\ell(\\cdot)$.\n\n(3) Please also note the difference in the order of summation and maximization between the proposed method and [1] as elaborated in the former reply. This also shows the difference between the two training objectives, and all these differences result in different solutions in the two works.\n\nQ2. The proposed method has been leveraged in many different applications. Please check the Google scholar and you would find that this method was employed even on time series problems like https://link.springer.com/chapter/10.1007/978-3-319-93040-4_53 . Therefore, it discards the validity of your comment (3).\n\nA2. Thanks for pointing out this interesting reference. Sorry that we did not pay attention to applications in time series before, and this reference is not in [1]\u2019s google scholar citation list. We have now added this reference in the related work section. Please see it in the revised version.\n\nQ3. Yes, you are right. As I already mentioned in Cons, \"the min-max (adversarial) framework used and the distribution over the data provides weighting augmentation. \". However, it does not clarify my concern on the novelty of your method and the similarity of your work to [1].\n\nA3. We agree that [1] is a relevant reference in the sense that it also reweights the augmented samples adversarially. We have also added discussion on it in Section 3 in the revised manuscript. Our proposed method is also different and novel in the following aspects.\n\n(1) The training objective and the construction of sample weights $P(y|x)$ are different as elaborated in our reply to Q1 and our former reply; \n\n(2) The solvers of the two are different. [1] uses linear programming to calculate the weights of augmented samples while we have a closed-form solution; \n\n(3) This closed-form solution brought training efficiency of the proposed method. Specifically, since we have the explicit solution of weights on augmented samples and the multiple forward and backward passes on these augmented samples can be computed in parallel, the whole training time is similar to the regular training counterpart. \n\nWe would be happy to answer any additional questions. Let us know if there are additional ways to raise your score. Thank you so much.\n\n[1] S. Behpour, K. Kitani, and B. Ziebart. \"Ada: Adversarial data augmentation for object detection.\" 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "gpdCaLr1j9s", "original": null, "number": 7, "cdate": 1606147269692, "ddate": null, "tcdate": 1606147269692, "tmdate": 1606147698373, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "OD0GqcPoeOR", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "To AnonReviewer2 Part 1", "comment": "Thanks for your review. We address the concerns as follows.\n\nQ1.\t\u201cThe novelty of the method is questionable for me as I have already seen a very close idea in the following paper: [1] https://ieeexplore.ieee.org/abstract/document/8658998 In [1], the min-max (adversarial) framework used and the distribution over the data provides weighting augmentation. It is unfortunate that this paper was not even cited.\u201d\n\nA1.\tThanks for pointing out this reference. We summarize the differences between this paper and [1] as follows.  \n\n(1)\tThe first difference is the training objective. [1] studies the problem of object detection, and a perturbed bounding box $y$ is an augmented sample. As the reviewer suggested, if we rewrite the expected loss for input $x$ (equation (1) in [1]) as $\\sum_{y}P(y|x)\\sum_{y^{\\prime}}f(y^{\\prime}|x)l(y^{\\prime},y)$, $\\sum_{y^{\\prime}}f(y^{\\prime}|x)l(y^{\\prime},y)$ can be viewed as the loss on augmented sample $y$ (bounding box) and $P(y\\mid x)$ is the weights on the augmented samples i.e. annotation distribution. \n\nOur main learning objective (3) is different from (2) in [1] because their weights $P(y| x)$ are independent of the output of the model (to be learned) $f(\\cdot)$ given $P(y\u2019|x)$. However, we compute the weights of the augmented samples according to the loss on them which depends on $f(\\cdot)$. \n\nMathematically, for a given original sample $x$, if we use the same symbols as [1], our objective in equation (4) can be rewritten as $\\max_{P(y|x)}\\sum_{y}P(y|x)\\sum_{y^{\\prime}}f(y^{\\prime}|x)[l(y^{\\prime}, y)]$. This is different from the term $\\sum_{y^{\\prime}}f(y^{\\prime}|x)\\max_{P(y|x)} \\sum_{y}P(y|x)l(y^{\\prime}, y)$ in objective (2) in [1] because one can not interchange the order between summation and maximization.\n\n(2)\tThe second difference is the solver, which is a direct cause of the difference of the objective. The authors in [1] use linear programming to iteratively compute annotation distribution $P(y\\mid x)$( Algorithm 1 in [1]), while we have a closed-form solution of the weights of the augmented samples in equation (6).\n\n(3)\tThe third difference is the application scenario. While for object detection, data augmentation can be done by directly generating perturbed labels (structured output of object detection), this method is not straightforward to be extended to other domains. On the other hand, our formulation can generally be applied in many domains.\n\n(4)\tThe method in [1] is more similar to adversarial training because (i) selecting bounding boxes that are maximally different from the ground truth mimics the process of generating the adversarial augmented samples and (ii) this generation process is included in each iteration of the learning process. However, for our proposed method, (i) the generation of augmented samples in our proposed method is decoupled with the learning process, and (ii) it can generally be applied on top of any data augmentation, regardless of whether the augmented samples are adversarially generated or not. The second paragraph in Section 2 of the paper clarifies the difference between the proposed method and adversarial training. We also added a discussion of [1] in the revised manuscript.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "Uwpysltggb1", "original": null, "number": 6, "cdate": 1606147213457, "ddate": null, "tcdate": 1606147213457, "tmdate": 1606147668149, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "OD0GqcPoeOR", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "To AnonReviewer2 Part 2", "comment": "Q2.\t\u201cGeneralization aspect. The authors of the paper presents their main objective to provide generalization while they have not provided comprehensive experiments or discussion to address this concern.\u201d\n\nA2.\tFirst of all, our method is highly theoretically motivated (please refer to Section 3.1). Minimizing the proposed MMEL loss enables the model to perform well under any reweighting strategy. Secondly, in Section 4, we conducted various experiments on both computer vision tasks and natural language tasks and discussed the empirical results. These results show that our proposed method has better accuracy results than the compared methods. \n\nQ3.\t\u201cAlthough the proposed method provides experiments on computer vision and natural language processing, I still suggest the authors to conduct more experiments considering more datasets from computer vision domain.\u201d\n\nA3.\tBelow we added more results with more augmentation methods, more architectures and more data sets on the computer vision domain.\n\n(1)\tSince our method can be applied on top of any data augmentation technique. We further combine our method with one widely-used data augmentation method Cutout [2] on WideResNet [3]. The baseline results on WideResNet are taken from [2]. The other settings are the same as Section 4.1 of our submission. As can be seen, applying both MMEL-H and MMEL-S on top of Cutout have better performance than the original Cutout.\n\n|     Method              |     C10_Wide28_10    |     C10_Wide40_2    |     C100_Wide28_10    |     C100_Wide40_2    |\n|-------------------------|----------------------|---------------------|-----------------------|----------------------|\n|     Baseline(Cutout)    |     96.9             |     95.9            |     81.6              |     74.8             |\n|     MMEL-H              |     97.0(+0.1)       |     96.75(+0.85)    |     82.68(+1.02)      |     79.97(+5.15)     |\n|     MMEL-S              |     97.38(+0.48)     |     96.65(+0.75)    |     82.06(+0.40)      |     78.26(+3.46)     |\n\n(2)\tWe also apply the proposed method on the ImageNet data set on ResNet. The results of the baseline are taken from https://pytorch.org/docs/stable/torchvision/models.html.\nDue to time and computation limit, we only report the results of the proposed method with hard loss (MMEL-H). We use random crop and horizontal flip (Krizhevsky et al., 2012) to augment the original training images. We directly adopt the label of the original training sample for all its augmented samples. For each original example, we generate 3 more augmented samples from it. The other training hyperparameters are the same as [4]. As can be seen, by reweighting the augmented samples, MMEL-H has higher performance than the baseline with original data augmentation, especially on ResNet34.\n\n|     Method          |     ResNet18-Imagenet    |     ResNet34-Imagenet    |     ResNet50-Imagenet    |\n|---------------------|--------------------------|--------------------------|--------------------------|\n|     Baseline(DA)    |     69.76                |     73.30                |     76.15                |\n|     MMEL-H          |     70.48(+0.72)         |     74.38(+1.08)         |     76.53(+0.38)         |\n\n\nReference:\n\n[1]\tS. Behpour, K. Kitani, and B. Ziebart. \"Ada: Adversarial data augmentation for object detection.\" 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.\n\n[2]\tD., Terrance, and G. W. Taylor. \"Improved regularization of convolutional neural networks with cutout.\" arXiv preprint arXiv:1708.04552 (2017).\n\n[3]\tZ. Sergey, and N. Komodakis. \"Wide residual networks.\u201d arXiv preprint arXiv:1605.07146 (2016).\n\n[4]\thttps://github.com/pytorch/examples/tree/master/imagenet\n\n[5] A. Krizhevsky, and I. Sutskever, and G. E. Hinton \u201cImageNet Classification with Deep Convolutional Neural Networks\u201d In Advances in neural information processing systems, pp. 1097\u20131105, 2012\u201d\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "AmNKoBnQyV8", "original": null, "number": 5, "cdate": 1606146254223, "ddate": null, "tcdate": 1606146254223, "tmdate": 1606147549442, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "ycls_pbsu3F", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "To AnonReviewer 1 Part 2 ", "comment": "Q5.\t\u201cIt is not clear how the hyperparameter lambda is tuned? Was it done using cross-validation? In the appendix the lambda is given as 1. Is that the best value?\u201d\n\nA5.\tIn this paper, we simply set both $\\lambda_{T}$ and $\\lambda_{P}$ as 1 without any tuning. A more careful tuning on them may lead to better performance. For example, we tune $\\lambda_{T}$ and $\\lambda_{P}$ in [0.5, 1.0, 2.0, 5.0] on task CoLA from the GLUE benchmark. \nCompared to the default setting ($\\lambda_{T}=\\lambda_{P}=1$), the best $\\lambda_{T}$ and $\\lambda_{P}$ respectively increase the accuracy from 61.7 to 62.8 and 63.5 for MMEL-S. The best $\\lambda_{P}$ on MMEL-H (there is no $\\lambda_{T}$ for MMEL-H) increases the accuracy from 62.1 to 62.8. Please note that the results of CoLA in Table 3 are averaged over five independent runs. For this particular run of this tuning task, the accuracy with default $\\lambda_{T}=\\lambda_{P}=1$ is 61.7 and 62.1 for MMEL-H and MMEL-S, respectively. \n\nQ6.\t\u201cCan you clarify the following sentence: \u201cRemark 3: If we ignore the KL divergence term in equation (3), due to the equivalence of minimizing cross-entropy loss and MLE loss...\u201d\n\nA6.\tIf we ignore the KL divergence term in equation (3), the objective becomes $\\frac{1}{N}\\sum_{i=1}^{N} E[\\ell(f_{\\theta}(z), y_{z})]$. It is the sum of negative expected log-likelihood if $\\ell(\\cdot, \\cdot)$ is the cross-entropy loss (which is commonly used for classification tasks). Then, minimizing equation (3) is equivalent to maximizing a log-likelihood loss. Please see page 8 in [3] for a more rigorous derivation. Under this condition, the equivalence of MMEL and GEM are then explained in the latter part of Remark 3.\n\nQ7.\t\u201cIs Eq 7. just Eq 5 but for a single example?\u201d\n\nA7.\tYes.\n\nQ8.\t\u201cFigure 1 is not really descriptive enough. Maybe make it one figure and use color and line style to show the difference.\u201d\n\nA8.\tThanks for this suggestion. We find it hard to put the two type of losses in one figure, so we mark the differences between them with different colors as suggested in Figure 1 in the revised manuscript. \n\nQ9.\t\u201cDA+UNI: does the implementation use bigger batch size to perform computations in parallel?\u201d\n\nA9.\tYes, similar to MMEL-H and MMEL-S described in Section 3.3.\n\nQ10.\t \u201cTypos: Section 3.1: an uniform -> a uniform, Remark 2: regularizedr -> regularizer\u201d \n\nA10.\t Thanks. We have corrected them in the revised manuscript.\n\nReference:\n[1] J. Martens. \"New insights and perspectives on the natural gradient method.\" arXiv preprint arXiv:1412.1193 (2014)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "ycls_pbsu3F", "original": null, "number": 4, "cdate": 1606146137690, "ddate": null, "tcdate": 1606146137690, "tmdate": 1606146282601, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "sfDtyrPHl6_", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "To AnonReviewer 1 Part 1", "comment": "Thanks for your review. We address the concerns as follows.\n\nQ1. \u201cThe method might show more advantage if tested in low data settings or small mini-batches. Have authors tried any setting with small training data? Or smaller mini-batches?\u201d\n\nA1. We added experiments with small training data and smaller mini-batches on natural language understanding task QNLI from the GLUE Benchmark. \n\n(1) For small training set, we randomly select a subset from the original training corpus of QNLI. The table below shows results with varying subset size of 10K, 20K, 40K, and 80K training samples. As can be seen, the proposed method has higher accuracies than the other baselines on all these small training sets. The performance gain over the Baseline is more obvious on smaller training sets. \n\n|     dataset size    |     Baseline    |     Baseline(DA+Long)    |     Baseline(DA+UNI)    |     MMEL-H    |     MMEL-S    |\n|---------------------|-----------------|--------------------------|-------------------------|---------------|---------------|\n|     10k             |     87.3        |     87.6                 |     89.3                |     89.8      |     89.5      |\n|     20k             |     88.1        |     88.5                 |     89.6                |     90.7      |     90.1      |\n|     40k             |     89.9        |     90.3                 |     91.1                |     91.5      |     91.5      |\n|     80k             |     91.0        |     91.4                 |     91.7                |     92.3      |     92.2      |\n|     108K            |     91.7        |     92.0                 |     91.9                |     92.2      |     92.4      |\n\n(2) For smaller batch size, besides the default batch size 32 used in the paper, we further conduct experiments on QNLI with smaller batch size 4, 8 and 16 in the table below. As can be seen, the proposed method is more efficient than Baseline(DA+Long) for all different batch size settings. The speedup gain is more obvious when the batch size is smaller than 16. In addition, decreasing batch size incurs more performance degradation on all three baselines than MMEL-H and MMEL-S.\n\n|     Batch-size    |     Baseline       |     Baseline(DA+Long)    |     Baseline(DA+UNI)    |     MMEL-H             |     MMEL-S        |\n|-------------------|--------------------|--------------------------|-------------------------|------------------------|-------------------|\n|                   |     acc    time    |     acc    time          |     acc    time         |     acc   time         |     acc   time    |\n|     4             |     90.5  5.67h    |     90.1   28.35h        |     91.0      5.78h     |     92.2 5.78h         |     92.0 5.78h    |\n|     8             |     91.3  2.83h    |     90.7   14.15h        |     91.4      2.97h     |     92.3 2.97h         |     92.2 2.97h    |\n|     16            |     91.2  2.40h    |     91.2   12.00h        |     91.7      2.52h     |     92.2 2.52h         |     92.1 2.52h    |\n|     32            |     91.6  2.19h    |     92.0   10.95h        |     91.9      3.43h     |     92.1 3.43h         |     92.2 3.43h    |\n\nQ2.\t\u201cThe method does not always beat DA+UNI that uses multiple augmentations but weights them uniformly.\u201d\n\nA2. (1)  As is also suggested by the reviewer, DA+UNI is also one of our contributions even though we set it as a baseline. \n \n(2) In addition, from Table 1, MMEL is much better than DA+UNI on image classification task, though the improvements in natural language understanding (NLU) tasks in Table 3 is less obvious. We speculate that this is because for textual data, augmented samples generated by word substitution can be much more diverse compared with augmented image data, and the number of augmented samples also greatly affects the accuracy as well as how to efficiently utilize these samples (i.e. sample re-weighting). Thus MMEL does not always beat DA+UNI on NLU tasks as they have the same number of augmented samples.\n\nQ3.\t\u201cTables: no standard deviation is given? In particular\u2026In particular, in Table 1, 0.5 can easily be within the standard deviation.\u201d\n\nA3.\tWe added mean and std results from 5 repetitions on both CIFAR and GLUE benchmark in Tables 1 and 3. Please also refer to our reply to Q4 for AnonReviewer3.\n\nQ4.\t\u201cTable 1: DA+UNI should have the same time as MMEL but for resnet56 it takes 7h while MMEL takes 4.5h to train\u201d\n\nA4.\tThanks for pointing this typo out. We fixed it in the revised version. \n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "ZrQavRum6yl", "original": null, "number": 3, "cdate": 1606145644717, "ddate": null, "tcdate": 1606145644717, "tmdate": 1606145644717, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "nsNEh5C0Lj-", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "To AnonReviewer3", "comment": "Thanks for your review. We address the concerns as follows.\n\nQ1. \u201cOne major concern is the performance comparison of varying numbers of augmented samples. Table 2 shows that often a smaller amount of augmented samples perform better. However, if the reweighting is indeed optimal, adding augmented samples should not hurt the performance.\u201d\n\nA1. The reviewer is correct that more augmented samples do not harm the performance. However, the performance saturates after a certain number of augmented samples, and adding more examples makes the performance fluctuate around this saturation point\u2019s performance. This is also why often the best performance is not necessarily achieved at the largest number of augmented samples, but at a smaller number. We also discussed this phenomenon at the end of Section 4.1 in the submission. To make this observation more reliable, we added results with mean and std from five independent runs in Table 2 in the revised manuscript. As can be seen from this table, the improvements brought by increasing augmented samples saturate after a certain number of augmented samples.\n\nQ2. \u201cIt is not clear when and why to use the hard or soft loss.\u201d\n\nA2.  (1) When: As is discussed in Section 3.2, the hard loss in equation (7) requires the hard label of each augmented sample, while the soft loss in equation (8) only requires the soft output probability from the model. Thus we use the soft loss when the augmented data ends up being far from the clean data and the appropriate label for it is unclear.\n\n(2) Why: In the absence of a reliable ground-truth label for the augmented data, using the hard loss which requires the ground-truth label may mislead the learning. As is also shown in Table 4, in natural language tasks of the GLUE benchmark, the augmented samples generated by word substitution usually have different semantic meanings (as explained in Section 3.3) from the original sample. In this case, if we do not use the teacher model to predict labels, using hard loss (MMEL-H Original) has much more severe loss degradation than using soft loss (MMEL-S Original).\n\nQ3. \u201cIn addition with the varying performance dependent on the number of augmentation samples this adds additional hyperparameters. The increase in performance might not be worth the increase in training time (see point 3).\u201d\n\nA3.  From Table 2, the performance improves with more augmented samples up to some certain number (Please also refer to our reply to Q1). Thus ideally we only need to stop increasing the number of augmented samples when we meet this saturation point. Empirically, we find that using 5 augmented samples for natural language understanding tasks of the GLUE benchmark and 10 augmented samples for image classification tasks on CIFAR achieves a good balance between the accuracy and training time.\n\nQ4. \u201cAre the results significant?\u201d\n\nA4. We added mean and std results from 5 repetitions on CIFAR in Tables 1&2 and GLUE benchmark in Table 3 in the revised manuscript. The small std in most cases indicates the stability of the proposed method. These results also show that the proposed method consistently outperforms the Baseline (DA) and Baseline (DA+UNI), while being more efficient in training than Baseline(DA+Long).\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}, {"id": "JEswRtei0d7", "original": null, "number": 2, "cdate": 1606145482340, "ddate": null, "tcdate": 1606145482340, "tmdate": 1606145482340, "tddate": null, "forum": "9G5MIc-goqB", "replyto": "9G5MIc-goqB", "invitation": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment", "content": {"title": "General response", "comment": "We thank all the reviewers for their insightful and valuable comments. Please refer to the response to each reviewer for detailed explanations. We have revised the manuscript as suggested by the reviewers. The main changes (highlighted in blue) we made include:\n1.\tIn Section 4, we add mean and std results for both image classification task CIFAR and the natural language understanding tasks from the GLUE benchmark. We do not report results for DA+Long because its training takes too much time. \n2.\tIn related work, we added some more discussion about the connection and difference between the proposed method and one reference mentioned by Reviewer2.\n\nThanks again for all the valuable comments and suggestions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1123/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reweighting Augmented Samples by Minimizing the Maximal Expected Loss", "authorids": ["~Mingyang_Yi1", "~Lu_Hou2", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1", "~Zhi-Ming_Ma1"], "authors": ["Mingyang Yi", "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Zhi-Ming Ma"], "keywords": ["data augmentation", "sample reweighting"], "abstract": "Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.", "one-sentence_summary": "a new reweighting strategy on augmented samples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yi|reweighting_augmented_samples_by_minimizing_the_maximal_expected_loss", "pdf": "/pdf/e4c9206df0bf95f0a614a0ac2cc2acd08f5125ce.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyi2021reweighting,\ntitle={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},\nauthor={Mingyang Yi and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Zhi-Ming Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9G5MIc-goqB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9G5MIc-goqB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1123/Authors|ICLR.cc/2021/Conference/Paper1123/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1123/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863393, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1123/-/Official_Comment"}}}], "count": 12}