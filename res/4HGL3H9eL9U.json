{"notes": [{"id": "4HGL3H9eL9U", "original": "uaLNal06O5l", "number": 3499, "cdate": 1601308388166, "ddate": null, "tcdate": 1601308388166, "tmdate": 1614985765493, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "MkUtlPJPl4x", "original": null, "number": 1, "cdate": 1610040367914, "ddate": null, "tcdate": 1610040367914, "tmdate": 1610473958917, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "I thank the authors and reviewers for their discussions about this paper. The proposed AT-GAN is a GAN-based method to generate adversarial examples. Similar methods (e.g. Song et al) have been proposed to use GANs to generate adv. examples more efficiently. Authors show their method has some numerical benefits. However, more experiments are needed to further justify it. Also, creating \"unrestrictive\" adv. examples can cause a risk of generating samples where the true label is flipped. Authors need to clarify it. Given all, I think the paper needs a bit of more work to be accepted. I recommend authors to address the aforementioned concerns in the updated draft.   \n\n-AC"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040367900, "tmdate": 1610473958896, "id": "ICLR.cc/2021/Conference/Paper3499/-/Decision"}}}, {"id": "d24gIqvaa8x", "original": null, "number": 3, "cdate": 1603901566094, "ddate": null, "tcdate": 1603901566094, "tmdate": 1606786254039, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review", "content": {"title": "a straightforward idea", "review": "The paper proposes AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from previous works, the study aims to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by \ufb01rst learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly for any input noise, denoted as non-constrained adversarial examples. Some experiments and visualizations show that AT-GAN can generate some diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models. \n\nOverall, the idea seems straightforward. Benefiting from the GAN, the proposed model could learn the distribution of adversarial examples to attach the target models. The paper is clearly written and some experiments are conducted. However, I have some concerns as below:\n\n1. In the loss function, $\\rho$ controls the difference between the outputs of the original and attach GANs, it is expected to see the performance and generated examples with different $\\rho$. \n\n2. The idea seems incremental. The main contribution is to transfer a pre-trained GAN to attach GAN to fool the classifiers. The novelty could be further summarized by highlighting the difference with most related works including but not limited to the aforementioned ones. The current manuscript makes the work seem like a straightforward combination of many existing approaches. \n\n3. Some experiment settings are not clear. A brief introduction to Model A to B should be given in the main paper, though the details is provided in Appendix.\n\nAs most concerns of mine are addressed by the rebuttal and I would like to rise my score. \n   \n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074733, "tmdate": 1606915763461, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3499/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review"}}}, {"id": "zuUJyBYwzYu", "original": null, "number": 7, "cdate": 1606275008399, "ddate": null, "tcdate": 1606275008399, "tmdate": 1606285245920, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "KHmEWeM1IMq", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment", "content": {"title": "Response to Review #4 (part 2/2)", "comment": "3.\t**Q**: I would like to know if transfer learning technique could be used to reduce the number of required adversarial examples.\n\n    **A**: AT-GAN transfers the conditional generator which can craft benign examples to generate adversarial examples. But different from transfer learning, the aim of AT-GAN is to generate the examples that fool the target classifier guaranteed by Eq. (3)\n\n    $$L_1 = \\mathbb{E}_{z\\sim p_z}[H(f(G_{attack}(z,y_s)), y_t)],$$\n\n    and are realistic to humans guaranteed by Eq. (4)\n\n    $$L_2 = \\mathbb{E}_{z\\sim p_z}[\\|G_{original}(z,y_s) + \\rho - G_{attack}(z,y_s)\\|]_p.$$\n\n    With the two loss functions, AT-GAN is trained to transfer the generator $G_{original}$ that models the distribution of benign examples to $G_{attack}$ that models the distribution of adversarial examples. This process is different from the training process of GANs and we do not need any adversarial examples for the transferring process.\n\n4.\t**Q**: The attack transferability.\n\n    **A**: We add experiments and use adversarial examples generated on Model A to attack Model C for MNIST dataset. The results are depicted as follows:\n\n    | | Nor. | Adv. | Ens. | Iter. |\n    |-|------|------|------|-------|\n    | FGSM | 46.7 | 4.2| 1.7| 4.6\n    | PGD | **97.5**| 6.5| 4.1| 4.1\n    |R+FGSM| 82.3 | 6.7| 4.8| 4.1\n    |Song's| 23.8| 20.8 | 20.6 | **20.1**\n    |AT-GAN| 65.3| **24.6** | **27.9**| 17.2\n\n    We can see that the examples generated by AT-GAN exhibit moderate transferability.\n\n5.\t**Q**: The adversarial examples used for adversarial training.\n\n    **A**: Adversarial training aims to defend various adversarial attacks but not limited to the adopted attack for adversarial training. Therefore, the examples we used are not generated by AT-GAN. We adopt three adversarial training in our experiments: a) adversarial training (Adv.) [1] uses adversarial examples generated by FGSM, b) ensemble adversarial training (Ens.) [2] uses adversarial examples generated by R+FGSM on the ensemble of models, c) Iterative adversarial training (Iter.) [3] uses adversarial examples generated by PGD.\n\n    [1] Ian Goodfellow, Jonathon Shlens, Christian Szegedy. Explaining and Harnessing Adversarial Examples. ICLR 2015.\n\n    [2] Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel. Ensemble Adversarial Training: Attacks and Defenses. ICLR 2018.\n\n    [3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR 2018.\n\n6.\t**Q**: Number of examples and time needed to train AT-GAN.\n\n    **A**: As in A3, we do not need any adversarial examples for the transferring process. For the training time, it takes about 8 minutes for transferring the generator of AT-GAN for Model A on MNIST. As we can craft numerous adversarial examples directly once the generator is transferred, we do not consider such time in the comparison for crafting 1,000 examples in the experiments. We have clarified it in the revision, thank you.\n\n7.\t**Q**: The generating capability, i.e., generating failure ratio, of AT-GAN.\n\n    **A**: We use the same input, and randomly pick 100 images for each category of MNIST generated by AT-GAN and the original generator, respectively. We then conduct human evaluation to determine whether each example is realistic. The evaluation results on the percentage of realistic images are as follows: \n\n    |   Category  |  0  |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  Average |\n    |-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|----------|\n    | Original(%)    | 100 | 100 | 93  | 94  | 98  |  96 | 99  | 100 | 98  | 100 |    97.8  |\n    | AT-GAN(%)   | 100 | 100 | 85  | 91  | 80  |  90 | 97  | 98  | 92  | 100 |    93.3  |\n\n    We see that adversarial examples in some categories (e.g. 2, 4) are harder to be semantically meaningful than other categories (e.g. 0, 1). On average, however, the generating capability is close to that of the original generator. \n\n    We have added the human evaluation in the revision. Thank you. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4HGL3H9eL9U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3499/Authors|ICLR.cc/2021/Conference/Paper3499/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836916, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment"}}}, {"id": "KHmEWeM1IMq", "original": null, "number": 6, "cdate": 1606274906086, "ddate": null, "tcdate": 1606274906086, "tmdate": 1606285168096, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "9qJB45VB5iR", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment", "content": {"title": "Response to Review #4 (part 1/2)", "comment": "Thank you for the valuable comments and suggestions. Below, we would like to address your main concerns: \n\n**General comments**: The main idea is to first train a normal GAN and then use the idea of transfer learning based on adversarial examples. The aim sounds good but the authors fail to clearly distinguish the idea with the exiting related methods theoretically or numerically. The idea of transferring is good (although not new), but after checking the implementation details, I have to say in the current version, the fact of transferring is quite limited.\n\n**A**: We try our best to clarify the differences of our work with existing related works, and address all your concerns in the following as well as in the revised paper. Note that we did not use any example for the transferring. We also add experiments following your suggestions, which could help improve our manuscript. Thank you.\n\n1.\t**Q**: comparison with existing related methods on generating adversarial perturbation (AdvGAN, AI-GAN).\n\n    **A**: All the perturbation-based adversarial attacks can be formulated as:\n    $$ min\\|\\delta\\|_p, s.t., f(x+\\delta) \\neq y, $$\n    where $y$ is the true label of $x$. Both AdvGAN and AI-GAN aim to train a generator that can craft adversarial perturbation $\\delta = G(x)$ ($x$ is an image):\n    $$ min_G |G(x)|_p, s.t., f(x+G(x)) = y_t \\neq y,$$\n    where $y$ is its label and $y_t$ is target label. It is consistent to the goal of perturbation-based adversarial attacks. \n\n    In contrast, AT-GAN aims to generate adversarial examples through modeling the distribution of adversarial examples by transferring a pre-trained generator (z is a noise):\n    $$ min_G |G(z,y) \u2013G_{ori}(z,y)|_p s.t. f( G(z,y) ) = y_t \\neq y.$$\n\n    The differences are: \n\n    a) Different input: AdvGAN and AI-GAN take natural images as input while AT-GAN takes random noise as input.\n\n    b) Different output: AdvGAN and AI-GAN output the adversarial perturbation for the input image while AT-GAN outputs the adversarial example directly.\n\n    c) Different training procedure: AdvGAN is similar to train a normal GAN and AI-GAN also considers the adversarial examples for training, while AT-GAN transfers a pre-trained generator to model the distribution of adversarial examples and do not need adversaries for transferring.\n\n    We agree that our method could not generate adversarial perturbation for a natural image, but the goal of our method is different. We aim to learn the distribution of the adversaries so that the output looks like a natural image but misclassified by the target model. Under such scenario, we could generate diverse adversaries that are not limited to the natural image. AdvGAN and AI-GAN also could not generate adversarial examples directly as we did. Moreover, generating non-constrained adversarial examples is harder and might be very useful in some scenarios. For instance, it can help implement adversarial training to improve the model robustness in few-shot learning.\n\n    Surely you could first borrow a normal GAN to generate image and then use this image to add perturbation by any perturbation based methods, not only AdvGAN, AI-GAN, but also any gradient based methods like FGSM, PGD. But this is out of the scope of this discussion. \n\n2.\t**Q**: comparison with the work of Song's.\n\n    **A**: Song's method searches over the neighborhood of the input noise for the pre-trained AC-GAN in order to find a noise whose output image is misclassified by the target classifier. Their method is essentially based on search, while AT-GAN is trained as an adversarial generative model. The generating capability of both Song\u2019s and ours rely on the GAN. We could also implement AT-GAN on other well-designed GANs for other datasets. Addressing your concern, we implement AT-GAN on CIFAR-10 dataset using StyleGAN2-ada (StyleGAN2 with adaptive discriminator augmentation) [1], a recently proposed conditional GAN. The target classifier is wide ResNet w32-10 [2] by normal training (Nor.) and Iterative training (Iter.). The attack success rates are as follows:\n\n    |   Model  |  PGD   |  FGSM  | AT-GAN |\n    |:--------:|:------:|:------:|:------:|\n    |  Nor.(%)   |  100.0 |  92.3  |  93.5  |\n    | Iter.(%)  |  54.6  |  49.2  |  73.0  |\n\n    On normally trained models, PGD achieves attack success rate of 100% while AT-GAN achieves attack success rate of 93.5%. However, the adversarially trained model exhibits little robustness against AT-GAN and AT-GAN achieves attack success rate of 73.0%. In Figure 5 in the Appendix D, we illustrate some generated adversarial examples on CIFAR-10 dataset. Thank you for the valuable suggestion. \n\n    [1] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila. Training Generative Adversarial Networks with Limited Data. NeurIPS 2020.\n\n    [2] Sergey Zagoruyko, Nikos Komodakis. Wide Residual Networks. BMVC 2016.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4HGL3H9eL9U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3499/Authors|ICLR.cc/2021/Conference/Paper3499/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836916, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment"}}}, {"id": "8x3IC1ZHGB0", "original": null, "number": 3, "cdate": 1606273914406, "ddate": null, "tcdate": 1606273914406, "tmdate": 1606275207584, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "mfwKnrx953P", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We appreciate the positive remarks that greatly encourage us, and the valuable suggestion made by the reviewer that have helped to improve the quality of our paper in the revised version. \n\n1.\t**Q**: The reasons of using AC-GAN and WGAN-GP as the pre-training stage.\n\n    **A**: There are two main reasons for adopting AC-GAN and WGAN-GP in the pre-training stage for our AT-GAN implementation. 1) In the literature, the combination of AC-GAN and WGAN-GP could build a powerful generative model and can craft realistic images on the evaluated datasets. 2) Song et al. [1] also utilize the same combination, and we follow their experimental setting for the fair comparison. \n\n    But AT-GAN is not limited to the above GANs. Actually, all conditional GANs that can craft realistic examples could be used for the implementation of AT-GAN in the pre-training stage. For instance, we add experiments on CIFAR-10 using StyleGAN2-ada (StyleGAN2 with adaptive discriminator augmentation) [2], and illustrate some generated examples in Appendix D.3. We have clarified it in the revision.\n\n    [1] Yang Song, Rui Shu, Nate Kushman, Stefano Ermon. Constructing Unrestricted Adversarial Examples with Generative Models. NeurIPS 2018.\n\n    [2] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila. Training Generative Adversarial Networks with Limited Data. NeurIPS 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4HGL3H9eL9U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3499/Authors|ICLR.cc/2021/Conference/Paper3499/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836916, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment"}}}, {"id": "zYPa7TeDoq4", "original": null, "number": 5, "cdate": 1606274435578, "ddate": null, "tcdate": 1606274435578, "tmdate": 1606274435578, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "pZPq2jKaLAw", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment", "content": {"title": "Response to Review #1 (part 2/2)", "comment": "2.\t**Q**: The idea seems incremental. The novelty could be further summarized by highlighting the difference with most related works including but not limited to the aforementioned ones.\n\n    **A**: AT-GAN aims to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries by transferring a pre-trained GAN, and there are no adversarial examples involved during the training. Once transferred, AT-GAN can directly generate adversarial examples from any input noise. \n\n    Here we highlight the differences with most related works as follows:\n\n    + **NAG, AdvGAN and AI-GAN vs. AT-GAN.** NAG [1], AdvGAN [2] and AI-GAN [3] focus on crafting adversarial perturbations by GANs. NAG [1] takes random noise as input and crafts image-agnostic adversarial perturbation. Such perturbation can be added to many natural images to craft the adversaries. AdvGAN [2] and AI-GAN [3] both use natural images as inputs, and generate the corresponding adversarial perturbations using GAN for the input image. AdvGAN fixes the target class for the generation, while AI-GAN uses projected gradient descent (PGD) attack to inspire the training of GAN and the target class is used as an input. In contrast, AT-GAN does not use any natural image as the input, but generates the adversaries directly from any random noise. Further, compared with AI-GAN, we do not use adversarial examples for the training. \n\n    + **Song's vs. AT-GAN.** Song's method [4] searches over the neighborhood of the input noise for the pre-trained AC-GAN in order to find a noise whose output image is misclassified by the target classifier. They define such adversaries as the unrestricted adversarial examples, however, their adversaries are still constrained by the original input noise. Their method is essentially based on search, while AT-GAN is trained as an adversarial generative model and our output is not constrained by any neighborhood. \n\n    + **PS-GAN vs. AT-GAN** PS-GAN [5] pre-processes an input seed patch (a meaningful small image) to adversarial patch that will be added to a natural image (such as a traffic sign) to craft an adversarial example, and an attention model is used to locate the attack area on the natural image. Their method uses GAN to generate meaningful adversarial patch based on the original patch, and paste that patch on a natural image. Though GAN is involved, their task is very different from ours.\n\n    In summary, existing works either are based on a search in a neighborhood of the input, or use a generative model to generate the perturbations or patches which will then be added to a natural image. Differs to existing works, we aim to model the distribution of adversarial examples by transferring a pre-trained GAN and generate non-constrained adversarial examples directly and quickly from any random noise. \n\n    We have highlighted the differences and make it clearer in the revision.\n\n    [1] Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu. NAG: Network for Adversary Generation. CVPR 2018.\n\n    [2] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song. Generating Adversarial Examples with Adversarial Networks. IJCAI 2018.\n\n    [3] Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li. AI-GAN: Attack-Inspired Generation of Adversarial Examples. arXiv Preprint arXiv:2002.02196, 2020.\n\n    [4] Yang Song, Rui Shu, Nate Kushman, Stefano Ermon. Constructing Unrestricted Adversarial Examples with Generative Models. NeurIPS 2018.\n\n    [5] Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie, Dacheng Tao. Perceptual-Sensitive GAN for Generating Adversarial Patches. AAAI 2019.\n \n3.\t**Q**: Some experiment settings are not clear. A brief introduction to Model A to B should be given in the main paper, though the details is provided in Appendix.\n\n    **A**:  We make the experiment settings clearer, and add a brief introduction of Model A to D to the main paper in the revision. Thank you again for the clarity check.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4HGL3H9eL9U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3499/Authors|ICLR.cc/2021/Conference/Paper3499/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836916, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment"}}}, {"id": "pZPq2jKaLAw", "original": null, "number": 4, "cdate": 1606274304012, "ddate": null, "tcdate": 1606274304012, "tmdate": 1606274304012, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "d24gIqvaa8x", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment", "content": {"title": "Response to Review #1 (part 1/2)", "comment": "We appreciate the reviewer\u2019s constructive suggestions and have performed the corresponding revisions. Below, we would like to address your main concerns.\n\n1.\t**Q**: It is expected to see the performance and generated examples with different $\\rho$.\n\n    **A**: We add experiments to investigate the impact of using different $\\rho$ in the loss function. As $\\rho$ could be constrained by both $\\ell_0$ and $\\ell_\\infty$ norm, we test various bounds, using Model A on MNIST dataset, for $\\rho$ in $\\ell_0$ and $\\ell_\\infty$, respectively.\n\n    a) We first fix $\\|\\rho\\|_\\infty=0.5$ and try various values for $\\|\\rho\\|_0$, i.e. 0, 100, 200, 300, 400 (the maximum possible value is 784 for 28$\\times$28 input). The attack success rates (ASR) are as follows:\n\n   $\\|\\rho\\|_0$|0|100|200|300|400\n    -|-|-|-|-|-\n    ASR(%)|98.9|98.8|98.7|96.7|95.8\n\n    We can observe that different values of $\\|\\rho\\|_0$ only have a little impact on the attack success rates, and the performances are very close for $\\|\\rho\\|_0$ = 0, 100, 200. Figure 6 in Appendix D further illustrates some generated adversarial examples, among which we can see there exist some slight differences on the examples. When $\\|\\rho\\|_0=0$, AT-GAN tends to change the foreground (body) of the digits. When we increase the value of $\\|\\rho\\|_0$ (100 and 200), AT-GAN is more likely to add tiny noise to the background and the crafted examples are more realistic to humans (for instance, smoother on digit 4). But if we continue to increase $\\|\\rho\\|_0$ (300 or 400), AT-GAN tends to add more noise and the quality of the generated examples decays. To have a good tradeoff on attack performance and generation quality, we set $\\|\\rho\\|_0=200$.\n\n    b)\tWe then fix $\\|\\rho\\|_0=200$ and test different values for $\\|\\rho\\|_\\infty$, i.e. 0, 0.1, 0.2, 0.3, 0.4, 0.5 (the maximum possible value is 1). The attack success rates (ASR) are as follows:\n\n    $\\|\\rho\\|_\\infty$|0|0.1|0.2|0.3|0.4|0.5\n    -|-|-|-|-|-|-\n    ASR(%)|98.9|99.2|98.9|98.9|98.9|98.7\n\n    We can observe that different values of $\\|\\rho\\|_\\infty$ have very little impact on the attack performance. Figure 7 in Appendix D further illustrates some generated adversarial examples, among which we can see that a little bit more noises are added for bigger $\\|\\rho\\|_\\infty$ but the differences are very tiny when $\\|\\rho\\|_\\infty = 0.2$ to 0.5. So we simply set $\\|\\rho\\|_\\infty=0.5$ in experiments, but other values of $\\|\\rho\\|_\\infty$ (0.2, 0.3, 0.4) also work. \n\n    We have added it in the revision. Thank you again for the valuable suggestion!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4HGL3H9eL9U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3499/Authors|ICLR.cc/2021/Conference/Paper3499/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836916, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Comment"}}}, {"id": "9qJB45VB5iR", "original": null, "number": 1, "cdate": 1603185748874, "ddate": null, "tcdate": 1603185748874, "tmdate": 1605023989607, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review", "content": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "review": "This paper is to train a generative neural networks that can output adversarial examples. The main idea is to first train a normal GAN and then use the idea of transfer learning based on adversarial examples. The aim sounds good but the authors fail to clearly distinguish the idea with the exiting related methods theoretically or numerically. The idea of transferring is good (although not new), but after checking the implementation details, I have to say in the current version, the fact of transferring is quite limited.  \n\nDetails:\n+ the idea of generating adversarial examples by a trained GAN is interesting.\n+ the writing is quite clear.\n-  lack of comparison with existing related methods. \n   Consider the core formulation, namely (2), which well describes the idea of this authors. But it is necessary to consider the following ideas: \n\n   1). generating adversarial permutation (AdvGAN, AI-GAN):  min_G \\| G(z,y) \\|_p, s.t., f(z+G((z,y)) = y_t \\neq y_s.\n   It is to train the difference of G_original and G_attack and I think in the training aspects, this is almost equal to the proposed idea. The authors try to argue that the proposed model does not require an input. But in my opinion, no input is a disadvantage:  if only adversarial examples are needed, AdvGAN etc. can feed an random input to original GAN and then add perturbations; but if one wants to attack a specific image, the proposed method will fail.  \n\n   2). attack a GAN to generate adversarial examples (Song's): min_z' \\|z - x\\|, s.t., f(G(z,y)) \\neq f(G(z'),y). \n   The author may argue the Song's attack procedure takes longer time. However, the there is no training time additionally needed . Moreover, I guess the generating capability of Song's idea, which relies on the GAN and there are many well-designed ones, is better than the proposed one.  I would like to see the generating performance of the proposed method on more complicated datasets, e.g., on CIFAR or other HIGH-RESOLUTION images. Another good point of Song's idea is that almost all the attacks on images could be parallelly used. I do not know whether its ASR could be easily improved. \n\n- The idea of transferring the original GAN to the attacking one is interesting. However, except of using the original GAN as the starting point, I cannot find other facts of \"transferring\". I would like to know if transferring learning technique could be used to reduce the number of required adversarial examples. \n\n- The attack transferbility has not been tested. Since there is adversarial samples involved, the obtained GAN is expected to be related to the victim model. \n\nAdditional questions, mainly for the experiments' result \n1. It is good that attack performance on adversarial trained NN is included. But where the adversarial examples come from? Are the examples are generated by AT-GAN?\n\n2. How many examples and time are needed to train the AT-GAN?\n\n3. Since the GAN has been changed, how about the generating capability, i.e., generating failure ratio of the AT-GAN should be reported. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074733, "tmdate": 1606915763461, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3499/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review"}}}, {"id": "mfwKnrx953P", "original": null, "number": 2, "cdate": 1603631740569, "ddate": null, "tcdate": 1603631740569, "tmdate": 1605023989545, "tddate": null, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "invitation": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review", "content": {"title": "A meaningful solution to find non-constrained adversarial examples by using adversarial transfer on generative adversarial net", "review": "This paper proposed the adversarial transfer on generative adversarial net (AT-GAN) to train an adversarial generative model that can directly produce adversarial examples. In the other way, AT-GAN could generate the adversarial examples directly for any input noise. Such a generative model was able to draw non-constrained adversarial examples.\n\nPros: \nThis paper is clearly written with reasonable paper organization covering background, model design, mathematical formula and experiments. The goal of this work is obvious with experimental justification. Mathematical description and experimental illustration are desirable to show the merit of this method.\n\nCons: \nThe reasons of using AC-GAN and WGAN-GP as the pre-train stage are missing.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3499/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3499/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AT-GAN: An Adversarial Generative Model for Non-constrained Adversarial Examples", "authorids": ["~Xiaosen_Wang1", "~Kun_He1", "~Chuanbiao_Song2", "~Liwei_Wang1", "~John_E._Hopcroft1"], "authors": ["Xiaosen Wang", "Kun He", "Chuanbiao Song", "Liwei Wang", "John E. Hopcroft"], "keywords": ["adversarial examples", "adversarial attack", "generation-based attack", "adversarial generative model", "non-constrained adversarial examples"], "abstract": "With the rapid development of adversarial machine learning, numerous adversarial attack methods have been proposed. Typical attacks are based on a search in the neighborhood of input image to generate a perturbed adversarial example. Since 2017, generative models are adopted for adversarial attacks, and most of them focus on generating adversarial perturbations from input noise or input image. Thus the output is restricted by input for these works. A recent work targets unrestricted adversarial example using generative model but their method is based on a search in the neighborhood of input noise, so actually their output is still constrained by input. In this work, we propose AT-GAN (Adversarial Transfer on Generative Adversarial Net) to train an adversarial generative model that can directly produce adversarial examples. Different from  previous works, we aim to learn the distribution of adversarial examples so as to generate semantically meaningful adversaries. AT-GAN achieves this goal by first learning a generative model for real data, followed by transfer learning to obtain the desired generative model. Once trained and transferred, AT-GAN could generate adversarial examples directly and quickly for any input noise, denoted as non-constrained adversarial examples. Extensive experiments and visualizations show that AT-GAN can efficiently generate diverse adversarial examples that are realistic to human perception, and yields higher attack success rates against adversarially trained models.\n\n ", "one-sentence_summary": "We propose to train an adversarial generative model called AT-GAN that aims to learn the distribution of adversarial examples, and can directly produce adversarial examples once trained.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|atgan_an_adversarial_generative_model_for_nonconstrained_adversarial_examples", "supplementary_material": "/attachment/ebde588d468e467cfea7f8052198fa191c3aaf36.zip", "pdf": "/pdf/d82ee47f908c32573b75d681aa950a0b82aab0b0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2GnmjM7e8v", "_bibtex": "@misc{\nwang2021atgan,\ntitle={{\\{}AT{\\}}-{\\{}GAN{\\}}: An Adversarial Generative Model for Non-constrained Adversarial Examples},\nauthor={Xiaosen Wang and Kun He and Chuanbiao Song and Liwei Wang and John E. Hopcroft},\nyear={2021},\nurl={https://openreview.net/forum?id=4HGL3H9eL9U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4HGL3H9eL9U", "replyto": "4HGL3H9eL9U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3499/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074733, "tmdate": 1606915763461, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3499/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3499/-/Official_Review"}}}], "count": 10}