{"notes": [{"id": "B1x3EgHtwB", "original": "ryl7OvlKPH", "number": 2262, "cdate": 1569439795642, "ddate": null, "tcdate": 1569439795642, "tmdate": 1577168237702, "tddate": null, "forum": "B1x3EgHtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_DaNnE7JU", "original": null, "number": 1, "cdate": 1576798744669, "ddate": null, "tcdate": 1576798744669, "tmdate": 1576800891494, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Decision", "content": {"decision": "Reject", "comment": "The paper develops linear over-parameterization methods to improve training of small neural network models. This is compared to training from scratch and other knowledge distillation methods. \n\nReviewer 1 found the paper to be clear with good analysis, and raised concerns on generality and extensiveness of experimental work. Reviewer 2 raised concerns about the correctness of the approach and laid out several other possibilities. The authors conducted several other experiments and responded to all the feedback from the reviewers, although there was no final consensus on the scores.\n\nThe review process has made this a better paper and it is of interest to the community. The paper demonstrates all the features of a good paper, but due to a large number of strong papers, was not accepted at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708354, "tmdate": 1576800256754, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Decision"}}}, {"id": "Hkef7hN4qH", "original": null, "number": 2, "cdate": 1572256793770, "ddate": null, "tcdate": 1572256793770, "tmdate": 1574115496481, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposes linear over-parameterization methods to improve training of small neural network models. The idea is simple -- each linear transformation in a network is overparameterized by a series of linear transformation which is algebraically equivalent to the original linear transformation. Number of experiments are conducted to show the effectiveness of the approach.\n\nThe proposed method is a simple application of over-parameterization to improve neural network model training. The motivation is clear and the proposed method is clearly presented. The paper is easy to understand and follow. Great analyses on the training behavior and generalization ability are conducted. Given the simplicity of the method, this could be a standard way of training small neural network model if the effectiveness of the method is observed more widely.\n\nThese are some concerns on the paper:\n\n1) The effectiveness of the approach is not necessarily significant in all experiments. For example, in Table 1, ExpandNet-FC and ExpandNet-CL were not effective. The same trend is observed in Table 2 for 400 epochs. Given that only ExpandNet-CK improves the performance, we could conclude that some intrinsic property of CK is important than over-parameterization. The good results for 90 epochs in Table 2 may mean linear over-parameterization yields faster convergence as suggested by Arora et al. 2018.\n\n2) The comparisons are not extensive. For example, we do not see Init for all models and \"w/ KD\" for ShuffleNetV2 in Table 2. Table 2 has \"N/A\". Table 3 and 4 do not have results with \"w/ KD\".  Knowledge transfer methods should be the baseline of the paper.\n\nMinor comments:\n\nIt will be interesting to see the results of the models used for Init.\n\nIt might be interesting to conduct experiments with a big model and see if we do not have any gains.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576619528917, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Reviewers"], "noninvitees": [], "tcdate": 1570237725358, "tmdate": 1576619528933, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Review"}}}, {"id": "Syl-HS8sjS", "original": null, "number": 5, "cdate": 1573770552854, "ddate": null, "tcdate": 1573770552854, "tmdate": 1573823165430, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "SyxThYwDYS", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment", "content": {"title": "Response to Review #3 (Part 1/2)", "comment": "\nThank you for your insightful and valuable review. We address your comments below and have modified our paper accordingly. We would appreciate any further feedback. \n\nQ: Without non-linearity, what is the added value and how we have better results?\n\nOver-parameterization has been shown both theoretically and empirically to facilitate neural network training. We believe this to be exactly our contribution: discovering a simple but effective method that takes advantage of over-parameterization during training by expanding layers, while this over-parameterization is not necessary at inference so that we can compress the better trained expanded networks back to original ones without losing any performance.  Note that it is not the same as naively adding new layers to the network with nonlinearity, which would give no direct way to do a lossless compression afterwards. This is a major contribution since no one has studied this aspect before.\n\nAs suggested, we conducted additional experiments and analysis to reject the proposed hypotheses and further evidence that the improvement is due to our expansion strategies.\n\n1) Q:  The improvements are possibly from implicit nonlinear operations between expanded layers.\n\nWe conducted experiments to compare the representation ability of float (32 bit) and double (64 bit) precision and to simulate the nonlinearity arising from truncation between expanded layers. Based on the experimental setting of Table 1, but with convolutional kernel size = 5, we trained a small network with float and double as baseline, and then trained an ExpandNet-CK with r=4 in float. In addition, we truncated the small network in double precision to float during training, to simulate the nonlinearity arising when multiplying 2 floats into 1 float. To be precise, we used double  to compute (W*x + b), and then converted the result to float to truncate the output feature maps. \n\n--------------------------------------------------------------------------------------------\n   Network                              |     CIFAR-10      |    CIFAR-100    \n--------------------------------------------------------------------------------------------\n   SmallNet(float)\t\t\t|   78.94 \u00b1 0.40   |   47.33 \u00b1 0.46\n   SmallNet(double)\t\t|   78.73 \u00b1 0.48   |   46.28 \u00b1 0.50\n   SmallNet(double) with\n   truncated non-linearity\t|   78.98 \u00b1 0.13   |   46.21 \u00b1 0.94\n   ExpandNet-CK (float)    \t|   79.90 \u00b1 0.38   |   48.26 \u00b1 0.14\n--------------------------------------------------------------------------------------------\n\nAs shown by the results reported in the table above, the networks with double precision do not outperform those with float precision. Furthermore, the nonlinearity obtained by truncation does not help during training. Our ExpandNet-CK consistently outperforms SmallNets with float or double or truncation. At test time, the outputs from an (uncompressed) ExpandNet and one that was compressed back to the SmallNet architecture are equal, up to precision error, which indicates that truncation does not affect the test performance.\n\nThus we can conclude that our improvement is not due to higher numerical precision or the implicit truncation nonlinearity arising from taking the products of several floats in the expanded linear layers.\n\n2) Q: Initialize the compact networks with the linear product of the well-trained non-linear counterpart of the expanded network.\n\nThis is an interesting suggestion. Based on the same experimental setting as in Table 1, we used the well-trained nonlinear counterparts to initialize the compact networks by computing the linear product of the expanded layers before training as suggested. The compact networks initialized using nonlinear ExpandNet-CL+FC achieve 77.02 \u00b1 0.35% (vs 79.98 \u00b1 0.28% using our initialization) on CIFAR-10 and 39.39 \u00b1 1.08% (vs 47.98 \u00b1 0.48% using our initialization) on CIFAR-100. Those initialized using nonlinear ExpandNet-CK+FC achieve 75.81 \u00b1 0.34% (vs 80.81 \u00b1 0.27% using our initialization) on CIFAR-10 and 34.88 \u00b1 1.41% (vs 49.82 \u00b1 0.25% using our initialization) on CIFAR-100. \n\nBoth experiments reveal that, while our initialization helps to improve our ExpandNet, using it to initialize the compact network is not effective.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x3EgHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2262/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2262/Authors|ICLR.cc/2020/Conference/Paper2262/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143964, "tmdate": 1576860552157, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment"}}}, {"id": "rkgS998soH", "original": null, "number": 6, "cdate": 1573771916781, "ddate": null, "tcdate": 1573771916781, "tmdate": 1573821612967, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "SyxThYwDYS", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment", "content": {"title": "Response to Review #3 (Part 2/2)", "comment": "\n\n3) Q: Initialize the compact networks with the linear product of the expanded networks.\n\t\nWe tried this before and found that it was not the reason behind the improvements. Following the experimental setting of Table 1 and Table 2, we initialized the compact networks with different expansion strategies on CIFAR-10, CIFAR-100 and ImageNet, respectively. The results are given in the table below.\n\n-------------------------------------------------------------------------------------------------------------------------\n   Network                            | Initialization       \t           |       CIFAR-10        |     CIFAR-100   \n-------------------------------------------------------------------------------------------------------------------------\n   SmallNet                            | Normal                           |     78.63 \u00b1 0.41     |    46.63 \u00b1 0.27\n   SmallNet                            | ExpandNet-FC               |     79.09 \u00b1 0.56     |    46.52 \u00b1 0.36\n   SmallNet                            | ExpandNet-CL               |     78.65 \u00b1 0.36     |    46.65 \u00b1 0.47\n   SmallNet                            | ExpandNet-CL+FC         |     78.81 \u00b1 0.52     |    46.43 \u00b1 0.72\n   SmallNet                            | ExpandNet-CK               |     78.84 \u00b1 0.30     |    46.56 \u00b1 0.23\n   SmallNet                            | ExpandNet-CK+FC        |     79.27 \u00b1 0.29     |    46.62 \u00b1 0.29\n   ExpandNet-CK+FC(ours) | Normal                           |     80.31 \u00b1 0.27     |    48.62 \u00b1 0.47\n-------------------------------------------------------------------------------------------------------------------------\n\nFrom these results and results in Table 1, we can see that, on CIFAR-10, compact networks initialized by Expand-FC and Expand-CL yield slightly better results than training the ExpandNets by normal initialization. However, the same trend does not occur on CIFAR-100 and ImageNet, where, with ExpandNet-CL initialization, MobileNet (90 epochs) gets 66.44% (vs 66.48% for the baseline and 69.40% for the ExpandNet-CL), MobileNetV2 (90 epochs) gets 63.07% (vs 63.75% for the baseline and 65.62% for the ExpandNet-CL) and ShuffleNet gets 56.91% (vs 56.89% for the baseline and 57.38% for the ExpandNet-CL). Moreover, compact networks initialized by ExpandNet-CK always yield worse results than training ExpandNet-CKs from scratch. \n\nTherefore, we believe that the benefits of our expansion strategies cannot be obtained by initialization using ExpandNets.\n\n2) and 3)  are great points that we have added them to our paper in Appendix E . Thanks again for these suggestions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x3EgHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2262/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2262/Authors|ICLR.cc/2020/Conference/Paper2262/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143964, "tmdate": 1576860552157, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment"}}}, {"id": "SyxeqmIjiB", "original": null, "number": 4, "cdate": 1573770120168, "ddate": null, "tcdate": 1573770120168, "tmdate": 1573817958192, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "Hkef7hN4qH", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment", "content": {"title": "Response to Review #2 (Part 2/2)", "comment": "    \n    1. 3) Q: The proposed method is a simple application of over-parameterization. The good results might yield from faster convergence of linear over-parameterization as suggested by Arora et.al. 2018.\n\nArora et al. 2018 only worked with linear models or linear layers. By contrast, we focus on practical, nonlinear, compact convolutional networks, and we propose to expand convolutional layers, which has not been studied before. Exploring how to expand convolutional layers is one of our contributions. \n\nIn their paper, Arora et al. performed a sanity test on MNIST with a CNN, by only expanding the fully-connected layers. According to our experiments, with Expand-FC ONLY, getting better results than the compact network is difficult.  In Appendix D, we perform a more thorough evaluation of the behavior observed by Arora et al. In short, the faster convergence they observed seems to be due to their use of a different regularizer, acting on the product of the parameter matrices of the expanded layers, rather than on the individual parameters. This, in turn, makes their model yield worse test error than the compact network, whereas our ExpandNets, which rely on standard regularization, achieve better results. See Appendix D of the revised paper for the detailed discussion.\n\n2) Missing results of 400 epochs and KD on ShuffleNetV2, Table 3 and Table 4.\n\nHere are the results of KD on ShuffleNet: ShuffleNet (w/KD) achieves 57.59% and ExpandNet-CL (w/KD) achieves 57.68% [ShuffleNet yields 56.89% and ExpandNet-CL 57.38%]. We are waiting for the results of 400 epochs. We will include these results in our paper when we get all of them.\n\nWe tend to disagree that knowledge transfer methods should be our main baselines. Our approach is complementary to knowledge transfer, and it can also be used on its own in the absence of teacher networks. In any event, Table 1 and 2 already indicate that, in most cases, baseline < baseline+KD < ExpandNet < ExpandNet+KD in terms of accuracy. The ShuffleNet results above confirm that the performance of our ExpandNets can be further boosted with the help of a teacher network.\n\nNote that using KD or knowledge transfer with YOLO and U-Net is not straightforward and has received very little attention so far. Doing so goes beyond the scope of this work.\n\n3) Initialize all models and apply the methods widely to big models.\n\nIn our experiments, we found that, on some datasets, the ExpandNets\u2019 nonlinear counterparts do not outperform the original models. Using these as initialization does not provide a good starting point. In other words, nonlinearity does not always help in deep networks and our initialization works much better when the baseline networks are quite small.\n\nWe did conduct some experiments on deeper and wider networks, but the improvements are not significant. As shown in Appendix A.4, Table 9, where we investigate the use of our Expand-CK on AlexNet with different number of channels, we found that the benefits decrease as the compact model size increases. This, we believe, further evidences that the benefits of our approach are due to over-parameterization.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x3EgHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2262/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2262/Authors|ICLR.cc/2020/Conference/Paper2262/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143964, "tmdate": 1576860552157, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment"}}}, {"id": "r1lZzyIoir", "original": null, "number": 2, "cdate": 1573768969333, "ddate": null, "tcdate": 1573768969333, "tmdate": 1573772124698, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment", "content": {"title": "Summary of Changes in New Version", "comment": "\nWe thank the reviewers for their valuable comments. We have revised our paper in the following way:\n\n1. As suggested by reviewer 2, we have added knowledge distillation for ShuffleNet in Table 2.\n\n2. As suggested by reviewer 2, we have added Appendix D to discuss the work of Arora et al. (2018) in detail and further highlight the differences with our work. \n\n3. As suggested by reviewer 3, we have added the analysis of our expansion strategies to Appendix E.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x3EgHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2262/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2262/Authors|ICLR.cc/2020/Conference/Paper2262/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143964, "tmdate": 1576860552157, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment"}}}, {"id": "H1lptzIjjB", "original": null, "number": 3, "cdate": 1573769861360, "ddate": null, "tcdate": 1573769861360, "tmdate": 1573771952254, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "Hkef7hN4qH", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment", "content": {"title": "Response to Review #2 (Part 1/2)", "comment": "\nThank you for your comments. We address your concerns below.\n\n1) Effectiveness of our approach.\n\nExpanding FC, CL or CK are different design choices of our approach, and we show them all in the paper for completeness. We demonstrate their effectiveness on 3 tasks (image classification, object detection, and semantic segmentation), using 5 different datasets  (ImageNet, PASCAL VOC, Cityscapes, CIFAR-10, CIFAR-100), and with 7 different compact network architectures (SmallNet3x3, SmallNet7x7, MobileNet, MobileNetV2, ShuffleNet, YOLO-LITE, U-Net). It is clear that our ExpaneNets are effective in all experimental settings. Although one can always run more experiments, this already constitutes strong evidence that our method works.\n\nWe nevertheless discuss your individual comments in more detail below.\n\n    1.1) Q: Expand-FC and Expand-CL are not significant in Table 1, and Table 2 for 400 epochs.\n\nTo be exact, ExpandNet-FC is the only configuration that does not achieve better results. However, it is merely one of our configurations. As shown in Appendix A.3 with multiple expansion rates and networks, ExpandNet-CL consistently outperforms the baselines. In addition, expanding FC and CL together consistently improves the training of small networks, as shown in Table 1. We also demonstrate the effectiveness of ExpandNet-CL on many large-scale image recognition tasks (classification using ImageNet; detection using Pascal VOC, segmentation using Cityscapes). This constitutes strong evidence that our approach is effective. \n\nFurthermore, as shown in Figure 2 and 3, ExpandNet-CL produces flatter minima and more zero-centered gradient cosine similarity, which indicates that ExpandNet-CL has a better training behavior and can reach solutions that generalize better. This is an important property of our convolutional expansion. \n\n\n    1.2) Q: Some intrinsic property of Expand-CK is more important than over-parameterization.\n\nWe conducted an additional ablation study for ExpandNet-CK to show that both ExpandNet-CK and over-parameterization are important.\n\nIn our method, the expansion rate (r) controls the number of parameters of ExpandNet-CK. Following the same experimental setting as for Table 1, where the baseline SmallNet achieves a top-1 accuracy of 78.63% on CIFAR-10 and 46.63% on CIFAR-100, we set the expansion rate in [0.25, 0.5, 0.75, 1.0, 2.0, 4.0], and report the corresponding results in the following table. For r < 1, the performance of ExpandNet-CK drops from 78.70% to 72.32% on CIFAR-10 and from 46.41% to 39.32% on CIFAR-100 as the number of parameters decreases. For r > 1, ExpandNet-CK yields consistently higher accuracy. Interestingly, with r = 1, ExpandNet-CK still yields better performance (79.22% vs 78.63% and 47.25% vs 46.63%) with fewer parameters (54.77K vs 66.19K and 60.62K vs 72.04K). \n\n--------------------------------------------------------------------------------------------\n   expansion rate |    #params (K)  |     CIFAR-10      |    CIFAR-100    \n--------------------------------------------------------------------------------------------\n   0.25                    |   37.91 / 43.76   |   72.32 \u00b1 0.62   |   39.23 \u00b1 0.84\n   0.50                    |   42.81 / 48.66   |   76.77 \u00b1 0.36   |   43.68 \u00b1 0.51\n   0.75                    |   48.43 / 54.28   |   78.70 \u00b1 0.42   |   46.41 \u00b1 0.52\n   1.00                    |   54.77 / 60.62   |   79.22 \u00b1 0.52   |   47.25 \u00b1 0.40\n   SmallNet           |   66.19 / 72.04   |   78.63 \u00b1 0.41   |   46.63 \u00b1 0.27\n   2.00                    |   87.32 / 93.17   |   79.97 \u00b1 0.18   |   48.13 \u00b1 0.42\n   4.00                    |  186.98 / 192.8  |   80.27 \u00b1 0.24   |   48.55 \u00b1 0.51\n--------------------------------------------------------------------------------------------\n(#params(K) denotes the number of parameters: CIFAR-10 / CIFAR-100)\n\nAltogether, these results indicate that, while ExpandNet-CK in itself helps to improve the results, combining it with over-parameterization further boosts the performance.\n\nWe also conducted an ablation study using different expansion rates and networks in Appendix A.3, Table 8, to investigate the importance of over-parameterization.\n\nNote that all the parameter numbers refer to the training parameters; at test time, our ExpandNets are compressed back, without any loss, so as to have the same number of parameters as the baseline compact network.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x3EgHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2262/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2262/Authors|ICLR.cc/2020/Conference/Paper2262/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143964, "tmdate": 1576860552157, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Authors", "ICLR.cc/2020/Conference/Paper2262/Reviewers", "ICLR.cc/2020/Conference/Paper2262/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Comment"}}}, {"id": "SyxThYwDYS", "original": null, "number": 1, "cdate": 1571416501218, "ddate": null, "tcdate": 1571416501218, "tmdate": 1572972361932, "tddate": null, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "invitation": "ICLR.cc/2020/Conference/Paper2262/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is extremely interesting and quite surprising. In fact, the major claim is that using a cascade of linear layers instead of a single layer can lead to better performance in deep neural networks. As the title reports, expanding layers seems to be the key to obtain extremely interesting results. Moreover, the proposed approach is extremely simple and it is well explained in Section 2 with equations (1) and (2). This paper can have a tremendous impact in the research in deep networks if results are well explained.\n\nHowever, in its present form, it is hard to understand why the claim is correct. In fact, the model presented in the paper has a major obscure point. Equation (1) and (2) are extremely clear. Without non-linear functions, equations (1) and (2) describe a classical matrix factorization like Principal Component Analysis. Now, if internal matrices have more dimensions of the rank of the original matrix, the product of the internal matrices is exactly the original matrix. Whereas, if internal matrices have a number of dimensions lower than the rank of the original matrix, these matrices act as filters on features or feature combination. Since the authors are using inner matrices with a number of dimensions higher than the number of dimensions of the original matrix, there is no approximation and, then, no selection of features or feature combinations. Hence, without non-linear functions, where is the added value of the method? How the proposed method can have better results. \nThere are some possibilities, which have not been explored:\n1) the performance improvement derives from the approximation induced by the representation of float or double in the matrices. The approximation act as the non-linear layers among linear layers.\n2) the real improvement seems to be given by the initialization which has been obtained by using the non-linear counterpart of the expansion; to investigate whether this is the case, the model should be compared with a compact model where the initialization is obtained by using the linear product of the non-linear counterpart of the expanded network. If this does not lead to the same improvement, there should be a value in the expansion.\n3) the small improvement of the expanded network can be given by the different initialization. In fact, each composing matrix is initialized randomly. The product of a series of randomly initialized matrices can lead to a matrix that is initialized with a different distribution where, eventually, components are not i.i.d.. To show that this is not relevant, the authors should organize an experiment where the original matrix (in the small network) is initialized with the dot product of the composing matrices. The training should be done by using the small network. If results are significantly different, then the authors can reject the hypothesis.\nIf the authors can reject (1), (2) and (3), they should find a plausible explaination why performance improves in their experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper2262/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2262/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shuxuan.guo@epfl.ch", "josea@nvidia.com", "mathieu.salzmann@epfl.ch"], "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks", "authors": ["Shuxuan Guo", "Jose M. Alvarez", "Mathieu Salzmann"], "pdf": "/pdf/941341ade81bd8ffe53e9af2baaf73c349ce5d48.pdf", "TL;DR": "This paper proposes linear expansion strategies building upon over-parameterization to facilitate practical compact network training. ", "abstract": "In this paper, we introduce a novel approach to training a given compact network. To this end, we build upon over-parameterization, which typically improves both optimization and generalization in neural network training, while being unnecessary at inference time. We propose to expand each linear layer of the compact network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can benefit from over-parameterization during training but can be compressed back to the compact one algebraically at inference. As evidenced by our experiments, this consistently outperforms training the compact network from scratch and knowledge distillation using a teacher. In this context, we introduce several expansion strategies, together with an initialization scheme, and demonstrate the benefits of our ExpandNets on several tasks, including image classification, object detection, and semantic segmentation. ", "keywords": ["Compact Network Training", "Linear Expansion", "Over-parameterization", "Knowledge Transfer"], "paperhash": "guo|expandnets_linear_overparameterization_to_train_compact_convolutional_networks", "original_pdf": "/attachment/051ba95e06e6a1b0e99e3daf9b6a7c1cca90dede.pdf", "_bibtex": "@misc{\nguo2020expandnets,\ntitle={ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks},\nauthor={Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x3EgHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x3EgHtwB", "replyto": "B1x3EgHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2262/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576619528917, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2262/Reviewers"], "noninvitees": [], "tcdate": 1570237725358, "tmdate": 1576619528933, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2262/-/Official_Review"}}}], "count": 9}