{"notes": [{"id": "HkeCbz2zAE", "original": null, "number": 5, "cdate": 1559572997940, "ddate": null, "tcdate": 1559572997940, "tmdate": 1559582763438, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "content": {"comment": "This is an awesome unifying approach between RL, EditDistance, and Imitation Learning.\nWill authors release the code as well?\nMeanwhile, I've been trying to implement the core part of the algorithm at https://github.com/SaeedNajafi/pytorch-ocd\n-- Thank You", "title": "code?"}, "signatures": ["~Saeed_Najafi1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Saeed_Najafi1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311711158, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkMW1hRqKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311711158}}}, {"id": "rkMW1hRqKX", "original": "rke4OqhcK7", "number": 963, "cdate": 1538087897472, "ddate": null, "tcdate": 1538087897472, "tmdate": 1546449228534, "tddate": null, "forum": "rkMW1hRqKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BklBdMH1lV", "original": null, "number": 1, "cdate": 1544667756574, "ddate": null, "tcdate": 1544667756574, "tmdate": 1545354505283, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Meta_Review", "content": {"metareview": "This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state-of-the-art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Exciting approach to training sequence-to-sequence models from scratch"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper963/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353018587, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353018587}}}, {"id": "rJe4U-Bcnm", "original": null, "number": 2, "cdate": 1541194060286, "ddate": null, "tcdate": 1541194060286, "tmdate": 1543863231409, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "content": {"title": "Exciting approach", "review": "The authors propose an alternative approach to training seq2seq models, which addresses concerns about exposure bias and about the typical MLE objective being different from the final evaluation metric. In particular, the authors propose to use a dynamic program to compute the optimal continuations of predicted prefixes (during training) in terms of edit distance to the true output, and then use a per-token cross entropy loss, with a target distribution that is uniform over all optimal next-tokens. The authors conduct a number of experiments, and show that this scheme allows them to attain state-of-the-art performance on end-to-end speech recognition, and that they can moreover do this without needing to pretrain the model with the MLE objective.\n\nThis is a very nice paper; it is generally well written, it gets excellent results, and it contains a comprehensive set of experiments and analysis. It is also quite exciting to see an approach to alleviating exposure bias that does not require pretraining with MLE. Accordingly, my suggestions mainly relate to the presentation and related work:\n\n - It seems a bit strange to argue that the proposed approach doesn't increase the time complexity over MLE. While technically true (as the authors note) if the vocabulary is bigger than the sequence lengths, the difference in (on policy) training time will presumably be felt when dealing with very long sequences, or with cases where the number of labels per time-step is small, like in character-level generation or in seq2seq style sequence labeling.\n\n - I think it's difficult to argue that the proposed approach isn't essentially a modification of imitation learning/learning-to-search algorithms like, say, AggreVaTe or LOLS (Chang et al., ICML 2015). As far as I can tell, the only differences are that cross entropy is used rather than a cost-sensitive classifier, and, perhaps, that the training is done in minibatches (with no aggregation).\n \n - Relatedly, while it is interesting that the loss uses all the optimal completion tokens, it should be noted that there is much work in transition-based parsing that adopts a learning-to-search approach and uses losses that incorporate multiple optimal next-predictions as given by a \"dynamic oracle\"; see Goldberg and Nivre (COLING, 2012) and others.  \n\n - I think it's also worth noting that training approaches like MIXER and others can target arbitrary rewards (and not just those where we can efficiently compute optimal next-steps), and so the proposed approach is a compelling competitor to MIXER-like approaches on problems such as machine translation or image captioning only to the extent that training with edit-distance is useful for such problems. Do you have a sense of whether training with edit-distance does indeed improve performance on such tasks?\n\nPros:\n- well written and interesting\n- good experiments, results, and analysis\n\nCons:\n - perhaps slightly more similar to previous work than is argued\n\n\nUpdate after author response: thanks for your response; I think the revised paper largely addresses my comments and those of the other reviewers, and I continue to hope it is accepted. Here are two small notes on the related work section of the revised paper:\n- In distinguishing OCD from DAgger, you note that the optimal policy is computed rather than provided at training time. In fact, structured prediction applications of SEARN (Daume III et al., 2009, which should also be cited) and DAgger often have this flavor too, such as when using them for sequence labeling (where optimal continuations are calculated based on Hamming distance).\n- Please include a reference to Goldberg and Nivre's (2012) dynamic oracle work.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "cdate": 1542234337161, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842595, "tmdate": 1552335842595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyg1e_DIRm", "original": null, "number": 9, "cdate": 1543038950657, "ddate": null, "tcdate": 1543038950657, "tmdate": 1543249744602, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Revision updates", "comment": "We are deeply grateful for the feedback we received. The paper has been revised to address all of the comments. The newly added parts of the paper are highlighted in green to enable easier comparison with the previous version. Our main updates are as follows:\n\n1) Improved and clarified the exposition of the objective function around Equation (6).\n2) Expanded and clarified the Related Work section and added the missing citations.\n3) Added the training & validation WER curves for MLE and OCD as Figure 3 to verify that even early on, OCD has stable training.\n\nWe are happy that the paper is well received by the research community so far.\n-- Authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}, {"id": "rkg1x8vI0m", "original": null, "number": 8, "cdate": 1543038438907, "ddate": null, "tcdate": 1543038438907, "tmdate": 1543038438907, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rJe4U-Bcnm", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Thank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.", "comment": "Dear Reviewer 3,\nThank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.\n\n> \u201cIt seems a bit strange to argue that the proposed approach doesn't increase the time complexity over MLE. While technically true (as the authors note) if the vocabulary is bigger than the sequence lengths, the difference in (on policy) training time will presumably be felt when dealing with very long sequences \u2026\u201d\n\nTheoretically, OCD and MLE have the same asymptotic time complexity, assuming vocabulary size is larger than the sequence length. When the vocabulary is small, but the sequences are long (e.g. genetic applications), OCD is less efficient than MLE. However, in terms of wall clock time, the OCD cost is amortized by the much larger neural net fprop/bprop computation time. Practically, sampling from the model introduces additional latency given that we will have to wait for previous tokens to be sampled. We have updated paper to state this more clearly and reference the Appendix, where we have provided a more detailed discussion of efficiency.\n\n> \u201cI think it's difficult to argue that the proposed approach isn't essentially a modification of imitation learning/learning-to-search algorithms like, say, AggreVaTe or LOLS (Chang et al., ICML 2015). As far as I can tell, the only differences are that cross entropy is used rather than a cost-sensitive classifier, and, perhaps, that the training is done in minibatches (with no aggregation)\u201d\n\nWe have updated the paper, especially the related work section, to make it more clear that OCD builds upon the imitation learning and learning to search algorithms. The use of mini-batches and cross entropy loss rather than cost sensitive classification are two important differences. As opposed to most imitation learning algorithms, we do not require pre-training or mixing with the expert policy. This removes a pesky tunable scheduling hyper-parameter. Another important difference is that most learning to search approaches (e.g. LOLS) resort to roll outs to approximate Q-values, and most imitation learning algorithms (e.g. AggreVaTe) assume availability of oracle completions. By contrast, we propose a dynamic programming algorithm to exactly and efficiently calculate optimal Q-values for edit distance. OCD demonstrates the applicability of imitation learning ideas to modern large-scale sequence-to-sequence problems and achieve state-of-the-art performance on speech recognition.\n\n> \u201c... it should be noted that there is much work in transition-based parsing that adopts a learning-to-search approach and uses losses that incorporate multiple optimal next-predictions as given by a \"dynamic oracle\"; see Goldberg and Nivre (COLING, 2012) and others\u201d\n\nThank you for the reference to related work on parsing. We have included the citation. Indeed, Goldberg & Nivre address a structure prediction problem based on a dynamic oracle, which is similar to our notion of optimal completions. That being said, they optimize a graph similarity metric using a margin-based loss, whereas OCD optimizes edit distance based on a KL loss. OCD is immediately applicable to parsing since Goldberg & Nivre can calculate \u03c0* for the corresponding graph similarity metric. This is an interesting direction for future work.\n\n > \u201cI think it's also worth noting that training approaches like MIXER and others can target arbitrary rewards (and not just those where we can efficiently compute optimal next-steps), and so the proposed approach is a compelling competitor to MIXER-like approaches on problems such as machine translation or image captioning only to the extent that training with edit-distance is useful for such problems.\u201d\n\nWe acknowledge that policy gradient formulations are more general. However, we believe that OCD is a viable replacement for MLE with better results, given its efficiency and ability to train from scratch. Hence, as of now for optimization of generic reward functions, our suggestion is to replace the MLE pretraining or mixing, with OCD pretraining or mixing. Our preliminary results suggest that EMBR training on top of OCD is more stable and faster than EMBR training on top of MLE. \n\f\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}, {"id": "SyxYgSvUC7", "original": null, "number": 7, "cdate": 1543038192998, "ddate": null, "tcdate": 1543038192998, "tmdate": 1543038192998, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "Hkx2Wz86h7", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Thank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.", "comment": "Dear Reviewer 2,\nThank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.\n\n> \u201cThe idea should also apply on many \u201cincremental rewards\u201d, for instance, BLEU scores in machine translation, etc. Do you have any comparison?\u201d\n\nYes, the main idea is applicable to other structured prediction problems as well. That being said, this paper focuses on edit distance for which exact computation of optimal Q-values is tractable. We are planning to extend OCD to other loss functions in the future.\n\n> \u201cWhat if the best suffix cannot be found using dynamic programming (when the evaluation metric is not edit-distance, but a sentence-level reward)?\u201d\n\nIf the optimal Q-values and optimal suffixes cannot be estimated using dynamic programming, then one can resort to bootstrapping based on Bellman optimality equations (e.g., Q-learning) to approximate optimal Q-values. Further, one can adopt ideas from OCD to consider concatenation of each prefix with various ground truth suffixes to lower bound optimal Q-values and speed up convergence.\n\n> \u201cCan the proposed algorithm be applied in other \u201cbigger\u201d tasks such as neural machine translation?\u201d\n\nOCD is applicable to large-scale datasets because the asymptotic complexity of OCD is the same as MLE. The Librispeech dataset comprises 1,000 hours of speech, we use 10000 tokens and is one of the largest publicly available speech recognition datasets. One can directly apply OCD with edit distance to machine translation, even though the evaluation metric is often BLEU score. \n> \u201cEq. (6) is not very clear. Do you first sample the whole sequence, and then supervise one token for each step? Or the whole suffix?\u201d\n\nThanks for the comment. We have improved the exposition around Eq. (6). Yes, we sample the whole sequence, and then each prefix receives the optimal target supervision in the form of one token extension. We did not rely on whole suffix supervision, as it is inefficient and requires some form of MLE again.\n\n> \u201cDo you have a comparison with the learning efficiency between MLE and OCD? Will it get unstable in the beginning of training as all the samples are wrong.\u201d\n\nWe added Figure 3 which shows the training and validation WER per training epoch. It shows that OCD is stable in the beginning of training (even if all of the sampled suffixes are wrong), while having a slightly slower curve. We emphasize that while the prefixes deviate from the ground truth initially, the target distribution is always optimal and just a re-alignment of ground truth sequence; consequently OCD can be trained from scratch without any need for MLE warm-starting.\n\n> \u201cMissing Reference: Ding & Soricut \u2026 This paper used a very similar idea as the proposed learning method which relies on incremental rewards to find the \u201coptimal\u201d suffix ...\u201c\n\nThanks for the reference. We acknowledge that previous work has discussed a temporal decomposition of terminal rewards based on the difference of optimal Q-values for consecutive state-action pairs (e.g., see Aggrevate [Ross & Bagnell, 2014]). We did not claim any credit for this. Ding et al. mention reward decomposition for the special case of ROUG score, but we cannot see a proper decomposition of rewards based on optimal Q-value differences in their paper. In our paper, we discuss the special case of edit distance, for which calculation of optimal Q-value differences is tractable. We updated the related work section and included a citation to Ding & Soricut.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}, {"id": "Syg5VVPUCm", "original": null, "number": 6, "cdate": 1543038001658, "ddate": null, "tcdate": 1543038001658, "tmdate": 1543038001658, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkeFupnrhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Thank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.", "comment": "Dear Reviewer 1,\nThank you for the valuable feedback and review. We have updated the paper accordingly. Our responses follow.\n\n> \u201cThis especially applies to the presentation of the approach around Eq. (6). It might not be straightforward for a reader to figure out how the tilde-sequences are obtained\u201d\n\nWe have improved the exposition around Eq. (6). Further, we have emphasised that the tilde-sequences are sampled i.i.d. from the output distribution corresponding to the model that is being optimised.\n\n> \u201cAs the objective is non-convex, in order to be able to reproduce the results it would be useful to provide some heuristics for choosing the initial solutions for the parameter vector.\u201d\n\nThe initial parameter vector (i.e., the weights of the neural network) are initialized via Xavier initialization [1]. Each weight is randomly drawn from a uniform distribution on the interval [-A, A], where A=sqrt(6. / (fan_in + fan_out)). We included the exact details (and hyper-parameters) in Appendix C. We have confirmed that running the optimization several times from different initializations results in similar results.\n\n[1]: Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}, {"id": "ryxxYXwUA7", "original": null, "number": 5, "cdate": 1543037815881, "ddate": null, "tcdate": 1543037815881, "tmdate": 1543037815881, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "BygnjVilT7", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Thanks, updated Related work accordingly.", "comment": "Thank you for the pointer. We have added a citation and included a discussion in the new version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}, {"id": "BygnjVilT7", "original": null, "number": 3, "cdate": 1541612708145, "ddate": null, "tcdate": 1541612708145, "tmdate": 1541612708145, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "content": {"comment": "Thanks for a nice paper. After reading your paper, I found it might be closer to the previous work in ASR in the following paper.\nLink:  http://www.mirlab.org/conference_papers/international_conference/ICASSP%202018/pdfs/0005839.pdf", "title": "Nicely suited fro ASR"}, "signatures": ["~Murali_Karthick_Baskar1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Murali_Karthick_Baskar1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311711158, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkMW1hRqKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311711158}}}, {"id": "Hkx2Wz86h7", "original": null, "number": 3, "cdate": 1541394948262, "ddate": null, "tcdate": 1541394948262, "tmdate": 1541533542568, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "content": {"title": "Interesting learning algorithms for autoregressive models without MLE pretraining", "review": "Quality and Clarity:\nThe writing is good and easy to read, and the idea is clearly demonstrated.\n\nOriginality:\nThe idea of never training over the ground-truth sequence, but training on sampled prefix and an optimized suffix is very novel. The similar idea is also related to imitation learning in other domains such as self-driving car where an oracle can give optimal instruction when exploring a new state. \n\nComments:\nThe paper proposed a very interesting training algorithm for auto-regressive models especially it does not require any MLE pre-training and can directly optimize from the sampling.\n\nHere are some questions:\n(1) The idea should also apply on many \u201cincremental rewards\u201d, for instance, BLEU scores in machine translation, etc. Do you have any comparison? What if the best suffix cannot be found using dynamic programming (when the evaluation metric is not edit-distance, but a sentence-level reward)?\n(2) Can the proposed algorithm be applied in other \u201cbigger\u201d tasks such as neural machine translation?\n(3) Eq. (6) is not very clear. Do you first sample the whole sequence, and then supervise one token for each step? Or the whole suffix?\n(4) Do you have a comparison with the learning efficiency between MLE and OCD? Will it get unstable in the beginning of training as all the samples are wrong.\n\n----------------------------\nMissing Reference:\nDing, Nan, and Radu Soricut. \"Cold-Start Reinforcement Learning with Softmax Policy Gradient.\" Advances in Neural Information Processing Systems. 2017.\n\nThis paper used a very similar idea as the proposed learning method which relies on incremental rewards to find the \u201coptimal\u201d suffix (for instance, edit-distance is a special example). It would be better to have some discussion,\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "cdate": 1542234337161, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842595, "tmdate": 1552335842595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkeFupnrhQ", "original": null, "number": 1, "cdate": 1540898161104, "ddate": null, "tcdate": 1540898161104, "tmdate": 1541533542157, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "content": {"title": "A nice twist to seq2seq models", "review": "The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation. In particular, a model trained in this way can be biased in the sense that training sequences typically have different sets of prefixes compared to test sequences. As a result, at the prediction time the model does not generalize well and for a given input sequence the decoder constructs a label sequence which reflects the training label sequences rather than the actual target label.\n\nTo address this shortcoming, the authors propose an approach based on edit distances and the implicit use of given label sequences during training. The main idea is to generate a label sequence with respect to the current parameter vector of a conditional probabilistic model (see Eqs. 2 & 3, as well as the objective in Eq. 6) and then based on the edit distance find the best possible completions for any prefix of that model-based label sequence. The training objective is then defined in Eq. (6): to each element in the output sequence the objective assigns the KL-divergence between a conditional distribution of the next element in the label sequence given a label prefix generated using the current model and the exponential family model based on edit distances given by the prefixes and optimal completions after the position of interest in the label sequence. The objective and the corresponding gradient can be computed efficiently using dynamic programming.\n\nIntuitively, the approach tries to find a parameter vector such that the decoder at a particular instance is likely to construct a label sequence with a small edit distance to the target label. As the training objective now considers all possible elements of the vocabulary given a prefix sequence, it is reasonable to expect that it performs better than MLE which only considers target vocabulary elements given target prefix sequences (e.g., compare Eqs. 3 & 6).\n\nThe experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art. I am not an expert in the field and cannot judge the related work objectively but can say that the context for their approach is set appropriately. I would, however, prefer more clarity in the presentation of the approach. This especially applies to the presentation of the approach around Eq. (6). It might not be straightforward for a reader to figure out how the tilde-sequences are obtained. As the objective is non-convex, in order to be able to reproduce the results it would be useful to provide some heuristics for choosing the initial solutions for the parameter vector. In Section 3, please also provide a reference to the appendix so that a reader can understand the conditional probabilistic model.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Review", "cdate": 1542234337161, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842595, "tmdate": 1552335842595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxAaU5s2X", "original": null, "number": 2, "cdate": 1541281477883, "ddate": null, "tcdate": 1541281477883, "tmdate": 1541281477883, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "B1gEfjSwhm", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "content": {"comment": "Thank you for the quick answer! Hope your paper gets in.\n-Lucas", "title": "Thanking the authors"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311711158, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkMW1hRqKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311711158}}}, {"id": "SkgkR9CV2X", "original": null, "number": 1, "cdate": 1540840135387, "ddate": null, "tcdate": 1540840135387, "tmdate": 1541039165238, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "rkMW1hRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "content": {"comment": "Hi, \n\nFirst of all, I'm a big fan of you paper. I thing it reads well and the idea is conceptually simple, yet powerful. I'm especially interested in applying OCD for natural language generation, and was wondering if you had experimented with word-level Seq2Seq models. There is evidence in the litterature [1] that exposure bias is more problematic in character-level vs word-level models, and I'm curious to see if OCD can lead to improvements for word-level models. \n\nThanks in advance!\n-Lucas\n\n[1] https://arxiv.org/abs/1610.09038", "title": "Word-level results"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311711158, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkMW1hRqKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311711158}}}, {"id": "B1gEfjSwhm", "original": null, "number": 1, "cdate": 1540999948145, "ddate": null, "tcdate": 1540999948145, "tmdate": 1540999948145, "tddate": null, "forum": "rkMW1hRqKX", "replyto": "SkgkR9CV2X", "invitation": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "content": {"title": "Word Vocabulary", "comment": "Hi Lucas,\n\nThank you for the comment and the interesting reference. In our Librispeech experiments, OCD significantly outperforms MLE when using a vocabulary of 10K tokens generated using Byte Pair Encoding (BPE). Many of these tokens are complete words. We expect similar gains over the MLE baseline when using a vocabulary of words. However, BPE has received more attention than word based encoding in the NLP community recently because of its seamless handling of unknown tokens.\n\nWe note that OCD can optimize word edit distance directly on a character-based model as well, by modifying the proposed dynamic programming algorithm. That being said, since OCD performs well on BPE, we think direct optimization of token edit distance based on BPE tokens is well suited for many real world applications.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper963/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.", "keywords": ["Sequence Learning", "Edit Distance", "Speech Recognition", "Deep Reinforcement Learning"], "authorids": ["sasabour@google.com", "williamchan@google.com", "mnorouzi@google.com"], "authors": ["Sara Sabour", "William Chan", "Mohammad Norouzi"], "TL;DR": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.", "pdf": "/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf", "paperhash": "sabour|optimal_completion_distillation_for_sequence_learning", "_bibtex": "@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper963/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614332, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkMW1hRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper963/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper963/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper963/Authors|ICLR.cc/2019/Conference/Paper963/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper963/Reviewers", "ICLR.cc/2019/Conference/Paper963/Authors", "ICLR.cc/2019/Conference/Paper963/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614332}}}], "count": 15}