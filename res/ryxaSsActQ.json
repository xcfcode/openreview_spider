{"notes": [{"id": "ryxaSsActQ", "original": "SJe_wP2FYm", "number": 128, "cdate": 1538087749104, "ddate": null, "tcdate": 1538087749104, "tmdate": 1545355397624, "tddate": null, "forum": "ryxaSsActQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.", "keywords": [], "authorids": ["wuyingting@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn", "wangrui.nlp@gmail.com"], "authors": ["Yingting Wu", "Hai Zhao", "Rui Wang"], "pdf": "/pdf/e4094f01a80524a4bc661cc502bf1a5c8aca2710.pdf", "paperhash": "wu|dual_skew_divergence_loss_for_neural_machine_translation", "_bibtex": "@misc{\nwu2019dual,\ntitle={Dual Skew Divergence Loss for Neural Machine Translation},\nauthor={Yingting Wu and Hai Zhao and Rui Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxaSsActQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgJmxwlg4", "original": null, "number": 1, "cdate": 1544740886882, "ddate": null, "tcdate": 1544740886882, "tmdate": 1545354514269, "tddate": null, "forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper128/Meta_Review", "content": {"metareview": "This paper proposes a new loss function that can be used in place of the standard maximum likelihood objective in training NMT models. This leads to a small improvement in training MT systems.\n\nThere were some concerns about the paper though: one was that the method itself seemed somewhat heuristic without a clear mathematical explanation. The second was that the baselines seemed relatively dated, although one reviewer noted that this seemed like a bit of a lesser concern. Finally, the improvements afforded were relatively small.\n\nGiven the high number of good papers submitted to ICLR this year, it seems that this one falls short of the acceptance threshold.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting, but heuristic idea. Experiments somewhat unconvincing."}, "signatures": ["ICLR.cc/2019/Conference/Paper128/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper128/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.", "keywords": [], "authorids": ["wuyingting@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn", "wangrui.nlp@gmail.com"], "authors": ["Yingting Wu", "Hai Zhao", "Rui Wang"], "pdf": "/pdf/e4094f01a80524a4bc661cc502bf1a5c8aca2710.pdf", "paperhash": "wu|dual_skew_divergence_loss_for_neural_machine_translation", "_bibtex": "@misc{\nwu2019dual,\ntitle={Dual Skew Divergence Loss for Neural Machine Translation},\nauthor={Yingting Wu and Hai Zhao and Rui Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxaSsActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper128/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353326657, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper128/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper128/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper128/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353326657}}}, {"id": "SJxB8-YThQ", "original": null, "number": 3, "cdate": 1541407053432, "ddate": null, "tcdate": 1541407053432, "tmdate": 1541534258264, "tddate": null, "forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "content": {"title": "An idea worth exploring but the paper has flaws.", "review": "The paper describes a new loss function for training, that can be\nused as an alternative to maximum likelihood (cross entropy), or\nas a metric that is used to fine-tune a model that is initially\ntrained using ML.\n\nExperiments are reported on the WMT 2014 English-German and\nEnglish-French test sets.\n\nI think this is an idea worth exploring but overall I would not\nrecommend acceptance. I have the following reservations:\n\n* I found much of the motivation/justification for the approach\nunconvincing - too heuristic and informal. What does it mean\nto \"overgeneralize\" or \"plunge into local optima\"? Can we say\nanything semi-formal about this alternative objective? \n\n* The improvements over ML are marginal, and there are a lot of moving\nparts/experimental settings in these models, i.e., a lot of\ntweaking. The results in tables 2 and 3 show a 0.36/0.34 improvement\nover ML using DSD. (btw, what is meant by \"DSD-deep\" or \"ML-deep\"? I'm\nnot sure these terms are explained?)\n\n* The comparison to related work is really lacking. The \"Attention is\nall you need\" paper (Vaswani et al.) reports 28.4/41.0 BLEU for these\ntest sets, respectively 3.4/5.96 BLEU points better than the results\nin this paper. That's a huge gap. It's not clear that the improvements\n(again, less than 0.5 BLEU points) will remain with a state-of-the-art\nsystem. And I think the paper is misleading in how it cites previous\nresults on these data sets - there is no indication in the paper that\nthese better results are in the literature.\n\nSome small things:\n\n* unplausible -> implausible\n\n* \"Husz (2015) showed that D(P || Q) is not identical to its inverse form\nD(Q || P)\" this is well known, predating 2015 for sure.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper128/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.", "keywords": [], "authorids": ["wuyingting@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn", "wangrui.nlp@gmail.com"], "authors": ["Yingting Wu", "Hai Zhao", "Rui Wang"], "pdf": "/pdf/e4094f01a80524a4bc661cc502bf1a5c8aca2710.pdf", "paperhash": "wu|dual_skew_divergence_loss_for_neural_machine_translation", "_bibtex": "@misc{\nwu2019dual,\ntitle={Dual Skew Divergence Loss for Neural Machine Translation},\nauthor={Yingting Wu and Hai Zhao and Rui Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxaSsActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "cdate": 1542234531691, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper128/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335655391, "tmdate": 1552335655391, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper128/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlTXPy5n7", "original": null, "number": 2, "cdate": 1541170980557, "ddate": null, "tcdate": 1541170980557, "tmdate": 1541534258047, "tddate": null, "forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "content": {"title": "Relatively low score on novelty but high score on experimental implementation", "review": "This paper presents a new loss objective for NMT. The main idea is to optimize an interpolation of KL(P|Q) and KL(Q|P), which is Kulback-Liebler Divergence computed at the word-level for model distribution Q and true distribution P. The motivation is that KL(P|Q) finds a Q that covers all modes of the data whereas KL(Q|P) finds a Q that concentrates on a single mode. So optimizing on the interpolation gets the best of both worlds. In my opinion, this is a relatively simple and known idea in ML (but perhaps not in MT? I'm not sure.) On the other hand, the NMT experiments are well-implemented and convincingly shows that it improves BLEU on a WMT dataset. \n\nIn general, the experiments look solid. I applaud the multiple baseline implementations, in particular even including the SMT baseline. The lack of transformer/CNN models is not a demerit in my opinion, since the focus is on loss objectives and the LSTM models are just as reasonable. \n\nThe paper is clearly written, with a few exceptions. It is not clear why you have to first train with ML before switching to the proposed DSD objective. As such, Section 4.5 should be prefaced with a motivation. Also, Figure 3 is hard to read with the two kinds of plots -- maybe split into two figures? \n\nAn open question is: does your model capture the issues of mode covering as mentioned in the motivation? It would be helpful to include analyses of the word-level distributions to quantify the differences (e.g. word entropy) between ML and various KL/DSD solutions. Also I would recommend showing train/test set perplexity scores of the various proposed and baseline methods. \n\nAs a minor point for argumentation: it is not clear that your proposal addresses the sequence-level loss vs word-level loss issue. It is conceivable, but it seems indirect and there is no quantifiable connection between the word-level loss (such as DSD) and a sequence-level loss. Or is there? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper128/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.", "keywords": [], "authorids": ["wuyingting@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn", "wangrui.nlp@gmail.com"], "authors": ["Yingting Wu", "Hai Zhao", "Rui Wang"], "pdf": "/pdf/e4094f01a80524a4bc661cc502bf1a5c8aca2710.pdf", "paperhash": "wu|dual_skew_divergence_loss_for_neural_machine_translation", "_bibtex": "@misc{\nwu2019dual,\ntitle={Dual Skew Divergence Loss for Neural Machine Translation},\nauthor={Yingting Wu and Hai Zhao and Rui Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxaSsActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "cdate": 1542234531691, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper128/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335655391, "tmdate": 1552335655391, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper128/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgORetD3m", "original": null, "number": 1, "cdate": 1541013711847, "ddate": null, "tcdate": 1541013711847, "tmdate": 1541534257778, "tddate": null, "forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "content": {"title": "Interesting idea, results are not quite there yet", "review": "This paper describes an alternative training objective to cross-entropy loss for sequence-to-sequence models. The key observation is that cross-entropy is minimizing KL(P|Q) for a data distribution P and a model distribution Q; they add another loss that minimizes the inverse KL(Q|P) to create their dual-skew divergence. The idea is tested in the context of neural MT, using a model similar to that proposed by Bahdanau et al. (2015) with results on English-to-French and English-to-German WNT 2014. In the context of beam search, improvements are small (<=0.5 BLEU) but statistically significant.\n\nThis is an interesting idea, and one I certainly wouldn\u2019t have thought of on my own, but I think it is currently lacking sufficient experimental support to warrant publication. The paper feels strangely dated, with most experiments on two-layer models, and only two citations from 2017. The experiments compare against an in-house maximum likelihood baseline (varying greedy-vs-beam search and model depth), and against a number of alternative training methods (minimum risk, scheduled sampling, RL) with numbers lifted from various papers. These latter results are not useful, as the authors (helpfully) point out that the baseline results in this paper are universally higher than the baselines from these other papers. Furthermore, it feels like methods designed to address exposure bias and/or BLEU-perplexity mismatch are not the right comparison points for this work, as it does not attempt to address either of these issues. I would instead be much more interested to see a comparison to label smoothing (Szegedy et al., 2015), which perhaps addresses some of the same issues, and which produces roughly the same magnitude of improvements. Also, the literature review should likely be updated to include Edunov et al., 2017. In general, the improvements are small (though technically statistically significant), the baseline models are somewhat shallow and the deltas seem to be decreasing as model depth grows, so it is hard to get too excited.\n\nSmaller concerns:\n\nFor Table 1, it would be helpful to explain why Baseline is not equal to \\Beta=1. With some effort, I figured out that this was due to the alpha term modifying the cross-entropy objective when \\Beta=1.\n\nIt would also be useful to tell us what \u201cswitching point\u201d was used for Table 1 and Figure 2.\n\nChristian Szegedy, Vincent Vanhoucke, SergeyIoffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. http://arxiv.org/abs/1512.00567.\n\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\u2019Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to sequence learning. In Proceedings of NAACL-HLT 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper128/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.", "keywords": [], "authorids": ["wuyingting@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn", "wangrui.nlp@gmail.com"], "authors": ["Yingting Wu", "Hai Zhao", "Rui Wang"], "pdf": "/pdf/e4094f01a80524a4bc661cc502bf1a5c8aca2710.pdf", "paperhash": "wu|dual_skew_divergence_loss_for_neural_machine_translation", "_bibtex": "@misc{\nwu2019dual,\ntitle={Dual Skew Divergence Loss for Neural Machine Translation},\nauthor={Yingting Wu and Hai Zhao and Rui Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxaSsActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper128/Official_Review", "cdate": 1542234531691, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxaSsActQ", "replyto": "ryxaSsActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper128/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335655391, "tmdate": 1552335655391, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper128/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}