{"notes": [{"id": "B1x1MerYPB", "original": "rJegPWgFvB", "number": 2158, "cdate": 1569439751076, "ddate": null, "tcdate": 1569439751076, "tmdate": 1577168289343, "tddate": null, "forum": "B1x1MerYPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3cdMnyN0iP", "original": null, "number": 1, "cdate": 1576798742005, "ddate": null, "tcdate": 1576798742005, "tmdate": 1576800894207, "tddate": null, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose using a noisy channel formulation which allows them to combine a sentence level target-source translation model with a language model trained over target side document-level information. They use reranking of a 50-best list generated by a standard Transformer model for forward translation and show reasonably strong results.  The reviewers were concerned about the efficiency of this approach and the limited novelty as compared to the sentence-level noisy channel research Yu et al. 2017. The authors responded in depth, adding results with another baseline which includes backtranslated data. I feel that although this paper is interesting, it is not compelling enough for inclusion in ICLR. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725452, "tmdate": 1576800277347, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Decision"}}}, {"id": "rye7MN6qjH", "original": null, "number": 5, "cdate": 1573733387359, "ddate": null, "tcdate": 1573733387359, "tmdate": 1573733387359, "tddate": null, "forum": "B1x1MerYPB", "replyto": "BJgcb06GsB", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thanks for your clarifications."}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1MerYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2158/Authors|ICLR.cc/2020/Conference/Paper2158/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145487, "tmdate": 1576860531575, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment"}}}, {"id": "Skxn36afjr", "original": null, "number": 1, "cdate": 1573211571905, "ddate": null, "tcdate": 1573211571905, "tmdate": 1573212414153, "tddate": null, "forum": "B1x1MerYPB", "replyto": "Sk97bk0tS", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment", "content": {"title": "Response-part 1", "comment": "Thank you for your review.\n\nRegarding the question about circumventing the data problem with back-translated documents. While this is a good idea, and there is evidence that it can work well (Junczys-Dowmunt, 2019), it is challenging to train such models well, whereas our model involves straightforward training procedures. Specifically, for back translation to succeed, monolingual data that will be back translated must be carefully selected, and, for good performance, you should filter likely bad translations; the ratio of back translated data and \u201creal\u201d data must be balanced, etc. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. We do have several results that we can add to the paper which we discuss here to convince you that our results are both interesting and \u201cgood\u201d.\n\nFirst, we did attempt to replicate the technique of Junczys-Dowmunt (2019), but found that in Chinese-English, it was difficult to learn a model that reliably generates the correct number of sentences (contra his findings), which makes a fair comparison challenging. But, to give some calibration for the relative power of back translation vs noisy channel modeling, we did generate a sentence-level proposal model on back translation and compare it to the performance of a sentence-level proposal model trained only on \u201creal\u201d parallel data:\n\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n|        |                        Model                                     | MT06 | MT03 | MT04 | MT05 | MT08 |\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n| 1     | Transformer baseline (q)                          | 49.40  | 49.42 | 50.11 | 48.76  | 41.58 |\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n| 2     | Backtranslation (q')                                    | 51.11 | 52.12 | 51.82 | 51.10  | 43.15 |\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n| 3     | Sent-reranker (using q as proposal)       | 52.25  | 52.21 | 52.35 | 51.28 | 44.27 |\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n| 4     | Doc-reranker (using q as proposal)        | 52.70  | 52.47 | 52.52 | 51.49 | 44.43 |\n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n|        | Sent-reranker + back translated              |            |            |            |            |            |\n| 5     | proposal (using q' as proposal)               |  52.95|  53.93 | 53.69 | 53.61  | 45.18 | \n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n|        | Doc-reranker + back translated               |            |            |            |            |            |\n| 6     | proposal (using q' as proposal)               |  53.56|  54.80 | 53.94 | 53.86  | 45.85 | \n+------+-------------------------------------------------------+---------+---------+---------+---------+---------+\n\nFrom these results, we see that while both techniques improve translation, i.e., both (2) and (3) are better than (1), sentence level back translation (2) is less effective than a noisy channel model reranker is (row 3), and, as we showed in the reviewed draft the doc-reranker is better again (row 4). Since we have a new model q\u2019, we can use it as a proposal model for our noisy channel reranker \u2014 effectively using the monolingual data twice. Happily, this improves results even further (rows 5-6). Thus, in addition to the challenges of making back translation work at all which we believe argues for the value of our model, we have evidence that (a) the noisy channel approach makes better use of monolingual data than back translation does; (b) using our inference strategy based on reranking samples from a proposal model, samples from a backtranslation-trained proposal model (q\u2019) can be improved further still, providing further evidence that the noisy channel model is well calibrated across a variety of qualities and that it picks up different things than backtranslation does. These results will be broadly of interest to the community, even if we haven\u2019t explored all imaginable back translation configurations. If you have a specific result that you think is particularly important to make this paper acceptable, please identify it, and we will run the comparison.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1MerYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2158/Authors|ICLR.cc/2020/Conference/Paper2158/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145487, "tmdate": 1576860531575, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment"}}}, {"id": "BJgcb06GsB", "original": null, "number": 3, "cdate": 1573211650102, "ddate": null, "tcdate": 1573211650102, "tmdate": 1573212090688, "tddate": null, "forum": "B1x1MerYPB", "replyto": "HJlp8ls19B", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review.\n\nRegarding the differences to Yu et al., 2017. While both papers indeed use a noisy channel decomposition, the novelty in this paper is the theoretical justification for training a model using only parallel sentences and monolingual documents, and then using it to infer document translations (an important task!). This asymmetry in available data is exactly the situation that exists in the world today, and our model, which addresses it directly and elegantly, will undoubtedly be of general interest. Moreover, while the Yu et al., 2017 model could be used on documents by concatenating their sentences to form a single long sequence, this would not let us use the conditional sentence independence assumptions that gives our model the flexibility to use just parallel sentences. Secondarily, the Yu et al. inference algorithm is specialized to their channel model, and it has a quadratic (in the length of the sentence) complexity, which would be prohibitive for sequences longer than a single sentence; in practice our inference technique is much faster. We will clarify these differences in the paper.\n\nRegarding whether our approach really needs parallel documents. First, there are two models in this paper- the joint translation model and proposal model we use to do inference. The joint translation model is only ever trained using parallel sentences. For inference, we use a proposal model that approximates the posterior, and we compare two variants: one that is trained using just parallel sentences (effectively, we assume independence between translations given the source document) and one that is trained with document context (see Table 2). As predicted, a proposal model that more closely matches the true posterior (i.e., the one with document context) is more effective than one that is less accurate (no document context), but the crucial result is that in both cases, document information has a positive impact on the performance of the system. The secondary result is that search is a hard problem, and while usable approximations exist, this is an important open question. We will clarify this."}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1MerYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2158/Authors|ICLR.cc/2020/Conference/Paper2158/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145487, "tmdate": 1576860531575, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment"}}}, {"id": "BklPU1CfjH", "original": null, "number": 4, "cdate": 1573211983018, "ddate": null, "tcdate": 1573211983018, "tmdate": 1573211983018, "tddate": null, "forum": "B1x1MerYPB", "replyto": "r1lfMoQ_YH", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review.\n\nRegarding the perceived lack of novelty to Yu et al., 2017, please see the response to Reviewer 2.\n\nRegarding baselines. We do have sentence-level LM results + sentence level proposal results (see Table 2). Regarding the Xia et al. (2017) method \u2014 which we argue would be a benchmark, not a baseline \u2014 we have provided some back translation results (see response to Reviewer 3), and we can add to the paper. Since back translation techniques are closely related, it does not seem important to add these techniques, especially since the Xia et al. method has not yet been established for document level translation (although it no doubt could be used for this), further it is not obvious that it would provide a convenient way to exploit the kind of data that we wish to exploit (monolingual documents, parallel sentences). The goal of this paper, and why readers should read it, is using a theoretically motivated approach to the document translation problem that solves a central data problem in MT, and characterizing its performance relative to some representative baselines.\n\nRegarding the difference in performance between document and sentence LMs. As we discuss in the paper, with citations to much prior work, the impact of fixing problems related to cross-sentence consistency has a minimal impact on BLEU, but the impact on human judgments can be much more significant. For this reason we also carried out a human evaluation, where the document reranker was favoured two-to-one by our evaluators.\n\nRegarding why the proposal model adds no value to the objective. The fact that the proposal model does not add new information to the objective is expected if Bayes rule yields a better estimate of the translation probability than its direct estimation (i.e., the proposal model). Thus, since we believe our component models (channel and language model) to be well estimated, we expected this redundant component to add no value, and we see this result as a confirmation that Bayesian arguments are trustworthy in this domain (deviations could be expected for a variety of reasons: e.g., poorly calibrated probability distributions, that parameters are chosen to maximize BLEU not to minimize the cross entropy under the posterior distribution). We will clarify this point in the paper.\n\nRegarding the effects of ensembling. We have compare ensembling 2 zh->en + 1 en->zh models:\n\n+------------------------------------------------------------------------------------------+------------+\n|                                     Model                                                                      | MT06     |\n+------------------------------------------------------------------------------------------+------------+\n| ensemble                                                                                                   | 50.04      |\n+------------------------------------------------------------------------------------------+------------+\n| Sent-reranker (sent-level transformer as the proposal model)       | 50.29     |\n+------------------------------------------------------------------------------------------+------------+\n| Doc-reranker (sent-level transformer as the proposal model)        | 50.93     |\n+------------------------------------------------------------------------------------------+------------+\n\nThe noisy channel approach outperforms ensembling in BLEU, as has been discussed in previous work on noisy channel approaches that show this isn\u2019t just an effect of ensembling. Also, notably, ensembling sentence level models will not address the document translation problem, nor will it enable us to use monolingual text data, which are two benefits that our technique has.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1MerYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2158/Authors|ICLR.cc/2020/Conference/Paper2158/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145487, "tmdate": 1576860531575, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment"}}}, {"id": "Hyxf1RTfjB", "original": null, "number": 2, "cdate": 1573211610151, "ddate": null, "tcdate": 1573211610151, "tmdate": 1573211610151, "tddate": null, "forum": "B1x1MerYPB", "replyto": "Sk97bk0tS", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment", "content": {"title": "Response-part 2", "comment": "Regarding speed. Search is indeed a hard problem in our model. We intend this paper to ask whether a well-motivated model performs well, and provide a reasonable (if imperfect) inference method. We show that it does work well. Now subsequent work can answer the question of how to make decoding fast. But search is a hard problem that has applications in many areas beyond translation, so this paper adds value to those who would work on this problem. We ourselves intend to work on this now that we know this model is effective, but we also argue that this is a good time to publish these results: others may be interested in knowing about yet another interesting search problem. We will clarify this."}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1MerYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2158/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2158/Authors|ICLR.cc/2020/Conference/Paper2158/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145487, "tmdate": 1576860531575, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Authors", "ICLR.cc/2020/Conference/Paper2158/Reviewers", "ICLR.cc/2020/Conference/Paper2158/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Comment"}}}, {"id": "r1lfMoQ_YH", "original": null, "number": 1, "cdate": 1571465994101, "ddate": null, "tcdate": 1571465994101, "tmdate": 1572972375679, "tddate": null, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** Paper summary **\nIn this paper, the authors propose a new re-ranking mechanism leveraging document-level information. Let X and Y denote two languages for ease of reference. The authors focus on X->Y translation and Y->X is a model used for re-ranking. Specifically,\n(1)\tTwo translation models X->Y and Y->X are trained, where X->Y is a document Transformer and Y->X is a sentence transformer.\n(2)\tTrain a language model P(Y) on document-level corpus (rather than sentence-level LM).\n(3)\tGiven a document with $I$ sentences (x^1, \u2026, x^I), translate each source sentence $x^i$ to K candidates.\n(4)\tUsing beam search guided by Eqn.(4) to find optimal translation paths, which is a combination of X->Y translation, Y->X translation, document-level language model and the number of words.\nThe authors work on NIST Chinese-to-English translation and WMT\u201919 Zh->En translation to verify the proposed algorithm.\n\n** Novelty **\nThe novelty is limited. Compared to the paper \u201cthe Neural Noisy Channel\u201d (Yu et. al, 2017), the authors use document Transformer and document-level LM for re-ranking, which is of limited novelty. \n\n** Details **\n1.\tSome baselines are missing from this paper: (A) dual inference baseline [ref1]; (B) X->Y is sentence-level transformer and LM is sentence-level LM, i.e., (Yu et. al, 2017), where P(Y|X) and P(X|Y) are sentence-level translation models.\n2.\tIn Table 1, the improvement of doc-reranker is not very significant compared to sent-reranker, ranging from 0.21 to 0.66. \n3.  In Table 4, \u201cChannel + LM\u201d and \"Proposal + Channel + LM\" achieved almost the same results. Does it mean that the \"proposal\" component do not work?\n4.\tMany models are used in this framework. I am not sure whether simple re-ranking or ensemble can outperform this baseline, e.g., 2 Zh->En + 1 En->Zh\n\n[ref1] Dual Inference for Machine Learning, Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, Tie-Yan Liu, IJCAI\u201917"}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299977184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Reviewers"], "noninvitees": [], "tcdate": 1570237726872, "tmdate": 1575299977194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review"}}}, {"id": "Sk97bk0tS", "original": null, "number": 2, "cdate": 1571840289517, "ddate": null, "tcdate": 1571840289517, "tmdate": 1572972375630, "tddate": null, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a simple approach for document-level machine translation. The idea is to use a language model on the target side and a reverse translation model to choose the best document-level translation. This is theoretically justified by Bayes\u2019 rule and the assumption that the sentences are conditionally independent. The authors implement this idea using a reranking model that rescores 50 candidate translations generated by a standard Transformer model for forward translation. \n\nThis is interesting work and the experimental results demonstrate the effectiveness of the approach. However, I am concerned about the (missing) comparison between the proposed approach and the approach that combines backtranslation and a document-level translator (e.g.  Doc-transformer). It seems to me that one could backtranslate a large monolingual corpus and use the resulting parallel documents as additional training data for a document-level translation model. How does the proposed approach compare to such a backtranslation approach?\n\nAnother concern is the speed of translation. It seems to me that the computational cost required for generating 50 candidates and reranking them is quite high. I would like to see some experimental results on the actual speed of translation. The aforementioned backtranslation approach should not have this problem, which also makes me unsure about the usefulness of the proposed approach in practice."}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299977184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Reviewers"], "noninvitees": [], "tcdate": 1570237726872, "tmdate": 1575299977194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review"}}}, {"id": "HJlp8ls19B", "original": null, "number": 4, "cdate": 1571954772648, "ddate": null, "tcdate": 1571954772648, "tmdate": 1572972375584, "tddate": null, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "invitation": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper describes a noisy channel approach for document-level translation, which does not rely on parallel documents to train. The approach relies on a sentence-level translation model (from target-to-source languages) and a document level language model (on target language), each is trained separately. For decoding, the paper relies on another proposal model (i.e., a sentence level translation model from source to target) and performs beam-search weighted by a linear combination of the scores of all three models. Experiments show strong results on two standard translation benchmarks.\n\nComments:\n-  The proposed approach is strongly based on the neural noisy channel model of Yu et al. 2017 but mainly extends it to context aware translation. While the paper is referenced, I believe more emphasis should be put on the differences of the proposed approach\n-  It seems that the Document Transformer uses parallel-documents to train, so I am wondering if you can still claim that your approach does not require parallel documents.\n-  In general, I think the paper is well written and results are compelling.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2158/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["leiyu@google.com", "lsartran@google.com", "wstokowiec@google.com", "lingwang@google.com", "lingpenk@google.com", "pblunsom@google.com", "cdyer@google.com"], "title": "Putting Machine Translation in Context with the Noisy Channel Model", "authors": ["Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer"], "pdf": "/pdf/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "abstract": "We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "keywords": ["machine translation", "context-aware machine translation", "bayes rule"], "paperhash": "yu|putting_machine_translation_in_context_with_the_noisy_channel_model", "original_pdf": "/attachment/c20527d2d5d628b86a30e752404bfcc02044223e.pdf", "_bibtex": "@misc{\nyu2020putting,\ntitle={Putting Machine Translation in Context with the Noisy Channel Model},\nauthor={Lei Yu and Laurent Sartran and Wojciech Stokowiec and Wang Ling and Lingpeng Kong and Phil Blunsom and Chris Dyer},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1MerYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1MerYPB", "replyto": "B1x1MerYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2158/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299977184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2158/Reviewers"], "noninvitees": [], "tcdate": 1570237726872, "tmdate": 1575299977194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2158/-/Official_Review"}}}], "count": 10}