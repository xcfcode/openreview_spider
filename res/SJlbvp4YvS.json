{"notes": [{"id": "SJlbvp4YvS", "original": "Syectw9wwS", "number": 586, "cdate": 1569439065166, "ddate": null, "tcdate": 1569439065166, "tmdate": 1577168234615, "tddate": null, "forum": "SJlbvp4YvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oAKcm2cgGf", "original": null, "number": 1, "cdate": 1576798700531, "ddate": null, "tcdate": 1576798700531, "tmdate": 1576800935404, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose to extend model-based/model-free hybrid methods (e.g., MVE, STEVE) to stochastic environments. They use an ensemble of probabilistic models to model the environment and use a lower confidence bound of the estimate to avoid risk. They found that their proposed method yields state-of-the-art performance over previous methods.\n\nThe valid concerns by Reviewers 1 & 4 were not addressed by the authors and although the authors responded to Reviewer 3, they did not revise the paper to address their concerns. The ideas and results in this paper are interesting, but without addressing the valid concerns raised by reviewers, I cannot recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729187, "tmdate": 1576800281737, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper586/-/Decision"}}}, {"id": "SJ06CesjB", "original": null, "number": 10, "cdate": 1573748421517, "ddate": null, "tcdate": 1573748421517, "tmdate": 1573748421517, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJezRg29sr", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment", "content": {"title": "Looking forward to the new version", "comment": "Thank you for the comments and clarifications! I look forward to seeing the new version."}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbvp4YvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper586/Authors|ICLR.cc/2020/Conference/Paper586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169238, "tmdate": 1576860553404, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment"}}}, {"id": "ByghG225sH", "original": null, "number": 9, "cdate": 1573731347531, "ddate": null, "tcdate": 1573731347531, "tmdate": 1573731377686, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SyextZ29sr", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment", "content": {"title": "Thanks for the clarification, but make sure to include them in the updated draft", "comment": "Thanks for the explanations. For the loss of the negative log-likelihood, yes I understand it's used for inferring the parameters from the reward samples. However, you should make it clearer how this idea is used and applied in RAVE more explicitly. In particular, since you mentioned that RAVE uses the PE model in Section 5, you should clearly state how you infer/estimate those parameters in the PE model. However, the log-likelihood is only mentioned for the non-ensemble case, which does not seem to be used later in RAVE. Btw, a quick suggestions is to add a sentence to clarify which model is used throughout the paper for RAVE right after Section 3.3, to reduce confusion.\n\nTo make the paper more accessible, I have the following additional suggestions: 1) add an algorithm framework (maybe in the appendix) summarizing the entire framework of RAVE, including how you infer the parameters, etc., i.e., essentially the entire approach you apply in the experiments; 2) change the rather non-standard and weird notation of putting parameter below the function (like in equation 4) to subscripts, or putting those parameters to the right hand side of the function and include them in the brackets as used in the standard function notation; 3) make sure that the brackets before the equation reference is separated from the previous words by a blank."}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbvp4YvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper586/Authors|ICLR.cc/2020/Conference/Paper586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169238, "tmdate": 1576860553404, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment"}}}, {"id": "SyextZ29sr", "original": null, "number": 8, "cdate": 1573728631534, "ddate": null, "tcdate": 1573728631534, "tmdate": 1573728631534, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "Ske-fbzGqr", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment", "content": {"title": "Reply", "comment": "We would like to thank the reviewer for the constructive comments. We are glad that the reviewer found the proposed method useful and novel.\n\nRegarding the comments:\n \n> What is the loss of the negative log-likelihood in (4) for?\nRAVE uses a probabilistic model to predict the reward, and we can maximize log-likelihood or minimize the negative log-likelihood to maximize its probability over the reward samples.\n\n \n>The authors may want to explain clearly what is the variance indicating in (8), although one can guess that it is closely related to (5). \n\n \nThanks for the suggestion. We would rewrite the symbols in(8) to make it more clear."}, "signatures": ["ICLR.cc/2020/Conference/Paper586/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbvp4YvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper586/Authors|ICLR.cc/2020/Conference/Paper586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169238, "tmdate": 1576860553404, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment"}}}, {"id": "SJezRg29sr", "original": null, "number": 6, "cdate": 1573728458378, "ddate": null, "tcdate": 1573728458378, "tmdate": 1573728458378, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SylI6P9iqr", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment", "content": {"title": " ", "comment": "We would like to thank the reviewer for the detailed review and constructive comments.\nWe do agree that the paper needs proofreading, and we will address the error mentioned in the future version. We also agree that our method based on MVE and STEVE is a bit complex. However, we argue that the extension on this line is not trivial, because model-based value expansion provides an alternative way to incorporate the dynamics model into MBRL methods, instead of using the dynamics model for the generation of additional training data.\n\nRegarding the comments:\n> MBRL  and MFRL performance in \"noisy environments and long trajectories.\"\nFor model-based RL methods, the authors of [1] have analyzed theoretically the value expansion error brought by the increasing rollout length H. Our empirical results on the toy environment also demonstrates the weakness of MBRL methods in a noisy environment. However, the experimental results of the paper[2] show that MBRL can outperform MFRL. On explanation is that in MFRL, the performance gap between learning from pixels and density vectors is still huge. For example, when learning from density observations, the performance on Halfcheetah-v1 can reach the maximum score, while learning from pixels reaches less than 800 scores. Compared with MFRL methods, which have a lower score, the MBRL method has better performance when learning from pixels.\n \n> Learning from pixels.\nWe agree that it might be hard to apply some MBRL methods in environments with pixel observations. However, we believe that these algorithms can be helpful in some real-world problems, where most of the sensor data is float vectors. (e.g., heat-sensor, sonar sensor). To address the issue of learning from pixels, predicting future rewards/values without predicting raw observations might be the right choice, like VPN[3] and I2A[4]. We agree that there is a trade-off between the proposed method and MFRL methods, and will discuss this issue in the future version.\n\n> DDPG is a weak baseline.\nWe agree that DDPG is a weak baseline, and that is why we further use TD3 and SAC, the best MFRL methods, as the baseline methods. The main concern of not including D4PG is that the improvement of our method might come from a multi-step rollout or using an ensemble of models. We use the same hyper-parameters, such as rollout steps and the number of ensemble models for RAVE and STEVE in our experiments. The performance gap between STEVE and RAVE shows that the performance gain of RAVE does not rise from these two techniques.\n \n> The use of ensembling is another confounder.\nAs we mentioned in the last paragraph, we use the same number of ensemble models for STEVE and RAVE, so it is clear the RAVE's gain does not rely on the ensemble technique, compared with STEVE. However, we agree that the impact of ensemble size on these methods is an interesting problem. We will study its importance in the future version.\n \n> The impact of the prediction horizon.\nThe reason why only STEVE and RAVE consider prediction horizons other than 1 is that they are MBRL methods, while other baselines are MFRL methods.\n \n> If the proposed solution actually helps on the toy problem.\nThanks for the suggestion. We would add the performance of rave on the toy environment in the future version.\n \n> Only one proposal for adaptive alpha is considered.\nThanks for the suggestion. We would add experiments of various kinds of adjustments on alpha in the future version.\n \n> No discussion is made on how much tuning was done to determine alpha and Z.\nWe set the maximum alpha to Z, and we maintain a variable recoding the recent range of the approximation error and normalize all the forward propagation error into (0, 1.5) in our experiment. \n \n>How the dynamic value of alpha changes during training.\nThanks for the suggestion. We would add a figure plotting the change of alpha in the future version.\n\n> Consider comparing on DMControl.\nThanks for the suggestion. We would consider the benchmarks in our future work.\n \n> Why does DDPG get worse over time on the RoboSchool tasks?\n\nWe use the DDPG implementation provided by the authors of STEVE. On explanation of the worse performance of DDPG in these environments is that these tasks are more challenging than previous tasks. Roboschool also provided simple environments such as Halfcheetah, but we only compare the algorithms on the most challenging environments of this library.\n \n[1] Feinberg et al., \"Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning.\" ICML 2018. https://arxiv.org/pdf/1803.00101.pdf\n[2] Hafner et al., \"Learning Latent Dynamics for Planning from Pixels.\" ICML 2019. https://arxiv.org/abs/1811.04551\n[3] Feinberg et al., \"Imagination-Augmented Agents for Deep Reinforcement Learning.\" NIPS2017. https://arxiv.org/abs/1707.03497\n[4] Weber et al., \"Imagination-Augmented Agents for Deep Reinforcement Learning.\" NIPS2017. https://arxiv.org/abs/1707.06203"}, "signatures": ["ICLR.cc/2020/Conference/Paper586/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbvp4YvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper586/Authors|ICLR.cc/2020/Conference/Paper586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169238, "tmdate": 1576860553404, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper586/Authors", "ICLR.cc/2020/Conference/Paper586/Reviewers", "ICLR.cc/2020/Conference/Paper586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Comment"}}}, {"id": "Bylh6JURtH", "original": null, "number": 1, "cdate": 1571868612004, "ddate": null, "tcdate": 1571868612004, "tmdate": 1572972576984, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel deep reinforcement learning algorithm at the intersection of model-based and model-free reinforcement learning: Risk Averse Value Expansion (RAVE). Overall, this work represents a significant but incremental step forwards for this \"hybrid\"-RL class of algorithms. However, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas.\n\nThe main strength of this work is its empirical section. The proposed algorithm is fairly compared against many relevant baselines on a variety of continuous control tasks, on both the Mujoco and Roboschool simulators, and demonstrates significant performance increases across all of them. The visualization in Figure 4 is interesting, and provides good insight into the issues with DDPG and STEVE, and the reasons for the success of RAME. Based on these results, the authors have convinced that RAME is a state-of-the-art algorithm.\n\nHowever, in spite of its strong performance on benchmarks, I believe that this paper needs a significant overhaul/rewrite before publication.\n\nOne major criticism I have is on the authors' treatment of the different types of uncertainty. The introductory sections spend a large amount of time on the differences between aleatoric and epistemic uncertainty, and various other related concepts. But when it comes to the actual new algorithm, that the authors only barely touched upon the (very important) point that V[Q^DVE] is actually a *mix* of epistemic and aleatoric uncertainties. When minimizing this term, it's not clear whether epistemic or aleatoric uncertainty is actually being reduced. (Based on the experiments, which report only expected values, not CVaR or anything of the sort, it seems the authors care only about reducing epistemic uncertainty; if so, it's unclear why they choose to minimize a term which includes aleatoric uncertainty too.) A more principled understanding of this quantity seems essential to this line of work. Similarly, \"risk\" typically refers to aleatoric uncertainty, but the risk-senstive component of RAME computes the confidence lower bound w.r.t. a mix of aleatoric and epistemic uncertainty. Calling this algorithm \"risk sensitive\" is likely to generate confusion, in my opinion.\n\nA few other points on presentation of ideas:\n- Switching from a deterministic to a stochastic model is a trivial extension of MVE/STEVE, and way too much time is spent on this point (even going so far as to name a new algorithm!). Section 4 contains no new insight, beyond \"deterministic models can be bad in stochastic environments\", which is obvious. Consider re-thinking your experiments for this section to help readers better understand *what* goes wrong.\n- In my opinion, RAME is as much a successor to TD3 as it is to STEVE. Taking the lower outcome of an ensemble of size 2 is equivalent to applying a penalty based on the stdev of the outcome distribution, with \u03b1=1. The introduction of the paper should be re-written to clearly point out that that RAME ~= TD3 + STEVE.\n- I'd also like to see a lot more ablations, on at least a few environments. There are at least four factors that need to be teased apart: 1) deterministic vs stochastic model 2) lower confidence bound penalty 3) adaptive \u03b1 for LCBP 4) STEVE reweighting. Does RAME require all of these elements to perform well? How do these elements interact?\n\nSome feedback on the writing:\n- There are various small factual errors. For example, in the very first paragraph, the Dyna algorithm is attributed to Kuturach et al, instead of Sutton (1990). (The algorithm from Kuturach is ME-TRPO.)\n- The notation is not very good; there are lots of symbols flying around everywhere, and it makes the ideas (which are fundamentally very simple) a bit difficult to parse. For example, naming each parameter-set individually, everywhere, is unnecessary.\n- There are some strange non-sequiturs and overall lack of flow.\n\nI think that this work has a lot of potential, and am especially impressed by the empirical results. I recommend rejection in its current form, but hope to see a revised version of this work appear at a conference in the near future."}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575493756200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper586/Reviewers"], "noninvitees": [], "tcdate": 1570237750003, "tmdate": 1575493756231, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Review"}}}, {"id": "S1etZbgy9B", "original": null, "number": 2, "cdate": 1571909888531, "ddate": null, "tcdate": 1571909888531, "tmdate": 1572972576939, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work , the authors combine model-based value expansion(MVE) with\nmodel-free reinforcement learning and also take into account the high-order stochastic characteristics\nof the environment to make the value expansion algorithm in modeling error propagation of dynamics risk-averse. \nThey propose a novel Hybrid-RL method, namely the Risk Averse Value Expansion(RAVE), that uses an ensemble of probabilistic dynamics models to generate imaginative rollouts and to model risk aversion\nof risks by estimating the lower confidence bound of the ensemble. They showed that by taking the risk (in terms of variance) of the dynamic propagation error into the account, RAVE can achieve comparable performance with other state-of-the-art baselines in benchmark experiments including MuJoCo and robo-school. Also they showed that RAVE reduced the variance of the return and thus prevented catastrophic consequences such as falling.\n\nI found this work interesting as the authors try to take the uncertainty of probabilistic dynamics model into account for estimating the value function and its confidence bounds in model-free RL. Utilizing such confidence interval of values, they can have a way of doing exploration while being risk-averse. While similar approaches of modeling the mean and variance of Q functions can be found in some existing work, such as boot-strapped DQN (Osband'16), and efficient exploration via Bayesian DQN (Azizzadenesheli'18), none of these work model the variance of the Q-function using the error propagation of the dynamics model. Through extensive experiments, it has also demonstrated the effectiveness of RAVE in terms of achieving good performance, approximation error in value estimation, as well as robustness to failure in standard mujoco benchmarks, which provides readers some detailed understanding on how this risk-averse uncertainty modeling/propagation technique helps in exploration. However, while i found this idea interesting, it appears to me that the current work is still quite preliminary (without theoretical concentration inequality based variance bounds for guiding exploration), and empirically, it would be great to compare this method with the aforementioned bootstrapping/bayesian based approaches. My other major comment is on claiming the proposed method to be \"risk-averse\", because in RL, risk-averse methods are commonly known to not only optimize the expected return but also to provide guarantees to other moments of the return, such as variance of CVaR. However, while the method used variance of the expected value (due to the error propagation of the dynamics) for exploration, I am not seeing the risk-averse optimization criteria being studied here. Therefore calling the RL method risk-averse might be a little mis-leading."}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575493756200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper586/Reviewers"], "noninvitees": [], "tcdate": 1570237750003, "tmdate": 1575493756231, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Review"}}}, {"id": "Ske-fbzGqr", "original": null, "number": 3, "cdate": 1572114697193, "ddate": null, "tcdate": 1572114697193, "tmdate": 1572972576892, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of high function approximation errors facing the stochastic environments when trying to combining model-based reinforcement learning (RL) with model-free RL. The paper begins by showing that previous methods like model-based value expansion (MVE) and stochastic ensemble value expansion (STEVE) can perform even worse than the pure model-free DDPG algorithm in a rather noisy environment. Following the ideas of MVE and STEVE, It then proposes a risk averse value expansion (RAVE) to replace the target Q function in the actor-critic algorithm, which is built upon an ensemble of probabilistic models (PE) and adopt the lower confidence bound as a surrogate of the target value as in the risk-sensitive RL. A simple yet intuitive approach for adaptively selecting the confidence bound \\alpha is also proposed. The experiments show that RAVE does improve over the state-of-the-art algorithms in several different environments, with a better draw-down control. In general, this paper is well-written and the idea of RAVE is novel as far as I know. But since I'm not very familiar with the specific literature of combing model-based and model-free RL, and since the idea of RAVE is relatively straightforward (but admittedly practically powerful and theoretically interesting), I choose to give a conservative accept to account the possibility that some existing works have followed very similar approaches. \n\nSome minor comments: 1) What is the loss of the negative log-likelihood in (4) for? 2) The authors may want to explain clearly what is the variance indicating in (8) more clearly, although one can guess that it is closely related to (5). "}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575493756200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper586/Reviewers"], "noninvitees": [], "tcdate": 1570237750003, "tmdate": 1575493756231, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Review"}}}, {"id": "SylI6P9iqr", "original": null, "number": 4, "cdate": 1572739006489, "ddate": null, "tcdate": 1572739006489, "tmdate": 1572972576839, "tddate": null, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "invitation": "ICLR.cc/2020/Conference/Paper586/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\nThis paper expands on previous work on hybrid model-based and model-free reinforcement learning. Specifically, it expands on the ideas in Model-based Value Expansion (MVE) and Stochastic Ensemble Value Expansion (STEVE) with a dynamically-scaled variance bias term to increase risk aversion over the course of learning, which the authors call Risk Averse Value Expansion (RAVE). Experimental results indicate notable improvements over their selected model-free and hybrid RL baselines on continuous control tasks in terms of initial learning efficiency (how many environment steps are needed to achieve a particular level of performance), asymptotic performance (how high the performance is given the same large number of environment steps), and avoidance of negative outcomes (how infrequently major negative outcomes are encountered over the course of training).\n\nReview\nThe core contribution of the paper is an extension to STEVE that uses an idea from risk-averse RL of biasing the underlying estimators away from high-variance predictions, and adds a dynamic weight to that bias.\n\nStrengths\n- The addition of a risk-aversion term to STEVE is a good contribution to the literature on safe RL. While it may be possible to criticize this contribution as somewhat trivial, I am disinclined to do so, as finding simple ideas that are effective is the hallmark of an important contribution, in my opinion.\n- Under the assumption that the baselines are fair, the empirical results show substantial improvements for a good selection of challenging tasks along three metrics: initial efficiency, asymptotic performance, and avoidance of negative outcomes.\n- The paper provides a careful and detailed section on the relevant preliminaries, giving precise notation that is expanded step-by-step from basic actor-critic approaches all the way through the description of RAVE. (Although see my comment below about possibly making the notation more concise.)\n- Overall, the paper\u2019s presentation is clear and natural. (Although see my comment below about a strong need for proofreading.)\n\nWeaknesses\n- The paper opens with a claim that MBRL has worse performance than MFRL in environments with \u201cnoisy environments and long trajectories\u201d. However, recent work [1] has shown that MBRL can outperform MFRL on many of the tasks considered in this paper using far fewer environment steps, even when training directly from pixels.\n- The experiments are all based on observations of the true state vectors rather than pixels, which dramatically simplifies working with imagination rollouts, as the dimensionality is so much lower. This hides the high computational cost and modeling challenges involved in using imagination-based MBRL or hybrid RL approaches in real-world environments, and (in my opinion) is a major shortcoming of this line of research (not only of this paper). Since the DDPG family of algorithms doesn\u2019t have to do rollouts, they have a strong advantage over this type of approach on more realistic settings where the true state is unknown and must be inferred from high-dimensional observations. If experiments are not going to be done using pixels, the discussion should at least mention the trade-offs involved between the proposed algorithm and the baselines in that setting.\n- The core baseline of the paper is DDPG, which is unnecessarily weak. D4PG [2] came out a year and a half ago and has the same high-level properties as DDPG, but outperforms it on all of the Mujoco tasks considered in this paper. Additionally, it includes some of the ideas presented in this work, but in a model-free setting, including a distributional treatment and multi-step rollouts, so it is easy to imagine that some of the experimental gains presented here would be erased when using it as a baseline.\n- The use of ensembling is another confounder in the experiments. Ensembling models almost always yields an improvement, so any technique that relies on some form of ensembling needs to additionally demonstrate that the gains presented are not solely due to ensembling. In this case, the experiments with STEVE and RAVE should minimally be performed with different numbers of models in their ensembles.\n- (Minor) Similarly, the prediction horizon can have a large impact, and only STEVE and RAVE consider prediction horizons other than 1. Showing how the horizon affects performance would help make the comparisons more fair.\n- Figure 1 shows a bias problem in some current approaches in a clear toy problem, but does not show whether RAVE addresses that problem. Even though this section precedes the presentation of RAVE, it is necessary to show that the proposed solution actually helps on the toy problem.\n- There are a number of problems with the proposal for adaptively computing the alpha parameter. In general, such adaptive hyperparameters add a great deal of complexity to hyperparameter tuning, so such suggestions should be either strongly motivated by theory or by empirical results. Neither seems to be the case here.\n  - Only one proposal for adaptive alpha (equation 13) is considered. Its justification is plausible, but it would be more convincing if other dynamic approaches were considered in the experiments. For example:\n    - alpha(env_step) = alpha * env_step / max_env_step\n    - alpha(s_t,a_t) = alpha * min{1.0, 1 / (Z * ||E[fhat(s_t,a_t)] - s_t+1||^2)}\n    - The opposite of the proposed approach, where alpha starts high and gets lower over the course of training.\n  - Indeed, a potentially useful quantity during evaluation, when s_t+1 is unknown, would be based on the variance of the predicted next state, rather than the difference of the expectation of the predicted next state and the true next state. E.g.:\n    - alpha(s_t,a_t) = alpha / Var[fhat(s_t,a_t)]\n    This formulation says that the precision of the prediction determines the confidence of the model, which is also intuitively reasonable (to me, anyway) and doesn\u2019t rely on knowing the future.\n  - (Minor) The adaptive variant appears to be only slightly better than alpha=0.5, but requires two hyperparameters that are unspecified in the main body of the paper \u2014 alpha and Z. The appendix lists those parameters, but no discussion is made on how much tuning was done to determine that pair of parameters.\n  - (Minor) A plot of how the dynamic value of alpha changes during training would be useful, at least in the appendix.\n\nRecommendation\nIn light of my comments above, I cannot currently recommend the acceptance of this paper at ICLR. However, I think that the core idea is likely to hold up under more careful experimental comparisons. If the authors submit a revised draft that addresses the substance of my concerns, I would be very likely to increase my rating. In particular, I would like to see much more careful experimental treatment of the idea, so that readers can have high confidence about the circumstances where RAVE is likely to be a good choice.\n\n[1] Hafner et al., \u201cLearning Latent Dynics for Planning from Pixels\u201d. ICML 2019. https://arxiv.org/abs/1811.04551\n[2] Barth-Maron et al., \u201cDistributed Distributional Deterministic Policy Gradients\u201d. ICLR 2018. https://arxiv.org/abs/1804.08617\n\n\nOther Comments and Questions\n- This paper needs a lot of proofreading. A few examples of errors that should be fixed before publication:\n  - \u201cequation. 2\u201d: This should read \u201cEquation 2\u201d, \u201cequation 2\u201d, \u201cEq. 2\u201d or \u201ceq. 2\u201d. The period in the last two options indicates that letters have been elided. The mistake of using a period where no elision has happened occurs throughout the paper.\n  - \u201cprophesy\u201d: This is not the correct word. Just say \u201cprediction\u201d.\n  - \u201cimage\u201d: This is often used when the correct word would be either \u201cimagine\u201d or \u201cimagination\u201d.\n  There are many other errors that could be fixed easily with the help of a native English speaker.\n- The mathematical notation in sections 3 through 5 is precise, but it is also a bit heavy. Consider whether there would be any ambiguity added if, for example, {\\hat Q}^DVE_{\\zeta_s,\\zeta_r,\\zeta_d,\\theta\u2019,\\phi\u2019} were instead notated {\\hat Q}^DVE_{\\zeta,\\theta\u2019,\\phi\u2019}.\n- Figure 2 adds nothing of value to the paper and should be removed.\n- Consider comparing on DMControl, which is the same set of tasks as Mujoco, but the scores are standardized such that each task has a maximum reward of 1000 per episode.\n- Why does DDPG get worse over time on the RoboSchool tasks? Without a clear explanation, it looks like a bug, and a bug like that calls into question the rest of the DDPG results as well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper586/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "authors": ["Bo Zhou", "Fan Wang", "Hongsheng Zeng", "Hao Tian"], "authorids": ["zhoubo01@baidu.com", "wangfan04@baidu.com", "zenghongsheng@baidu.com", "tianhao@baidu.com"], "keywords": ["reinforcement learning", "model-based RL", "risk-sensitive", "sample efficiency"], "TL;DR": "We extend the model-based value expansion methods with risk-averse learning and achieve state-of-the-art results on challenging continuous control benchmarks.", "abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "pdf": "/pdf/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "paperhash": "zhou|risk_averse_value_expansion_for_sample_efficient_and_robust_policy_learning", "original_pdf": "/attachment/14b062bf6d157f7f3358ed599c51ec26d9dcaba3.pdf", "_bibtex": "@misc{\nzhou2020risk,\ntitle={Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning},\nauthor={Bo Zhou and Fan Wang and Hongsheng Zeng and Hao Tian},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbvp4YvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbvp4YvS", "replyto": "SJlbvp4YvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575493756200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper586/Reviewers"], "noninvitees": [], "tcdate": 1570237750003, "tmdate": 1575493756231, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper586/-/Official_Review"}}}], "count": 10}