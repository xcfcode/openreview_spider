{"notes": [{"id": "BkglSTNFDB", "original": "SkxMQ3EvwH", "number": 509, "cdate": 1569439031529, "ddate": null, "tcdate": 1569439031529, "tmdate": 1583912024074, "tddate": null, "forum": "BkglSTNFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Tex4z872T", "original": null, "number": 1, "cdate": 1576798698461, "ddate": null, "tcdate": 1576798698461, "tmdate": 1576800937347, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "In this paper, the authors extended Q-learning with UCB exploration bonus by Jin et al. to infinite-horizon MDP with discounted rewards without accessing a generative model, and proved nearly optimal regret bound for finite-horizon episodic MDP. The authors also proved PAC-type sample complexity of exploration, which matches the lower bound up to logarithmic factors. Overall this is a solid theoretical reinforcement learning work.  After author response, we reached a unanimous agreement to accept this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729386, "tmdate": 1576800281966, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper509/-/Decision"}}}, {"id": "HJgxIUpioS", "original": null, "number": 12, "cdate": 1573799495644, "ddate": null, "tcdate": 1573799495644, "tmdate": 1573799857082, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BylcpEQYsS", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Thanks for the response and new comments", "comment": "We sincerely thank the reviewer for the quick response and new comments.\n\nRegarding the definition of PAC in infinite horizon settings: Actually this has been extensively discussed in the past two decades. It is believed that sample complexity of exploration is the most natural PAC measurement in this setting. Plese see [1, 2, 3] (below) for more detailed discussion on the performance measurement in infinite horizon settings.\n\n\n>>Actually I do not agree with this. You can set your Q-value in the last stage to be the Q-value of the first stage of the previous iteration. Then even the middle ones (1+R/2) are guaranteed to have near-optimal actions.\n\nIf  we understand correctly, the reviewer is proposing replacing Q-value of the last stage by Q-value of the first stage in the algorithm. (Pleasae correct us if our understanding is wrong.) However, the resulting Q-learning algorithm does not converge. This is because in the finite horizon algorithm there is no discount factor, and replacing the Q-value in the last stage will lead to an unbounded increase in Q-value. If algorithm is modified so that discount factor is included, then the algorithm will be like the one we proposed in the paper. In this case, it is possible to bound the number of sub-optimal steps of this algorithm using our analysis framework. However, this approach is no longer a black box reduction to finite horizon setting because the algorithm is changed.\n\n\n>>I would guess you can use the same argument in your paper to bound the number of errors made by the regret algorithm.\n\nYes, we agree that the argument in our paper (namely carefully designed Condition 1 and Condition 2) can be used to bound the number of errors. This argument is one of the technical contribution of our paper.\n\n[1] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309\u20131331, 2008.\n[2] Kakade, Sham Machandranath. On the sample complexity of reinforcement learning. Diss. University of London, 2003.\n[3] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model- free reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pp. 881\u2013888. ACM, 2006.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "HyxP6tXTtB", "original": null, "number": 1, "cdate": 1571793343477, "ddate": null, "tcdate": 1571793343477, "tmdate": 1573742141747, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper considered a Q-learning algorithm with UCB exploration policy for infinite-horizon MDP, and derived the sample complexity of exploration bound. The bound was shown to improve the existing results and matched the lower bound up to some log factors. The problem considered is interesting and challenging. However, I have a few major concerns.\n\n1. Assumptions: The Condition 2 is on the term $X_t^{(i)}$ which is the i-th largest item of the gap $\\Delta_t$ (the difference between the value function and the Q function fo the optimal policy). How to verify this condition in practice? Do you need this condition for all $t$? Does this condition depend on the choice of length $R$? The conditions are listed without any discussions.  \n\n2. Algorithm: The Algorithm depends on the choice of the parameter $R$. How to choose $R$ in practice?\n\n3. Writing: This paper is not well written. There are many typos and grammar errors. In addition, the key components Sections 3.3 and 3.4 are very hard to follow. For instance, in Section 3.3, the authors first introduced Condition 1 and then Condition 2, and then claimed that Condition 2 implied Condition 1. Similarly, in Section 3.4, Lemma 1 was introduced before Lemma 2. Then the authors claimed that Lemma 2 implies Lemma 1. If would be easier to follow if the authors only introduced the latter result, and then discuss the former result as a remark.  \n\n~~~~~~~~~~~~~~~~~~~~~~\nAfter rebuttal: Thanks the authors for addressing my questions. My first two major concerns have been nicely addressed. Therefore, I would like to increase my rating to 6.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575713021812, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper509/Reviewers"], "noninvitees": [], "tcdate": 1570237751110, "tmdate": 1575713021830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Review"}}}, {"id": "r1eZ4jA9iS", "original": null, "number": 11, "cdate": 1573739305188, "ddate": null, "tcdate": 1573739305188, "tmdate": 1573739305188, "tddate": null, "forum": "BkglSTNFDB", "replyto": "S1leQbZqir", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Thanks for the feedback", "comment": "We thank the reviewer for the feedback.\n\nFirst, we would like to clarify that both Condition 1 and Condition 2 are not assumptions for our theorem to hold. We do not have additional assumptions beyond the problem formulation. Instead, they are simply used as a proxy for sample complexity in our proof.\n\nBelow we answer the three questions raised by the reviewer:\n\n1.Do we need Condition 2 for all t?\nWe do not require Condition 2 to hold for all t. Instead, we count the number of times such that Condition 2 is violated, which is an upper bound for sample complexity. This counting is done in the series of inequalities on page 7 (above the final bound), where it can be noted that the second line counts the number of times such that Condition 2 is violated.\n\n2.Does this condition depend on the choice of length R?\nThe condition itself does depend on the choice of $R$, in the sense that the logarithmic of $R$ is involved in the definition of $M$.\n\n3.How to verify Condition 2 in practice? \nCondition 2 is not an assumption of the theorem. It is a statement to make the roadmap of the proof easier to understand. Verifying whether Condition 2 holds at a particular time step is similar to verifying whether the algorithm is performing bad at a particular time. Currently we think this is hard but unnecessary.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "S1leQbZqir", "original": null, "number": 10, "cdate": 1573683480216, "ddate": null, "tcdate": 1573683480216, "tmdate": 1573683480216, "tddate": null, "forum": "BkglSTNFDB", "replyto": "SyeQKO1HoB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "my questions were not answered", "comment": "Thanks the author for addressing my Question 2. However, I do not think my Question 1 has been clearly addressed.\n\nCondition2: Do you need this condition for all t? Does this condition depend on the choice of length R? How to verify Condition 2 in practice? "}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "HJlWhQI79B", "original": null, "number": 4, "cdate": 1572197289368, "ddate": null, "tcdate": 1572197289368, "tmdate": 1573627087687, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #4", "review": "Summary: This paper adapts the UCB Q-learning method to the inifinite-horizon discounted MDP setting. With an analysis similar to that of Jin et al (2018), it shows that this algorithm achieves a PAC bound of (1-gamma)^-7 |S||A|/eps^2, improving previous best-known bound (Delayed Q-learning, Strehlet et al, 2006, (1-gamma)^-8 |S||A|/eps^4) for this case.\n\nEvaluation: As I see this paper a direct extension of that of Jin et al (2018), I am afraid I have to recommend a rejection. \n\nHere are some more detailed comments:\n\nSignificance: \nThis paper studies the RL problem for the infinite-horizon discounted MDP setting. This is an important setting in reinforcement learning. However, the bound is not optimal as the dependence of (1-gamma) is significantly larger than the lower bound. Moreover, both the algorithm and analysis are direct extensions of that of Jin et al, I do not see a huge technique improvement. \n\nTechnique Novelty:\nAs stated in the paper, the major difficulty is that the inf-horizon case does not have a set of \"consecutive episodes\". Therefore the \"learning error at time t cannot be decomposed as errors from a set of consecutive time\nsteps before t, but errors from a set of non-consecutive time steps without any structure.\" However, I do not see a major technological innovation is needed to get around this issue. As a result, the analysis and algorithm in this paper are very similar to that of Jin et al 2018, who nearly implicitly contain the results in this paper.\n\nFurthermore, I would think there is a (likely) very simple reduction from the inf-horizon to finite-horizon: break the inifinite horizon into episodes of length R = O((1-\\gamma)^-1 log(eps^-1)). Now, although the MDP does not restart, but it can be treated as restarting at a history-dependent initial state distribution at the beginning of every episode. So, an optimal finit-horizon algorithm in this setting is at most epsilon worse than the optimal inf-horizon algorithm, no matter where/when you start. With little to no modification, we can see that Jin et al work in this setting. Thus, we obtain an algorithm for the inf-horizon as well.\n\nA good match for this conference?\nAs this paper is an adaptation of a previously known Q-learning algorithm to a slightly different setting in RL, I do not see how it fits the \"learning representation\" paradigm. Of course, Q-function can be argued as a representation of the MDP model, but this Q-function itself is not a new concept in this paper.\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575713021812, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper509/Reviewers"], "noninvitees": [], "tcdate": 1570237751110, "tmdate": 1575713021830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Review"}}}, {"id": "BylcpEQYsS", "original": null, "number": 9, "cdate": 1573627074090, "ddate": null, "tcdate": 1573627074090, "tmdate": 1573627074090, "tddate": null, "forum": "BkglSTNFDB", "replyto": "r1xYsw1rjS", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Thanks for explanation", "comment": "Thanks for the explanation. I now agree that the technique sufficiently differs from Jin et al.\nThis is mainly due to the fact that you need to run the algorithm forever and there is no hope to restart even if you stuck at some bad state (this also demonstrates that the def of PAC is not very good ...). \n\n>>The algorithm in Jin et. al finds a time-dependent policy. Running finite-horizon algorithm can only guarantee near-optimal behavior at step 1, 1+R, 1+2R, etc. For other steps, the policy given by Jin et. al can be suboptimal. For example, at step 1+R/2, the policy given by finite horizon algorithm only maximizes over the reward of remaining R/2 steps, which cannot guarantee optimal bound in the infinite horizon setting.\n\nActually I do not agree with this. You can set your Q-value in the last stage to be the Q-value of the first stage of the previous iteration. Then even the middle ones (1+R/2) are guaranteed to have near-optimal actions.\n\n\n>>Sublinear regret does NOT imply finite sample complexity of exploration. For example, if an algorithm takes sub-optimal moves at step 1, 4, 9, \u2026, t^2, \u2026, the regret is bounded by . However, the sample complexity of exploration of this algorithm is unbounded. Therefore, no direct reduction can be made from sample complexity of exploration to regret.\n\nI would guess you can use the same argument in your paper to bound the number of errors made by the regret algorithm.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "r1xYsw1rjS", "original": null, "number": 4, "cdate": 1573349280826, "ddate": null, "tcdate": 1573349280826, "tmdate": 1573349638304, "tddate": null, "forum": "BkglSTNFDB", "replyto": "HJlWhQI79B", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "First of all, we thank the reviewer for giving detailed technical comments. The major concern of the reviewer is that there exists simple reduction from infinite-horizon setting to the episodic setting; and it is straightforward to generalize Jin et al (2018) to obtain our results.\n \nHowever, the reduction given by the reviewer is incorrect. Below we explain why the reduction doesn\u2019t hold, and this clearly illustrates the subtle differences between the infinite-horizon setting and the episodic case. In fact, as we already emphasized in the paper (Section 3.2), infinite-horizon setting cannot be solved by reducing to finite-horizon setting as long as we consider sample complexity instead of regret.\n\nWe focus on sample complexity of exploration in infinite horizon setting, which is a standard measure widely used in previous results in this setting, while Jin et al proved a regret bound. Please note that sublinear regret does NOT imply finite sample complexity of exploration.\n\nThe reduction giving by the reviewer does NOT work for infinite horizon setting.\n1.\tThe algorithm in Jin et. al finds a time-dependent policy. Running finite-horizon algorithm can only guarantee near-optimal behavior at step 1, 1+R, 1+2R, etc. For other steps, the policy given by Jin et. al can be suboptimal. For example, at step 1+R/2, the policy given by finite horizon algorithm only maximizes over the reward of remaining R/2 steps, which cannot guarantee optimal bound in the infinite horizon setting.\n2.\tSublinear regret does NOT imply finite sample complexity of exploration. For example, if an algorithm takes sub-optimal moves at step 1, 4, 9, \u2026, t^2, \u2026, the regret is bounded by $\\sqrt{T}$. However, the sample complexity of exploration of this algorithm is unbounded. Therefore, no direct reduction can be made from sample complexity of exploration to regret.\n\n\nResponse to other comments:\n\nRegarding the dependence of  1-gamma: \nPreviously best-known model-free algorithm for infinite horizon setting is Delayed Q-learning, which achieves a bound of $\\tilde{O}(\\frac{SA}{\\epsilon^4(1-\\gamma)^8})$. We also show that Delayed Q-learning cannot achieve near-optimal bound due to the inefficient usage of samples in Appendix D. Compared to this bound, our result is a significant improvement since we match the lower bound in terms of epsilon as well as S and A up to logarithmic factors. Besides, The previously best claimed result of model-based algorithms is 1/(1-gamma)^6, which is close to our result (1/(1-gamma)^7) and also significantly above the lower bound. Further improving the dependence on 1-gamma is a future direction.\n\nRegarding the technical novelty: \nAs stated in section 3.2, there are two major difficulties. Firstly, since we need to bound sample complexity of exploration, we need to establish convenient sufficient condition for being epsilon-optimal. We carefully design Condition 1 and Condition 2 to solve this problem. Secondly, we need to decompose errors to that of non-consecutive steps without any structure. See section 3.2 for detailed discussion.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "SyeQKO1HoB", "original": null, "number": 7, "cdate": 1573349499435, "ddate": null, "tcdate": 1573349499435, "tmdate": 1573349499435, "tddate": null, "forum": "BkglSTNFDB", "replyto": "HyxP6tXTtB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We thank anonymous reviewer 1 for the review. \n\nCondition 2 is a sufficient condition for near-optimal reward. Lemma 1 proved that the number of steps violating condition 2 is bounded. Please note that our algorithm achieves finite sample complexity of exploration in any MDPs, regardless of the suboptimality gap.\n\nOnce the desired performance of our algorithm (namely, epsilon) is determined, $R$ can be chosen as $R=\\ln \\left(\\frac{3}{\\epsilon (1\u2212\\gamma)}\\right) /(1 \u2212 \\gamma).$\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "HJxbUu1SoB", "original": null, "number": 6, "cdate": 1573349449136, "ddate": null, "tcdate": 1573349449136, "tmdate": 1573349449136, "tddate": null, "forum": "BkglSTNFDB", "replyto": "r1gUR9K1qr", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Response  to Reviewer 2", "comment": "We thank anonymous reviewer 2 for the review. \n\nComparison to other previous works: The most notable prior work in model-free PAC-RL is Delayed Q-learning  (Strehl et al. 2006), which has a $\\tilde{O}\\left(SA/\\epsilon^4(1-\\gamma)^8\\right)$ sample complexity of exploration bound. Our improved dependence on $\\epsilon$ comes from two factors.\n\n1. Strehl et al. treat all mistakes over $\\epsilon$ on the same footing. In our analysis, we require large mistakes to happen rarely, but allow small mistakes to happen with larger probabilities. This is mainly captured by Condition 2.\n\n2. Delayed Q-learning needs $\\epsilon^{-2}$ samples to update the Q-value by $\\epsilon$, which results in slow learning progress at the start. This is illustrated by our example in Appendix D. In comparison, UCB bonus allows faster progress at the start.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "B1xjlOyroB", "original": null, "number": 5, "cdate": 1573349362550, "ddate": null, "tcdate": 1573349362550, "tmdate": 1573349362550, "tddate": null, "forum": "BkglSTNFDB", "replyto": "H1x2aR8Z9S", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank anonymous reviewer 3 for the review. \n\nRegarding the example in section 3.2: Please note that \u2018sample complexity of exploration\u2019 counts the number of sub-optimal steps. We agree that steps taken in $s_1$ maybe near-optimal. When the agent leaves state $s_1$ and enters other state, it will take sub-optimal steps, which contributes to the sample complexity of exploration.\nFor the finite horizon case, when the leaving probability is small, the regret could be small.\n\nIn summary, this example showed that there are cases where an algorithm is efficient when using regret as complexity measure and inefficient when using sample complexity of exploration. Therefore there is no straight-forward reduction from infinite horizon case to finite horizon case.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkglSTNFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper509/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper509/Authors|ICLR.cc/2020/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170369, "tmdate": 1576860541346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper509/Authors", "ICLR.cc/2020/Conference/Paper509/Reviewers", "ICLR.cc/2020/Conference/Paper509/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Comment"}}}, {"id": "r1gUR9K1qr", "original": null, "number": 2, "cdate": 1571949261639, "ddate": null, "tcdate": 1571949261639, "tmdate": 1572972586642, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nIn this paper, the authors extend the UCB Q-learning algorithm by Jin et al. (2018) to infinite horizon discounted MDPs, and prove a PAC bound of \\tilde{O}(SA/\\epsilon^2 (1-\\gamma)^7) for the resulting algorithm. This bound improves the one for delayed Q-learning by Strehl et al. (2006) and matches the lower-bound in terms of \\epsilon, S, and A. \n\n\nComments:\n- Overall, the paper is well-written and well-structured. Although most of the results are in the appendix, the authors have done a good job in what they reported in the paper and what they left for the appendix. \n- I personally think ICLR is not a good venue for this kind of papers. Places like COLT and journals are better because they allow the authors to report most of the results in the main paper, and give more time to the reviewers to go over all the proofs. \n- I did not have time to go over all the proofs in the appendix but I checked those in the paper. I did not find any error, but the math can be written more rigorously. \n- The algorithm is quite similar to the one by Jin et al. (2018), which is for finite-horizon problems. The authors discuss on Page 4 that why despite the similarity of the algorithms, the techniques in Jin et al. (2018) cannot be directly applied to their case. It would be good if the authors have a discussion on the novelty of the techniques used in their analysis, not only w.r.t. Jin et al. (2018), but w.r.t. other analysis of model free algorithms in infinite-horizon discounted problems. \n- What makes this work different than Jin et al. (2018), and is its novelty, is obtaining a PAC bound. The resulting regret bound is similar to the one in Jin et al. (2018), but the PAC bound is new. It would be good if the authors mention this on Page 4. This makes it more clear in which sense this work is different than that by Jin et al. (2018). \n- It is important to write in the statement of Condition 2 that \"i\" changes between 0 and \\log_2 R. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575713021812, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper509/Reviewers"], "noninvitees": [], "tcdate": 1570237751110, "tmdate": 1575713021830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Review"}}}, {"id": "H1x2aR8Z9S", "original": null, "number": 3, "cdate": 1572069059978, "ddate": null, "tcdate": 1572069059978, "tmdate": 1572972586600, "tddate": null, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "invitation": "ICLR.cc/2020/Conference/Paper509/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends Jin et al. (2018)'s idea to infinite horizon and improves the best known sample complexity to $\\tilde{O}(\\frac{SA}{\\epsilon^2 (1-\\gamma)^7})$. The derivation is similar to Jin's paper except a very careful selection on the pseudo-horizon length $H$, where $H$ is given in finite horizon and work as the decaying rate for $\\alpha_k$, but for infinite horizon when we need to decide how to pick $H$.\n\nTheoretical Soundness: I didn't check every step of the proof, but I the steps I check is correct and I can feel that the derivation is solid.\n\nNovelty: In section 3.2, the authors discuss the difference between finite case and infinite case. I don't agree with the example if the starting state $s_1$ is with leaving probability less than $T^{-1}$. In this case, the latter rewards counted for $V(s_1)$ will multiply by $\\gamma^t$ which is pretty small, and will not contribute too much on the error. But I do agree that the case of infinite horizon is different since we need to carefully decide the decaying rate of $\\alpha_k$, which is definitely related to $\\frac{1}{1-\\gamma}$ but we need to figure out the relation. I think that is the most difficult part of the whole proof and this is the main technical contribution for the paper.\n\nMinor issue: In algorithm box I didn't see $V^{\\pi_t}, \\hat{Q}_t$ which occurs a lot in the derivation. Maybe change $\\hat{V}$ and $\\hat{Q}$ in the way align to the derivation?\n\nOverall I think this paper is sufficient to get in the conference. But to be honest since I lack the background of PAC-RL, I would remain a conservative of weak accept and would like to hear more discussions from other reviewers and authors to finalize my decision."}, "signatures": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper509/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP", "authors": ["Yuanhao Wang", "Kefan Dong", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "dkf16@mails.tsinghua.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["theory", "reinforcement learning", "sample complexity"], "TL;DR": "We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result.", "abstract": "A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \\emph{without} accessing a generative model. We show that the \\textit{sample complexity of exploration} of our algorithm is bounded by $\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously best known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\\epsilon$ as well as $S$ and $A$ up to logarithmic factors.", "pdf": "/pdf/3a261a8730f71c072aeea18bbd7d9a757ebc725d.pdf", "paperhash": "wang|qlearning_with_ucb_exploration_is_sample_efficient_for_infinitehorizon_mdp", "_bibtex": "@inproceedings{\nWang2020Q-learning,\ntitle={Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\nauthor={Yuanhao Wang and Kefan Dong and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkglSTNFDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f18e675c2d09159773e2a027f9c388e9156c4d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkglSTNFDB", "replyto": "BkglSTNFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575713021812, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper509/Reviewers"], "noninvitees": [], "tcdate": 1570237751110, "tmdate": 1575713021830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper509/-/Official_Review"}}}], "count": 14}