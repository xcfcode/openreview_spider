{"notes": [{"id": "SyMDXnCcF7", "original": "B1euLbAqF7", "number": 1368, "cdate": 1538087967148, "ddate": null, "tcdate": 1538087967148, "tmdate": 1551821585337, "tddate": null, "forum": "SyMDXnCcF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxPSP9NlV", "original": null, "number": 1, "cdate": 1545017151500, "ddate": null, "tcdate": 1545017151500, "tmdate": 1545354472821, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Meta_Review", "content": {"metareview": "This paper provides a mean-field-theory analysis of batch normalization. First there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. They then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions.\n\nThe reviewers (and random commenters) found this paper very interesting. The reviewers were unanimous in their vote to accept.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting and surprising findings with a mean-field-theory analysis of batch normalization"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1368/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352864298, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352864298}}}, {"id": "HJeOL_au6Q", "original": null, "number": 4, "cdate": 1542146128396, "ddate": null, "tcdate": 1542146128396, "tmdate": 1542146128396, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "rkx3VuuPpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "content": {"comment": "Thanks for the detailed reply!", "title": "thanks"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613575, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyMDXnCcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613575}}}, {"id": "BJlt5Odvam", "original": null, "number": 6, "cdate": 1542060177295, "ddate": null, "tcdate": 1542060177295, "tmdate": 1542060177295, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "HkgmPcrZpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your careful review and useful comments! Overall, in response to your review and that of referee 3 we will include a more intuitive discussion of our results in the next revision of our text.\n\nTo reply to your other specific comments,\n\n1) The intuition for batchnorm can be put in a more general setting. If a function f: X -> Y tends to spread out small clusters in the input space almost evenly in the output space, then one can expect that its gradients will be large typically. In our case, a batchnorm network can be understood as a function that sends a batch of inputs to a batch of outputs. In the appendix, we showed that the correlation between two different batches tend to a constant value independent of the input batches. No matter how close two input batches are, the output batches will have the same \u201cdistance\u201d from each other -- small movements in the input space leads to large movements in the output space. Thus we can expect the gradients to be large as well. We have added a new figure to the Appendix to further support this intuition. In it, we pass through a linear batchnorm network 2 minibatches. Both minibatches contain points on the same circle and 1 point off the circle that is unique to each minibatch. While the circle in each minibatch will remain an ellipse as they are propagated through the network, the angle between the planes spanned by them increasingly becomes chaotic with depth.\n\n3) As observed in [1] and [2], depthwise convergence to covariance fixed points is bad for training, and the best networks are either moderately deep or initialized such that the depthwise convergence rate to the fixed point is as slow as possible. We observe that deep networks whose activation statistics resemble a non-BSB1 fixed point typically feature worse gradient explosion than BSB1 networks. This seems to be because the nonlinearities that induce these fixed points increase rapidly (for example, polynomials with high degrees), so that the corresponding derivatives are also large, causing gradient explosion. \n\n(The reason that rapidly increasing nonlinearities don\u2019t converge to BSB1 fixed points is that, after a spontaneous symmetry-breaking, begins a \u201cwinner-take-all\u201d covariance dynamics, in which the activations of a few examples in the batch suddenly dominates those of the others in the batch, and this dominance persists across each layer.)\n\n4) We were a bit confused by what was meant by \u201cpractice\u201d here. We have thoroughly verified that for realistic input distributions (MNIST and CIFAR10) and common initialization strategies (weights that are randomly distributed) our theory makes accurate prediction. Moreover, we have shown that these predictions can be connected to practice in the sense that they predict whether or not the network can be trained.\n\nHaving said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.\n\nIf this did not properly address your question, please feel free to let us to know and we will improve this response!\n\n[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein. Deep Information Propagation (https://arxiv.org/abs/1611.01232)\n[2] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626165, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1368/Authors|ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626165}}}, {"id": "rkx3VuuPpQ", "original": null, "number": 5, "cdate": 1542060084279, "ddate": null, "tcdate": 1542060084279, "tmdate": 1542060084279, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "Sklr8el-6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your thoughtful comments about our paper, we\u2019re very happy that you found it interesting! We absolutely agree about the need for precise analysis to disentangle the many complexities that compete in deep learning.\n\nRegarding your question, great observation! Indeed, they are manifestations of the same underlying phenomenon. Below around L=50, the amount of gradient explosion is small enough that it doesn\u2019t significantly deteriorate performance --- this is shown in figure 2. At the same time, the gradients are small compared to the corresponding weights, so that after the first few steps, the weights themselves don\u2019t change much --- this is shown in figure 3a. If the weights don\u2019t change much, then the gradient dynamics remain roughly the same --- this is shown in figure 3b and 3c. Conversely, for L > 50, gradient explosion dominates the weight matrices, so much that |W| at time 1 is roughly the same as the norm of the corresponding gradient. After 1 step, the exponentially decreasing norms (with depth) of the weights attenuate the gradient explosion. This is because of the batchnorm property dBN(ax)/d(ax) = 1/a dBN(x)/dx. Thus in figure 3b and 3c we see gradient vanishing for L > 50 after 1 step of SGD.\n\nWe indeed used the \u201cback-prop weight\u201d assumption, and we will be more explicit about its usage in the next revision.\n\nWe are also extremely interested in carrying out the calculation for residual networks!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626165, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1368/Authors|ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626165}}}, {"id": "H1gQCvdvTm", "original": null, "number": 4, "cdate": 1542059979498, "ddate": null, "tcdate": 1542059979498, "tmdate": 1542059979498, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "r1lORJDq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your review and very useful comments! We\u2019re happy you found our manuscript interesting.\n\nTo address your comments:\n\n1) Thank you for pointing out that we had not defined the delta. Here delta is the Kronecker delta defined so that \\delta_{a,b} = 1 if a = b and 0 if a != b. In the context of the variance of the multivariate normal distribution, the delta function indicates that the different neurons in each layer have zero covariance. We\u2019ll add an explicit discussion of this fact to the manuscript.\n\n2) Thanks for pointing this out, we\u2019ll correct it in the next revision.\n\n3) It is true that the extent to which randomized weights describe trained networks is unclear. However, it is true that most commonly used weight initialization schemes are random. For example, He initialization [1] and Xavier initialization [2] strategies are both special cases of the setup considered here. We therefore view our theory as a theory of neural networks at initialization. (There are, however, initialization schemes that are not random and that are not described by our theory).\n\n[1] K. He, X. Zhang, S. Ren, J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. (http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) \n[2] X. Glorot, Y. Bengio, Y. W. Teh, M. Titterington. Understanding the difficulty of training deep feedforward neural networks. (http://proceedings.mlr.press/v9/glorot10a.html) "}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626165, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1368/Authors|ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626165}}}, {"id": "SkxSjPODaQ", "original": null, "number": 3, "cdate": 1542059933416, "ddate": null, "tcdate": 1542059933416, "tmdate": 1542059933416, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "BJxndjHwnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for the review and careful reading of our paper! We\u2019re glad that you found it of interest. On revision we will fix the typos that you identified.\n\nRegarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1]. When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs. At this point the network becomes untrainable. To reconcile this with the commonsense intuition that \u201cdeeper is better\u201d, our answer is twofold.\n\n1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn\u2019t approach its fixed point over depths often considered in machine learning. When this is the case one can safely increase the depth without sacrificing accuracy.\n\n2) It seems that the role of depth in performance is more subtle than standard intuition would dictate. For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy.\n\nIn the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.\n\n[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein. Deep Information Propagation (https://arxiv.org/abs/1611.01232)\n[2] G. Yang and S. S. Schoenholz. Mean Field Residual Networks (https://arxiv.org/abs/1712.08969) \n[3] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626165, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1368/Authors|ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626165}}}, {"id": "Sklr8el-6Q", "original": null, "number": 3, "cdate": 1541632077291, "ddate": null, "tcdate": 1541632077291, "tmdate": 1541804023842, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "content": {"comment": "I finally found the time to read this paper, and was glad that I did so.\n\nThe conclusion that at initialization batch norm actually harms at large depth is quite surprising, and this cannot be reached without precise analysis. This is the unique strength of the mean-field framework, in contrast with many other theoretical studies.\n\nI have a question. The authors plot the depth scale in Fig 2. The depth scale, which is based on calculations at initialization, looks very predictive as in Fig 2, but as Fig 3 suggested, the behavior at t=10 is much different from the behavior at initialization. Yet interestingly the limit depth in Fig 2 (about L=50) coincides so well with the pictures in Fig 3, where we see there is a sort of phase transition around L=50. Are they related somehow?\n\nA suggestion. The back-propagation calculation seem to assume that the \"back-prop weight\" is independent of the \"forward-prop weight\". If the assumption is used, I think it should be mentioned, since this assumption is highly non-trivial.\n\nIt would be interesting to carry out the calculations for residual architectures.", "title": "comments on the work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613575, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyMDXnCcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613575}}}, {"id": "HkgmPcrZpX", "original": null, "number": 3, "cdate": 1541655131302, "ddate": null, "tcdate": 1541655131302, "tmdate": 1541655131302, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "content": {"title": "Interesting and counter-intuitive results about batch-normalization", "review": "This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights. There are a number of interesting predictions made in this paper on the basis of this analysis. The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.\n\nComments:\n\n1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?\n\n2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.\n\n3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry? For instance, are BSB1 fixed points good for training neural networks?\n\n4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations. For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice. It would be good to mention this in the introduction or the conclusions.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "cdate": 1542234244786, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335931795, "tmdate": 1552335931795, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lORJDq3m", "original": null, "number": 2, "cdate": 1541201872361, "ddate": null, "tcdate": 1541201872361, "tmdate": 1541533192062, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "content": {"title": "The detailed analysis of the training of DNN with the batch normalization is quite interesting. ", "review": "\nThis paper investigates the effect of the batch normalization in DNN learning.\nThe mean field theory in statistical mechanics was employed to analyze the\nprogress of variance matrices between layers. \nAs the results, the batch normalization itself is found to be the cause of gradient explosion. \nMoreover, the authors pointed out that near-linear activation function can improve such gradient explosion. \nSome numerical studies were reported to confirm theoretical findings.\n\nThe detailed analysis of the training of DNN with the batch normalization is quite interesting. \nThere are some minor comments below.\n\n- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?\n- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3. \n- The randomized weight is not very practical. Though it may be the standard approach of mean field,\nsome comments would be helpful to the readers. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "cdate": 1542234244786, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335931795, "tmdate": 1552335931795, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxndjHwnm", "original": null, "number": 1, "cdate": 1541000051822, "ddate": null, "tcdate": 1541000051822, "tmdate": 1541533191858, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "content": {"title": "Interesting work, strong results", "review": "This paper provides a new dynamic perspective on deep neural network. Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers. Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system. Local performance around the fixed point is explored. Extensions are provided to include the batch normalization. I believe this paper may stimulate some interesting ideas for other researchers.\n\nTwo technical questions:\n\n1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point. How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough? This somewhat conflicts the commonsense of \"the deeper the better?\" \n\n2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Review", "cdate": 1542234244786, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335931795, "tmdate": 1552335931795, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgsnvp59X", "original": null, "number": 1, "cdate": 1539131314869, "ddate": null, "tcdate": 1539131314869, "tmdate": 1539131314869, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "rygMio2X57", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "content": {"title": "Clarification", "comment": "Thanks for your interest! \n\n1) The numerator and denominator are correct, but I think the meaning might be slightly unclear. When we write |\\nabla_{W^l}L|^2, we mean the norm of the gradient of the loss with respect to the weights in layer l. Thus, in fig. 3 we plot the ratio of the size of the gradient of the loss with respect to the weights in layer 10 compared with layer L for a network of depth L (note that both gradients are for a network of depth L). When gradients explode, the norm of the gradient increases during backprop so that the magnitude is larger nearer to the \"input\" to the network than the \"output\". Thus, this is essentially saying that the gradient in the 10'th layer of the network is exponentially larger for deeper networks than for shallower ones.\n\n2) \\gamma = 1, \\beta = 0 at initialization for figure 3. In all experiments, \\gamma and \\beta were free parameters that could be learned during training.\n\n3) The choice of 10 was arbitrary and the results don't depend strongly on the choice. The only thing that one has to be careful of is that our theoretical results are \"asymptotic\" in the depth. So if you look at layers that are too close to the inputs you can get transient effects that change the clean exponential scaling. \n\nPlease let us know if you have any further questions or if this response was unclear.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1368/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626165, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyMDXnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1368/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1368/Authors|ICLR.cc/2019/Conference/Paper1368/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626165}}}, {"id": "rygMio2X57", "original": null, "number": 2, "cdate": 1538669465784, "ddate": null, "tcdate": 1538669465784, "tmdate": 1538669465784, "tddate": null, "forum": "SyMDXnCcF7", "replyto": "SyMDXnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "content": {"comment": "Interesting work, I have a few questions about Fig. 3. Were the numerator and denominator of the y-axes flipped accidentally? The current labeling seems to imply that the gradient wrt each of the quantities in a,b,c is monotonically decreasing with depth L at t=0, which I wouldn't expect for layers with constant width. Also, what was gamma set to in this case (e.g. fixed, or learnable with initialization X)? And any reason as to why layer 10 was chosen as normalization? This confused me initially because I only saw in-text references to \"10\" in the context of training steps.", "title": "Clarification re Fig. 3"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1368/Reviewers/Unsubmitted"], "writers": ["~Angus_Galloway1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean Field Theory of Batch Normalization", "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.", "keywords": ["theory", "batch normalization", "mean field theory", "trainability"], "authorids": ["gregyang@microsoft.com", "jpennin@google.com", "vinaysrao@google.com", "jaschasd@google.com", "schsam@google.com"], "authors": ["Greg Yang", "Jeffrey Pennington", "Vinay Rao", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "TL;DR": "Batch normalization causes exploding gradients in vanilla feedforward networks.", "pdf": "/pdf/2171c2c7a748443c6ba04c4e2be436f1bb09e9e2.pdf", "paperhash": "yang|a_mean_field_theory_of_batch_normalization", "_bibtex": "@inproceedings{\nyang2018a,\ntitle={A Mean Field Theory of Batch Normalization},\nauthor={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMDXnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1368/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613575, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyMDXnCcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1368/Authors", "ICLR.cc/2019/Conference/Paper1368/Reviewers", "ICLR.cc/2019/Conference/Paper1368/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613575}}}], "count": 13}