{"notes": [{"id": "HJlQfnCqKX", "original": "HJl4QbjtKm", "number": 1249, "cdate": 1538087946831, "ddate": null, "tcdate": 1538087946831, "tmdate": 1550896318831, "tddate": null, "forum": "HJlQfnCqKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xq2DCXe4", "original": null, "number": 1, "cdate": 1544968114215, "ddate": null, "tcdate": 1544968114215, "tmdate": 1545354489629, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Meta_Review", "content": {"metareview": "The paper suggests a new measurement of layer-wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A layer-wise geometric margin distribution is used to calibrate the generalization ability, with extensive experimental support yet lacking a theory."}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1249/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352908273, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352908273}}}, {"id": "Ske1HVNc07", "original": null, "number": 6, "cdate": 1543287862891, "ddate": null, "tcdate": 1543287862891, "tmdate": 1543292410245, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "SylJSZ_STX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Addressing your comments.", "comment": "Thank you for the review. We address your concerns below.\n\n#What benefit can be acquired when using geometric margin defined in the paper.#\nThe geometric distance is the actual distance between a point \u201cx\u201d and the decision boundary f(x)=0, i.e. d1=min_x ||x|| s.t. f(x)=0.This term is usually used in contrast to functional distance defined as d2=f(x). If x is on the decision boundary, d1=d2=0, but otherwise d1 and d2 can differ. Note that d2 can change by simple reparametrization. For instance, consider a linear decision boundary f(x)=w.x. In this case, geometric distance d1=f(x)/||w|| and d2=f(x). Let F(x)=(c*w).x, i.e. just scaling the weights by factor c. This does not change the decision boundary. For such F, d1 remains the same, but d2 scales with c. One can force a condition to make margins equal in both scenarios: by making the closet point to the decision boundary to have distance 1. However, this requires introducing an inequality per point, similar to SVMs. With geometric margin, we can work with an unconstrained optimization and directly apply gradient descent or SGD.\n\n#Why does normalization make sense?#\nOur normalization allows direct analysis of the margins across different models with the same topology (or different datasets trained on the same network), which is otherwise difficult due to the positive homogeneity of ReLU networks. For example, suppose we have two networks with exactly the same weight, and then in one of the networks, we scale weight_i by constant positive factor c and the weight_{i+1} by 1/c (i is a layer index), the predictions of the two networks remain the same; however, their unnormalized margin distribution will be vastly different and the normalized version will be exactly the same.\n\n#Why does the middle layer margin can help? #\nThere is no reason we can assume a-priori that maximizing only input or output margin (for example) is enough for good generalization. As shown in our ablation results in Tables 1 and 4, the combination of multiple layers performs significantly better. If we cut a deep network at any stage, we can treat the first half of the network as a feature extractor and the second half as the classifier. From this perspective, the margins at middle layer can be just as important as the margins in the output layer or input layer. Lastly, we note that Elsayed et. al. show that optimizing margin at multiple layers provides significant benefits for generalization and adversarial robustness. \n\n#Why a linear (linear log) relation between the statistic and generalization gap.#\nWe are not claiming this is the true relationship between the statistics and the generalization gap. The true relationship may very well be nonlinear and one could perform a nonlinear regression to predict the gap, but it would need regularization and more data to avoid overfitting while a linear combination of simple distributional features already attains high quality prediction (according to CoD, k-fold cross validation and MSE) across 700+ pretrained models. This suggests that a linear relationship is indeed a very close *approximation*.\n\n#I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here.#\nWe assume the reviewer is referring to theorem 1.1 of Bartlett et al. If one wishes to compute the gap to be the inside of the soft big O, the result will be much larger than the error emitted by our prediction, and will require picking appropriate gamma and delta values. We further note the following: the case study of Bartlett et. al. (section 2) explicitly show in their diagrams (Figures 2 and 3) the normalized distribution as evidence of generalization prediction power (instead of the bound itself) and this normalized distribution is closely related to but is not directly their bounds (they drop the log terms); extracting the statistics in a sense quantifies their case study. Before submitting the paper, we also had personal communication with one of the authors of Bartlett et. al., and the author agreed that our comparison was fair. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "HygHuQVq07", "original": null, "number": 5, "cdate": 1543287661374, "ddate": null, "tcdate": 1543287661374, "tmdate": 1543291771099, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Summary of revisions and responses to all reviewers.", "comment": "We thank all the reviewers for their comments, suggestions and questions. We have responded to each reviewer\u2019s individual comments below. We have modified the paper as follows to address common questions posed by the reviewers:\n\n1. Using negative examples: we have added linear fits to both test accuracy and generalization gap and shown comparisons with and without negative examples. Table 3 in Appendix 7 (page 13) shows these results. We see that using negative margins predicts accuracy better than the generalization gap. However, as noted above, we chose to predict generalization gap, and in that case, a log relationship provides much stronger prediction, but log transform cannot use negative margin values. \n2. To answer R2\u2019s question about the importance of hidden layers, we show in Table 4, Appendix 7, the results of fitting every single layer and compare to fitting all layers together. No single layer, input, hidden or output performs as well as the combination. We also provide intuition for why it is important from a theoretical perspective to use margins at hidden layers (Section 3). \n\nWe have added to the main body or appendix of the paper a few smaller edits:  \n1. typos identified by R1 (Eq. 4)\n2. more compact notations for Table 1\n\nClarifying explanations:\n1. Why we choose to discard negative margins (Sec. 3.1)\n2. Why we use both a linear and log regression model (Sec. 3.3)\n3. Mean square error computations (Tables 1, 3, and 4)\n4. Why we chose evenly spaced layers for our margin computations. (end of Section 3.2)\n5. Added references suggested by reviewers and commenter.\n\nLastly, we will release all the trained CIFAR-10 and CIFAR-100 models. We hope this work along with the model dataset will open up interesting avenues for future research.\n\nWe hope the rebuttal and revision have addressed the reviewers\u2019 questions and comments. \n\nThank you!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "BylGPEVq07", "original": null, "number": 7, "cdate": 1543287898415, "ddate": null, "tcdate": 1543287898415, "tmdate": 1543287898415, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "Ske1HVNc07", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Addressing your comments (contd.)", "comment": "#If you do a regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)#\nIn the Appendix (Section 9.1 and 9.2), we already show both cross-architecture and cross-dataset comparisons, which achieve good predictive accuracy but worse than the result on a single architecture. However, when we tried using the result from cnn alone to predict the generalization gap of residual network or vice versa (not included in the paper), the result does not signify any interesting correlation. Nevertheless, we would like to emphasize that the regression is shared (and gives an accurate prediction) across other significant changes such as channel sizes, batchnorm/group norm, regularization, learning rate, dropout change (presented in appendix section 6)\n\n# Novelty #\nAs you correctly pointed out, our work and Barlett et. al. build on the broad notion of \u201cmargin distribution\u201d and \u201cnormalization\u201d. However, there are significant differences:\n1. Bartlet\u2019s definition of margin relies only on f_i-f_j, which only reflects margin in the output space, as opposed to (f_i-f_j)/||d/dx f_i - d/dx f_j|| which approximates margin in input (or any hidden) space.\n2. The normalization used in Bartlett et al. is a complexity measure which is drastically different from our normalization that captures more direct geometric properties of the activations. Specifically, Bartlett\u2019s normalization relies on the spectral complexity of a network which involves spectral norm of weight matrices and reference matrices. In our work, the normalization is defined based on the total variance of the activations of the hidden layers directly (Eqs 4 and 5). \n4. Barlett et. al. do *not* show any linear relationship between margin and test performance or gap. \nThe above distinctions lead to very different predictions on the generalization gap as shown in our results (Figure 2 and Table 1). In fact, the choice of distributional features and normalization scheme are crucial for accurate prediction of the generalization gap.\n\nFurthermore, we note again that the normalization scheme of Bartlett et. al. cannot be used as-is for residual networks and is not applicable to hidden layers, a drawback not present in our normalization. Finally, we have conducted a far larger scale of experiments as compared to Bartlett et. al. to verify the effect of each prediction scheme of the generalization gap. Like we mentioned in our response to reviewer 1, we will be releasing the 700+ realistic models we used in the paper as a dataset where researchers can easily test theories on generalization, which is one of the first of its kind. \n\nRegarding Liao et. al. 2018, as stated in the paper, their proposed normalized loss leads to a significant *decrease* in output margin confidence, which is the opposite of what is desirable. Furthermore, normalized cross-entropy loss is different from margin-based loss, so we do not think their observation takes away the novelty of our paper just because both works illustrate linearity.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "H1xTRJmc3m", "original": null, "number": 2, "cdate": 1541185493301, "ddate": null, "tcdate": 1541185493301, "tmdate": 1543279465419, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "content": {"title": "Well written; technically weak", "review": "After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. \n\nThe paper is well written and is mostly clear. (1st line on page 4 has a typo, \\bar{x}_k in eq (4) should be \\bar{x}^l?)\n\nNovelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since:\n\n(i). The fact that \"normalized\" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; \n(ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; \n(iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. \n\nTechnical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them?\n\nExperiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, \"Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors.\". How can the authors explain this? \n\nBy using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "cdate": 1542234271270, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335905747, "tmdate": 1552335905747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylJSZ_STX", "original": null, "number": 4, "cdate": 1541927222845, "ddate": null, "tcdate": 1541927222845, "tmdate": 1541927222845, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "content": {"title": "An empirical study towards the prediction power based on the margin distribution at each layer.", "review": "The author(s) suggest using geometric margin and layer-wise margin distribution in [Elsayed et al. 2018] for predicting generalization gap.\n\npros,\na). The author shows large experiments to support their argument.\n\ncons,\na). No theoretical verification (nor convincing intuition) is provided, especially for the following questions,\n    i) what benefit can be acquired when using geometric margin defined in the paper.\n    ii) why does normalization make sense beyond the simple scaling-free reason. For example, spectral complexity as a normalization factor in [Bartlett et al. 2017] is proposed from the fact, that the Lipschitz constant determines the complexity of network space.\n    iii) why does the middle layer margin can help? \n    iv) why a linear (linear log) relation between the statistic and generalization gap.\n\nFurther question towards experiment,\ni) I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here.\nii) If you do regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)?\n\nFinally, I'm not sure the novelty is strong enough since the margin definition comes from [Elsayed et al. 2018] and the strong linear relationship has been shown in [Bartlett et al. 2017, Liao et al. 2018] though in different settings.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "cdate": 1542234271270, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335905747, "tmdate": 1552335905747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gSCYhmTX", "original": null, "number": 4, "cdate": 1541814733160, "ddate": null, "tcdate": 1541814733160, "tmdate": 1541814812794, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "H1xTRJmc3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Novelty, Experiments, Technical Details", "comment": "\nWe thank you for your insightful review.\n\n## NOVELTY ##\n\nR2: \u201cThe fact that normalized margins are correlated with generalization was shown in Bartlett Fig 1\u201d.\n\nAs you pointed out, both works build on the broad notion of \u201cmargin distribution\u201d and \u201cnormalization\u201d. However, there are significant differences:\n1. Margin in Bartlett uses f_i-f_j that can only reflect output margins, as opposed to (f_i-f_j)/||d/dx f_i - d/dx f_j|| that works for any layer.\n2. We do not use margin distribution itself to predict the generalization gap, but rather distributional features that involve \u201cnonlinear transform\u201d of the distances (quartiles or moments).\n3. Normalization in Bartlett\u2019s uses norm of weight matrices, which is drastically different from geometric spread of activations (variance) we use (Eqs 4 and 5). Also their cannot be used as-is for residual networks, a drawback not present in our normalization. \n\nThese distinctions result in very different predictions of the generalization, as clearly shown in our Fig 2 and Table 1. In fact, the choice of distributional features and normalization are crucial for accurate prediction of the generalization gap.\n\nFinally, we have conducted a far larger scale of experiments, and will be releasing the 700+ realistic models used in the paper so that researchers can easily test generalization theories. This is the first of its kind. \n\n\n## TECHNICAL ##\n\n# Missing Absolute Value in Eq (3) #\n\nThere is no incorrectness; we deliberately adopt \u201csigned distance\u201d. The polarity reflects which side of the decision boundary the point is. Even Eq (7) of Elsayed that you mentioned quickly evolves to signed distance in their Eq (8).\n\n# Why Negative Distance Implies Misclassification #\n\nIt was our oversight not to mention that \u201ci\u201d in our Eq (3) corresponds to the ground truth label. We will clarify this in the final version. In this case, f_i-f_j>0 (i.e. distance is positive) implies correct classification and f_i-f_j<0 implies misclassification. \n\n# Why Negative Points are Ignored #\n\nWe indeed investigated using negative distances. We observed that:\n\n1. Modern deep architectures often achieve near perfect classification on training data. Hence, the contribution of negative distances to the full distribution is negligible in most trained models.\n\n2.  A small fraction of models do have notable misclassification (due to data augmentation or heavy regularization). For these models, we found that margin distribution computed with only positive samples predicted the generalization gap better than (or at par with) the full distribution. However, we observed that the latter is indeed a better predictor of test accuracy (just not the gap). Since we focus our narrative on the generalization gap, we decided to omit these results from the main paper; however, we will include these results in the appendix.\nWe also note that there is no technical problem in using margin distribution with only positive samples, e.g. Bartlett\u2019s work \u201cThe Sample Complexity of Pattern Classification with Neural Networks\u201d develops a generalization bound by such samples (paragraph above their Theorem 2).\n\n\n## EXPERIMENTS ##\n\n# Why 4 Layers and Why Even Spacing #\n1. This leads to a fixed-length signature vector, hence agnostic to the architecture and depth.\n2. Computing signature across all layers is expensive for large deep models.\n3. Larger signature would require more pre-trained networks to avoid overfitting in regression phase. Given that each pre-trained network is only one sample in the regression task, creating a large pool of models is prohibitively expensive. Our study with 700 realistic sized pre-trained networks is perhaps already beyond the common practice for such empirical analysis. \n4. The even spacing is merely a natural choice of minimal commitment and already achieves near perfect prediction (CoD close to 1) is some scenarios. However, it is possible to examine other configurations.\n\n# Log/Linear #\nWe are not sure if we understand the question. We provide an answer below, but if this is not what you meant, please let us know. We investigate the use of signature components in two ways: 1. Directly as the input to linear regression, 2. Applying an element-wise log to them before using them as input of the linear regression. In either case, the regression remains linear in optimization variables, but with the log transform we effectively regress the product of signature components to the gap value.\n\n# Other Criteria (MSE, AIC, etc.) #\nWe have pointed out that the coefficient of determination already captures the MSE along with the scale of the error; however, for completeness, we will include this result in the appendix. We report k-fold cross validation results as well, which is known to be asymptotically equivalent to AIC (Stone M. (1977) An asymptotic equivalence of choice of model by cross-validation and Akaike\u2019s criterion)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "S1e5qVsQaQ", "original": null, "number": 3, "cdate": 1541809297905, "ddate": null, "tcdate": 1541809297905, "tmdate": 1541814786466, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "Hkl7VhOqhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Addressing your comments", "comment": "We would like to thank you for your review and suggestions. We are very glad that you liked the empirical analysis of generalization gap and margin distribution statistics. On that note, while not mentioned in the paper, we are in preparation to release the  700+ models we used in the paper as a dataset where researchers can easily test theories on generalization. We believe this will be one of the first datasets for studying generalization on realistic and modern network architectures and we hope it will be instrumental in the ongoing generalization research.\n\n\n## Construction of Signature from Pairwise Distances (i,j) in Eq (5) ##\n\nFor computational efficiency, we picked we pick ground truth label as \"i\" (as you correctly pointed out), and the highest non-ground truth logit as \"j\", and compute the distance between the two classes. While aggregating all pairwise distance might be more comprehensive, the complexity scales roughly quadratically with the number of classes. As such, we made the design choice to use the top two classes. In cases where the class with the highest logit is not the ground truth (hence misclassification with negative distance), we discard the data point. We will further explain this choice below. We mention this detail in the text but we will make sure it is more clear.\n\n\n## Notation (i,j) instead of {i,j} to Emphasize Orderedness ##\n\nThank you for the suggestion. We agree and will incorporate this in the revision to avoid confusion.\n\n\n## Why Only Positive Distances in Margin Distribution ##\n\nYou are right that when \u201ci\u201d is the ground truth label, the sign of the distance indicates whether the point is correctly classifier or is misclassified. \n\nWe indeed investigated using negative distances when computing the margin distribution. We observed that:\n\n1. Modern deep architectures often achieve near perfect classification on training data. Hence, the contribution of negative distances to the full distribution is negligible in most trained models.\n\n2.  A small fraction of models do have notable misclassification (due to data augmentation or heavy regularization). For these models, we found that margin distribution computed with only positive samples predicted the generalization gap better than (or at par with) the full distribution. However, we observed that the latter is indeed a better predictor of test accuracy (just not the gap). Since we focus our narrative on the generalization gap, we decided to omit these results from the main paper; however, we will include these results in the appendix.\nWe also note that there is no technical problem in using margin distribution with only positive samples, e.g. Bartlett\u2019s work \u201cThe Sample Complexity of Pattern Classification with Neural Networks\u201d develops a generalization bound by such samples (paragraph above their Theorem 2).\n\n\n## Typo ##\n\nThank you for pointing out the typo. It will be fixed in revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "Hkl7VhOqhX", "original": null, "number": 3, "cdate": 1541209131350, "ddate": null, "tcdate": 1541209131350, "tmdate": 1541533295627, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "content": {"title": "A nice empirical paper with good intuitions and encouraging results", "review": "This paper does not even try to propose yet another \"vacuous\" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some \"prescriptive\" insights (per Sanjeev Arora) towards understanding generalization in deep neural nets.\n\nI have no major complaints but for a few questions regarding clarifications,\n1. From Eq.(5), such distances are defined for only one out of the many possible pairs of labels. So when forming the so-called \"margin signature\", how exactly do you compose it from all such pair-wise distances? Do you pool all the distances together before computing the statistics, or do you aggregate individual statistics from pair-wise distances? And how do you select which pairs to include or exclude? Are you assuming \"i\" is always the ground-truth label class for $x_k$ here?\n\n2. In Eq.(3), the way you define the distance (that flipping i and j would change the sign of the distance) is implying that {i, j} should not be viewed as an unordered pair, in which case a better notation might be (i, j) (i.e. replacing sets \"{}\" with tuples \"()\" to signal that order matters).\n\nAnd why do you \"only consider distances with positive sign\"? I can understand doing this for when neither i nor j corresponds to the ground-truth label of x, because you really can't tell which score should be higher. But when i happens to be the ground-truth label, wouldn't a positive distance and a negative distance be meaningful different and therefore it should only be beneficial to include both of them in the margin samples?\n\nAnd a minor typo: In Eq.(4), $\\bar{x}_k$ should have been $\\bar{x}^l$?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Review", "cdate": 1542234271270, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335905747, "tmdate": 1552335905747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgvJXxt2Q", "original": null, "number": 1, "cdate": 1541108446853, "ddate": null, "tcdate": 1541108446853, "tmdate": 1541108471002, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "BJgDuN3On7", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "content": {"title": "Thanks for comments", "comment": "Thank you for your helpful comments.\n\n### References ###\n\n1. We agree that the interaction of margin and generalization has been subject to a great amount of research in classical ML literature. This makes it impossible to provide a comprehensive survey in a conference paper. So we had to narrow the scope of related works to recent papers that address generalization/margin in the case of *deep* models. Nonetheless, we will be happy to include the references on SVM and clustering that you suggested.\n\n2. Regarding the other ICLR2019 submission you mentioned, obviously we were not aware of it prior to ICLR submission deadline (and it is not available on arxiv either). We are aware of that submission, but it seems to have some issues (reading the comments for the paper).\n\n### Linear Assumption ###\n\n1. Regarding your suspicion of linear relationship between margin and generalization gap: we are not directly relating the two using a linear map. Note that we are converting the margin distribution to a feature vector via a nonlinear map (quartiles/moments), and it is these features that are regressed to the generalization gap by a linear map. This is a widely used idea for nonlinear regression; e.g. as in kernel SVM for regression (nonlinear feature space followed by linear fitting). One could also train a nonlinear (deep) neural net to predict the gap, but it would need regularization and more data to avoid overfitting while a linear combination of simple distributional features already attains high quality prediction (see next point) across ~700 pretrained models. The latter suggests that a linear relationship is indeed a very close approximation.\n\n2. The point of the paper is not to claim an optimal feature set, but to leverage *simple* and *easy to compute* features that could be extracted from the distribution (like quartiles or moments) can yet give a reasonable prediction of the generalization gap that is much better than recent theoretical upper bounds in the literature. We hope this could be a step toward constructing *practical* algorithms for improving generalization in deep networks. Regarding mathematical proof for why these features should explain the generalization gap: while such results would be very interesting, it is quite ambitious if not impossible. Nevertheless, we assess the quality of the linear fit using one of the standard statistical tools created for this purpose: Coefficient of Determination (CoD).  As mentioned in the paper, in some scenarios we observe CoD=0.97 (max is 1.0) which indicates a reasonably good fit.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607978, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlQfnCqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1249/Authors|ICLR.cc/2019/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607978}}}, {"id": "BJgDuN3On7", "original": null, "number": 1, "cdate": 1541092462774, "ddate": null, "tcdate": 1541092462774, "tmdate": 1541092462774, "tddate": null, "forum": "HJlQfnCqKX", "replyto": "HJlQfnCqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1249/Public_Comment", "content": {"comment": "Introducing the theory of margin distribution into the framework of deep learning is an interesting idea. And it seems that there is a related work [Optimal margin Distribution Network, Submission to ICLR 2019], which has tried to design a new loss function based on margin distribution and theoretically proved its generalization effect. As I know, the influence of margin distribution has always been a concern for generalization theory. [Schapire, 1998] [Wang, 2011] [Gao, 2013], and there are several new algorithms based on the theory of margin distribution in both SVM [Zhang, 2017] and Clustering [Zhang, 2018] frameworks. I think that authors should read these papers and add references to them.\nRegarding the content of the paper, I am confused about the linear (or log() ) estimation of the generalization gap: \"$\\hat{g} = a^T \\phi(\\theta) + b$\". Does this formula have a theoretical analysis or some statistical models to explain it? It seems unreasonable to directly explain the relationship between margin distribution and generalization with a simple linear relationship. I expect that the authors can theoretically give a formula to explain the relationship between the generalization gap and the margin distribution.\n\n\n[Optimal margin Distribution Network, Submission to ICLR 2019] Anonymous. \u201cOptimal margin Distribution Network\u201d Submitted to International Conference on Learning Representations 2019\n[Schapire, 1998] Schapire, R., Freund, Y., Bartlett, P. L., Lee, W. Boosting the margin: A new explanation for the effectives of voting methods. Annuals of Statistics 26 (5), 1651\u20131686. 1998\n[Wang, 2011] Wang, L. W., Sugiyama, M., Yang, C., Zhou, Z.-H., Feng, J. \u201cA refined margin analysis for boosting algorithms via equilibrium margin.\u201d Journal of Machine Learning Research 12, 1835\u20131863. 2011\n[Gao, 2013] Gao, W., and Zhou, Z.-H. \"On the doubt about margin explanation of boosting.\" Artificial Intelligence 203, 1-18. 2013\n[Zhang, 2017] Zhang, T., Zhou, Z.-H. \"Multi-Class Optimal Margin Distribution Machine.\" International Conference on Machine Learning. 2017.\n[Zhang, 2018] Zhang, T., Zhou, Z.-H. \"Optimal Margin Distribution Clustering.\" Proceedings of the National Conference on Artificial Intelligence, 2018.\n", "title": "Some comments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1249/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "abstract": "As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\nOur measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.", "keywords": ["Deep learning", "large margin", "generalization bounds", "generalization gap."], "authorids": ["ydjiang@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "authors": ["Yiding Jiang", "Dilip Krishnan", "Hossein Mobahi", "Samy Bengio"], "TL;DR": "We develop a new scheme to predict the generalization gap in deep networks with high accuracy.", "pdf": "/pdf/e45c63ac8fbcb8d842a89e7192e5db72b0fcc0de.pdf", "paperhash": "jiang|predicting_the_generalization_gap_in_deep_networks_with_margin_distributions", "_bibtex": "@inproceedings{\njiang2018predicting,\ntitle={Predicting the Generalization Gap in Deep Networks with Margin Distributions},\nauthor={Yiding Jiang and Dilip Krishnan and Hossein Mobahi and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlQfnCqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1249/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311643348, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlQfnCqKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1249/Authors", "ICLR.cc/2019/Conference/Paper1249/Reviewers", "ICLR.cc/2019/Conference/Paper1249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311643348}}}], "count": 12}