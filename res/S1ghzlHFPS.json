{"notes": [{"id": "S1ghzlHFPS", "original": "BylwOXxKDH", "number": 2188, "cdate": 1569439763966, "ddate": null, "tcdate": 1569439763966, "tmdate": 1577168279184, "tddate": null, "forum": "S1ghzlHFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "D5j0__OBk", "original": null, "number": 1, "cdate": 1576798742751, "ddate": null, "tcdate": 1576798742751, "tmdate": 1576800893473, "tddate": null, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2188/-/Decision", "content": {"decision": "Reject", "comment": "While reviewers find this paper interesting, they raised number of concerns including the novelty, writing, experiments, references and clear mention of the benefit. Unfortunately, excellent questions and insightful comments left by reviewers are gone without authors\u2019 answers. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725330, "tmdate": 1576800277191, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2188/-/Decision"}}}, {"id": "r1e_z2XijB", "original": null, "number": 1, "cdate": 1573760015847, "ddate": null, "tcdate": 1573760015847, "tmdate": 1573760015847, "tddate": null, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2188/-/Official_Comment", "content": {"title": "Thanks and clarification. ", "comment": "Thanks very much to the reviewers -- these are high-quality reviews.  We appreciate the time you spent on the paper and the thoughtful feedback. \n\nOur presentation was written too quickly, and more careful writing would have answered some of your main concerns.  In the model, we do have ways to handle parameter sharing (last sentences in sections 3.2, 3.3 and 3.4) and event type composition (section A.4). In the experiments, we tuned hyper-params (including # hidden nodes) for the baseline model (which is indeed a strong multivariate point process) as well as for our model, so the gain is from the design improvement.  We will clarify these points in the next version.\n\nUnrelatedly, our technical approach has evolved and deepened considerably since we submitted this version. We don't think it would be appropriate to deeply change our ICLR submission at this late stage, so we'll just submit our next version to the next conference. We'll certainly take your comments into account as well -- thanks again!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2188/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2188/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1ghzlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2188/Authors", "ICLR.cc/2020/Conference/Paper2188/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2188/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2188/Reviewers", "ICLR.cc/2020/Conference/Paper2188/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2188/Authors|ICLR.cc/2020/Conference/Paper2188/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145044, "tmdate": 1576860535814, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2188/Authors", "ICLR.cc/2020/Conference/Paper2188/Reviewers", "ICLR.cc/2020/Conference/Paper2188/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2188/-/Official_Comment"}}}, {"id": "BJxQnlqy9S", "original": null, "number": 1, "cdate": 1571950762519, "ddate": null, "tcdate": 1571950762519, "tmdate": 1572972371599, "tddate": null, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper builds an interesting connection between Datalog rules and temporal point processes. The novelty of the approach is to factorize the latent state of LSTM into different blocks that represent three major interactions between temporal events, including: dependency, affects, and updates. The design of the node blocks within the hidden state allows the modeling of fine-grain structure of a given event type. Based on the Datalog program and the logic rules, the intensity function of the temporal point process can be formulated from facts in a database. The problem of enabling a flexible family of intensity functions is one of the most important topics in point processes, and a paper advancing knowledge in this area is certainly welcome.\n\nThe paper is in general well written. Section 2.2 can be more clarified by explicitly comparing the concepts of blocks and entities using \"mind(Alice)\" and \"body(Alice)\" before introducing the hidden state h_mind(Alice)(t). It took me some time going back and forth to understand these examples here. With respect to the design of the Datalog interface, it looks like it covers the assertion involving two arguments. Since these arguments affect the partition of the number of node blocks, it would be more clear to illustrate how to design the node blocks as the number of arguments increases (say beyond 2 arguments). In fact, if we know the number of entities in each event type, say the number of node blocks to partition is 3 per hidden state in advance, we can leverage three separate small LSTMs each of which has the private hidden state with the same number of nodes as that in one of the node blocks. Then, we can determine the interactions among these separate small LSTMs based on the logic rules, so it will be helpful to elucidate the additional advantages of partitioning these node blocks in the same hidden state. The proposed technique mainly considers how to incorporate the block design into the LSTM hidden states as a general sequence model. What is the unique characteristics of Neural Hawkes Process have been particularly exploited from this perspective? It looks like it can be applied to other LSTM-based approach as long as the predictions are functions of the hidden states. For the synthetic experiments, it is obvious that single Neural Hawkes process has more challenges to fit the mixture of processes. It will be more convincing to compare with a mixture model, like \"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering\" with the proposed approach, and the same as in the real experiments. Also, a standard test-of-goodness fit like QQ-plot will also be more useful to improve the experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575425360416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2188/Reviewers"], "noninvitees": [], "tcdate": 1570237726440, "tmdate": 1575425360437, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review"}}}, {"id": "r1lm7jx-9H", "original": null, "number": 2, "cdate": 1572043547013, "ddate": null, "tcdate": 1572043547013, "tmdate": 1572972371551, "tddate": null, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review for Temporal Modeling via Logical Specification of Factorial\nLSTMs\n\nThis paper addresses a key problem in machine learning: how to control\nthe inductive bias of a model in an interpretable way.  The paper\ncontributes a Datalog-based language that allows a human to hand-code\nstructural assumptions (typically based on domain knowledge) that are\nautomatically translated into sparsity patterns in the parameter\nmatrices of an ML model (in this case, a neural Hawkes process,\nalthough the idea would [probably] generalize to other cases).  The\nlanguage plus structured-neural-Hawkes process is demonstrated on a\nfew very small problems, with mixed results.\n\nThis paper is borderline.  However, I tend to favor rejection because\nwhile the ideas are very interesting (and potentially impactful),\nvalidation of the claims is weak.\n\nContributions:\n\nOn the positive side:\n\nA Datalog interface to specifying structural zeros in parameter\nmatrices is a good idea.  The language is natural, and the high-level\nmapping from structure and objects to low-level parameters seems\nreasonable and potentially useful.\n\nThe method makes it easier to specify an inductive bias.  This is a\nstep in the right direction; but at its heart, this paper does not do\nanything that couldn't have been done by hand - it only makes it\neasier.\n\nThe method is potentially more interpretable than other attempts at\ncontrolling inductive bias (for example, simple weight regularizaion),\nbut see below for why this might be a red herring.\n\nThe paper is very nicely written.  It's clear that a lot of attention\nto detail went into writing it.  Well done.\n\nWeaknesses:\n\nThere are a few major points to criticize about this paper.\n\nFirst, there is no clear learning or prediction benefit.  The results\nare mixed: while it appears that the SHP learns faster than the\nunstructured HP, they appear to be asymptoting at the same point.\nThis is perhaps to be expected, as the structural zeros introduced by\nthe corresponding Datalog program effectively reduce the parameter\ncount, but the shape of the learning curves is unchanged.\n\n(Also: please include error bars in Fig. 2(a1) and 2(a2))\n\nThe proper comparison would probably be to a low-rank parameter\nmatrix, where the parameter count is similarly reduced, but in an\nunstructured way.  That would allow us to disentangle \"parameter count\nreduction\" from \"inductive bias\", which is currently not done in the\npaper. \n\nThe results in Figure 3c are mixed - it appears that SHP is only\nbetter in 1/4 of the cases; in all other cases, the error bars seem to\nindicate that there is no predictive power.\n\nFinally, I am concerned that the method may give a false sense of\nexplainability to the model - why it is true that a highly structured,\nsymbolic language is being used to craft an inductive bias, there is\nno \"symbol grounding\".  That is, there is no guarantee that the neural\npart of the learning algorithm will use the parameters in the way the\nhuman intended it to, because the parameters are ultimately\ndisconnected from the symbols.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575425360416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2188/Reviewers"], "noninvitees": [], "tcdate": 1570237726440, "tmdate": 1575425360437, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review"}}}, {"id": "Skx3yBR8qH", "original": null, "number": 3, "cdate": 1572426979764, "ddate": null, "tcdate": 1572426979764, "tmdate": 1572972371504, "tddate": null, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": " Summary: \n\n \nThe paper proposed to use Datalog rules to specify the design of the LSTM architecture for event data in continuous time. The LSTM module will be used to model the rate of the events. By incorporating Datalog rules, the paper aims to encode informed inductive biases into the model. \n\nComments:\n\n    After reading the entire paper, I think the main idea of this paper is to use sparse and structured weight matrices (called structural zeros\u201d in the paper) to substitute the dense weight matrices in the original LSTM, and to split the hidden state into blocks where each block refer to a different world\u2019s state.  \n\n\n         How to design the structured weight matrices and how to define the node blocks, it is informed by the  Datalog rules. This design, however, will lead to a huge weight matrix and a very  long hidden state once the types of events and number of entities grow. The proposed model will face a severe scalability issue. From this point of view, only \u201cstructural zeros\u201d weight matrices are not enough for an elegant model.                \n\n \n\n        How to smartly share parameters and how to control the number of parameters will be an interesting direction to explore. This submission touches on this a bit but not in a principled way. For example, in Eq. 18(a) and 18(b), the embedding vectors for the grounded predicate is a summation of the embedding vectors of the entities and the predicates. This final embedding is empirically validated or is based on some permutation invariant property? This needs more clarification or some references.\n\n \n\n \n        2.      The presentation needs to be polished. The current writing is not easy to follow. Especially for section 3. The architecture design needs to be clarified more. When I read this part, I felt a little difficult to map the Datalog rules to your model. \n        3        Since you are learning the vector embeddings for event types and entities, what are the advantages of this compared to the marked point process model, where the event types and entities are treated as discrete markers and are a much more parsimonious model. The Datalog rules can also be defined on the marker level by introducing a structured dependency structure over the markers. What are the potential benefits of learning the embeddings? The explanation is missing in this paper. \n\n       4     Lack of references. The proposed neural-symbolic architecture shares some similarities to the following papers: \n          (1) End-to-End Differentiable Proving \n\n          (2) DeepProbLog: Neural Probabilistic Logic Programming \n\n          (3) Neural Logic Machines.\n          \n        What are your contributions and differences in terms of the neural-symbolic architecture design?\n\n \n       As for introducing logic rules to guide event predication, this is not a new topic. Here is a list of references:\n\n \n         (1) PEL-CNF: Probabilistic event logic conjunctive normal form for video interpretation. \n          (2) A general framework for recognizing complex events in Markov logic. \n\n          (3) Learning Bayesian networks for clinical time series analysis. \n\n          (4) Logical Hierarchical Hidden Markov Models for Modeling User Activities. \n\n          (5) Slice Normalized Dynamic Markov Logic Networks. \n       5.         Lack of strong baselines. The paper only did a small-scale experiment study. It only compares a neural Hawkes process model. The experimental evaluation also needs stronger baselines. Specifically, methods that can handle continuous-time (e.g. marked point process) or probabilistic logic methods that can discretize time (as mentioned in the above references). The baselines are not quite strong and appear a bit arbitrary in the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2188/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyuanmei@gmail.com", "gqin@jhu.edu", "chokkyvista06@gmail.com", "jason@cs.jhu.edu"], "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs", "authors": ["Hongyuan Mei", "Guanghui Qin", "Minjie Xu", "Jason Eisner"], "pdf": "/pdf/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "TL;DR": "Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.", "abstract": "Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.\n", "keywords": ["factorized LSTM", "temporal point process", "event streams", "structural bias", "Datalog"], "paperhash": "mei|informed_temporal_modeling_via_logical_specification_of_factorial_lstms", "original_pdf": "/attachment/91dcbe5f60f1e52c8558cc244c58521e3828e0ef.pdf", "_bibtex": "@misc{\nmei2020informed,\ntitle={Informed Temporal Modeling via Logical Specification of Factorial {\\{}LSTM{\\}}s},\nauthor={Hongyuan Mei and Guanghui Qin and Minjie Xu and Jason Eisner},\nyear={2020},\nurl={https://openreview.net/forum?id=S1ghzlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1ghzlHFPS", "replyto": "S1ghzlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2188/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575425360416, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2188/Reviewers"], "noninvitees": [], "tcdate": 1570237726440, "tmdate": 1575425360437, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2188/-/Official_Review"}}}], "count": 6}