{"notes": [{"id": "S1gtclSFvr", "original": "rJlvZkZFvS", "number": 2479, "cdate": 1569439888911, "ddate": null, "tcdate": 1569439888911, "tmdate": 1577168238449, "tddate": null, "forum": "S1gtclSFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hTPnoV1iuk", "original": null, "number": 1, "cdate": 1576798750005, "ddate": null, "tcdate": 1576798750005, "tmdate": 1576800885847, "tddate": null, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2479/-/Decision", "content": {"decision": "Reject", "comment": "This paper describes how they extend a previous phrase-based neural machine translation model to incorporate external dictionaries. The reviewers mention the small scale of the experiments, and the lack of clarity in the writing, and missing discussion on computational complexity. Even though the method seems to have the potential to impact the field, the paper is currently not strong enough for publication. The authors have not engaged in the discussion at all. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724705, "tmdate": 1576800276395, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2479/-/Decision"}}}, {"id": "HJx5Mz-xiH", "original": null, "number": 3, "cdate": 1573028370474, "ddate": null, "tcdate": 1573028370474, "tmdate": 1573028370474, "tddate": null, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposed an end-to-end phrase-to-phrase NMT model (NP2MT). I think the contribution of this paper is incremental and the idea is of less novelty. In general, the model is largely based on the NPMT model, where modification is the introduce of phrases in the source sentences. Then the author proposed the memory module strategy. In the experiments, the performance improves significant when using out of domain dictionary, but less significant for in-domain dictionary. I also have a concerns about the experiments. The dataset used in this paper seems not convincing to me. By my own experience, the performance on small dataset for either LSTM or Transformer is not stable. The authors just tested the model performance on WMT test set. I think at least the WMT training data should be used for training as well. Another question is the details about the training time and decoding time, since the dynamic programming is used.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574791943423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2479/Reviewers"], "noninvitees": [], "tcdate": 1570237722250, "tmdate": 1574791943438, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review"}}}, {"id": "SJlq8XkRFB", "original": null, "number": 1, "cdate": 1571840849578, "ddate": null, "tcdate": 1571840849578, "tmdate": 1572972332992, "tddate": null, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a phrase-based encoder-decoder model for machine translation. The encoder considers all possible phrases (i.e. word sequences) up to a certain length and compute phrase representations using bidirectional LSTMs from contextual word embeddings computed with another bidirectional LSTM layer. The decoder also considers possible segmentations and computes contextual representations for the previously generated segments. Each word in the current segment is generated by a Transformer model by attending to all phrases in the source sentence. The authors present a dynamic programming method for considering all possible segmentations in decoding. They also present a method for incorporating a phrase-to-phrase dictionary built by Moses into the decoding process.\n\nI like the idea of phrase-to-phrase translation and the relatively simple architecture proposed in the paper. At the moment, however, I am not quite sure how practical their approach is. One reason is the experimental setting. Both of the datasets used in the experiments are quite small and it is not clear how the proposed model performs when several millions of sentence pairs are available for training. \n\nAnother reason is that the computational cost of the proposed model is not really clear. The authors state that it is much more efficient than NPMT but it is not clear how it compares to the standard Transformer approach. It seems to me that the computational cost of their model is highly dependent on the value of P (maximum length of phrases). \n\nAt first, I thought the decoder was implemented with LSTMs, but I realized that it was actually implemented with a Transformer by reading the appendix. I think this should be explained in the main body of the paper. I am also wondering how the authors\u2019 model compares to a standard seq-to-seq model whose decoder is implemented with a Transformer.\n\nThe equation in section 2.2 seems to suggest that the model prefers segmentations with small numbers of segments. I am wondering if there is any negative effect on the translation quality.\n\nHere are some minor comments:\n\np.2 valid of -> valid\np.4 lookup -> look up?\np.4 forr -> for\np.4 indict -> indicate?\np.5 Table 1 -> Table 1"}, "signatures": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574791943423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2479/Reviewers"], "noninvitees": [], "tcdate": 1570237722250, "tmdate": 1574791943438, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review"}}}, {"id": "BJgZqngyqB", "original": null, "number": 2, "cdate": 1571912841332, "ddate": null, "tcdate": 1571912841332, "tmdate": 1572972332942, "tddate": null, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission belongs to the field of machine translation. In particular, it looks at the problem of phrase-to-phrase translation (previously used state of the art approach) using neural network approaches. The main idea behind this paper is to use a segmental (analogue of phrases) form of neural networks on the source side and attention mechanism to align those segments with segments generated on the target size. This submission additionally describes how an external dictionary can be incorporated using a heuristic approach. I believe this submission could be of wide interest to the machine learning community. I find experimental validation to be satisfactory whilst presentation unsatisfactory for the following reasons:\n\n1) notation\n\nGiven that you are dealing with two sets of sequences (source, target side), segments on both sides, attention linking both sides I find it strange that the notation used is not carefully introduced and clearly explained. What is ${\\bf g}_{{\\bf z}<z_k}$ using precise mathematical language? How it is different from ${\\bf g}_{{\\bf z}<k}$ and what that is? The same for ${\\bf y}_{<t}^{z_k}$, a*, d* and all other variables. In order to help the reader understand your approach it is fundamental to be precise and not ambiguous about each (!) symbol you are using. \n\n2) Algorithm 1, 2 and Figure 1\n\nThe algorithmic description was meant to help the reader to understand the process. Unfortunately I have to disagree that is has accomplished this purpose. Please make sure you have unambiguously explained every single term, explicitly say what you are running argmax over, etc. Please also make sure you are discussing/describing the algorithms/figures in your submission. Given the non-trivial nature, lack of proper introduction into the notation used, you cannot simply point the reader to it and not discuss it. \n\nMinor comments:\n\nPlease refrain from using \"vanilla model\" unless you can cite a publication defining exactly what that is.\nPlease explain how did you derive dictionary. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2479/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2479/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574791943423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2479/Reviewers"], "noninvitees": [], "tcdate": 1570237722250, "tmdate": 1574791943438, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2479/-/Official_Review"}}}, {"id": "HkxaB1i7uS", "original": null, "number": 1, "cdate": 1570119492662, "ddate": null, "tcdate": 1570119492662, "tmdate": 1570119685207, "tddate": null, "forum": "S1gtclSFvr", "replyto": "S1gtclSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2479/-/Public_Comment", "content": {"comment": "Thanks for your nice work! Now transformers prevail, it is really surprising to see traditional seq2seq models have such impressive performance.\nIs it possible to use a Transformer encoder instead? I guess the performance will further improve.\n", "title": "Is it possible to extend the encoder to Transformers?"}, "signatures": ["~Chong_Ruan1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chong_Ruan1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com"], "title": "Neural Phrase-to-Phrase Machine Translation", "authors": ["Jiangtao", "Feng", "Lingpeng Kong", "Po-sen Huang", "Chong", "Wang", "Da", "Huang Jiayuan", "Mao", "Kan", "Qiao", "Dengyong", "Zhou"], "pdf": "/pdf/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "abstract": "We present Neural Phrase-to-Phrase Machine Translation (\\nppmt), a phrase-based translation model that uses a novel phrase-attention mechanism to discover relevant input (source) segments to generate output (target) phrases. We propose an efficient dynamic programming algorithm to marginalize over all possible segments at training time and use a greedy algorithm or beam search for decoding. We also show how to incorporate a memory module derived from an external phrase dictionary to \\nppmt{} to improve decoding. %that allows %the model to be trained faster %\\nppmt is significantly faster %than existing neural phrase-based %machine translation method by \\cite{huang2018towards}. Experiment results demonstrate that \\nppmt{} outperforms the best neural phrase-based translation model \\citep{huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \\citep{vaswani2017attention}.", "keywords": [], "paperhash": "jiangtao|neural_phrasetophrase_machine_translation", "original_pdf": "/attachment/8c714e998aaa49fae25f97183655dd37175ebb47.pdf", "_bibtex": "@misc{\njiangtao2020neural,\ntitle={Neural Phrase-to-Phrase Machine Translation},\nauthor={Jiangtao and Feng and Lingpeng Kong and Po-sen Huang and Chong and Wang and Da and Huang Jiayuan and Mao and Kan and Qiao and Dengyong and Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gtclSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gtclSFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179697, "tmdate": 1576860585046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2479/Authors", "ICLR.cc/2020/Conference/Paper2479/Reviewers", "ICLR.cc/2020/Conference/Paper2479/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2479/-/Public_Comment"}}}], "count": 6}