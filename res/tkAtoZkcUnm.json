{"notes": [{"id": "tkAtoZkcUnm", "original": "czLLig6juhE", "number": 1944, "cdate": 1601308214242, "ddate": null, "tcdate": 1601308214242, "tmdate": 1616021347034, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XFcL8TiJgX", "original": null, "number": 1, "cdate": 1610040485455, "ddate": null, "tcdate": 1610040485455, "tmdate": 1610474090785, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All reviewers tend towards accepting the paper, and I agree."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040485441, "tmdate": 1610474090770, "id": "ICLR.cc/2021/Conference/Paper1944/-/Decision"}}}, {"id": "tge1iOd-HOE", "original": null, "number": 3, "cdate": 1603904885325, "ddate": null, "tcdate": 1603904885325, "tmdate": 1606823588374, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review", "content": {"title": "Paper is marginally above the acceptace threshold", "review": "*****  Paper's Summary  *****\n\nThe authors proposed an algorithm named Neural Thompson Sampling (NeuralTS) for solving contextual multi-armed bandit problems. NeuralTS uses deep neural networks for dealing with exploration and exploitation. In the paper, the authors proved the sub-linear regret of NeuralTS, which is also verified using experiments. \n\n\n*****  Paper's Strengths  *****\n\nAs NeuralTS uses the deep neural network, it can be used for estimating non-linear reward function in the contextual bandits problem.\n\nThe authors proved the sub-linear regret using the recent theoretical results from deep learning. The regret upper bound is similar (in terms of the number of contexts and effective dimension) to the regret bound of existing methods.\n\nThe performance of NeuralTS matches with the state-of-the-art baselines. In some cases, the performance is even better than the existing methods.\n\n\n*****  Paper's Weaknesses  *****\n\nThe weak point of the paper is its novelty. The result incorporates recent deep learning and contextual bandits results [Zhou et al. 2019] (paper in ICML 2020) with the existing Thompson Sampling variant for contextual bandits problem.\n\nSince the neural network (parameters m and L) is fixed before using the algorithm, it may not be possible to estimate any arbitrary reward function with the fixed neural network. Therefore, NeuralTS can have linear regret for the cases where the reward function can not be estimated.\n\n\n*****  Comments  *****\n\nIt is difficult to understand the second part of Assumption 3.4. More clarity may help readers.\n\nThe experiments can be repeated 50 or more times to get a better confidence interval.\n\n\n*****  Questions for the Authors  *****\n\nPlease address the above weaknesses of the paper.\n\nHow are the values of '\\nu,' 'L,' and 'm' set in the experiments?\n\nWhy does NeuralTS need T as input? \n\n\n***** Post Rebuttal *****\n\nI thank authors for the clarifications! After reading the rebuttal and comments of other reviewers, I am increasing my score. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107302, "tmdate": 1606915796226, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1944/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review"}}}, {"id": "yaDcQ4EEY1j", "original": null, "number": 3, "cdate": 1605686127639, "ddate": null, "tcdate": 1605686127639, "tmdate": 1606028443825, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "LDBwsAmn1VA", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for your detailed comments and suggestions.\n\nQ1: Definition of $m$\n\nA1: $m$ is the neural network width. We have added the definition in the revision. \n\nQ2: Definition of $\\mathbf{g}$\n\nA2: $\\mathbf{g}$ is the gradient of the neural network, i.e., $\\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) := \\nabla_{\\boldsymbol{\\theta}}f(x; \\boldsymbol{\\theta})$.  We have added the definition in the revision. \n\nQ3: The contribution is incremental\n\nA3: We disagree respectfully.  Firstly, as in the linear and kernelized cases, in neural bandit, TS and UCB share some similarities but they rely on different tools in the regret analysis.  Second, our contribution is not a trivial combination of existing TS analysis and NTK: if we directly apply the Linear TS analysis, then directly perturbing the neural network parameter will push the neural network parameter out of the regime that can be handled by existing theoretical tools of NTK. This requires us to randomize the reward rather than the parameters.  Third, since our covariance matrix is calculated via the gradient of the network at time $t$, analysis of Kernel TS is not applicable to our algorithm because we cannot get direct access to the NTK kernel. Instead, we can only approximate the covariance matrix from $\\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta})$. To solve this problem, we introduce some novel techniques to deal with the intrinsic randomness from neural network learning and the randomness from the sampling distribution, as we have done in Lemmas 4.2 and 4.3. Finally, from a practical perspective, TS is preferred in various application scenarios over deterministic strategies like UCB when action randomization is critical, as we have shown in the delayed reward section. Therefore, we believe it is important to have a Neural TS algorithm with a provable regret guarantee.\n\nQ4: Many parameters depend on $T$. Is it possible to get rid of the dependence $T$ by playing some doubling trick?\n\nA4: Yes, it is possible to get rid of the dependence on $T$ by the doubling trick. Recall that Theorem 3.5 suggests that to obtain a sublinear regret, the network width $m$ needs to satisfy that $m \\geq \\text{poly}(T)$. Therefore, we need to know $T$ before we run the algorithm to set $m$. For the case where $T$ is unknown, we can use the standard doubling trick (e.g., Cesa-Bianchi & Lugosi, 2006) to set $m$ adaptively.  To do that, we decompose the time interval $(0, +\\infty)$ as a union of non-overlapping intervals $[2^s, 2^{s+1})$. When $2^s \\leq t < 2^{s+1}$, we run NeuralTS with the input $T = 2^{s+1}$. It can be verified that similar $\\tilde O(\\tilde d\\sqrt{t})$ regret still holds.  We have added a discussion about this as Remark 3.9 in the revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tkAtoZkcUnm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1944/Authors|ICLR.cc/2021/Conference/Paper1944/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853988, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment"}}}, {"id": "O5mfJhD2fHY", "original": null, "number": 6, "cdate": 1605686482869, "ddate": null, "tcdate": 1605686482869, "tmdate": 1605686482869, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment", "content": {"title": "Response to All Reviewers", "comment": "We thank all reviewers for their helpful feedback and constructive suggestions. We have addressed all your questions and concerns and updated our paper. The updates in the revised paper are highlighted in red color. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tkAtoZkcUnm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1944/Authors|ICLR.cc/2021/Conference/Paper1944/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853988, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment"}}}, {"id": "dAsxMmOSij", "original": null, "number": 5, "cdate": 1605686435774, "ddate": null, "tcdate": 1605686435774, "tmdate": 1605686435774, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "q0VN1wLVZ5d", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thanks for your positive and helpful comments. \n\nQ1: Can you address the decomposition of regret between $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}_0$?\n\nA1: We believe there is a misunderstanding. According to our initialization scheme described in line 2 of Algorithm 1, and the construction of the context vector described in Assumption 3.4, the output of the initial neural network for any context vector is 0. Therefore, only using the initial neural network (of parameter $\\boldsymbol{\\theta}_0$) without updating the parameter will lead to a linear regret. \n\n\nQ2: \u201cSampling the estimated reward from posterior\u201d is a confusing statement\n \nA2: We would like to clarify that Neural TS indeed samples from the reward posterior distribution, rather than sampling the parameter, as described in Line 6 of Algorithm 1.  This is one of the differences from previous Thompson Sampling algorithms. In specific, we sample from a Gaussian distribution of reward, where the mean is the estimated reward by the neural network. Then we pull the arm by taking the argmax of the sampled value. Therefore, \u201cSampling the estimated reward from posterior\u201d is a correct description of the algorithm. \n\nQ3: Foster and Rhaklin\u2019s work is neither a UCB exploration nor a TS exploration. \n\nA3: Thank you for pointing it out. We have revised the description.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tkAtoZkcUnm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1944/Authors|ICLR.cc/2021/Conference/Paper1944/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853988, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment"}}}, {"id": "BifyH4e5hg5", "original": null, "number": 4, "cdate": 1605686373058, "ddate": null, "tcdate": 1605686373058, "tmdate": 1605686373058, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tge1iOd-HOE", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks for your detailed comments and suggestions.\n\nQ1: Technical novelty compared to Neural UCB and other TS works \n\nA1: Firstly, from linear to kernel version, TS and UCB are similar to each other. Thus it is not surprising that our result is similar to the Neural UCB one. However, our contribution is not a trivial combination of the TS analysis and NTK: if we directly apply the Linear TS analysis, then directly perturbing the neural network parameter will make the neural network parameter out of the regime of NTK, which will make it difficult to do the analysis. Furthermore, since our covariance matrix is calculated via the gradient of the network at time $t$, which makes the analysis of Kernel TS not applicable to our algorithm. To solve this problem, we introduce some novel techniques to deal with the intrinsic randomness from neural network learning and the randomness from the sampling distribution, as we have done in Lemmas 4.2 and 4.3. Therefore, we believe that our result is not a trivial combination of Neural UCB\u2019s result with other TS tricks in the bandit literature. From the practical perspective, since TS is preferred in various application scenarios when action randomization is critical, as we have shown in the delayed reward section, we believe it is important to have a Neural TS algorithm with a provable regret guarantee.\n\nQ2: Since the neural network (parameters m and L) is fixed before using the algorithm, it may not be possible to estimate any arbitrary reward function with the fixed neural network. Therefore, NeuralTS can have linear regret for the cases where the reward function can not be estimated.\n\nA2: The reviewer is right that the $\\sqrt{T}$ regret does not hold for arbitrary reward functions. As discussed in Remark 3.6, a sufficient condition to guarantee $\\sqrt{T}$ regret is that $h$ lies in the function space induced by NTK. \n\n\nQ3: How are the values of '\\nu,' 'L,' and 'm' set in the experiments?\n\nA3: We describe how to set these parameters in Section 5 and Appendix A of the original submission (as well as the current version): $L=2$, $m=100$, and $\\nu$ is selected by a grid search on $\\{1, 10^{-1}, 10^{-2}, \\cdots, 10^{-5}\\}$. For most environments, $\\nu = 10^{-4}$ gives good performance. Furthermore, we also tried $m=1000$ but the results are similar to those of $m=100$. \n\n\nQ4: Second part of Assumption 3.4.\n\nA4: The duplication assumption made in contexts ensures the initial output of the neural network $f(\\mathbf{x}; \\boldsymbol{\\theta})$ is $0$ upon initialization, as described by the paragraph right after Assumption 3.4.\n\n\nQ5: Experiments can be repeated 50 times.\n\nA5: Thank you for your suggestion. Since neural networks training takes a long time, just as other neural network methods, we have tried our best to repeat our experiments 20 times during the author\u2019s response. We have updated our results in Figures 1 and 3. The relationship between the performance of these algorithms remains unchanged. We will repeat the experiments 50 times in the camera-ready of our paper.\n\nQ6: Why does Neural TS need input $T$?\n\nA6: Theorem 3.5 suggests that, in order to obtain a sublinear regret, the network width $m$ needs to satisfy that $m \\geq \\text{poly}(T)$. Therefore, we need to know $T$ at the beginning in order to set $m$. However, we can get rid of the dependence on $T$ by the doubling trick (Cesa-Bianchi & Lugosi, 2006). To do that, we decompose the time interval $(0, +\\infty)$ as a union of several disjoint intervals $[2^s, 2^{s+1})$. When $2^s \\leq t < 2^{s+1}$, we run NeuralTS with the input $T = 2^{s+1}$. It can be verified that similar $\\tilde O(\\tilde d\\sqrt{t}))$ regret still holds.  We have added a discussion about this as Remark 3.9 in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tkAtoZkcUnm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1944/Authors|ICLR.cc/2021/Conference/Paper1944/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853988, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment"}}}, {"id": "lERCPJBZU7O", "original": null, "number": 2, "cdate": 1605685310755, "ddate": null, "tcdate": 1605685310755, "tmdate": 1605686184684, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "UgfxVCRYdv", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thanks for your detailed comments and constructive suggestions. \n\nQ1: Is there a disconnect between theory and practice regarding the network width $m$?\n\nA1: We agree with the reviewer that the NTK theory requires a large value of $m$, while in practice $m$ can be much smaller (as in our experiments).  The disconnection has its root in the current deep learning theory based on the neural tangent kernel and is not specific to our work.  We have added a discussion on this point in Remark 3.8 and Section A.1.\n\n\nQ2: Definition of $\\mathbf{g}$.\n\nA2: $\\mathbf{g}$ is the gradient of the neural network, i.e., $\\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) := \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{x}; \\boldsymbol{\\theta})$.  We have added the definition in the revision. \n\n\nQ3: How is the computation required by these bandit algorithms?\n\nA3: Thank you for your suggestion. We have added a run time plot (Figure 5 in Appendix A.3) to compare the implementation complexity of these algorithms. According to Figure 5, Neural TS and Neural UCB are both 2~3 times slower than the $\\epsilon$-greedy algorithm which is caused by calculating the gradient of the network parameter for each input context. Moreover, BootstrapNN is about 5+ times slower than $\\epsilon$-greedy, since it needs to train several networks in each round. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tkAtoZkcUnm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1944/Authors|ICLR.cc/2021/Conference/Paper1944/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853988, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Comment"}}}, {"id": "UgfxVCRYdv", "original": null, "number": 1, "cdate": 1603868083843, "ddate": null, "tcdate": 1603868083843, "tmdate": 1605024322836, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review", "content": {"title": "Good paper overall; would have liked more discussion on the assumptions", "review": "The paper proposes neural thompson sampling (TS) - a method to run TS without assuming that the reward is a linear function of the context, as is generally assumed in literature. This is not the first paper to use neural networks for TS, however existing papers either a) used TS only in the last layer, or b) maintained uncertainty over the weights and sampled the entire neural network. This paper instead maintains a single network that computes the mean of the reward distribution of an arm. \n\nThe paper also is the first paper to provide regret guarantees for a neural TS algorithm. Experiments show that their algorithm performs better than other baselines.\n\nMy concern with the theoretical results is a missing discussion on their utility with respect to the assumptions. The necessary assumption for all results in the paper is Condition 4.1, which assumes that m, the width of the network is larger than T^6 L^6 K^6. With T as the horizon, this assumes a neural network width of 10^18 even for a modest horizon of 1000 (as used in the experiments). I don't think the experiments used this width in their implementation. I would like if the authors point out this disconnect for the benefit of the readers, and have a discussion section. I believe this assumption may be necessitated by the use of NTK.\n\nSecond, the algorithm uses the function g to model the variance of the distribution, but I did not find any discussion. Is g assumed to be known? If not, how is it learned?\n\nI like that the authors study the delayed reward experiments as it is often the case in practical situations. What will also be useful is to discuss the implementation complexity (computation required to decide the next arm to be sampled) of various algorithms (ideally through a plot). Some algorithms may be faster than others, and readers can use this plot to make an informed choice.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107302, "tmdate": 1606915796226, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1944/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review"}}}, {"id": "q0VN1wLVZ5d", "original": null, "number": 2, "cdate": 1603874375172, "ddate": null, "tcdate": 1603874375172, "tmdate": 1605024322769, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review", "content": {"title": "Novel and efficient Thompson sampling algorithm for neural networks", "review": "Summary:\n\nThe paper proposes a novel Thompson sampling algorithm for neural networks which can be applied to any arbitrary, bounded reward function. While existing works apply Thompson sampling (TS) to neural networks in a heuristic way (e.g., sampling parameters in the last layer only), this algorithm considers the posterior distribution of all the parameters and is the first to provide a theoretical, tight regret upper bound. The work builds on the neural tangent kernel theory, which enables the use of techniques developed for linear reward functions (Agrawal and Goyal, 2013). Actually, the paper is analogous to the work of Zhou et al. (2020) which proposed the NeuralUCB algorithm by combining the neural tangent kernel theory and the Linear UCB methodology (Abbasi-Yadkori et al., 2011).\n\nReason for Score:\n\nI vote for accepting. I believe the contribution of the paper is significant, as it is the first to construct a valid posterior distribution for the parameters in the neural networks. The method also achieves a tight regret upper bound that matches the bound of NeuralUCB. The experiments show that empirically, neuralTS performs better than the state of the art, neuralUCB.\n\nPros\n\nThe paper provides an efficient way for doing directed exploration in bandits with neural network reward models.\nThe presentation of the algorithm, theorem, and experiments is clear and concrete.\n\nCons\n\nAccording to the proofs, the neural TS algorithm that uses initial parameter \\theta_0 instead of \\theta_t should have smaller regret than the current algorithm. That is to say, according to the proofs, the regret can be decomposed as (the regret of the algorithm which does not update \\theta ) + (a term related to the difference of \\theta_t and \\theta_0).  I think the authors should add discussion about this point.\n \nMinor comments\n\n In the last line of page 1:\n\u201cSampling the estimated reward from posterior\u201d is a confusing statement. Actually, we sample a parameter from the posterior. The same goes for \u201csampling the reward\u201d between equations (2.2) and (2.3)\n\nIn the last line of Related work, authors mention Foster and Rhaklin. I don\u2019t think they use UCB exploration. They also use a randomized algorithm although it isn't TS.\n \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107302, "tmdate": 1606915796226, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1944/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review"}}}, {"id": "LDBwsAmn1VA", "original": null, "number": 4, "cdate": 1603945174938, "ddate": null, "tcdate": 1603945174938, "tmdate": 1605024322704, "tddate": null, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "invitation": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review", "content": {"title": "Interesting idea, incremental contribution", "review": "This work proposes a neural network based thompson sampling algorithm for general bounded reward contextual bandit problems.  They provide a theoretical regret guarantee for the proposed algorithm with the help of recent advances of Neural Tanget Kernel.\n\nThe main techniques that use the gradient of the neural networks and NTK have been developed in a recent paper, Neural UCB, as cited by the authors as well. Based on estimations of mean and variance developed in the Neural UCB, NeuralTS is a variant that uses a Gaussian posterior on the top of those estimations. The theoretical contribution is adapting some well-known tricks in the bandit literature to analyze the regret of TS algorithm. That is why I think the contribution is quite incremental.\n\nThe authors provide sufficient experimental data points to justify the advances of the proposed Neural TS against all the existing benchmarks, which is a plus.\n\nBesides, I have the following detailed comments:\n\n1. In Section 2, I didn't find a clear definition of $m$. I have to infer that it is the nework width until Algorithm 1.\n2. In the entire paper, I didn't find a definition for $g(x,\\theta)$. I have to infer its meaning by reading Neural UCB paper.\n3. In Theorem 3.5, many parameters depend on the time horizon T. However, in some use case, we may not have the information of T and some algorithms have this advantage of so-called any-time regret bound. Is it possible to get rid of this dependence by playing some doubling trick?", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1944/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1944/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Thompson Sampling", "authorids": ["~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Lihong_Li1", "~Quanquan_Gu1"], "authors": ["Weitong ZHANG", "Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "keywords": ["Deep Learning", "Contextual Bandits", "Thompson sampling"], "abstract": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $O(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|neural_thompson_sampling", "supplementary_material": "/attachment/fbdaf4cccd934b838e2675b31a7b627e0481bc67.zip", "pdf": "/pdf/d0c2efae754e3efbba6032b8b7d232a28b2bf5bc.pdf", "one-sentence_summary": "We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021neural,\ntitle={Neural Thompson Sampling},\nauthor={Weitong ZHANG and Dongruo Zhou and Lihong Li and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tkAtoZkcUnm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tkAtoZkcUnm", "replyto": "tkAtoZkcUnm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1944/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107302, "tmdate": 1606915796226, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1944/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1944/-/Official_Review"}}}], "count": 11}