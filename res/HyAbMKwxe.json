{"notes": [{"id": "BkeUYNfAcm", "original": null, "number": 4, "cdate": 1539347582105, "ddate": null, "tcdate": 1539347582105, "tmdate": 1539347582105, "tddate": null, "forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "invitation": "ICLR.cc/2017/conference/-/paper46/public/comment", "content": {"title": "forex , crpto currency and binary investments and investors for beginners", "comment": "Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail."}, "signatures": ["~Blossom_de_chase1"], "readers": ["everyone"], "nonreaders": [""], "writers": ["~Blossom_de_chase1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751898, "id": "ICLR.cc/2017/conference/-/paper46/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAbMKwxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper46/reviewers", "ICLR.cc/2017/conference/paper46/areachairs"], "cdate": 1485287751898}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488573999344, "tcdate": 1478099462484, "number": 46, "id": "HyAbMKwxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyAbMKwxe", "signatures": ["~Nicolas_Le_Roux2"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396324187, "tcdate": 1486396324187, "number": 1, "id": "HJnisMIdg", "invitation": "ICLR.cc/2017/conference/-/paper46/acceptance", "forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This is a well written paper that proposes the adaptation of the loss function for training during optimization, based on a simple and effective tighter bound on classification error. The paper could be improved in terms of a) review of related works; b) more convincing experiments.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396324704, "id": "ICLR.cc/2017/conference/-/paper46/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396324704}}}, {"tddate": null, "tmdate": 1485713754973, "tcdate": 1482152406789, "number": 2, "id": "rkkJ9LHNg", "invitation": "ICLR.cc/2017/conference/-/paper46/official/review", "forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "signatures": ["ICLR.cc/2017/conference/paper46/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper46/AnonReviewer3"], "content": {"title": "Review & questions", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.\n\nThe paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \"uncertain\" label. \n\nWhile the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): \n(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and \n(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. \n\nThe reason why I am confused is that \"The standard approach to supervised classification\", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.\n\n=== question:\nIn the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest \"Assuming the class is chosen according to p(y|X, \u03b8)\"), or the more standard deterministic classifier argmax_y P(y|x, theta) ?\n\nAs far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). \n\n=== comments:\n- The section \"allowing uncertainty in the decision\" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) \"Classification with a Reject Option using a Hinge Loss\" or Sayedi et al. (2010) \"Trading off Mistakes and Don\u2019t Know Predictions\".\n\n- there seems to be a \"-\" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.\n\n- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.\n\nFinal comments:\nI think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the \"smoothed\" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional \"uncertain\" label. I increase my score from 5 to 6.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512716282, "id": "ICLR.cc/2017/conference/-/paper46/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper46/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper46/AnonReviewer2", "ICLR.cc/2017/conference/paper46/AnonReviewer3", "ICLR.cc/2017/conference/paper46/AnonReviewer1"], "reply": {"forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512716282}}}, {"tddate": null, "tmdate": 1485292033848, "tcdate": 1482199531125, "number": 3, "id": "SkmefGIEg", "invitation": "ICLR.cc/2017/conference/-/paper46/official/review", "forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "signatures": ["ICLR.cc/2017/conference/paper46/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper46/AnonReviewer1"], "content": {"title": "review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. \nConsequently, the optimized upper bound (log-loss) gets looser. \n\nAs a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. \n\nI would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. \n\nThere is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).\n\n[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.\n[2] Direct Loss Minimization for Structured Prediction, McAllester et al. \n[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.\n\nFinal comment:\nI believe the material presented in this paper is of interest to a wide audience at ICLR.\nThe problem studied is interesting and the proposed approach is sound. \nI recommend to accept the paper and increase my score (from 7 to 8). \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512716282, "id": "ICLR.cc/2017/conference/-/paper46/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper46/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper46/AnonReviewer2", "ICLR.cc/2017/conference/paper46/AnonReviewer3", "ICLR.cc/2017/conference/paper46/AnonReviewer1"], "reply": {"forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512716282}}}, {"tddate": null, "tmdate": 1483459145722, "tcdate": 1483459145722, "number": 3, "id": "SyGI9HKSg", "invitation": "ICLR.cc/2017/conference/-/paper46/public/comment", "forum": "HyAbMKwxe", "replyto": "SJAJ3rHNl", "signatures": ["~Nicolas_Le_Roux2"], "readers": ["everyone"], "writers": ["~Nicolas_Le_Roux2"], "content": {"title": "Reply to reviewer 2", "comment": "Per the reviewer's suggestion, the paper has been reorganized with the connection to RL delayed to the end of the paper.\n\nWe are not sure which figures the reviewer refers to regarding the lack of legend but would be happy to make further changes if it makes the paper clearer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751898, "id": "ICLR.cc/2017/conference/-/paper46/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAbMKwxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper46/reviewers", "ICLR.cc/2017/conference/paper46/areachairs"], "cdate": 1485287751898}}}, {"tddate": null, "tmdate": 1482158666183, "tcdate": 1482158666183, "number": 2, "id": "HJGLfdrNl", "invitation": "ICLR.cc/2017/conference/-/paper46/public/comment", "forum": "HyAbMKwxe", "replyto": "SJAJ3rHNl", "signatures": ["~Nicolas_Le_Roux2"], "readers": ["everyone"], "writers": ["~Nicolas_Le_Roux2"], "content": {"title": "Reply to reviewer 2", "comment": "Thank you for your review and comments.\n\nThe link with RL is made to show that these two communities optimize different losses, even though they care about the same objective. I will make the connection clearer.\n\nCould you elaborate on what you mean by \"pushed further\"? Do you mean evaluating its impact on a larget set of models? If so, do you have specific models in mind? Rather than performing an extended set of experiments, which will always be too limited for some, I hope to provide a compelling enough argument for everyone to try it on their log-likelihood optimized classifier of choice.\n\nI shall rewrite and improve unclear parts of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751898, "id": "ICLR.cc/2017/conference/-/paper46/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAbMKwxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper46/reviewers", "ICLR.cc/2017/conference/paper46/areachairs"], "cdate": 1485287751898}}}, {"tddate": null, "tmdate": 1482158388450, "tcdate": 1482158388450, "number": 1, "id": "S1nVbOSEx", "invitation": "ICLR.cc/2017/conference/-/paper46/public/comment", "forum": "HyAbMKwxe", "replyto": "rkkJ9LHNg", "signatures": ["~Nicolas_Le_Roux2"], "readers": ["everyone"], "writers": ["~Nicolas_Le_Roux2"], "content": {"title": "Deterministic vs. stochastic classifiers", "comment": "Thank you for your review and comments. I apologize if some parts were unclear and will modify the paper accordingly.\n\n- In the experiments, the deterministic classifier is used. Interestingly, in the iterative scheme, the randomized classifier converges to the deterministic one, something which is not true in general.\n\nYou are right that, in the deterministic case (the one studied), the algorithm does not minimize an upper bound on the classification error but rather an upper bound on a smooth version of that error (replacing the step function with a sigmoid). I will make this clearer.\n\nI will also read your additional references and update the paper accordingly.\n\nI hope this clarifies any misunderstandings you might have had."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751898, "id": "ICLR.cc/2017/conference/-/paper46/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAbMKwxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper46/reviewers", "ICLR.cc/2017/conference/paper46/areachairs"], "cdate": 1485287751898}}}, {"tddate": null, "tmdate": 1482148838562, "tcdate": 1482148838562, "number": 1, "id": "SJAJ3rHNl", "invitation": "ICLR.cc/2017/conference/-/paper46/official/review", "forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "signatures": ["ICLR.cc/2017/conference/paper46/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper46/AnonReviewer2"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.\n\nAn interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "pdf": "/pdf/7e08b8baf70b21e37be0ee34f5ae5e5adeff8f19.pdf", "paperhash": "roux|tighter_bounds_lead_to_improved_classifiers", "conflicts": ["criteo.com"], "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512716282, "id": "ICLR.cc/2017/conference/-/paper46/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper46/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper46/AnonReviewer2", "ICLR.cc/2017/conference/paper46/AnonReviewer3", "ICLR.cc/2017/conference/paper46/AnonReviewer1"], "reply": {"forum": "HyAbMKwxe", "replyto": "HyAbMKwxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper46/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512716282}}}], "count": 9}