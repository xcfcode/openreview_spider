{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362392700000, "tcdate": 1362392700000, "number": 3, "id": "88txIZ2gY7lJh", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "fm5jfAwPbOfP6", "replyto": "fm5jfAwPbOfP6", "signatures": ["anonymous reviewer ef61"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "review": "This paper argues that inference in Boltzmann machines can be performed using neurons modelled according to the Linear Nonlinear-Poisson model. The LNP model is first presented, then one variant of inference procedure for Boltzmann machine is introduced and a section shows that LNP neurons can implement it. Experiments show that the inference procedure can produce reconstructions of handwritten digits.\r\n\r\nPros: the LNP model is presented at length and LNP neurons can indeed perform the operations needed for inference in the Boltzmann machine model.\r\nCons: the issue of learning the network itself is not tackled here at all.\r\nWhile the mapping between the LNP model and the inference process in the machine is particularly detailed here, I did not find this particularly illuminating, given that restricted Boltzmann machines were designed with a simple inference procedure with only very simple operations.\r\nI find this paper provides too little new insight to warrant acceptance at the conference."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "decision": "conferencePoster-iclr2013-workshop", "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "pdf": "https://arxiv.org/abs/1210.8442", "paperhash": "shao|linearnonlinearpoisson_neuron_networks_perform_bayesian_inference_on_boltzmann_machines", "authors": ["Yuanlong Shao"], "authorids": ["shaoyl85@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362383640000, "tcdate": 1362383640000, "number": 4, "id": "B4qSE6NM3ZEOV", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "fm5jfAwPbOfP6", "replyto": "fm5jfAwPbOfP6", "signatures": ["Yuanlong Shao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you very much for the valuable reviews and references! I learned quite a lot from reading the suggested papers.\r\n\r\n--> For Reviewer caa8: \r\n\r\n- Regarding the question raised in the end of your review, I think a somewhat related question is why neurons use spikes and whether we shall follow that in our computational model. I was previously approaching this question by exploring whether the combined semi-stochastic algorithm does something similar to simulated annealing, resulting in better local optima estimated in variational inference. But I failed to show this kind of superiority. In all the experiments I did, pure variational inference performs better than semi-stochastic inference in the context of classification with DBM on MNIST (probably because the DBM is well learned and the posteriors are well shaped with only one significant mode, also see the next paragraph). \r\n\r\nAnother possibility of answering this is that, according to neural coding literature, such as works from Aurel Lazar, spikes are efficient ways of encoding time-varying real-valued functions if they are band limited (smooth in some sense). So neurons may be constrained by the energy they can use and have to choose the spiking approach. If that is the case, the 'randomness' of LNP is not that important, what's important of spikes are their property of reconstructing the function. And indeed more realistic neuron models such as Leaky Integrate-and-Fire (LIF) and Hodgkin-Huxley (HH) are not random (Chapter 5 of <6> provide a review about randomness vs. chaotic property of neurons). Are the spikes generated only to meet the reconstruction requirement? I think this worth further investigation. What I do found after submitting this paper is that, in the classification experiments I just mentioned, LIF networks work much better than LNP networks because the converged spiking pattern in LIF is periodic and the activation we get from convolving the recent spiking history is more stable, but if I replace the pseudo-random number generator in LNP by a quasi-random number generator (an effort to get more evenly distributed random samples), LNP and LIF behaves similarly. Another finding is that, in my recent GPU-cluster implementation of stochastic networks, the only way I can balance the data transmission with the computation is by transmitting packed binary spikes among computation nodes. So maybe LIF and HH are merely certain kinds of quasi-random generators used to do variational inference in an economic way. \r\n\r\n--> For Reviewer 4490\r\n\r\n- I think the Poisson-unit BM direction is interesting, and I will definitely explore this in the future. Thanks for the review. In the supplemental material, I have a detailed justification about in what sense the discrete time Bernoulli model can be considered as approximations to the continuous time Poisson model. So for this paper, I think the foundation is OK.\r\n\r\n- I agree to your comments that this paper is specific to the deep Boltzmann machine architecture. I didn't rule out the possibility that neurons can represent other models. But as long as they can represent Boltzmann machines with hidden variables, then the representation power is promised. As a compact universal approximator, even if behavior experiments reveals different types of probability models, they are not necessarily conflicting and may still be implemented by DBM. In addition, what's in my paper is extendable to high-order Boltzmann machines, as long as convolution with D function in section 2 of my paper can be eliminated (which I already validated on LIF and will put in my future publications). \r\n\r\n- As to whether it is enough to have positive weights only, I'm not sure. If the weights are symmetric, then the question is whether a Boltzmann machine with positive weights only is still universal for knowledge representation. I believe the answer is yes, because if yes, then the brain could be doing learning by using positive weights only in most of the normal time but add additional negative weights on demand when serious mistakes are made which need to be revised effectively. This way positive weights will be dominating but not exclusive. So yes I also think this issue needs to be delayed when we deal with neurally plausible learning. If Boltzmann machine with positive only weights and a constant bias is still universal, the original Boltzmann machine without the softmax manipulation in my paper could suffice for LNP modeling. \r\n\r\n- For whether the network will diverge to other parts of the posterior, I think this is very probable, since variational inference is a local optimum algorithm, and by my justification in the supplemental material, the semi-stochastic inference algorithm also approaches the local optima of variational inference. The good news is that by <3>, if a model is well learned, variational inference is good in the sense that the variational lower bound is close to the true likelihood, we can interpreting this result as the true posterior is highly single-moded. This alleviates the problem of local optima. \r\n\r\n- For low mean firing rate, I already considered this. A broader question is that if the nonlinear activation is not sigmoid (such as the Figure 1 in my paper), or if the maximum output of the activation function is low, can we still consider the LNP as a inference procedure. My answer is yes. In deriving the variational inference, we obtain the sigmoid activation from differentiating the KL-divergence loss function. If the activation is not sigmoid, we can easily reverse this derivation to obtain the new loss function, and by taking a difference to the original loss function, we get a regularization term. With different ways of fitting LNP to a Boltzmann machine, the regularization term can be different. Most of the time such regularization term is such that they favors low activity, which is reasonable. In this way, a different activation function can be regarded as a regularized variational inference. I can put more about this issue in the next revision if the paper is accepted. \r\n\r\n--> In the following I put a brief discussion of the reference papers provided by the reviewers. Please let me know if I made any mistake on my interpretation of these papers, as I read them in a hurry. \r\n\r\n- J\u00f3zsef Fiser, Pietro Berkes, Gerg\u0151 Orb\u00e1n, M\u00e1t\u00e9 Lengyel. Statistically optimal perception and learning: from behavior to neural representations', Trends Cogn Sci. 2010. 14(3):119-130\r\n\r\nThis paper previously inspired me a lot. Although there is no specific computational models proposed in this paper, most of the schemes which they discussed are what I'm following in my paper. For example, (1) they mentioned that sampling-based approaches, compared to parameter-based approach, has direct option for learning, but they lack the basis for experimental test. My work connects abstract computational models to LNP spiking neurons, allowing direct test (my on-going work right now extends this to Leaky Integrate-and-Fire and Hodgkin-Huxley as well). (2) They also mentioned that parameter-based approaches may suffer from the exponential explosion in the required size of neurons, while connectionist models such as Boltzmann machine rely on distributed coding which don't suffer from this problem. Furthermore, Boltzmann machines may be compact universal approximators. In this sense, modeling knowledge representation in terms of Boltzmann machines would be safe as long as the learnability issue can be addressed. (3) They also mentioned the 'spontaneous activity'. One of the issues when interpreting spikes as samples is that Monte-Carlo methods such as Gibbs sampling does not converge to the right distribution when all neurons sample together in parallel, while my approach provides an alternative viewpoint as a stochastic approximation of variational inference. The variational distribution one can get can be considered as an approximation of a mode of the joint posterior <4>, thus the activities in different neurons will be correlated according where the mode is located, this leads to an explanation of 'spontaneous activity'. (4) They also mentioned that inference and learning should be considered together when talking about representation. I didn't have it in my current paper, but my on-going work, built on this model, relates STDP to backpropagation, and by <1>, error-driven learning is hopeful to yield consistent probabilistic models with a properly chosen learning scheme. I will make these works available once the learning rule is tested in actual learning tasks. Also, if one favors likelihood-based learning such as contrastive divergence, the description on how hippocampus works in <2> implies that the positive phase of contrastive divergence, if implemented by variational inference as in <3>, can be preserved as short-term memory in the brain. The negative phase of contrastive divergence may be implemented by dreaming <5> (these early works about unlearning/reverse learning are about Hopfield Networks. But HN is a thresholded version of variational inference in Boltzmann machines with hidden variables, thus in terms of representation and learning, they are highly related to each other). \r\n\r\n- Reichert, D. Deep Boltzmann Machines as Hierarchical Generative Models of Perceptual Inference in the Cortex. Ph.D. Thesis. 2012.\r\n\r\nFor now as what I understand, I think this paper is more of an 'analogical model', the clues they use to relate the stochastic property of spiking to machine learning is through Gibbs sampling, which has the issue I mentioned above, while my focus is more on interpreting neural spiking as variational inference. \r\n\r\n- Shi, L., & Griffiths, T. L. (2009). Neural implementation of hierarchical Bayesian inference by importance sampling. Advances in Neural Information Processing Systems 22\r\n\r\nTo my understanding, this paper is about the implementation of importance sampler via one-hidden layer feedforward neural networks and then use it as building block to construct a hierarchical model for both top-down generative and bottom-up inference procedure. Thus if this is neurally plausible, it stands for other things that biological neural networks can do, which does not conflict to my work that neurons can perform approximate inference on DBM. The two lines of research can be separately proceeded.\r\n\r\n- Ma WJ, Beck JM, Pouget A (2008) Spiking networks for Bayesian inference and choice. Current Opinion in Neurobiology 18, 217-22.\r\n\r\nThis paper is about the Probabilistic Population Code, which is another alternative for how neurons represent probability, belonging to the parameter-based approach discussed in the Fiser etc. 2010 paper above. \r\n\r\n- Pecevski D, Buesing L, Maass W (2011) Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons. PLoS Comput Biol 7(12): e1002294\r\n\r\nThis paper is most interesting to me for now. I cannot comment on it before I read it carefully. I will come back with another post a bit later. \r\n\r\nReference List: \r\n\r\n<1> Joshua V. Dillon, Guy Lebanon. Stochastic Composite Likelihood. Journal of Machine Learning Research 11 (2010) 2597-2633.\r\n\r\n<2> O'Reilly, R. C., Bhattacharyya, R., Howard, M. D., & Ketz, N. (2011). Complementary learning systems. Cognitive Science\r\n\r\n<3> Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. Arti\ufb01cial Intelligence, 5(2):448C455, 2009.\r\n\r\n<4> Thomas Minka. Divergence measures and message passing. Technical report, Microsoft Research, 2005.\r\n\r\n<5> Francis Crick and Graeme Mitchison, The function of dream sleep, Nature 304, 111 - 114 (14 July 1983); doi:10.1038/304111a0.\r\n\r\n<6> Wulfram Gerstner and Werner M. Kistler. Spiking Neuron Models: Single Neurons, Populations, Plasticity. Cambridge University Press, 1 edition, 2002."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "decision": "conferencePoster-iclr2013-workshop", "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "pdf": "https://arxiv.org/abs/1210.8442", "paperhash": "shao|linearnonlinearpoisson_neuron_networks_perform_bayesian_inference_on_boltzmann_machines", "authors": ["Yuanlong Shao"], "authorids": ["shaoyl85@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362262200000, "tcdate": 1362262200000, "number": 2, "id": "QQ1JEKYFTIQhj", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "fm5jfAwPbOfP6", "replyto": "fm5jfAwPbOfP6", "signatures": ["anonymous reviewer 4490"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "review": "This paper proposes a scheme for utilizing LNP model neurons to perform inference in Boltzmann Machines.  The contribution of the work is to map a Boltzmann Machine network onto a set of LNP model units and to demonstrate inference in this model.\r\n\r\nThe idea of using neural spiking models to represent probabilistic inference is not new (see refs. at end).  The primary contribution of this work is to take a learned deep Boltzmann machine from the literature, and to implement this network using LNP neurons, with the necessary modifications.  Therefore, the contribution is specific to the deep Boltzmann machine architecture.  The existing work in the literature often takes a different approach: taking realistic neural models and asking how these models can represent variables probabilistically.\r\n\r\nPros:\r\nDeveloping mappings between machine learning algorithms and neural responses is an important direction.\r\nTo my knowledge, the implementation of a deep-BM with spiking neurons is novel.\r\n\r\nCons:\r\nThe clarity of the text and presentation of the mathematics needs improvement.\r\nThe resulting model suffers from some non-biological phenomenology.\r\nThe empirical results are not very compelling.\r\nI would liked to have seen a comparison to the existing approaches for using spiking neurons to implement inference.  Particularly: [2-5]. Is there not a mapping from those models to the deep BM?  Why is the proposed mapping necessary, or what are the limitations of those existing proposals for a deep BM?\r\n\r\n\r\nOther comments:\r\nThe paper provides a lengthy introduction to LNP and inference.  I would encourage the author to justify the various details that are introduced, and those that are not directly relevant for the proposed network, should be left out.  In general, the exposition needs clarification.\r\n\r\nThe proposed network seems like a logical series of steps, but the end result leads to a biologically implausible network, (at least when considering known properties of cortex).  I think a broad approach might be warranted for this problem. For example, starting from the LNP model and using this model as an element in a Boltzmann machine.\r\n\r\nA related note: Isn't it just more plausible to estimate a deep-network with positive only weights? (to deal with Dale's law) There is likely some work to be done there, but it seems this direction wouldn't require the paired neurons you have here.  Or a network with realistic excitatory-inhibitory ratio?\r\n\r\nWhy not start with a Poisson-unit Boltzmann machine, and examine its properties? see (Welling et al. 2005)\r\n\r\nI found the empirical evaluation to be weak.  I don't understand how running the network is a demonstration of correct inference.  Wouldn't we expect each of these networks to diverge and sample different parts of the posterior?\r\n\r\nThe statistics in Figure 5 need more justification.  I did not understand why these are relevant, or what degree of variability should be acceptable.\r\n\r\nThere are, of course, a variety of biological issues that seem to be incongruent with the proposal.\r\nIn cortex the distribution of excitatory to inhibitory neurons is 4:1.  The current proposal seems to require 1:1.\r\nThe pairing of neuron weights seems unlikely, but maybe this could be solved through learning?\r\nWhat about mean firing rates?, are these consistent between model and cortical responses?\r\n\r\nThe title is a little misleading. I might suggest something more like:\r\nNetworks of LNP neurons are capable of performing Bayesian inference on Boltzmann machines.\r\n\r\nThe work of Shi and Griffiths 2009 seems highly relevant, and addresses some of the questions posed by the author.\r\n\r\nNote that Dale's law is not generally applicable, but I am not sure about any refuttaion in cortex, which I assume is where you would imagine the deep network. (see co-transmission)\r\n\r\n[1] Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums with an application to information retrieval. Advances in Neural Information Processing Systems 17, pages 1481-1488. MIT Press, Cambridge, MA.\r\n\r\n[2] Shi, L., & Griffiths, T. L. (2009). Neural implementation of hierarchical Bayesian inference by importance sampling. Advances in Neural Information Processing Systems 22\r\n\r\n[3] Ma WJ, Beck JM, Pouget A (2008) Spiking networks for Bayesian inference and choice. Current Opinion in Neurobiology 18, 217-22.\r\n\r\n[4] Pecevski D, Buesing L, Maass W (2011) Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons. PLoS Comput Biol 7(12): e1002294\r\n\r\n[5] J\u00f3zsef Fiser, Pietro Berkes, Gerg\u0151 Orb\u00e1n, M\u00e1t\u00e9 Lengyel. Statistically optimal perception and learning: from behavior to neural representations', Trends Cogn Sci. 2010. 14(3):119-130"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "decision": "conferencePoster-iclr2013-workshop", "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "pdf": "https://arxiv.org/abs/1210.8442", "paperhash": "shao|linearnonlinearpoisson_neuron_networks_perform_bayesian_inference_on_boltzmann_machines", "authors": ["Yuanlong Shao"], "authorids": ["shaoyl85@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361988540000, "tcdate": 1361988540000, "number": 1, "id": "1JfiMxWFQy15Z", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "fm5jfAwPbOfP6", "replyto": "fm5jfAwPbOfP6", "signatures": ["anonymous reviewer caa8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "review": "The paper provides an explicit connection between the linear-nonlinear-poisson (LNP) model of biological neural networks and the Boltzmann machine. The author proposes a semi-stochastic inference procedure on Boltzmann machines, with some tweaks, that can be considered equivalent to the inference of an LNP model.\r\n\r\nAuthor's contributions:\r\n(1) Starting from the LNP neuron model the author, in detail, derives one (Eq. 5) that closely resembles a single unit in a Boltzmann machine. \r\n(2) A semi-stochastic inference (Eq. 10) for a Boltzmann machine that combines Gibbs sampling and variational inference is introduced.\r\n(3) Several tweaks (Sec. 4) are proposed to the semi-stochastic inference (Eq. 10) to mimic Eq.5 as closely as possible.\r\n\r\nPros)\r\nAs I am not an expert in biological neurons and their modeling, it is difficult for me to assess the novelty fully. Though, it is interesting enough to see that the inference in the biological neuronal network (based on the LNP model) corresponds to that in Boltzmann machines. Despite my lack of prior work and details in biological neuronal models, the reasoning seems highly detailed and understandable. I believe that not many work has explicitly shown the direct connection between them, at least, not on the level of a single neuron (Though, in a high level of abstraction, Reichert (2012) used a DBM as a biological model). \r\n\r\nCons)\r\nIf I understood correctly, unlike what the title seems to claim, the network consisting of LNP neurons do 'not' perform the exact inference on the corresponding Boltzmann machine. Rather, one possible approximate inference (the semi-stochastic inference, in this paper) on the Boltzmann machine corresponds to the LNP neural network (again, in a form presented by the author). \r\n\r\nI can't seem to understand how the proposed inference, which essentially samples from the variational posterior and use them to compute the variational parameters, differs much from the original variational inference, except for that the proposed method adds random noise in estimating variational params. Well, perhaps, it doesn't really matter much since the point of introducing the new inferernce scheme was to find the correspondance between the LNP and Boltzmann machine. \r\n\r\n\r\n\r\n\r\n= References =\r\nReichert, D. Deep Boltzmann Machines as Hierarchical Generative Models of Perceptual Inference in the Cortex. Ph.D. Thesis. 2012."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "decision": "conferencePoster-iclr2013-workshop", "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "pdf": "https://arxiv.org/abs/1210.8442", "paperhash": "shao|linearnonlinearpoisson_neuron_networks_perform_bayesian_inference_on_boltzmann_machines", "authors": ["Yuanlong Shao"], "authorids": ["shaoyl85@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358316900000, "tcdate": 1358316900000, "number": 63, "id": "fm5jfAwPbOfP6", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "fm5jfAwPbOfP6", "signatures": ["shaoyl85@gmail.com"], "readers": ["everyone"], "content": {"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines", "decision": "conferencePoster-iclr2013-workshop", "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "pdf": "https://arxiv.org/abs/1210.8442", "paperhash": "shao|linearnonlinearpoisson_neuron_networks_perform_bayesian_inference_on_boltzmann_machines", "authors": ["Yuanlong Shao"], "authorids": ["shaoyl85@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}