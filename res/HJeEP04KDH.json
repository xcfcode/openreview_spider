{"notes": [{"id": "HJeEP04KDH", "original": "Syl3VIvdDH", "number": 1172, "cdate": 1569439324356, "ddate": null, "tcdate": 1569439324356, "tmdate": 1577168212950, "tddate": null, "forum": "HJeEP04KDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "drJCq17qTk", "original": null, "number": 1, "cdate": 1576798716417, "ddate": null, "tcdate": 1576798716417, "tmdate": 1576800920093, "tddate": null, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Decision", "content": {"decision": "Reject", "comment": "The paper investigates quantization for speeding up RL. While the reviewers agree that the idea is a good one (it should definitely help), they also have a number of concerns about the paper and presentation. In particular, the reviewers feel that the authors should have provided more insight into the challenges of quantization in RL and the tradeoffs involved. After having read the rebuttals, the reviewers believe that the authors are on the right track, but that the paper is still not ready for publication. If the authors take the reviewer comments and concerns seriously and update their paper accordingly, the reviewers believe that this could eventually result in a strong paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713043, "tmdate": 1576800262566, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Decision"}}}, {"id": "SJxbS6petH", "original": null, "number": 1, "cdate": 1570983224775, "ddate": null, "tcdate": 1570983224775, "tmdate": 1574437528596, "tddate": null, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. It shows that overall, reducing the precision of the neural network in DRL algorithms from 32 bits to 16 or 8 bits doesn't have much effect on the quality of the learned policy. It also shows how this quantization leads to a reduced memory cost and faster training and inference times.\n\nI don't think this paper contributes with many novel results in the field, with most results being known or expected. The result that is interesting, in my opinion, is not properly explored.  The paper is well-written but it is a bit repetitive. It seems to me that the first 3 pages could be compressed in 1, as the same information is introduced over and over again. \n\nWith respect to the results being known, quantization is known to succeed in supervised learning tasks. In a deep reinforcement learning algorithm, when you apply post-training quantization in a deep reinforcement learning algorithm, mainly when that algorithm uses a value function (e.g., A2C or DQN), the problem is reduced to a regression problem. It is no different than a supervised learning problem. One has the original network\u2019s prediction and they need to match that prediction. The complexities introduced in the reinforcement learning problem (bootstrapping, exploration, stability) don\u2019t exist anymore as they arise during training. Thus, it doesn\u2019t seem to me that these results are novel or surprising. In a sense it is neat to see that eventual errors do not compound, but that\u2019s it. If I were to write this paper I would make this set of experiments much shorter just as a sanity check. One thing that I feel is missing is a notion of the impact of the quantization not in the rewards accumulated but in the policy/value function. How often does the quantized agent take a different action than the original agent, for example? Does it happen often but only when it doesn\u2019t matter, or is it rare?\n\nThe quantization during training is potentially interesting. It was not properly explored though. I wonder if the quantization during training has a regularization effect, which is known to improve agent\u2019s performance in reinforcement learning (e.g., Cobbe et al., 2018, Farebrother et al., 2018). Does the agent generalize better when using a network with fewer bits of precision? How does this change impact training? These are all questions that could potentially make the results in this paper novel (i.e., quantization as a form of regularization), but as it is now, the results are not that surprising.\n\nImportantly, there are important details missing in the paper that make it hard for me to evaluate the validity of the results presented. Are the results reported over multiple runs? What is the version of the Atari games used, is it the one with stochasticity? How much variance do we have if we replicate this process over different networks that perform well? These are questions I would like to see answered because they also inform us about the impact of the proposed idea. For example, if by repeating this experiment multiple times one observe a high variance, it might mean that different models might be impacted in different ways.\n\nThe results in the \u201creal-world\u201d (Pong is not real-world) are not that surprising as well. Basically they show that if one uses a network with lower precision training and inference are faster, which, again, is not surprising. \n\nThere\u2019s also an important distinction in the results that is not discussed in the paper: DQN estimates a value function while methods such as PPO directly estimate a policy. The reason DQN might have a wider distribution is exactly because it is estimating a different objective. These are important details that should be acknowledged and discussed in the paper. In my opinion, for this paper be relevant, it should have a very thorough evaluation of these different dimensions of reinforcement learning algorithms, with explicit discussions about it. Variance, the impact of quantization during learning, the distinction between parametrizing policies versus value functions, etc.\n\nFinally, there are some aspects of the presentation of this paper that could also be improved. Aside from typos, below are some other comments on the presentation.\n- There\u2019s no such thing as Atari environment, it is either Arcade Learning Environment (Bellemare et al., 2013) or Atari games.\n- I\u2019d introduce/explain quantization in the beginning of the second paragraph of the Introduction for those not familiar with the term.\n- No references are provided for the environments used. You should refer to Bellemare et al.\u2019s (2013) work as well as Brockman et al.\u2019s (2016).\n- Is it really necessary to explain Fp16 quantization as it is done now, with even a picture of two bytes? I\u2019d expect most readers are familiar with how numbers are represented in a computer.\n- The equation for Uniform Affine Quantization is pretty much the same as the one in the Section Quantization Aware Training. All these \u201crepetitions\u201d, or discussions that are common-knowledge give the impression that the paper is trying to fill all the pages without necessarily having enough content.\n- The references are not standardized (e.g., sometimes names are shortened, sometimes they are not) and the paper \u201cEfficient inference engine on compressed deep neural network\u201d is cited twice.\n\n\nReferences:\n\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, Michael Bowling: The Arcade Learning Environment: An Evaluation Platform for General Agents. J. Artif. Intell. Res. 47: 253-279 (2013)\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba: OpenAI Gym. CoRR abs/1606.01540 (2016)\n\n\nKarl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, John Schulman: Quantifying Generalization in Reinforcement Learning. CoRR abs/1812.02341 (2018)\n\nJesse Farebrother, Marlos C. Machado, Michael Bowling: Generalization and Regularization in DQN. CoRR abs/1810.00123 (2018)\n\n\n------\n\n\n>>> Update after rebuttal: I stand by my score after the rebuttal. \n\nThe rebuttal did acknowledge some points I made to me the paper took a gradient update towards the right direction. I don't think the paper is quite there yet though. It is repetitive, spending too much time with basic concepts, and it still ignores small details that matter (e.g., calling it Atari Arcade Learning). I strongly recommend the authors to follow my recommendations closely and then submit the paper again to a next conference. The discussion about generalization is potentially interesting, going beyond the regularization for exploration aspect. A better discussion about quantization during learning is also essential. The first three pages could probably be compressed by half.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576262857910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Reviewers"], "noninvitees": [], "tcdate": 1570237741294, "tmdate": 1576262857924, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review"}}}, {"id": "Syx1z1vioH", "original": null, "number": 3, "cdate": 1573773062580, "ddate": null, "tcdate": 1573773062580, "tmdate": 1573775791720, "tddate": null, "forum": "HJeEP04KDH", "replyto": "SJxbS6petH", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for your thorough and insightful feedback. We especially appreciate your ideas on quantization as a form of regularization, which is now a core component in our paper and a  direction of major interest. We\u2019ve summarized the main points below:\n\nQuantization as a form of regularization in RL-Training:\nFollowing the reviewer feedback, we\u2019ve now established a relationship between using traditional regularizers (specifically layer-norm), quantization-aware training, and the amount of exploration/exploitation that an agent does (section 4). Specifically, we\u2019ve found that quantization-aware training, like traditional regularizers, increases agent exploration and that more heavily quantized models tend to explore more.\n\nTo quantify exploration, we look at the variance in the distribution of action values produced by the policy. Since during training an action is sampled from this distribution, if the policy produces a high variance action distribution (meaning it is much more confident in some actions over others), it is less likely to take other sets of actions and hence corresponds to lower exploration. Conversely, if the action distribution has low variance, the model is more likely to take one action versus another and hence corresponds to high exploration.\n \nWe\u2019ve plotted the variance in action distribution of different policies during different points in the training process and provide a more detailed analysis of the data in section 4 (Quantization as Regularization). Briefly, we found that\nLayer norm, a traditional form of regularization, reduces the variance in action distribution and hence increases exploration\nQuantization aware training likewise reduces the variance in action distribution and hence increases exploration\nHigher levels of quantization (e.g: quantization aware training at 2 bit vs 8 bit) increases exploration more than lower levels of quantization\nQuantization aware training yields policies that explore more, but attain the same levels of reward. Thus, action distribution variances are not lower because the model is less trained, but because the quantization actively facilitates exploration.\n\nOverall, the data indicates that quantization-aware training, like traditional regularizers (layer norm https://arxiv.org/abs/1710.10686) regularize RL training by increasing exploration. \n\nPost-training Quantization of reinforcement learning is same as supervised learning and thus it is not surprising that it works. \n[Authors]: We agree with the reviewer that post-training quantization in RL can be formulated as supervised learning. However, since reinforcement learning has a feedback loop, our work studies the effects of quantization on sequential decision and how the error compounds because of quantization. We quantitatively demonstrate that the error does not accumulate, which is an important finding for RL because if the error compounds then quantization benefits cannot be leveraged to improve system performance (Section 5). \n\nTo show post training quantization applied to RL can improve system performance, we apply quantization to point-to-point navigation policy and achieve higher closed-loop control frequency on resource-constrained aerial robots. A higher control frequency translates to higher responsiveness and agility (http://rpg.ifi.uzh.ch/docs/RAL19_Falanga.pdf) of the aerial robot. Hence, understanding the effect of quantization w.r.t to compounding error is vital to exploit the system side benefits of quantization. \n\nThe results in the \u201creal-world\u201d (Pong is not real-world) are not that surprising as well. Basically they show that if one uses a network with lower precision training and inference are faster, which, again, is not surprising. \n\n[Authors]:Originally, we used \u201creal-world\u201d to imply the usefulness of the approach but not the application (Pong). To avoid any confusion, we have updated the paper to remove any reference that suggests Pong is a real-world environment. \n\nOne of our main contributions is to show that quantization applied to RL training can speed up the training process (by as much as 1.6X, Section 5) without impacting convergence. Speeding up the training can reduce the infrastructure cost thereby making it cheaper to train RL policies. We used Pong as a proxy RL environment to show the results. Given the promising results, we plan to use low-precision training to speed up RL training (e.g., robotics task covered in Section 5).\nHow are the results reported? How many runs? What are the versions of the game?\n[Authors]: The results reported for post-training quantization are averaged over 100 episodes. The version of Atari games used is from gym specifically the NoFrameskip with 4 images stacked together. We updated the paper to with right attribution for the different environments used in this work.\n\nFor quantization aware training, we train each policy at least 3 times and then for evaluating the reward from that policy we average it over 100 episodes.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeEP04KDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1172/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1172/Authors|ICLR.cc/2020/Conference/Paper1172/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160130, "tmdate": 1576860561938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment"}}}, {"id": "BkgkuaIooS", "original": null, "number": 2, "cdate": 1573772647331, "ddate": null, "tcdate": 1573772647331, "tmdate": 1573772681042, "tddate": null, "forum": "HJeEP04KDH", "replyto": "B1ls5I2aYr", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "On behalf of all the authors, I would like to thank you for your feedback.\n\nIt is unclear how results will generalize to real-environments (i.e Pong or Cheetah are not real world environments)\n[Authors] Thank you for the comment. We have updated the paper to remove any reference that suggest pong as a read-world environment. The purpose of evaluating on simple environments was to use it as baseline before moving onto complex tasks such as training RL policy for robot control. The details of applying quantization to RL policies are covered in Section V  (Quantized policy for deployment and Figure 5). In short, we show that one can increase the closed-loop frequency from 5Hz to 90Hz without much drop in the success rate.\n\nHow is quantization affecting RL in a different way compared to supervised learning? For example does it boost exploration?\n\nWe\u2019ve established a relationship between using traditional regularizers (specifically layer-norm), quantization-aware training, and the amount of exploration/exploitation that an agent does. We\u2019ve added a significant section on quantization and exploration in section 4 (Quantization as Regularization). \n\nIn summary, we use the variance in the model\u2019s inferred action distribution as a proxy for exploration. Higher variance in the action distribution implies less exploration as the model is biased to choosing one particular action during training. Our main additions are:\nLayer norm, a traditional form of regularization, reduces the variance in action distribution and hence increases exploration\nQuantization aware training likewise reduces the variance in action distribution and hence increases exploration\nHigher levels of quantization (e.g: quantization aware training at 2 bit vs 8 bit) increases exploration more than lower levels of quantization\nQuantization aware training yields policies that explore more, but attain the same levels of reward. Thus, action distribution variances are not lower because the model is less trained, but because the quantization actively facilitates exploration.\n\nNeed information about experiments (number of runs).\nThe results reported are averaged over 100 episodes. The version of Atari games used is from gym specifically the NoFrameskip with 4 images stacked together as inputs to the models. We use pybullet (Python API for Bullet Physics Engine) for half-cheetah, walker2d environments.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeEP04KDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1172/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1172/Authors|ICLR.cc/2020/Conference/Paper1172/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160130, "tmdate": 1576860561938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment"}}}, {"id": "rkgpN68iiH", "original": null, "number": 1, "cdate": 1573772597193, "ddate": null, "tcdate": 1573772597193, "tmdate": 1573772597193, "tddate": null, "forum": "HJeEP04KDH", "replyto": "HJeTTeOscB", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment", "content": {"title": "Response to Reviewer#1", "comment": "On behalf of all the authors, I would like to thank you for your feedback.\n\nIn the definition of Q_n(W): isn't delta equal to |W| / 2^n?\n[Authors:] In Q_n(w) , delta = (|max(W, 0)| + |min(W, 0)|) / 2^n. This effectively captures the range of the parameter weight values of W and divides this range into 2^n distinct segments. Dividing by delta effectively bins the unquantized values into these segments.\n\nIn Figure 5: your results show that the \"int8\" method has a significantly lower success rate than \"fp32\". Could you provide some discussion as to why this is the case?\n[Authors:] Int8 shows a significantly lower success rate because we use post-training quantization to quantize both weights and activations. Importantly, quantization of activations is difficult since activations often exhibit a larger range of values (and the range of activations is unknown without doing forward passes on a representative sample of inputs). We\u2019ve updated the paper with this information.\n\nTypos: Page 4, \"is a applied\"; Page 5, \"full connected weights\"; Page 8, \"of a accurate\"\n[Authors]: We have updated the paper and corrected typos.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeEP04KDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1172/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1172/Authors|ICLR.cc/2020/Conference/Paper1172/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160130, "tmdate": 1576860561938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Authors", "ICLR.cc/2020/Conference/Paper1172/Reviewers", "ICLR.cc/2020/Conference/Paper1172/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Comment"}}}, {"id": "B1ls5I2aYr", "original": null, "number": 2, "cdate": 1571829394637, "ddate": null, "tcdate": 1571829394637, "tmdate": 1572972503311, "tddate": null, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Training and deployment of DRL models is expensive. Quantization has proven useful in supervised learning, however it is yet to be tested thoroughly in DRL. This paper investigates whether quantization can be applied in DRL towards better resource usage (compute, energy) without harming the model quality. Both quantization-aware training (via fake quantization) and post-training quantization is investigated. The work demonstrates that policies can be reduced to 6-8 bits without quality loss. The paper indicates that quantization can indeed lower resource consumption without quality decline in realistic DRL tasks and for various algorithms.\n\nThe researchers propose a benchmark called QUARL that allows them to evaluate the effectiveness of quantization as well as the impact of quantization across a set of established DRL algorithms (e.g., DQN, DDPG, PPO) and environments (e.g., OpenAI Gym, ALE). Quantizations tested: fp32 -> fp16, int8, uniform affine.\n\nThe idea is simple and carries over from (image-based) supervised learning. The experiments are exhaustive and have to the best of my knowledge not yet been conducted. The conclusions indicate the advantage of quantization, however it is unclear how these results would generalize to real environments (the environments used are after all still simple benchmarks, e.g., half-cheetah or pong). The results are also not entirely surprising or impactful: how is quantization impacting reinforcement learning in a different way than supervised learning? E.g., DQN is supervised learning of a Q-value function against a target. What secondary effects does quantization have on the learning procedure: e.g., does it boost exploration behavior or does it regularize training? We also know that some of these tasks can be solved by extremely small models (https://arxiv.org/abs/1806.01363), while the models used in this work are significantly larger: is quantization working simply because the network capacity is large enough to allow it? These could be investigated in more detail. Furthermore, I'm also missing some experimental setup details: e.g., how many seeds were used for all of the experiments (which is known to greatly affect the results on the benchmarks used in this paper)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576262857910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Reviewers"], "noninvitees": [], "tcdate": 1570237741294, "tmdate": 1576262857924, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review"}}}, {"id": "HJeTTeOscB", "original": null, "number": 3, "cdate": 1572729029152, "ddate": null, "tcdate": 1572729029152, "tmdate": 1572972503266, "tddate": null, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "invitation": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper studies the effect of quantization on training reinforcement learning tasks. Specifically, the paper applies post-training quantization and quantization aware learning to various tasks and record the effects on accuracy and training speed.\n\nOverall, the empirical evaluations suggest that quantization does not significantly hurt the performance of RL training among a wide range of tasks. On several tasks, the authors showed that quantization can significantly reduce memory usage and speed up the inference time. On the other hand, the improved efficiency comes at the cost of accuracy or lower rewards (2% - 5% error as shown in section 4) and (> 5% in terms of success rate as shown in Figure 5).\n\nWhile it is expected that quantization should decrease the accuracy of the trained model, it is not entirely clear how one should evaluate the trade-off presented in the work. Some natural questions that I believe deserve more discussions are:\n-- Are the kinds of accuracy cost the best one could hope for using these methods?\n--  Is there still room for improvement in terms of reducing the cost of accuracy?\n\nDetailed comments:\n-- In the definition of Q_n(W): isn't $\\delta$ equal to |W| / 2^n?\n-- In Figure 5: your results show that the \"int8\" method has a significantly lower success rate than \"fp32\". Could you provide some discussion as to why this is the case?\n-- Typos: Page 4, \"is a applied\"; Page 5, \"full connected weights\"; Page 8, \"of a accurate\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1172/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["srivatsan@seas.harvard.edu", "f20170472@goa.bits-pilani.ac.in", "maxlam@g.harvard.edu", "zishenwan@g.harvard.edu", "sandrafaust@google.com", "vj@eecs.harvard.edu"], "title": "Quantized Reinforcement Learning (QuaRL)", "authors": ["Srivatsan Krishnan", "Sharad Chitlangia", "Maximilian Lam", "Zishen Wan", "Aleksandra Faust", "Vijay Janapa Reddi"], "pdf": "/pdf/3c38626473eec8e2b2a8e3fe849fde6988fde094.pdf", "TL;DR": "We conduct 350+ experiments to show that RL models can be quantized to 6-8 bits without harming quality; show narrower weight distribution facilitates quantization; show quantization speeds training by 50% and inference by 18x.", "abstract": "Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, we show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate the real-world applications of quantization for reinforcement learning. We use half-precision training to train a Pong model 50 % faster, and we deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.", "keywords": ["Deep Reinforcement Learning", "Quantization"], "paperhash": "krishnan|quantized_reinforcement_learning_quarl", "code": "https://github.com/quarl-iclr/quarl", "original_pdf": "/attachment/feedcffba72387abda0905bfe4616004f7de4f31.pdf", "_bibtex": "@misc{\nkrishnan2020quantized,\ntitle={Quantized Reinforcement Learning (Qua{\\{}RL{\\}})},\nauthor={Srivatsan Krishnan and Sharad Chitlangia and Maximilian Lam and Zishen Wan and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeEP04KDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeEP04KDH", "replyto": "HJeEP04KDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1172/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576262857910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1172/Reviewers"], "noninvitees": [], "tcdate": 1570237741294, "tmdate": 1576262857924, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1172/-/Official_Review"}}}], "count": 8}