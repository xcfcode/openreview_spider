{"notes": [{"id": "ZHJlKWN57EQ", "original": "6FGSMC9uC-v", "number": 2294, "cdate": 1601308252858, "ddate": null, "tcdate": 1601308252858, "tmdate": 1614985742257, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "TnIqz0QKizy", "original": null, "number": 1, "cdate": 1610040395579, "ddate": null, "tcdate": 1610040395579, "tmdate": 1610473990667, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "After reading the paper, reviews and authors\u2019 feedback. The meta-reviewer agrees with reviewers that the paper has limited novelty and could be more clear about mix precision training. Therefore this paper is rejected.\n\nThank you for submitting the paper to ICLR. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040395566, "tmdate": 1610473990650, "id": "ICLR.cc/2021/Conference/Paper2294/-/Decision"}}}, {"id": "AHYkIuHCjdF", "original": null, "number": 1, "cdate": 1603724462801, "ddate": null, "tcdate": 1603724462801, "tmdate": 1606760643456, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review", "content": {"title": "This submission rehashes BF16 labeling it incorrectly as 'pure 16b' and creating the wrong perception that they are first to question the so-called 'folklore' of 16b MAC not enough for training.", "review": "During the rebuttal, I concluded that this submission is highly confusing, rather misleading.  I was led to believe the authors are in fact talking 'pure 16b MAC' - meaning 16b FP multiplies and 16b accumulate. After reading their responses to R4, I now learnt that they in fact are using 32b accumulates as is already standard practice in BF16 - If so, they have little or nothing new to offer. Rounding discussions the paper focuses on become highly secondary.  BF16 is already well-understood and accepted. Their writeup was highly misleading to say the least. I change my rating based on this.  \n\nThey use a representative set of benchmarks which include, CNN-based Resnet, recommendation proxy DLRM, and NLP proxy BERT. \n\nNovelty is limited, but critical: They reiterate prior observations that accurate weight updates are more critical for maintaining accuracy than forward-backward gradient updates. Former is what suffers when round-to-nearest cancels out small updates. Stochastic rounding has also been published before and shown to still miss the accuracy mark with >0.,1% accuracy gap for some important benchmarks. Novel part of this work is suggesting a Kahan summation based software fix on top of stochastic rounding to overcome the accuracy loss.\n\nPrimary issue: This is a very confusing and misleading writeup.  Authors should have clearly spelt out that they are in fact talking about BF16 - which is well-publicized for years now as 16b mule and 32b accumulate - and not labelled it as 'pure 16b MAC' - which is already known to exist as well, and proven to be not sufficient for DL training.  In light of this, this work has very little value-add. I change my rating now to 'clear reject' for their misleading writing style.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099671, "tmdate": 1606915771348, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2294/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review"}}}, {"id": "J0YJ0DUYyJ5", "original": null, "number": 12, "cdate": 1606256381770, "ddate": null, "tcdate": 1606256381770, "tmdate": 1606256381770, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "wmdtwNgJYJv", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Revised draft updated.", "comment": "Thank you for the prompt reply and helpful discussion. We just uploaded the revised draft with comments incorporated for your further review and reference."}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "V-88kr3GuG", "original": null, "number": 11, "cdate": 1606244511212, "ddate": null, "tcdate": 1606244511212, "tmdate": 1606248975401, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Draft Revision Uploaded", "comment": "Dear reviewers,\n\nThanks again for all the thoughtful and detailed comments on our initial submission draft. \n\nWe have uploaded a revised version of our draft for your further review and reference. \n\nIn this updated draft, we incorporated the comments and answer the questions in both the main paper and the appendix. This uploaded version has all the planned edits we mentioned in the review replies (The updated text is marked by blue fonts for convenience.).\n\nThanks,\n\nAuthors of the submission.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "ZAf-MZFZym0", "original": null, "number": 10, "cdate": 1606189499690, "ddate": null, "tcdate": 1606189499690, "tmdate": 1606241096540, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "HwjASu5bXP", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "To the best of our knowledge our statement is correct", "comment": "We thank the reviewer for the prompt reply. The linked does show input and output types but the description of what happens in the hardware for each instruction is still opaque. To support our statement, we refer to more details here:\nhttps://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21730-inside-the-nvidia-ampere-architecture.pdf\n\nThe document above (slides 9-15) goes through the accumulator widths that we are referencing (specifically for GPUs). For training, this official Nvidia document shows that they use a FP32 accumulator for the tensor cores in both A100 and V100 **for all training types**. There was one FP16 accumulator added **only for inference** in the new A100, but there are no added TOPs in the table (slides 12). Regardless, our original claim of FP32 (higher precision) accumulators being the de-facto hardware support for training holds true.\n\nIt is important to note that the pure 16-bit training we study is not the same as the mixed precision literature linked above. The details above are hardware support, hidden from users and default in all ML accelerators that we are aware of. The mixed precision work introduces FP32 model weights and optimizer states that need to be stored in memory (increasing memory units and requiring real hardware FP32 unit support, not just a higher-precision accumulator). This is why our work, in comparison, is named as pure Bfloat16 training as discussed in the intro and preliminary sections."}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "HwjASu5bXP", "original": null, "number": 9, "cdate": 1606186754452, "ddate": null, "tcdate": 1606186754452, "tmdate": 1606189311984, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "6Wc6yv-7zWF", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Not sure about that ", "comment": "Thank you for the response. I do not understand this statement in the paper:\n\n> Importantly, for a 16-bit FMAC unit, the accumulator a has higher-than-16-bit\nprecision. This higher precision accumulator is standard in modern hardwares because it ensures\nthe precision of complex operators like matrix multiplication (Henry et al., 2019; Markidis et al.,\n2018). The result in the accumulator then needs to be rounded to 16-bits before it is output from\nthe FMAC unit (e.g. before writing to memory).\n\nLooking at the documentation on NVIDIA's site it seems certainly possible to do the MAC in 16 bits (as one might naively expect: [NVIDIA reference for bfloat fma](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH____BFLOAT16__ARITHMETIC.html#group__CUDA__MATH____BFLOAT16__ARITHMETIC_1g463bf603ed3b2eba19de9ab7d37aad44)\n\n---\n\nNow, it is a different argument to say that you are targeting mixed precision training where the multiplies happen in BFloat16 and the accumulates in 32 bits, but then I think the title of the paper should be \"Revisiting Mixed Precision Training with BFloat16 and FP32 accumulates\" or something like that, and I don't think you can say \"pure 16-bit training\" as you do through out the paper.\n\nBut in that case the main point of comparison should be other mixed-precision training (and not 32-bit training) such as perhaps the following:\n\n[Mixed-Precision Training of Deep Neural Networks](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n\nNote that they say on that page:\n\n> We found that accumulation into single precision is critical to achieving good training results. Accumulated values are converted to half precision before writing to memory.\n\nThus I think it is important to NOT be doing accumulates in FP32 if one wants to claim 16-bit purity (since this mixed mode _is_ what makes it mixed-precision). \n\nAnd perhaps it is possible that your technique may help the NVIDIA method above by not requiring the FP32 shadow copy of weights.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "wmdtwNgJYJv", "original": null, "number": 8, "cdate": 1606147243363, "ddate": null, "tcdate": 1606147243363, "tmdate": 1606147243363, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "wHZOvB0gg_w", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "Thank you for clarifying and responding to my feedback. I believe the picture is a bit clearer now, but the revised draft is still missing so I cannot see the updates. Overall, my view of this submission has not changed significantly, so I will not raise my score further."}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "LFsxTemk0-Y", "original": null, "number": 7, "cdate": 1605674475524, "ddate": null, "tcdate": 1605674475524, "tmdate": 1606118976953, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "AHYkIuHCjdF", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank R3 for the favorable consideration on the importance of insights from our study. We refer to the general reply for questions shared across reviewers and resolve the remaining comments in the below. \n\nWe agree with R3 that FP32 support is needed for general purpose processors. However, we believe eliminating the requirement of 32-bit compute units can be of practical importance for accelerators specialized for training generic deep learning models. E.g. as discussed in the general reply for common questions, both 32-bit training and mixed precision training require compute units with 32-bit multiply for operations inside optimizers. In contrast, the 16-bit training algorithms we study only require faster and more energy efficient units with 16-bit multiply; this benefit is especially pronounced for training large SOTA models using optimizers with sophisticated multiplications such Adam.  As accelerators for DL training are active research topics and open a large industrial market for both startups and hardware giants, we are very excited to continuously explore the feasibility of accelerators only using <32 bit compute units. To clarify the above, we will add discussions in the preliminary section.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "wHZOvB0gg_w", "original": null, "number": 6, "cdate": 1605674333053, "ddate": null, "tcdate": 1605674333053, "tmdate": 1606117420953, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "6T1y9Alj2t", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank R1 for the thoughtful comments and refer to the general reply for shared questions; we resolve the remaining comments in the below. \n\nR1 raises the question when a least square regression will be overparameterized and suggests alternative elaboration on the overparameterized assumption. Our overparameterized assumption refers to the underdetermined least-squares regression models. In such cases, the model dimensionality (input feature dimension) is fixed but can be a very large value (relative to the number of samples). We consider this setting to reflect the overparameterized nature of deep neural networks. We agree with R1 that alternatively it is also very intuitive to elaborate the same technical condition as an assumption where the training data is generated from an underlying linear model. We will better elaborate on the overparameterization assumption in the beginning paragraph in Section 3.1.\n\nWe agree with R1 that it is intrinsically interesting to compare floating point low precision training algorithms to fixed point ones. Though the focus of our paper is on floating point training algorithms which are widely adopted in emerging model-agnostic accelerators, we are very eager to explore this comparison in future work to reveal more insights for accelerator designers in an even further horizon. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "d_R4CHHSBNp", "original": null, "number": 2, "cdate": 1605673460573, "ddate": null, "tcdate": 1605673460573, "tmdate": 1605677527578, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "General response to shared comments (1/2)", "comment": "We thank the reviewers for their thoughtful comments and detailed questions. First, we address questions and comments common among multiple reviewers in this reply. Next, we discuss the individual comments for each reviewer one by one.\n\nIn this paper, we study whether 16-bit training, which requires only 16-bit compute units for emerging deep learning accelerators, can match the model accuracy attained by 32-bit training. Such accelerators promise better efficiency (e.g. chip area, energy consumption) or more overall available compute capacity than accelerators which still require 32-bit floating point compute support. \n\n**The goal of our paper** is not to propose new techniques to enable < 16 bit precision training for certain types of models [1, 2]. Instead, we aim to reveal what is the minimal (and simplest) set of model-agnostic algorithmic techniques to achieve strong accuracy for SOTA models on emerging/future accelerators, robustly across many model domains (e.g. CV, NLP, Speech, Recommender). Towards this end, our study challenges the practitioner\u2019s folklore that mixed precision (16-bit & 32-bit) is needed to maximize model accuracy for training generic DL models. This suggests the feasibility of new deep learning accelerators that only require modern 16-bit units for generic model training.\n\nIt is important to note that these widely-adopted modern 16-bit units do not eliminate 32-bit higher precision accumulation in MAC operations. This higher precision accumulation is inexpensive compared to 16-bit multiply with respect to hardware design in modern 16-bit MAC units but is critical to the numerical accuracy of operations such as matrix multiplication and convolution. As a result, this higher precision accumulation in MAC units will likely continue to be standard in all modern hardware accelerators (including GPUs and TPUs) [6, 7, 8]. To the best of the authors knowledge, we are the first to provide such a study to inform hardware designers about the minimal algorithmic support needed for model-agnostic accelerators that can train SOTA models across application domains using only 16-bit compute units. We agree with the reviewers that these points should be made explicitly and clearly in our draft and we will modify the introduction of our draft accordingly.\n\n**The goal of our theoretical analysis** is different and complementary to the previous theory in [3, 4]. The theories in [3, 4] prove upper bounds on convergence. These upper bounds show that stochastic rounding for weight updates can achieve good convergence even in the worst-case. On the other hand, we prove lower bound on convergence for standard nearest rounding for weight updates. This lower bound shows that nearest rounding for weight updates can substantially degrade the convergence even in the most optimistic case. By combining the insights from our lower bound with those from previous upper bounds, we can finally close the loop and inform the future accelerator designers that 1) only supporting nearest rounding is not enough to maximize model accuracy and 2) to alleviate this, stochastic rounding for model weight updates is one of the minimal support requirement in future model-agnostic training accelerators.\n\nIn addition, the theoretical analysis in [3, 4] focuses on rounding for fixed-point numbers, while we study rounding for floating point numbers commonly adopted in accelerators for model-agnostic training. Our analysis reveals that the convergence limit depends on the magnitude of the optimal solution which can be arbitrarily large. This can be a fundamental barrier for convergence which does not emerge in training with fixed point numbers. As suggested by R1 and R2, we will incorporate this discussion into Section 3.1 and related work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "J0TtxpQEccc", "original": null, "number": 3, "cdate": 1605673804959, "ddate": null, "tcdate": 1605673804959, "tmdate": 1605674896768, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "d_R4CHHSBNp", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "General response to shared comments (2/2)", "comment": "**Efficiency of stochastic rounding**: As suggested by R1, R3 and R4, we will elaborate existing efficient implementations of stochastic rounding in Section 3.2 to clarify the hardware efficiency. Specifically in recent literature [5], stochastic rounding for model weight updates can be implemented in an inexpensive way; it does not require any expensive multiplication or division in modern hardware design. Instead, it only requires 1) generating random bit sequence with a shift register [6], 2) add random sequence to the lower mantissa bits and 3) truncate; these operations are inexpensive relative to other optimizer operations regardless of the model size. Second, we note that the minimal technique in our study only requires stochastic rounding for model weight updates while even cheaper nearest rounding remains for forward, backward and optimizer operations other than weight updates. \n\n**Speed and memory efficiency of 16-bit training with Kahan summation**: Despite the fact that 16-bit Kahan accumulation requires an additional 16-bit auxiliary value, it still has advantages over 32-bit and mixed precision training in terms of speed and memory efficiency. In more details, optimizers in both 32-bit and mixed precision training operate on 32-bit weights (for mixed precision, the weights here refer to the master copy) and 32-bit optimizer states (such as momentum). On the other hand, in 16-bit training with Kahan summation, the optimizers leverage fully 16-bit weights, optimizer states and auxiliary variables. \n\n*Regarding the speed*, optimizers in 32-bit and mixed precision training require compute units with 32-bit multiply, while optimizers in 16-bit training with Kahan summation can leverage units with 16-bit multiply. Given that 2) modern MAC units with 16-bit multiply can be implemented with 1.5X throughput compared to those with 32-bit multiply [10] and 3) Kahan summation only introduce 3 additional add/subtract operations which are inexpensive relative to multiply (e.g. Adam has 9 major multiply operations), optimizers in 16-bit training with Kahan summation can demonstrate meaningful speedup. \n\n*Regarding the memory efficiency*, if we use Adam optimizer as an example, 16-bit training with Kahan summation costs 33% and 43% lower memory for weights plus optimizer states respectively than 32-bit training and mixed precision training (mixed precision training has both 16-bit and 32-bit weights in memory). We note that these benefits here are especially pronounced for training large SOTA models such as Megatron [11], Microsoft Zero[12] with billions of model weights using sophisticated optimizers such as Adam with many multiply operations. Following R1, R3 and R4\u2019s suggestion, we will add the above discussion in Section 3.2 to clarify the system efficiency advantage of 16-bit training with Kahan summation.\n\n**Reference**\n\n[1] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks, Sun, Xiao, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan, 2019\n\n[2] Training Deep Neural Networks with 8-bit Floating Point Numbers, Wang, Naigang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan, 2018\n\n[3] Analysis of Quantized Models, Hou, Lu, Ruiliang Zhang, and James T. Kwok, 2018\n\n[4] Training Quantized Nets: A Deeper Understanding, Li, Hao, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein, 2017\n\n[5] Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent, De Sa, Christopher, Matthew Feldman, Christopher R\u00e9, and Kunle Olukotun, 2017\n\n[6] Cloud TPU: Codesigning Architecture and Infrastructure (https://www.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf), Clifford Chao Brennan Saeta, 2019\n\n[7] BFloat16 Processing for Neural Networks on Armv8-A (https://community.arm.com/developer/ip-products/processors/b/ml-ip-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a), Nigel Stephens, 2019\n\n[8] NVIDIA Tensor Core Programmability Performance & Precision, Markidis, Stefano, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S. Vetter, 2018\n\n[9] QPyTorch: A Low-Precision Arithmetic Simulation Framework, Zhang, Tianyi, Zhiqiu Lin, Guandao Yang, and Christopher De Sa, 2019\n\n[10] FPU Generator for Design Space Exploration, Galal, Sameh, Ofer Shacham, John S. Brunhaver II, Jing Pu, Artem Vassiliev, and Mark Horowitz, 2013\n\n[11] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, Shoeybi, Mohammad, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro, 2019\n\n[12] Turing-NLG: A 17-billion-parameter Language Model by Microsoft, 2020\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "HCiENkTrQEe", "original": null, "number": 5, "cdate": 1605674187759, "ddate": null, "tcdate": 1605674187759, "tmdate": 1605674187759, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "7frBEYS7OlR", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank R2 for the detailed feedback on both the theory and empirical aspects. We refer to the general reply for questions in common and discuss the remaining questions in the below.\n\nR2 asked what is the precision of optimizer states, such as momentum in SGD. In our experiments, we also use BFloat16 precision for momentum in the SGD optimizer, and for first / second moments in the Adam optimizer. This ensures that all the optimizer operations can directly use 16-bit input and generate 16-bit output using 16-bit compute units. To make this more clear, we will increment the discussion on the optimizer implementation using stochastic rounding or Kahan summation for model weight updates in Appendix B.\n\nRegarding the generalization of our analysis on least-squares models, the insights that nearest rounding for model weight updates degrades convergence due to cancellation of updates empirically generalize to deep learning models. E.g. in Appendix D.3, we show that up to 80% of model updates can get cancelled due to BFloat16 nearest rounding for weight updates in a DLRM recommender model; this leads to ~3% AUC degradation compared to 32-bit training. We are also excited to study rigorous theory generalization to deep learning models feasible for analysis as future works.\n\nR4 also very sharply noticed that in Table 2, standard 16-bit training demonstrates one magnitude worse validation perplexity on Wiki103 datasets than 32-bit training. This large discrepancy is valid. It is because that validation perplexity value is exponential with respect to the validation loss. Thus the difference of validation loss can be significantly magnified by validation perplexity which is the standard metric for language modeling tasks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "6Wc6yv-7zWF", "original": null, "number": 4, "cdate": 1605674088676, "ddate": null, "tcdate": 1605674088676, "tmdate": 1605674088676, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "PiMZzwLC_9a", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank R4 for the detailed feedback and discussion. We refer to the general reply on common questions and resolve the other comments in the following.\n\nR4 suggested that we should run experiments with TPUs or recent GPUs to achieve evaluation when the accumulation operation in MAC is also in 16-bit precision. Unfortunately these hardware accelerators (like all of the existing ones that we know of) use higher precision accumulators in MAC units. In more details, the BFloat16 unit we considered takes 16-bit input for multiplication and uses 32-bit higher precision for the accumulation in MAC operations. \n\nSuch higher precision accumulation is inexpensive compared to multiplication but important to the numerical accuracy of operations such as matrix multiplication and convolutions. Thus it is the standard design practice in today\u2019s TPUs and GPUs [6, 8]. Because there is minimal reward from a hardware design perspective in eliminating higher precision accumulators, studying 16-bit precision accumulation in MAC operation is beyond the scope of this paper (albeit still very interesting conceptually). Therefore, QPyTorch can support exact compute simulation for the BFloat16 units [9] in our study that exist (and will continue to exist) in modern hardware; we use it to flexibly evaluate numerical techniques. Using TPUs or new GPUs would still result in the same numerical output for BFloat16 units as in the QPyTorch because these accelerators are also using 32-bit instead of 16-bit MAC accumulation [6, 8]. We thank R4 for bringing up this point as we very much agree that it needs to be addressed more clearly and explicitly in our draft and we will modify the introduction to address this point.\n\nR4 also noticed that the training accuracies are different at the early stage for 32-bit and standard 16-bit training in Figure 1. This is because we use strong smoothing for the curves for clear visualization. Both training algorithms start from the same initialization and the training accuracy gradually diverges. We will include a less smoothed curve in the appendix D.1 to clarify this observation and link from the main paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZHJlKWN57EQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2294/Authors|ICLR.cc/2021/Conference/Paper2294/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Comment"}}}, {"id": "6T1y9Alj2t", "original": null, "number": 2, "cdate": 1603804630437, "ddate": null, "tcdate": 1603804630437, "tmdate": 1605024245224, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review", "content": {"title": "Limited novelty, but extensive empirical evaluation", "review": "### Summary\nThis work reinvigorates half precision training as an alternative to either full single precision or mixed half and single precision. The authors demonstrate that the nearest rounding is the culprit for the worse performance of half precision training compared to single precision, due to cancelling small updates. They then propose two known techniques that can mitigate this effect, stochastic rounding and Kahan summation. Empirically, they demonstrate on an extensive suite of tasks that the proposed adjustments lead to half precision performance that is almost on par to single precision. \n\n### Pros\n- Solid experimental evaluation and results on a large variety of tasks\n- Both modifications that need to done are simple and straightforward\n\n### Cons\n- Limited novelty\n\n### Recommendation \nOverall, even though the paper does not have a lot of novel content, the experimental evaluation is thorough and demonstrates that the improvements close the gap to single precision training. Therefore I am keen on accepting this work, although admittedly not by much. \n\n### Detailed feedback\nThe paper is relatively well written and easy to follow. The authors nicely motivate their modifications which clearly show that the gap between half and single precision training is almost closed. My main point of criticism is that the novelty of this paper is relatively small, as the techniques proposed for performing the quantized weight update are, firstly, not new and, secondly, have been used in previous works for similar reasons. More specifically, performing stochastic rounding for the weight update in order to make progress when the magnitude of the update is small (thus rounded to zero) was also proposed at [1], which also showed successful 16 bit training (albeit for outdated tasks). As for Kahan summation; that has been proposed before for training quantised neural networks at [2] (again as a means to avoid making no progress when the parameter update is small). As a result, the true contributions of this work lie on the extensive empirical evaluation along with the theoretical analysis of rounding in a simple linear model. \n\nAs for other feedback and questions\n- At section 3.1 you first assumption A1 seems peculiar, as you state that you work on a least squares regression problem, but you assume that the model is overparametrized. How can this be the case? In a linear model, the amount of parameters is bounded by the dimensionality of the input (and it doesn\u2019t seem that you perform any feature expansion for x). I believe that a more reasonable statement would be to assume that the actual data are generated by a linear model, hence there exists a true solution w^*.\n- Stochastic rounding requires the generation of random numbers; can this step also be done accurately in bfloat16?\n- From the evaluation it seems that Kahan summation performs better but it increases the memory size by a factor of 2; how does this fare memory wise to having the weights in single precision and is it a worthy tradeoff?\n- I believe that comparisons against other methods for quantised training that use fixed point instead of floating point would be interesting, e.g., [3, 4]. This would highlight the differences between these two formats and show whether the more expensive floating point operations are necessary.\n\n[1] Deep Learning with Limited Numerical Precision, S. Gupta, A. Agrawal, K. Gopalakrishnan, P. Narayanan, 2015\n[2] Training Deep Neural Networks in Limited Precision, H. Park, J. H. Lee, Y. Oh, S. Ha, S. Lee, 2018\n[3] Training and Inference with Integers in Deep Neural Networks, S. Wu, G. Li, F. Chen, L. She, 2018\n[4] Per-Tensor Fixed-Point Quantization of the Backpropagation Algorithm, C. Sakr, N. Shanbhag, 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099671, "tmdate": 1606915771348, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2294/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review"}}}, {"id": "7frBEYS7OlR", "original": null, "number": 3, "cdate": 1603901224509, "ddate": null, "tcdate": 1603901224509, "tmdate": 1605024245165, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review", "content": {"title": "A good submission but lack novelty", "review": "This paper explores the possibilities of reducing the precision of the weight update operation (i.e. AXPY ops) from 32 bit to 16 bit in today\u2019s BFloat16 training framework. To enable 16 bit update, the authors proposed two techniques, i.e. stochastic rounding and Kahan summation. The authors use a simple least-squares regression model to theoretically explain the model accuracy degradation due to nearest rounding, then experimentally demonstrate the effectiveness of two techniques on a range of deep learning models and dataset. \n\nStrong points:\n\nThis paper is very well written. The problem is well addressed, and the solutions are well explained. The authors also provided both theoretical analysis and comprehensive experimental results. \n\n\nWeek points:\n\n1). Novelty: Both the problem addressed, and the solutions proposed in this paper have been reported in recent publications. In particular, both (https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers) and (https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks) investigated the reduced precision of weight update from 32 bit to 16 bit or less. Moreover, the former introduced stochastic rounding and the later introduced round-off residue which basically the same as the Kahan accumulation technique discussed in this paper. Although, both papers discussed this topic in the FP8 training frameworks, while this paper in the BFloat16 framework, the basic concepts are the same. The authors of this paper summarized the techniques nicely, however, the novelty limited.\n\n2). On the same note, theoretical analysis on the impact of rounding mode on quantized weight update were also discussed in recent publications, such as (https://arxiv.org/abs/1706.02379 and https://openreview.net/forum?id=ryM_IoAqYX). It would have been nice to include these discussions as background knowledge and to distinguish this work from others. \n\nTo clarify:\n\n1). The proposed Kahan Summation method created a second tensor to store/accumulate the quantization error. Both weight and rounding error tensors are in 16 bit which in total, effectively, is 32 bit. Since AXPY is a very fast operation, this method does not seem to save much in terms of memory or speed. \n\n2). Today, SGD is often used with momentum, could the authors comment on the precision of momentum accumulation. And how about other popular optimizers, such as Adam?\n\n3). The analysis, i.e. Theroem 1 and 2, is based on a simple least-square regression model. Can this theory generalize to deep learning models?\n\n4).  On Table 2, last two rows of last column, the value seems to be inconsistent with other data in the same row. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099671, "tmdate": 1606915771348, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2294/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review"}}}, {"id": "PiMZzwLC_9a", "original": null, "number": 4, "cdate": 1604289680891, "ddate": null, "tcdate": 1604289680891, "tmdate": 1605024245105, "tddate": null, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "invitation": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review", "content": {"title": "Serious problems with the approach", "review": "I think the use of QPyTorch for the experiments here invalidates the results since the intermediate matrix multiplies are done in single precision (FP32), and so are more optimistic than a pure 16-bit implementation. (This is both according to the authors Sec 4, experiment setup; and according to the QPyTorch paper arxiv:1910.04540, Sec 3 intro.) For these kinds of experiments to be meaningful, they have to be done on native 16-bit hardware which luckily is becoming more common, e.g., Google's TPUs or the newer NVIDIA GPUs.\n\nThere are two other problems. First, it is not clear how stochastic rounding would be implemented in hardware. Doing it for every MAC operation could likely be even more expensive than just doing 32-bit MAC operations, since it involves the generation of random numbers, division, etc. Second, Kahan summation takes up twice the weight storage, so a more detailed calculation is needed to compare any hardware/energy savings to use that instead of just 32-bit.\n\nAs an aside, it may be interesting in Figure 1 to zoom in on the initial part of training to understand where the difference between 32-bit and standard 16-bit comes from in early training since at that point, the gradients are generally larger than later on in training.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2294/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2294/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting BFfloat16 Training", "authorids": ["~Pedram_Zamirai1", "~Jian_Zhang1", "~Christopher_R_Aberger1", "~Christopher_De_Sa2"], "authors": ["Pedram Zamirai", "Jian Zhang", "Christopher R Aberger", "Christopher De Sa"], "keywords": ["16-bit training", "Low precision training", "Deep learning hardware"], "abstract": "State-of-the-art generic low-precision training algorithms use a mix of 16-bit and 32-bit precision, creating the folklore that 16-bit precision alone is not enough to maximize model accuracy. As a result, deep learning accelerators are forced to support both 16-bit and 32-bit compute units which is more costly than only using 16-bit units for hardware design. We ask can we do pure 16-bit training which requires only 16-bit compute units, while still matching the model accuracy attained by 32-bit training. Towards this end, we study pure 16-bit training algorithms on the widely adopted BFloat16 compute unit. While these units conventionally use nearest rounding to cast output to 16-bit precision, we show that nearest rounding for model weight updates can often cancel small updates, which degrades the convergence and model accuracy. Motivated by this, we identify two simple existing techniques, stochastic rounding and the Kahan accumulation, to remedy the model accuracy degradation in pure 16-bit training. We empirically show that these two techniques can enable up to 7% absolute validation accuracy gain in pure 16-bit training. This leads to 0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit precision training across seven deep learning applications. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zamirai|revisiting_bffloat16_training", "supplementary_material": "/attachment/a7cde7410972f587b44212facee88809a8a53a11.zip", "pdf": "/pdf/c963b3088d2e92502c5717f55f45863c53e67708.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KcyBHq5b8_", "_bibtex": "@misc{\nzamirai2021revisiting,\ntitle={Revisiting {\\{}BF{\\}}float16 Training},\nauthor={Pedram Zamirai and Jian Zhang and Christopher R Aberger and Christopher De Sa},\nyear={2021},\nurl={https://openreview.net/forum?id=ZHJlKWN57EQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZHJlKWN57EQ", "replyto": "ZHJlKWN57EQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2294/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099671, "tmdate": 1606915771348, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2294/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2294/-/Official_Review"}}}], "count": 17}