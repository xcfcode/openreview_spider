{"notes": [{"id": "HJglg2A9FX", "original": "HJeEh09qYm", "number": 1044, "cdate": 1538087912150, "ddate": null, "tcdate": 1538087912150, "tmdate": 1545355375381, "tddate": null, "forum": "HJglg2A9FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkx_gGEMxN", "original": null, "number": 1, "cdate": 1544860144269, "ddate": null, "tcdate": 1544860144269, "tmdate": 1545354533079, "tddate": null, "forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Meta_Review", "content": {"metareview": "This paper addresses the problem of learning with outliers, which many reviewers agree is an important direction. However, reviewers point to issues with the experiments (missing baselines, ablations, etc.) and are concerned that the assumptions in the theoretical analysis are too strong. These were potentially addressed in a revised version of the paper, but the revisions are so major that I do not think it is appropriate to consider them in the review process (and it is hard to assess to what extent they address the issues without asking reviewers to do a thorough re-appraisal, which goes beyond the scope of their duties). I encourage the authors to take reviewer comments into account and prepare a more polished version of the manuscript for future submission.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "problems with experiments and assumptions; post-deadline revision too large"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1044/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352986020, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1044/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1044/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352986020}}}, {"id": "Hke_ZBQBp7", "original": null, "number": 3, "cdate": 1541907712270, "ddate": null, "tcdate": 1541907712270, "tmdate": 1543302285691, "tddate": null, "forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "content": {"title": "Tackles important problem but needs more fleshing out", "review": "The authors propose an iterative method for discarding outlying training data: first, learn a model on the entire training dataset; second, identify the training examples that have high loss under the learned model; and then alternate between re-learning the model on the training examples that do not have high loss, and re-identifying the training examples with high loss under the new model. This method works for both supervised and unsupervised learning, and the authors show that in theory, their method has some convergence properties in the mixed linear regression and Gaussian mixture model settings. The authors also run some experiments on neural networks and datasets with synthetic noise to show the benefits of their proposed method.\n\nThe problem of noisy datasets is relevant to almost all machine learning problems in the real world, and the authors' method shows promise as a straightforward way to increase performance on such noisy data. However, my opinion is that the authors need to make a stronger case, theoretically and/or experimentally, for why their method should be preferred to other methods. Detailed comments follow.\n\n== Experiments ==\n1) No comparisons are provided to other outlier detector methods (e.g., based on nearest neighbors, distance to centroid, influence functions, etc.) or techniques that also purport to deal with noisy labels (e.g., by modifying the learning algorithm or loss function). While there are too many existing methods to expect the authors to benchmark against all of them, it's important to at least have a couple of representative comparisons. \n\n2) It'd be nice to have an ablative analysis to tease out the factors behind the gain in accuracy. For example, is the iteration important, or would a single pass suffice? How robust is the algorithm to tau, the fraction of data to discard? (The authors do test initializing randomly vs. initializing on the full dataset.) \n\n3) The systematic label noise scenario seems to favor the authors' method (though the authors claim that it is a harder scenario than random label noise). It'd be helpful to see if the method works against random noise.\n\n== Theory ==\n4) The assumptions seem very restrictive. For example, for mixed linear regression (section 4), the features of all examples are assumed to be drawn i.i.d. from an isotropic Gaussian (so even the bad samples are drawn from the same distribution as the good samples; and all features are independent). To my knowledge, this is not a \"standard and widely studied\" assumption. For the Gaussian mixture model (section 5), a similar isotropic Gaussian assumption is made for each mixture. \n\n5) Beyond the independence assumptions mentioned above, the initialization results make additional assumptions on the \"bad\" data (e.g., average distance of the good vs. bad parameters) that I found hard to parse. How strong are these assumptions? Do they hold on real datasets?\n\n6) The convergence results (Theorems 1 and 3) have a constant term sigma in them. This is surprising and seems to me to considerably weaken the result -- one would expect that the dependence on sigma will decrease with n.\n\nI think either a strong experimental or strong theoretical section would be sufficient for me to recommend acceptance. However, the paper currently shows potentially interesting experimental/theoretical results but does not do a comprehensive job of either side.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "cdate": 1542234319066, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860303, "tmdate": 1552335860303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gAK0CrRQ", "original": null, "number": 3, "cdate": 1543003781754, "ddate": null, "tcdate": 1543003781754, "tmdate": 1543003781754, "tddate": null, "forum": "HJglg2A9FX", "replyto": "BkeEAjqS3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank the reviewer for the feedback. Regarding your concerns, we want to make several clarifications:\n\nReview: \"As said in related work, a soft version of this paper\u2019s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.\"\n\nResponse: The main innovations of our paper over the existing work is:\n1.    For GMMs and Mixed Linear Regression, we provide strong statistical consistency guarantees, while the previous work just provided loss-function minimization and generalization, or robustness for only single linear regression and very small fractions. In fact for Mixed Linear Regression our results provide the best guarantees, even when compared to methods specially made for that problem (while our method is just an instance of a more general idea)\n2.    We provide strong empirical evidence that the idea of retaining the best samples has merit for deen neural classifiers and generative models, something not shown in previous work.\n3.    The previous work on the soft version had one more penalty parameter (needed to prevent the \u201csoft\u201d from becoming \u201chard\u201d), and choosing that parameter is not easy, while still being important to success - even if one knows the fraction of bd samples. We do not have such a parameter problem.\n\nReview: \"In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  \u201cstrict inequality\u201d property in the 3rd line in Theorem 2, because it should correspond to the  \u201cstrict inequality\u201d property in the 2nd line in Theorem 1.\"\n\nResponse: We have updated the theorems of the paper (please see the new version), and these do not have this issue. \n\nReview: \"Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to \nA Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017\nThe comparison to these type of methods need to be included. \"\n\nResponse: We thank the reviewers for bringing this relevant work to our attention. We have included this in the related work section of the updated version.\n\nReview: \"Theorem 2 does not give the probability, only mentioning \u201chigh probability\u201d. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that\"\n\nResponse: By high probability, we mean with probability 1-n^{-c} for some constant c, which is consistent for all the theorems. We have made this explicit in the updated version.\n\nReview: \"In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.\"\n\nResponse: In our revised version, in Appendix E, we provide a comparison with the 'distance to centroid' method to give an idea of ILFB's advantage over other simple algorithms. We also show (in Appendix E) the significant improvement over ILFB's 1-step counterpart, which is a natural algorithm to deal with noisy data. We observe empirically that a batch version of our algorithm (gradient descent over a subset of a batch of samples with smaller loss) is unstable, and would stuck at bad local results. More specifically, in classification task, the batch version improves over the baseline, but is worse than our current algorithm. In the generation task, the batch version will get stuck. We have not included the results for the batch version, since it may be due to the reason that we did not select hyperparameters correctly (maybe a much smaller learning rate would work). On the other hand, our current algorithm does not need any hyperparameter adjustment if one knows how to train the network in the standard, noiseless setting. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614713, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJglg2A9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1044/Authors|ICLR.cc/2019/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614713}}}, {"id": "SJlFk0RHAX", "original": null, "number": 2, "cdate": 1543003616761, "ddate": null, "tcdate": 1543003616761, "tmdate": 1543003616761, "tddate": null, "forum": "HJglg2A9FX", "replyto": "Hke_ZBQBp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank the reviewer for the feedback. Regarding your concerns, we want to make several clarifications:\n\nReview: \"1) No comparisons are provided to other outlier detector methods or techniques that also purport to deal with noisy labels. While there are too many existing methods to expect the authors to benchmark against all of them, it's important to at least have a couple of representative comparisons.\n2) It'd be nice to have an ablative analysis to tease out the factors behind the gain in accuracy. For example, is the iteration important, or would a single pass suffice? How robust is the algorithm to tau, the fraction of data to discard?\"\n\nResponse: In our revised version, we include a more holistic comparison in Appendix E. First, we include a comparison with the centroid method. Second, according to the reviewer's suggestion, we have an ablation study, and show the benefit of doing the iterative update compared to a single pass, and the robustness of the algorithm to tau. In fact, our original experiments with tau being 5% less than the true value is not hard to be satisfied in practice, since in the worst case, one can sweep over all possible tau s with an increment of 5% each time.  In Appendix A (Figure 5), we also include a simulation showing the robustness to tau in the linear regression setting. \n\nReview: \"3) The systematic label noise scenario seems to favor the authors' method (though the authors claim that it is a harder scenario than random label noise). It'd be helpful to see if the method works against random noise.\"\n\nResponse: In Appendix E (Table 6), we add new experiments showing our algorithm works decently. In the random label noise setting, the contribution of the bad samples are 'canceled out' naturally due to the randomness assumption. It is empirically observed (see [1]) in this setting that when the sample size is large enough, even if we train with the naive method, we would still get pretty good accuracy. \n[1] Deep Learning is Robust to Massive Label Noise. David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit\n\nReview: \"4) The assumptions seem very restrictive. For example, for mixed linear regression (section 4), the features of all examples are assumed to be drawn i.i.d. from an isotropic Gaussian. To my knowledge, this is not a \"standard and widely studied\" assumption. For the Gaussian mixture model (section 5), a similar isotropic Gaussian assumption is made for each mixture.\"\n\nResponse: Our assumptions are indeed standard:\nThe following GMM papers make exactly the same assumptions:\n[2] On Learning Mixtures of Well-Separated Gaussians. Oded Regev, Aravindan Vijayaraghavan. FOCS 2017\n[3] Global Analysis of Expectation Maximization for Mixtures of Two Gaussians. Ji Xu, Daniel Hsu, and Arian Maleki. NIPS 2016\n\nThe following Mixed Linear Regression papers make stronger assumptions where they consider the noiseless setting:\n[4] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear regression. In ICML 2014\n[5] Kai Zhong, Prateek Jain and Inderjit S. Dhillon.  Mixed Linear Regression with Multiple Components. NIPS 2016\n\nOnly very recently the following paper considers the setting where the covariance for each component needs not be identity.\n[6] Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal complexity. COLT 2018\nHowever, (1) they consider the noiseless case, (2) their runtime is exponential in the number of mixture components, even for recovering a single component. \n\nReview: \"5) Beyond the independence assumptions mentioned above, the initialization results make additional assumptions on the \"bad\" data (e.g., average distance of the good vs. bad parameters) that I found hard to parse. How strong are these assumptions? Do they hold on real datasets?\"\n\nResponse: In the updated paper we have removed the initialization, since in any case that only guaranteed convergence to the top component. We instead emphasize (both here and in the revised paper) that the local convergence result implies that we will converge to the true component closest to our initial point - this can thus be easily used with multiple different \u201cfar away\u201d initializations to get a good estimate for all components. \n\nReview: \"6) The convergence results (Theorems 1 & 3) have a constant term sigma in them. This is surprising and seems to me to considerably weaken the result -- one would expect that the dependence on sigma will decrease with n.\"\n\nResponse: For Gaussian mixture model setting, even if we do thresholding starting from the true mean, the other component will contribute a fraction (a function of sigma) of bad samples, which will make the next iteration estimate to be biased within the noise ball. Similarly things happen for mixed linear regression.  We have added this part of discussion to the revised paper. We also experimentally validate the performance under different noise levels in Appendix C.2 and D.2.. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614713, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJglg2A9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1044/Authors|ICLR.cc/2019/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614713}}}, {"id": "ryg2OnAr07", "original": null, "number": 1, "cdate": 1543003252471, "ddate": null, "tcdate": 1543003252471, "tmdate": 1543003252471, "tddate": null, "forum": "HJglg2A9FX", "replyto": "BJe1_7BF2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank the reviewer for the feedback. Regarding your concerns, we want to make several clarifications: \n\nReview: \u201cMy main problem with this work is the difficulty in understanding whether the reason our training model produces a large loss on some examples is due to them being bad examples or is because the model is not good enough and needs improvement. \u201c\n\nResponse: Our work does not consider the problem of how much to train / avoid overtraining a deep classifier; this is of course an important issue but is more broader and orthogonal to our focus. We use the standard technique of (a) running a fixed maximum number of epochs, and then (b) choosing the model of these that has the best validation set performance. Indeed our objective is to show that sample rejection (using our method) can work with existing training choices.\n\nReview: \"The theoretical work is related to linear regression and Gaussian Mixture model but the experiments are relayed to Deep Neural Nets! So I am not sure if this setup makes sense. Either both should be for DNN or neither should be.\"\n\nResponse: We do have experiments for the Gaussian Mixture and Mixed Linear Regression models as well, in the appendix; they match the theory results presented in terms of showing strong statistical consistency. It would indeed be great to get similar strong statistical consistency results for Deep Neural Nets; however, these do not exist even for the standard case when there is no bad data (i.e. even given samples generated according to a DNN, it is not known if the DNN parameters can be consistently found by standard SGD-based training). Rather, our DNN experiments show that the basic IDEA - rejecting high-loss samples - has (empirical) value even in DNN settings.\n\nReview: \"In Gaussian Mixture model, there are multiple components and each commonest has its own parameteres. So not sure (1) why the authors assume only mean parameter. (2) Given that Gaussian mixture model assumes multiple components, doesn't it automatically address the problem by putting the samples from different distribution in a different component?\"\n\nResponse: For point (1) above, Our GMM analysis assumes the means of each component are the only unknown parameters, the covariances are known to be \\sigma I for each component. This is a standard analytical setting for GMMs, see e.g. [RV17] and many references therein. \nFor point (2) above, indeed different components means different distributions. Note however that it may still not be easy to distinguish them in an unlabeled dataset. \nOverall: our algorithm for the GMM can be best understood as a \u201c1-means\u201d algorithm, where we try to find the SINGLE gaussian that best fits a subset of the samples, given a dataset of samples coming from multiple overlapping gaussians. Our theorem shows that such a 1-means algorithm succeeds.\n\n[RV17] On Learning Mixtures of Well-Separated Gaussians. Oded Regev, Aravindan Vijayaraghavan. FOCS 2017\n\nReview: \"The parameter \\tau is set to 5 percent less that the true ratio of good samples (correct labels). This seems a pretty bias choice and implicitly applied that one needs to know the true value of this ratio which is a huge expectation. The authors need to investigate the effect of the changes of this value on the performance of their proposed framework!\"\n\nResponse: Indeed we do need to know an upper bound on what fraction of data that is corrupted (but obviously not which precise samples are corrupted). We note that even with this knowledge there do not exist methods that can effectively learn good models. We do not need to know the fraction exactly.\nThat said, we did do preliminary investigations of what happens if our assumed fraction is incorrect - i.e. we retain more samples in each step than is warranted by the data. Empirically, it seems the quality of the model gently degrades with this over-estimation, while still being better than the baseline standard practice of just blindly using all data. In our revised version, we have included more simulations (Appendix A) and experiments (Appendix E) to show that our algorithm is not very sensitive to mis-specified tau.  \n\nReview: \"The experiment with GAN is very wired. How can you expect to have a data set with 20 percent of its examples be bad cases. The authors need to justify that such cased can happen in real applications. \"\n\nResponse: Indeed if a lot of quality human effort has been spent in developing a curated dataset, there will not be significant fractions of errors. However the motivation for our work is that this effort is expensive, and sloppy data curation can be a reality if this expense is not undertaken. Say for example one wants to make a GAN for impressionist paintings; ideally one would need a clean dataset of only such paintings - curated from a larger one of all kinds of paintings. Bad curation can result in significant fraction of samples being non-impressionist. Similar examples are easy to imagine in natural language etc.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614713, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJglg2A9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1044/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1044/Authors|ICLR.cc/2019/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers", "ICLR.cc/2019/Conference/Paper1044/Authors", "ICLR.cc/2019/Conference/Paper1044/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614713}}}, {"id": "BJe1_7BF2m", "original": null, "number": 2, "cdate": 1541129062868, "ddate": null, "tcdate": 1541129062868, "tmdate": 1541533471317, "tddate": null, "forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "content": {"title": "A well written paper but has major problems", "review": "This paper introduces a framework for situation when the training samples are not pure. The idea is a simple approach by training a model and removing a portion of examples from the training set based on the loss of the model. The authors provide some theoretical study on two models: linear regression and Gaussian mixture model and utilize deep neural network to show their framework performs well experimentally. \n\nMy main problem with this work is the difficulty in understanding whether the reason our training model produces a large loss on some examples is due to them being bad examples or is because the model is not good enough and needs improvement. For example, one can always overtrain a classifier such that it classifies the training examples perfectly. Now the question become how much should I train my classifier. In case of Deep Neural Networks for example, the number of epochs can change the loss occurred by classifiers on the examples and it is not easy to know when to stop training in order to utilize the procedure introduced in this work.\n\n\nThe theoretical work is related to linear regression and Gaussian Mixture model but the experiments are relayed to Deep Neural Nets! So I am not sure if this setup makes sense. Either both should be for DNN or neither should be.\n\nI am not sure if I understand Section 5 and the discussion related to the Gaussian Mixture Model. In Gaussian Mixture model, there are multiple components and each commonest has its own parameteres. So not sure (1) why the authors assume only mean parameter. (2) Given that Gaussian mixture model assumes multiple components, doesn't it automatically address the problem by putting the samples from different distribution in a different component?\n\nPage 5 typo: closest point closest to\n\nThe parameter \\tau is set to 5 percent less that the true ratio of good samples (correct labels). This seems a pretty bias choice and implicitly applied that one needs to know the true value of this ratio which is a huge expectation. The authors need to investigate the effect of the changes of this value on the performance of their proposed framework! To me, it seems that the results can be hugely affected by the value of this parameter. \n\nThe experiment with GAN is very wired. How can you expect to have a data set with 20 percent of its examples be bad cases. The authors need to justify that such cased can happen in real applications. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "cdate": 1542234319066, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860303, "tmdate": 1552335860303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkeEAjqS3Q", "original": null, "number": 1, "cdate": 1540889548138, "ddate": null, "tcdate": 1540889548138, "tmdate": 1541533471112, "tddate": null, "forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "content": {"title": "novelty over early work? different angle to view this problem - sparse learning. Proofs/statements need to more rigorous.", "review": "This paper provides an algorithm that excludes the bad training data in the training process and obtain a more accurate model for both supervised and unsupervised learning problem. The paper gives the theoretical guarantee for mixed linear regression and Gaussian mixture model, and also conducts the experiments for deep image classification and deep generative models.\n\nMajor Concerns:\n1, As said in related work, a soft version of this paper\u2019s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.\n\n2, In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  \u201cstrict inequality\u201d property in the 3rd line in Theorem 2, because it should correspond to the  \u201cstrict inequality\u201d property in the 2nd line in Theorem 1.\n\n3. Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to \n\nA Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017\n\nThe comparison to these type of methods need to be included. \n\nMinor Concerns:\n1, Theorem 2 does not give the probability, only mentioning \u201chigh probability\u201d. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that\n\n2, In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1044/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iteratively Learning from the Best", "abstract": "We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models.  We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset.  The experimental results show  significant improvement over the baseline methods that ignore the existence of bad labels/samples. ", "keywords": ["noisy samples", "deep learning", "generative adversarial network"], "authorids": ["shenyanyao@utexas.edu", "sanghavi@mail.utexas.edu"], "authors": ["Yanyao Shen", "Sujay Sanghavi"], "TL;DR": "We propose a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings.", "pdf": "/pdf/f1be2489d05ba40c3f7cf760da4cc1e88290a945.pdf", "paperhash": "shen|iteratively_learning_from_the_best", "_bibtex": "@misc{\nshen2019iteratively,\ntitle={Iteratively Learning from the Best},\nauthor={Yanyao Shen and Sujay Sanghavi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJglg2A9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1044/Official_Review", "cdate": 1542234319066, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJglg2A9FX", "replyto": "HJglg2A9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1044/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860303, "tmdate": 1552335860303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1044/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}