{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488587660849, "tcdate": 1478296729077, "number": 478, "id": "Hku9NK5lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hku9NK5lx", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "content": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396608019, "tcdate": 1486396608019, "number": 1, "id": "B1upnfUOl", "invitation": "ICLR.cc/2017/conference/-/paper478/acceptance", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers unanimously recommended accepting the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396608541, "id": "ICLR.cc/2017/conference/-/paper478/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396608541}}}, {"tddate": null, "tmdate": 1485911313206, "tcdate": 1485911313206, "number": 17, "id": "SJKMS2Cvl", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "BJfmdQRPx", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "conv parameters", "comment": "Thanks for your comment. Unfortunately, we focused on the fully-connected layers, and have not yet had a chance to apply our regularizer framework to convolutional layers. Thank you for pointing out that the parameters of resnet-152 mostly come from the convolutional layers. To apply our regularization to such deep network is an exciting future work direction. However, since every convolutional layer of resnet-152 has quite a restricted size, good compression would probably require reduction on the depth of the network."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485662622440, "tcdate": 1485662622440, "number": 14, "id": "rJIit1oDe", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "HJISXyive", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "timit", "comment": "Thanks again for your reply. As you mentioned, the higher performance could be obtained by larger/deeper or convolutional models. However, for this paper, we focus on the task of compression, so we restrict our model to be a fully-connected model of size 3 x 2048 (one model from [Abdel-rahman Mohamed 2012] in our paper reference). We use the same hmm in [Abdel-rahman Mohamed 2012], which is a monophone model, and triphone models would probably improve the results. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485661153884, "tcdate": 1485660990378, "number": 13, "id": "HJISXyive", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "Skyi5Ccvg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "timit", "comment": "Yajie's PDNN recipe (https://www.cs.cmu.edu/~ymiao/kaldipdnn.html) also hits ~17.8 PER.\n\nHinton et al., 2012 (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf Table 1) hit 20.x w/ monophones (and I presume you used triphones which should only yield better results) -- however they have a deeper model. But this was back in 2012 and things have improved so much more since then...\n\nA (very incomplete table) from https://github.com/syhw/wer_are_we have all results with sub 18% PER (except for the NIPS 2009 paper).\n\nMore cited results (see Table 5):\nhttps://arxiv.org/pdf/1603.00223v2.pdf\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485658774803, "tcdate": 1485658774803, "number": 12, "id": "Skyi5Ccvg", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "BkNbx09we", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "timit", "comment": "Thanks for your reply. I think the Kaldi s5 recipe utilizes a different training/validation/test split from the most published results, and often gives much higher numbers than the ones published. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485656060273, "tcdate": 1485656060273, "number": 11, "id": "BkNbx09we", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "B1_Q_2qwg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "timit", "comment": "Actually, the Kaldi TIMIT recipe (standard recipe out-of-the-box) achieves 18.x PER ... so 23.x PER is really quite off the mark.\n\nhttps://github.com/kaldi-asr/kaldi/blob/master/egs/timit/s5/RESULTS"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485649952383, "tcdate": 1485649952383, "number": 10, "id": "B1_Q_2qwg", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "BJAQHUcPg", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "timit", "comment": "Thanks for your comment. The paper focuses on the task of compressing fully-connected layers rather than pushing the performance of the state-of-the-art. Therefore, the model we choose is of restricted size. Specifically, for the TIMIT result, we train a fully-connected model of size 3 x 2048, and the reported performance is reasonable for such model. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1485624614431, "tcdate": 1485624614431, "number": 9, "id": "BJAQHUcPg", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "timit", "comment": "I didn't read the paper in detail, but for the TIMIT results the authors reported 23.x PER, this is actually quite poor and quite off from SOTA. SOTA is around 16.5 PER and even end-to-end methods (w/o HMMs) can achieve ~17.6% (see Alex Graves CTC/RNN transducer paper and Jan Chorowski's Attention paper)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1484865648856, "tcdate": 1484865648856, "number": 8, "id": "BkKdx6C8g", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "SkFKTRbVx", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "All components applied jointly for good compression results", "comment": "Thanks for pointing out our strong empirical results. We are working on making the paper clearer. The diversity, sparsity, and tying weights are the three key components of the proposed method. Applying any of the component individually does not yield nearly as good results. Please see the discussion with AnonReviewer3 for more details on this point.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1484865536765, "tcdate": 1484865536765, "number": 7, "id": "SkYZl6AUx", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "rJrHMeMVe", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "All components applied jointly for good compression results", "comment": "Thanks for your questions about applying the components of the proposed method independently. Applying regularization alone gives diversity similar to what we reported in the paper but at a cost of much higher sparsity. Also, even worse, performance degrades as well in this case.  In fact, we began our investigation without using sparse initialization and weight tying, and we found that performance is far worse in this case.  Applying sparse initialization or weight tying independently would have no effect on the compression task. Overall, we achieve good compression results only when sparse initialization, density-diversity penalty, and weight tying are all applied simultaneously. A good compression requires low diversity (which is achieved by applying the density-diversity penalty), high sparsity (which is achieved by applying both the density-diversity penalty and the sparse initialization) and no loss of performance (which is achieved by training with weight tying). The other components of the algorithm proposed in the paper (e.g. applying the density-diversity penalty with a certain small probability), are included to make the method more computationally efficient.  We have added a paragraph to the end of section 3.4 in the latest version of the paper, and we will be happy to add more discussions into the final version of the paper, should we be given the opportunity.\n\nWe are also optimistic about the applicability of our method to large networks. First, the experiments on TIMIT uses a network with matrices of size 2048 * 2048, i.e., reasonably representative of modern big networks. Moreover, sorting can be greatly accelerated by: 1) sorting in parallel; 2) using the sparse structure of the weight matrices, as also pointed out by the other reviewers; and 3) using the sorting results of previous batches.\n\nRegarding the \"quick verification\", the penalty is not applied at all during the weight tying phase, and the weight tying phase would not change the diversity or sparsity of the trained weight matrices. The weight tying phase is solely for the purpose of getting the compressed network performance similar to the original, uncompressed performance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1484865430262, "tcdate": 1484865430262, "number": 6, "id": "Sy0ckTCIx", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "Hy4g70FEg", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "Clarification on the table results", "comment": "Thanks for pointing out that our paper is strong. Regarding the confusion about the results tables, 1-sparsity (or density, as we will now call it based on your comments) is presented instead of sparsity.  Sparsity represents the fraction of weights that are zero, while we report percentage of non-zero entries. Thus, our methods achieve very sparse weight matrices. We choose to report density instead of sparsity because density (sometimes known as the percentage of non-zero entries) is used in related work (e.g., the \"deep compression\" paper, Han et al., 2015a); also, density is often a small number and, hence, is easier to read in plots; finally, and perhaps most importantly, for both density and diversity, lower is better meaning they are more mutually consistent."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1482445547684, "tcdate": 1482445547684, "number": 3, "id": "Hy4g70FEg", "invitation": "ICLR.cc/2017/conference/-/paper478/official/review", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer1"], "content": {"title": "Very good paper. Extremely easy to read and understand. Exciting ideas. Very good results. a few typos.", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.\n\nThis density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.\n\nRegularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.\n\nAs weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.\n\nThe training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.\n\nThe experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.\n\n\nThe paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.\n\nThe result tables are a bit confusing unfortunately.\n\nminor issues:\n\np1\nenglish mistake: \u201cwhile networks *that* consist of convolutional layers\u201d.\n\np6-p7\nTable 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:\n In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.\nI assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512572815, "id": "ICLR.cc/2017/conference/-/paper478/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer3", "ICLR.cc/2017/conference/paper478/AnonReviewer1"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512572815}}}, {"tddate": null, "tmdate": 1481929277325, "tcdate": 1481929277325, "number": 2, "id": "rJrHMeMVe", "invitation": "ICLR.cc/2017/conference/-/paper478/official/review", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.\n\nThe approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?\n\nI think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. \n\nA quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?\n\nPreliminary rating:\nI think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.\n\nMinor notes:\nPlease resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512572815, "id": "ICLR.cc/2017/conference/-/paper478/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer3", "ICLR.cc/2017/conference/paper478/AnonReviewer1"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512572815}}}, {"tddate": null, "tmdate": 1481927480506, "tcdate": 1480736299725, "number": 3, "id": "rJNVAn1ml", "invitation": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer3"], "content": {"title": "Lot of Aspects to this Algorithm", "question": "There are a number of different things going on here: \n- spare initialization\n- the proposed Density-Diversity Penalty applied with some probability\n- mode subtraction after the Density-Diversity Penalty is applied\n- weight tying (weights with the same value after the above steps accumulate and share gradient updates)\n \nCan you speak to the importance of each of these to your overall results? Have you done any sensitivity analysis with respect to the probability of applying the penalty or the fraction of sparsity in the initialization? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959259622, "id": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer1", "ICLR.cc/2017/conference/paper478/AnonReviewer3"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959259622}}}, {"tddate": null, "tmdate": 1481923968832, "tcdate": 1481923968832, "number": 1, "id": "SkFKTRbVx", "invitation": "ICLR.cc/2017/conference/-/paper478/official/review", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer2"], "content": {"title": "Good results", "rating": "6: Marginally above acceptance threshold", "review": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512572815, "id": "ICLR.cc/2017/conference/-/paper478/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer3", "ICLR.cc/2017/conference/paper478/AnonReviewer1"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512572815}}}, {"tddate": null, "tmdate": 1481697503661, "tcdate": 1481697503651, "number": 5, "id": "Hk_kYwRQl", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "BJGYjHTXx", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "Compare to other results", "comment": "Thank you for pointing out the most recent work on the compression task. Our proposed regularizer enforces high sparsity and low diversity simultaneously, and thus we should compare the final compression rate instead of sparsity alone. We compare our compression rates to Han's work using the same compression rate calculation, which considers the sparse matrix representation as well as bits for encoding, and we report the numbers in paragraphs 3-4 of section 4.1 and paragraph 3 of section 4.2.\n\nThe compression rate reported in Guo's work is not the same compression rate shown in our work or Han's work. Indeed, Guo et al. use the inverse of the overall sparsity. As mentioned by the reviewer, the final compression rate could be comparable, and especially for the LeNet-300-100 model, which is a smaller fully-connected network and therefore harder to compress, our compression rate should be significantly better.  Furthermore, the reported accuracy in Guo's work is quite worse than that of the uncompressed model.\n\nFor Ullrich's work, comparing the LeNet-300-100 model as reported in paragraph 1 of section 6.1 of Ullrich's work, our method gives better sparsity as well as significantly better accuracy. However, it is hard to compare the final results, because for Ullrich's work, a more aggressive encoding method of the sparse matrix is used and Huffman encoding is also applied.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1481624441732, "tcdate": 1481624441724, "number": 4, "id": "BJGYjHTXx", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "Hy0-CIJXg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Results", "comment": "The results have not been compared to a recent NIPS paper by  Guo et al., 2016. Furthermore, there is a very similar paper in spirit submitted to ICLR that claims to achieve even better results (Soft Weight-Sharing for Neural Network Compression by\nUllrich et.al. ). \n\nFor example Guo et al., 2016 prunes 99.1% of weights in LeNet-5. Whereas you report 92%. \nThe final compression rate could, however, be competitive nevertheless if you can quantize more than others. \n\nCould you please also provide compression rates as proposed by Han et al., 2015.\n\nYiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In Advances In Neural Information Processing Systems, pp. 1379\u20131387, 2016.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2, 2015"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1481330138423, "tcdate": 1481330138418, "number": 3, "id": "HyzJRTuQl", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "Hy0-CIJXg", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "Experiment results", "comment": "Thank you for pointing out our strong empirical results. We will be running more experiments on datasets such as CIFAR. We also note that there are TIMIT dataset results in the paper, and the MNIST data is actually quite difficult for the task of compression, since the model considered (the LeNet network) is very small and hard to compress. But indeed, one can always add more experimental results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1481330111794, "tcdate": 1481330111788, "number": 2, "id": "BJO666OQl", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "SybRMsy7l", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "Possible improved generalization", "comment": "Thank you for pointing out the possible improvement on generalization power of the proposed regularizer. We think the proposed regularization method could also improve the performance of the model, if the certain pattern in the data is learned as enforced by the regularization. However, the proposed regularization is a natural fit for the model compression task, so we focus on the compression results in the present paper.\n\nFor deep neural networks, it is not clear whether smaller models will always generate better results, because 1) empirically, we often find that a deeper and wider model gives better performance, and 2) theoretically, there is recent work showing that the local optima of the network are close to the global optimum if the network is big enough. So in general, we don't think that the compressed models are necessarily better in terms of performance. Our primary motivation in this work is to enable the good performance offered by deep models to be achievable on lower-resource devices."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1481330066383, "tcdate": 1481330066375, "number": 1, "id": "By9cTTO7l", "invitation": "ICLR.cc/2017/conference/-/paper478/public/comment", "forum": "Hku9NK5lx", "replyto": "rJNVAn1ml", "signatures": ["~Shengjie_Wang1"], "readers": ["everyone"], "writers": ["~Shengjie_Wang1"], "content": {"title": "Importance of different steps", "comment": "Thank you for asking about the importance of different steps as well as suggesting the sensitivity test. The four parts mentioned are all required to get the desired compressed model performance with both low density and low diversity. The sparse initialization is more important for the low density objective, and the regularizer is more important for the low diversity objective. The weight tying in the end focuses on obtaining performance as good as that of the original, uncompressed model.\n\nApplying the penalty with a certain probability is for training efficiency. Ideally, we would like to apply the penalty at every mini-batch. However, we find that even with a certain small probability of applying the penalty, we can obtain the desired compression result with much greater training speed.\n\nFor sparse initialization, we vary the initialization sparsity fraction from 5% to 20%, and when comparing the highest and lowest values on the TIMIT dataset, we observe 0.2% absolute difference for phone accuracy, 1.17%, 2.48% and 2.72% absolute differences for learned sparsity of the first three weight matrices, respectively, and almost no difference for learned diversity. Overall, we believe the performance is not very sensitive to the fraction of sparse initialization (this answers your question about sensitivity). Given the opportunity, we will add this information to the final paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559883, "id": "ICLR.cc/2017/conference/-/paper478/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hku9NK5lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper478/reviewers", "ICLR.cc/2017/conference/paper478/areachairs"], "cdate": 1485287559883}}}, {"tddate": null, "tmdate": 1480729459110, "tcdate": 1480729289380, "number": 2, "id": "SybRMsy7l", "invitation": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer1"], "content": {"title": "Compression and Regularisation", "question": "Your paper only considers the target problem of compression and does not pretend to apply to regularisation in the sense that the point is usually to improve generalization performance. However given the known links between the two, (i.e. Occam's razor, Minimum Description Length, and more generally smaller models being known to generalize better), have you investigated the impact on generalisation ? Do you think that your method definitely won't help with respect to generalization (and then why shouldn't it), or do you think it should help w.r.t generalisation (but then why didn't you talk about it in the paper) ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959259622, "id": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer1", "ICLR.cc/2017/conference/paper478/AnonReviewer3"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959259622}}}, {"tddate": null, "tmdate": 1480711686252, "tcdate": 1480711686248, "number": 1, "id": "Hy0-CIJXg", "invitation": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "signatures": ["ICLR.cc/2017/conference/paper478/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper478/AnonReviewer2"], "content": {"title": "Better results than state of the art", "question": "Good results. The technique looks a little bit complicate and obfuscated. Do you have results on more competitive tasks? For instance CIFAR, SVHN... and more... MNIST use to have always room for improvement in many directions... \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \u201cdensity-diversity penalty\u201d regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive.", "pdf": "/pdf/2cb01001f2f89ca1125217eba88e499f0a8704af.pdf", "TL;DR": "We propose a new ''density-diversity penalty'' to fully-connected layers to get significantly high sparsity and low diversity trained matrices, while keeping the performance the same.", "paperhash": "wang|training_compressed_fullyconnected_networks_with_a_densitydiversity_penalty", "keywords": ["Deep learning"], "conflicts": ["washington.edu"], "authors": ["Shengjie Wang", "Haoran Cai", "Jeff Bilmes", "William Noble"], "authorids": ["wangsj@cs.washington.edu", "haoran@uw.edu", "bilmes@uw.edu", "william-noble@u.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959259622, "id": "ICLR.cc/2017/conference/-/paper478/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper478/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper478/AnonReviewer2", "ICLR.cc/2017/conference/paper478/AnonReviewer1", "ICLR.cc/2017/conference/paper478/AnonReviewer3"], "reply": {"forum": "Hku9NK5lx", "replyto": "Hku9NK5lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959259622}}}], "count": 23}