{"notes": [{"id": "SJg5J6NtDr", "original": "H1xybw5BPS", "number": 312, "cdate": 1569438946136, "ddate": null, "tcdate": 1569438946136, "tmdate": 1583912032961, "tddate": null, "forum": "SJg5J6NtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "TDvtFBHBRW", "original": null, "number": 1, "cdate": 1576798693032, "ddate": null, "tcdate": 1576798693032, "tmdate": 1576800942364, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposed a meta-learning approach that learns from demonstrations and subsequent RL tasks.\nThe reviewers found this work interesting and promising. There have been some concerns regarding the clarity of presentation, which seems to be addressed in the revised version. Therefore, I recommend acceptance for this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722720, "tmdate": 1576800274079, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper312/-/Decision"}}}, {"id": "HkxkRDmBsr", "original": null, "number": 2, "cdate": 1573365703484, "ddate": null, "tcdate": 1573365703484, "tmdate": 1573767093793, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "rJe17q5Z9H", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment", "content": {"title": "Submission updated to improve presentation", "comment": "We thank the reviewer for the thoughtful comments, and have updated the paper to improve the presentation:\n\n>>> The context should be better introduced [...]\n\nWe have updated Section 3.4 to explain in more detail how the policies condition on demonstration/trial data by extracting context vectors.\n\n>>> It is unclear to me how the reward signal is used to learn the meta-policy is phase 2 [...]\n\nAs you noted, the phase II policy incorporates the trial reward information into the context embedding which is fed to the actor network that selects actions in phase II. It should learn how to use the reward information directly through the meta-training process. Suppose the trial trajectory has low reward, then the phase II policy should not reinforce the poor trial behavior otherwise it will receive a high loss for that task (Equation 3). We\u2019ve updated the portion of Section 3.3 (right after Equation 3) to state this explicitly. \n\n>>> The reward functions are not properly explained in the paper [...]\n\nWe\u2019ve updated the Appendix A.3 and Appendix B to include our description of the environment cost functions.\n\n>>> If we take 40 random observations and the reward signal is sparse, wouldn't it be quite likely that the observations do not contain any reward values?\n\nWe\u2019ve updated Appendix A.3 & C.2 to clarify this. An episode has a maximum of 50 timesteps, corresponding to 5 seconds, but ends early if the task is successfully completed before then. As a result, most demonstrations are under 50 timesteps and we decided to sample 40 timesteps (with replacement) because most of the demonstrations, barring a few outliers, were <= 40 timesteps long. When sampling the 40 timesteps, we always included the last timestep at the end. Since our episodes terminate on success, the last timestep will contain the reward signal unless the agent failed.\nThe practice of subsampling trajectories to be a fixed length follows prior works on one-shot imitation learning (e.g. Finn et al. One-Shot Visual Imitation Learning via Meta-Learning; James et al. Task Embedded Control Networks)."}, "signatures": ["ICLR.cc/2020/Conference/Paper312/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper312/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg5J6NtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper312/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper312/Authors|ICLR.cc/2020/Conference/Paper312/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173283, "tmdate": 1576860543523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment"}}}, {"id": "B1xEKDrsir", "original": null, "number": 6, "cdate": 1573767035944, "ddate": null, "tcdate": 1573767035944, "tmdate": 1573767077249, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "Syxmdc1CKB", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment", "content": {"title": "Please let us know if the response has addressed your concerns", "comment": "Dear Reviewer 2,\n\nCould you let us know if our response has addressed the concerns raised in your review? We would be happy to provide further revisions or experiments to address any remaining issues, and would appreciate a response from you on the points that we raised."}, "signatures": ["ICLR.cc/2020/Conference/Paper312/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper312/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg5J6NtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper312/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper312/Authors|ICLR.cc/2020/Conference/Paper312/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173283, "tmdate": 1576860543523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment"}}}, {"id": "Syg18_mrir", "original": null, "number": 3, "cdate": 1573365831122, "ddate": null, "tcdate": 1573365831122, "tmdate": 1573365831122, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "SJxKT8BYtS", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment", "content": {"title": "Updated with more discussion of limitation and future improvements", "comment": "Thank you for the feedback. We address your concern below.\n\n>>> [...] adding a more detailed discussion on the limitations can improve the impact of the paper further.\n\nWe\u2019ve expanded the Discussion section to include a paragraph on limitations (as illustrated by our failure analysis) and future plans to address them."}, "signatures": ["ICLR.cc/2020/Conference/Paper312/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg5J6NtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper312/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper312/Authors|ICLR.cc/2020/Conference/Paper312/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173283, "tmdate": 1576860543523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment"}}}, {"id": "Bkxx84qesH", "original": null, "number": 1, "cdate": 1573065800365, "ddate": null, "tcdate": 1573065800365, "tmdate": 1573065800365, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "Syxmdc1CKB", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment", "content": {"title": "Updated submission to address questions", "comment": "We thank the reviewer for the thoughtful comments and suggestions. We have included some corrections and re-uploaded the paper. To respond to the questions in order:\n\n1. Thank you for catching this typo, \\theta should be \\phi in Eq. 4. We\u2019ve corrected this in the updated paper.\n\n2. We\u2019ve updated Section 3.4 with a more concrete explanation of how the policies are implemented and to explain how the conditioning on demos or trials works. \\pi_\\theta(a_t|s_t, {d_i,k}) means that for any task, the phase I policy is conditioned on the task demonstrations (i.e., the policy infers the task from the demos). Concretely, the phase I policy has a demo embedding network which creates an embedding vector from the demo data, which we feed into the actor network that produces actions (bottom half of Fig. 3 and Fig. 7). Similarly, the phase II policy has both demo and trial embedding networks to condition on both demos and trial experience. Meta-training \\theta and \\phi amounts to meta-learning the weights of the embedding and actor networks for the phase I and II policies, respectively. The demonstrations are important even after meta-training \\theta because at meta-test time the evaluation tasks are unknown prior to receiving the demos: the agent will be presented with a pair of new objects and the evaluation task could be to: a) pick one of them up, b) to slide one into the other, c) to pick and place, and so on. Conditioning on the demos helps the phase I policy infer what to do in the trial phase.\n\n3. You are correct that the meta-parameters \\theta and \\phi are fixed during meta-testing. Since the policies infer the task using the embeddings, it is not necessary to update \\theta or \\phi within a task. We\u2019ve updated Section 3.3 to clarify this.\n\n4. You are correct that due to the architecture of the neural network policies, the state/observation dimension is fixed across tasks. However, the content of the state may vary between tasks: for example, the objects present in the image are different for each task. We\u2019ve updated Section 3.4 to clarify this."}, "signatures": ["ICLR.cc/2020/Conference/Paper312/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg5J6NtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper312/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper312/Authors|ICLR.cc/2020/Conference/Paper312/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173283, "tmdate": 1576860543523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper312/Authors", "ICLR.cc/2020/Conference/Paper312/Reviewers", "ICLR.cc/2020/Conference/Paper312/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Comment"}}}, {"id": "SJxKT8BYtS", "original": null, "number": 1, "cdate": 1571538625439, "ddate": null, "tcdate": 1571538625439, "tmdate": 1572972611257, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper proposes an approach for combining meta-imitation learning and learning from trial-and-error with sparse reward feedback. The paper is well-written and the experiments are convincing. I found the idea of having separate networks for the two phases of the algorithm (instead of recollecting on-policy trial trajectories) interesting and possibly applicable in other similar settings. \n\nThe paper has a comprehensive analysis of the advantages of the method over other reasonable baselines which do not have the trial-and-error element. However, in terms of the limitation of the method, the paper only has a small failure analysis section in the appendix; adding a more detailed discussion on the limitations can improve the impact of the paper further. "}, "signatures": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878315556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper312/Reviewers"], "noninvitees": [], "tcdate": 1570237753953, "tmdate": 1575878315569, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Review"}}}, {"id": "Syxmdc1CKB", "original": null, "number": 2, "cdate": 1571842667480, "ddate": null, "tcdate": 1571842667480, "tmdate": 1572972611224, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work focuses on meta learning from both demonstrations and rewards. The proposed method has certain advantages over previous methods:\n(1) In comparison to meta-imitation, it enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. \n(2) In comparison to meta-reinforcement learning, it can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration.\n\nI am a bit confused by the writings and don't clearly understand the algorithm.\n1. In eq. 4, should \\theta be \\phi?\n\n2. What does \\pi_\\theta(a_t|s_t, {d_i,k}) mean? I'd like to see an example formulation of the policy. When the parameter \\theta is well trained, does the policy still take demonstrations {d_i,k} as inputs? If yes, why are demonstrations needed? \n\n3. In meta-testing (Algo. 2), will \\theta and \\phi be updated? Currently it looks like that the two parameters are fixed in meta testing. If so, this is a bit strange. Why not update the policies after sampling demonstrations (step 4) and collecting trials (step 5)?\n\n4. \"The state space S, reward ri, and dynamics Pi may vary across tasks.\" I think all the tasks should share the same input space; otherwise, the meta-trained two policies cannot be applied to a test task with different input space.  For example, if the states are 100x100 images in meta training, how to apply the policies to a meta-testing task with 100x200 images as states? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878315556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper312/Reviewers"], "noninvitees": [], "tcdate": 1570237753953, "tmdate": 1575878315569, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Review"}}}, {"id": "rJe17q5Z9H", "original": null, "number": 3, "cdate": 1572084247495, "ddate": null, "tcdate": 1572084247495, "tmdate": 1572972611182, "tddate": null, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "invitation": "ICLR.cc/2020/Conference/Paper312/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a meta-learning approach that can learn from demonstrations and subsequent reinforcement learning trials. The approach works in two phases. First, multiple demonstrations are collected for each task and a meta-learning policy is obtained by embedding the demonstrations of a single task in a context vector that is given to the actor policy.  Subsequently, the meta-policy is used to collect trajectories which are also evaluated with a (sparse) reward. These trajectories, along with the demonstrations, are again used by a second meta-learning policy (with a concatenated context from demonstrations and trajectories) to obtain the final policy. The algorithm is tested on a simple point reaching task as well as on a more complex 3d physics simulation that contains pick-and-place tasks, pushing tasks and button pressing.\n\nThe approach seems really interesting and I think combining demonstrations with reinforcement learning for meta learning is a very promising approach. This is also underlined by the experimental results presented in the paper. The only concern I have is the presentation of the paper. The algorithmic description is very short and many things are left unclear. I would also like to see a bit more ablation studies in the paper. More comments below:\n\n- Just from the text it is not clear how the meta-learning actually works and you have to study the figures in detail to understand that a context is extracted from the demonstrations as well as the RL trajectories. The context should be better introduced (even though this has been shown already in different papers, I think it is a good strategy to write the  \nin a self-contained way).\n- It is unclear to me how the reward signal is used to learn the meta-policy is phase 2. I understand that it is concatenated to the embedding before forming the context (again this is not really described in text but only in the figure), but it is quite unclear to me how the reward is used as optimality criteria. How do you reinforce good trajectories and decrease the influence of poor trajectories? This is maybe one of the most important parts of the paper and needs to be described in much more detail.\n- The reward functions are not properly explained in the paper, even not for the toy task (reaching). This should at least be done in the appendix.\n- I would like to see more ablation studies. For example, it is mentioned that 40 observations are used from the demonstrations and the trajectories. How is this number picked and how does it influence the performance? Also, typically, how long is a single trajectory? It is also unclear to me, if we take 40 random observations and the reward signal is sparse, wouldn't it be quite likely that the observations do not contain any reward values?\n- How does K influence the performance of the algorithm?\n\n \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper312/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayz@stanford.edu", "ejang@google.com", "kappler@google.com", "alexherzog@google.com", "khansari@google.com", "wohlhart@google.com", "yunfeibai@google.com", "kalakris@google.com", "slevine@google.com", "cbfinn@cs.stanford.edu"], "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards", "authors": ["Allan Zhou", "Eric Jang", "Daniel Kappler", "Alex Herzog", "Mohi Khansari", "Paul Wohlhart", "Yunfei Bai", "Mrinal Kalakrishnan", "Sergey Levine", "Chelsea Finn"], "pdf": "/pdf/dbe739d65d07b15df28fc4f40e29d648966e4165.pdf", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "code": "https://drive.google.com/open?id=1f1LzO0fe1m-kINY8DTgL6JGimVGiQOuz", "keywords": ["meta-learning", "reinforcement learning", "imitation learning"], "paperhash": "zhou|watch_try_learn_metalearning_from_demonstrations_and_rewards", "_bibtex": "@inproceedings{\nZhou2020Watch,,\ntitle={Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards},\nauthor={Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg5J6NtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b96e1af00f52f18c33d9a8e29af5b32feb9ed813.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg5J6NtDr", "replyto": "SJg5J6NtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper312/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878315556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper312/Reviewers"], "noninvitees": [], "tcdate": 1570237753953, "tmdate": 1575878315569, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper312/-/Official_Review"}}}], "count": 9}