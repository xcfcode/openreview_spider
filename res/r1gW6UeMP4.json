{"notes": [{"id": "r1gW6UeMP4", "original": "SJle6UeGDV", "number": 5, "cdate": 1552185016842, "ddate": null, "tcdate": 1552185016842, "tmdate": 1599502517387, "tddate": null, "forum": "r1gW6UeMP4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks", "authors": ["Mihir Kale", "Aditya Siddhant", "Sreyashi Nag", "Radhika Parik", "Anthony Tomasic", "Matthias Grabmair"], "authorids": ["mihirkale815@gmail.com", "siddhantaditya01@gmail.com", "sreyashinag28@gmail.com", "radhikaparik@gmail.com", "anthony.tomasic@gmail.com", "mgrabmair@andrew.cmu.edu"], "keywords": ["transfer learning", "contextual embeddings", "meta embeddings"], "TL;DR": "Extract contextual embeddings from off-the-shelf supervised model. Helps downstream NLP models in low-resource settings", "abstract": "Pre-trained word embeddings are the primary\nmethod for transfer learning in several Natural Language Processing (NLP) tasks. Recent\nworks have focused on using unsupervised\ntechniques such as language modeling to obtain these embeddings. In contrast, this work\nfocuses on extracting representations from\nmultiple pre-trained supervised models, which\nenriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and crosslingual settings indicate that such supervised\nembeddings are helpful, especially in the lowresource setting, but the extent of gains is dependent on the nature of the task and domain.", "pdf": "/pdf/11a73aebef858802eb61860802c37e613c3d898a.pdf", "paperhash": "kale|supervised_contextual_embeddings_for_transfer_learning_in_natural_language_processing_tasks"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "ICLR.cc/2019/Workshop/LLD"}, {"id": "S1luM8aHtN", "original": null, "number": 1, "cdate": 1554531855944, "ddate": null, "tcdate": 1554531855944, "tmdate": 1555512024678, "tddate": null, "forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Official_Review", "content": {"title": "Very good paper. Clear and well motivated approach. Detailed experiments and discussion. ", "review": "Summary\nThe paper proposes a technique to transfer knowledge from trained models to task/domains/languages where such models cannot be trained due to the lack of training data. This is done in contrast to using only pretrained embeddings such as GloVe or ELMO which are trained in an unsupervised manner, and independently from any downstream task. \nThe proposed approach assumes the existence of pre-trained models either for different tasks to the targeted one, or the same task but on different domains or languages. All these models are trained in a supervised manner where the last layer in the model is an output layer (before applying a Softmax). The authors project the feature representations that are generated from the pre-trained models using the Convex Combination method to unify and combine the features from all the models. Finally, they concatenate the resulting representation with either GloVe or ELMO. \nIn their experiments on different tasks, domains, and languages they showed that the proposed approach achieves an increase in the F1-Score ranging between 5%-7% compared to only using the pretrained embeddings.  \n\nPros.\n1.\tThe experimental setting fits the workshop. Specifically, training a model on limited data where the results are on par with models trained on all the available data.  \n2.\tThe proposed approach is sound. Mainly, where it does not require retraining models that were used for other tasks/domains/languages. \n3.\tThe experiments are designed to cover the different knowledge transfer settings: different tasks, the same task but different domains, and the same task but different languages. \n4.\tOverall, the paper is clear and well-written, and the authors provided a good discussion on the results. Furthermore, the results are well-presented and easy to interpret. \n5.\tFrom a reproducibility perspective, the authors provided all the experimental details in their paper. \n\nCons.\n1.\tIn the Related Work section, it is not clear how the work under Supervised Transfer Learning is different from this work.  \n2.\tIt is not clear how the GloVe representation was combined from the word level to the document/sentence level. If it was averaged for all the words, please mention that in the Experiments Setup section. \n\nFurther comments.\nIn addition to the proposed future work provided by the authors, I suggest the following:\n1.\tIn addition to only using pretrained embedding as a baseline, compare the performance of your model to the performance of SOTA of each task but with limited data. \n2.\tRun the experiments on multiple subsets of the data (1k each) and report the average performance with SD. \n3.\tFor the cross-task setting: would adding more (models trained on different) tasks always have a positive effect? Would having a large number of tasks eventually lead to a generic representation which is similar to simply using ELMO again? \n4.\tMinor formatting-related issues\n\u2022\tCitation.  Please note the difference between having the author name inside or outside the brackets: XX (2018) discussed \u2026 / the model is provided in (XX 2019 \n\u2022\tIn Sec.3 Approach, the acronym SRL was used without providing the full name (Semantic Role Labelling is mentioned in the first line of the introduction). \n", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks", "authors": ["Mihir Kale", "Aditya Siddhant", "Sreyashi Nag", "Radhika Parik", "Anthony Tomasic", "Matthias Grabmair"], "authorids": ["mihirkale815@gmail.com", "siddhantaditya01@gmail.com", "sreyashinag28@gmail.com", "radhikaparik@gmail.com", "anthony.tomasic@gmail.com", "mgrabmair@andrew.cmu.edu"], "keywords": ["transfer learning", "contextual embeddings", "meta embeddings"], "TL;DR": "Extract contextual embeddings from off-the-shelf supervised model. Helps downstream NLP models in low-resource settings", "abstract": "Pre-trained word embeddings are the primary\nmethod for transfer learning in several Natural Language Processing (NLP) tasks. Recent\nworks have focused on using unsupervised\ntechniques such as language modeling to obtain these embeddings. In contrast, this work\nfocuses on extracting representations from\nmultiple pre-trained supervised models, which\nenriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and crosslingual settings indicate that such supervised\nembeddings are helpful, especially in the lowresource setting, but the extent of gains is dependent on the nature of the task and domain.", "pdf": "/pdf/11a73aebef858802eb61860802c37e613c3d898a.pdf", "paperhash": "kale|supervised_contextual_embeddings_for_transfer_learning_in_natural_language_processing_tasks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Official_Review", "cdate": 1553713421596, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421596, "tmdate": 1555511819411, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Skgkz9GcYN", "original": null, "number": 2, "cdate": 1554815495461, "ddate": null, "tcdate": 1554815495461, "tmdate": 1555511880572, "tddate": null, "forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Official_Review", "content": {"title": "Missing fine-tuning experiments", "review": "The authors propose a method for combining encoded representations from pretrained supervised models for transfer learning. The authors argue that in low-resource settings, the combined representations can transfer useful supervision signals that unsupervised representations cannot. The authors evaluate their work on an SRL task (cross-task) and NER (cross-domain and cross-lingual).\n\nThe authors present a complete piece of work, from a summary of transfer learning techniques, to a description of their convex combination technique, to their experiments. The paper follows a logical structure, and is generally well written.\n\nThe authors make a strong argument for the practicality of their technique. They give a thorough description of the off-the-shelf models and architectures they use, as well as a relatively static hyperparameter set. Assuming the relevant pretrained models are available, the projection layers and combination weights are easy to implement. The model indeed differs from ELMo in that the task-specific models are not jointly learned and thus need to be projected into the same space.\n\nHowever, I'm concerned some key evaluations are missing. The most critical missing comparison is to ELMo/ULMFiT/BERT fine-tuned using the available labels for the transfer task (especially for cross-task and cross-domain). The most directly comparable one here is ELMo. The authors learn the exact same weights as ELMo fine-tuning would for their task: the decoder, the per-task weight, and the per-representation weight. It's likely that fine-tuning these alone would yield the performance gains obtained from concatenation with the proposed model. In general, a more complete ablation is needed besides [GloVe/ELMo] vs. [GloVe/ELMo] + proposed, e.g. proposed alone, GloVe + ELMo. In addition, the efficacy of the projection layers is not clear. The folk knowledge that a single linear can project unsupervised word vectors between languages likely does not apply here; a more complex projection might be needed to reconcile task-specific models. The authors give little justification for their projection approach and do not validate it experimentally.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks", "authors": ["Mihir Kale", "Aditya Siddhant", "Sreyashi Nag", "Radhika Parik", "Anthony Tomasic", "Matthias Grabmair"], "authorids": ["mihirkale815@gmail.com", "siddhantaditya01@gmail.com", "sreyashinag28@gmail.com", "radhikaparik@gmail.com", "anthony.tomasic@gmail.com", "mgrabmair@andrew.cmu.edu"], "keywords": ["transfer learning", "contextual embeddings", "meta embeddings"], "TL;DR": "Extract contextual embeddings from off-the-shelf supervised model. Helps downstream NLP models in low-resource settings", "abstract": "Pre-trained word embeddings are the primary\nmethod for transfer learning in several Natural Language Processing (NLP) tasks. Recent\nworks have focused on using unsupervised\ntechniques such as language modeling to obtain these embeddings. In contrast, this work\nfocuses on extracting representations from\nmultiple pre-trained supervised models, which\nenriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and crosslingual settings indicate that such supervised\nembeddings are helpful, especially in the lowresource setting, but the extent of gains is dependent on the nature of the task and domain.", "pdf": "/pdf/11a73aebef858802eb61860802c37e613c3d898a.pdf", "paperhash": "kale|supervised_contextual_embeddings_for_transfer_learning_in_natural_language_processing_tasks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Official_Review", "cdate": 1553713421596, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper5/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421596, "tmdate": 1555511819411, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1xMK3nf9E", "original": null, "number": 1, "cdate": 1555381369693, "ddate": null, "tcdate": 1555381369693, "tmdate": 1555510977653, "tddate": null, "forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks", "authors": ["Mihir Kale", "Aditya Siddhant", "Sreyashi Nag", "Radhika Parik", "Anthony Tomasic", "Matthias Grabmair"], "authorids": ["mihirkale815@gmail.com", "siddhantaditya01@gmail.com", "sreyashinag28@gmail.com", "radhikaparik@gmail.com", "anthony.tomasic@gmail.com", "mgrabmair@andrew.cmu.edu"], "keywords": ["transfer learning", "contextual embeddings", "meta embeddings"], "TL;DR": "Extract contextual embeddings from off-the-shelf supervised model. Helps downstream NLP models in low-resource settings", "abstract": "Pre-trained word embeddings are the primary\nmethod for transfer learning in several Natural Language Processing (NLP) tasks. Recent\nworks have focused on using unsupervised\ntechniques such as language modeling to obtain these embeddings. In contrast, this work\nfocuses on extracting representations from\nmultiple pre-trained supervised models, which\nenriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and crosslingual settings indicate that such supervised\nembeddings are helpful, especially in the lowresource setting, but the extent of gains is dependent on the nature of the task and domain.", "pdf": "/pdf/11a73aebef858802eb61860802c37e613c3d898a.pdf", "paperhash": "kale|supervised_contextual_embeddings_for_transfer_learning_in_natural_language_processing_tasks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper5/Decision", "cdate": 1554736070581, "reply": {"forum": "r1gW6UeMP4", "replyto": "r1gW6UeMP4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736070581, "tmdate": 1555510968172, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}