{"notes": [{"id": "P63SQE0fVa", "original": "BOOFKSavAJT", "number": 2808, "cdate": 1601308311486, "ddate": null, "tcdate": 1601308311486, "tmdate": 1614985759426, "tddate": null, "forum": "P63SQE0fVa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ShQbB_VWdht", "original": null, "number": 1, "cdate": 1610040374779, "ddate": null, "tcdate": 1610040374779, "tmdate": 1610473966814, "tddate": null, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a deep reinforcement learning approach for solving minimax multiple TSP problem. Their main algorithmic contribution is to propose a specialized graph neural network to parameterize the policy and used a clipped idea to stabilize the training. Unfortunately, the reviewers remain to be unconvinced by the experiments after the rebuttal and the writing need to be significantly improved. Also, it would be worthwhile to study how the proposed method can generalize to other problems. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040374763, "tmdate": 1610473966793, "id": "ICLR.cc/2021/Conference/Paper2808/-/Decision"}}}, {"id": "gOu69LaeVD-", "original": null, "number": 3, "cdate": 1603951295537, "ddate": null, "tcdate": 1603951295537, "tmdate": 1606778881648, "tddate": null, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review", "content": {"title": "Insufficient performance and contribution", "review": "The authors propose an RL framework, called ScheduleNet trained by clipped REINFORCE, for minmax multiple traveling salesman problem (minimax mTSP), which uses a clipping idea to stabilize learning process as PPO does. The authors empirically show the feasibility of the proposed framework.\n\n- Unfortunately, the proposed method has poorer performance than existing works, in particular, OR-Tool. This decreases the merit significantly.\n\n- In addition, it is hard to find contribution from proposing new RL method since the stabilizing effect of the clipped REINFORCE is shown in only limited environment (only minimax mTSP).\n\n- Table 2 is not completed.\n\n- The nature of \"minimax\" mTSP should be more clearly represented and exploited. Currently, the proposed RL framework seems to work for other formation of mTSP, and also it seems not to exploit the nature of minimax.\n\n- In order for showing novelty of the proposed method, it might be useful to devise and investigate actor-critic methods sharing the main idea. In the submission, only footnote 1 simply mentions the hardness of learning value function.\n\n- The behavior of ScheduleNet need to be studied further, e.g., when your algorithm works well, and not.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088258, "tmdate": 1606915765414, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2808/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review"}}}, {"id": "wHDCNRZBDyA", "original": null, "number": 9, "cdate": 1606288470884, "ddate": null, "tcdate": 1606288470884, "tmdate": 1606288470884, "tddate": null, "forum": "P63SQE0fVa", "replyto": "v1peR8EJf-J", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Response to the reviewer 2 - Continued", "comment": "1- Experimental evaluation leaves many questions unanswered;\nWe agree that our first submission leaves questions a lot. From the revised manuscript, we did our best to answer the unanswered question. The major \"answered\" questions are as follows:\n\n1. We clarify the set of instances where we are better than OR-Tools on the random uniform dataset. To summarize, ScheduleNet shows the cases winning the OR-Tools for every number of cities $N$ and the number of workers $m$  pairs. The winning ratio varies across the $N$ and $m$: ScheduleNet becomes stronger whenever the problems become larger.\n\n2. We confirmed and provided the results about \"scalability\" in the computational time of the proposed method. In short, for large instances, our approach is faster than OR-Tools. This becomes fascinating since we also show that the proposed method most likely produce better solutions than OR-Tools.\nWe further analyze the effectiveness of the proposed return normalization scheme and RL training method. (1) \n\n3. We show that the proposed return leads the RL agents to better performance than two dense rewards, which are practically considered to use training RL algorithms for solving scheduling problems. (2) Clipped REINFORCE, which can be seen as a simplified PPO, actually results in better outcomes than PPO. We hypothesize this phenomenon originates from the value prediction error of critics. We also provide the distribution of critic targets (makespan) with high volatility and even have multi-modalities, which make training critic challenging.\n\nPlease refer to section 6 of the revised manuscript that thoroughly explains the \"answers.\"\n\n2- Motivation for tackling yet another variant of the TSP is not very strong, in that it is unclear that practitioners solving mTSP in practice would be interested in using the proposed method. No discussion of how the ideas presented here could extend to other variants of TSP or significantly improve performance on some class of instances that are of great interest to the community or an application domain.\n\n**Practical motivation**: Solving mTSP, especially when the number of cities $N$ and workers $m$, are practically demanding.  Let us think about the scenarios where a giant fully autonomous warehouse with the retrieving robots are in there. In such facilities, the number of items (naturally considered to be a city) and the number of retrieving robots (salesman) can be huge. In this practical setup, ScheduleNet can be better than the other baseline discussed in the manuscript.\n\nAlso, this practical setup invokes a very different aspect of the mTSP problem. Unlike the problems we majorly discussed in the manuscript, the new tasks emerge during physically \"solving\" mTSP in the real world. \n\nThis scenario is often referred to as \"dynamic\" scheduling problems. The proposed method (construction heuristics) is the only method that can solve such dynamic problems without resolving the whole new mTSP. The other methods, such as exact method or construction heuristics (e.g., OR-Tools and seq-to-seq RL methods), require solving the problem from scratch.\n\n**Extention to the general scheduling problems**: We plan to extend the focus of this research to more general scheduling problems, including: \n\t1. the problems the AI community starts to deal with (e.g., Multi-robot reward collection (MRRC) and mCVRP).\n\t2. the more practically demanding-yet-challenging problems such as Job-shop scheduling problems (JSSP); especially the flexible JSSP (FJSSP). (F)JSSP arises from various contexts. From daily-life usage: e.g., job scheduling in over the multiple CPU/GPU cores, to the high-tech industries such as semi-conduction manufacturing systems. \n\nWe designed our problem the formulation and policy learning procedure being as much as independent from a specific type of scheduling problems such that, in future, the mentioned scheduling problems would be solved with the minimal change of method.\n\n3- Submission seems to have been rushed, with missing citations and weird captions in a couple places.\n\nWe apology for the first submission not tidy in terms of contents and grammar also. We wish that this revised manuscript can answer the questions made from the reviewing process.\n\n+ We fixed the missing citations and the weird captions. Thank you for the comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "v1peR8EJf-J", "original": null, "number": 3, "cdate": 1606230054894, "ddate": null, "tcdate": 1606230054894, "tmdate": 1606285482022, "tddate": null, "forum": "P63SQE0fVa", "replyto": "7tweN75oHy7", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Response to the reviewer 2", "comment": "Thanks for the review. We revised the manuscript such that the questions are well answered.\n\nwe have modified the introduction to further highlight the motivation and novelties of the current study. Please check the updated manuscript\n\nAnswers for question 1: We repeat the experiments for each pair of $n$ and $m$ 500 times to secure the statistical supports on validating the performance of each method.  Additionally, we also report standard deviations for each pair of $N$ and $m$.\n\nAnswers for question 2: In principle, solving the random mTSP instances with an exact method, such as formulating minmax mTSP as mixed-integer linear programming (MILP) and solve it with CPLEX is possible. However, we didn\u2019t choose to solve the random instances since it usually takes a prohibitively large computation time, especially large instances $N\\geq100$. Also, from the mTSP literature (not-necessarily deep + (RL) methods), we confirmed that the new algorithms are often compared to the existing heuristics. \n\nWe choose to use OR-Tools for as the major target of comparisons since the OR-Tools often shows the reliable good outcomes for the Minmax mTSP problems where the optimal solution (or at least, the upper and lower bound are known cases).\n\nAnswers for question 3: We delete the result of Hu et al. from the table. Instead, we additionally implement the heuristics and evaluate performances of the heuristics on the same mTSP instances to further investigate the properties of the proposed methods.  In summary, our method scores better-or-similar performance on various mTSP scenarios. And we confirmed that Schedulenet can win the OR-Tools on random mTSP dataset and in benchmark setup, Our method produces slightly large makespans but beats the meta-heuristic baselines which are optimizing the parameters for each instance.\n\nAnswers for question 4: We replace the benchmark results of CPLEX with the so-far-known best upper bound. \n\nAnswers for question 5: We update the manuscript to define the meta-heuristic algorithms, self-organization Map (SOM), ant-colony Optimization (ACO), evolutionary algorithm (EA), and add citations for the results.\n\nAnswers Q6: We manually tuned the \u2018search limits\u2019 parameters of the OR-Tools described in the above link. \nEspecially, we allows longer run times for solving large-sized maps to the OR-Tools. However, as we discussed in the manuscript, OR-Tools\u2019 results consistently tend to be strong in smaller worker cases (m <=5) and weak in large worker cases (m=7).\n\nAnswers Q7. Running time results. As per request, we included the computation time of ScheduleNet and comparison to OR-Tools in the Appendix. Empirical results show that ScheduleNet is faster, and its computation time is invariant to the topology (construction) of the underlying mTSP instance, and only depend on the problem size (m+N).\n\nAnswers Q8: We deliberately choose the hyperparameters to become deep learning standards. e.g., $2^n$-numbered of neurons, ReLU activations, etc. Also, we set the hyperparameters for training and RL methods as similar to one of the stable-baseline PPO2, which is considered to be the standard PPO implementation now.  We included the hyperparameters and details of Training, Heuristics in the Appendix.\n\nAnswers Q9. Relation to VRP... Have you considered comparing ScheduleNet to VRP learning approaches from the literature, such as Nazari et al. (which you cite)?\n\nThank you for asking this question. We want to emphasize, that although mTSP is the special case of the CVRP (C = infinity), the problem formulations and approaches that were used by Nazari et. al. and Kool et. al. cannot be directly applied to solve minmax multi-worker (m>1) problems. Both Nazari et.al. and Kool et. al. mentioned that they are tackling CVRP problems with m=1, i.e. single-vehicle cases. Extending their works to multi-worker cases, was one of the main motivations of the ScheduleNet, and, indeed, we are planning to extend our approach to multi-vehicle capacitated VRP (mCVRP) and Flexible Job Shop Scheduling Problem (FJSSP).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "VQm326aeCwO", "original": null, "number": 8, "cdate": 1606284266398, "ddate": null, "tcdate": 1606284266398, "tmdate": 1606284266398, "tddate": null, "forum": "P63SQE0fVa", "replyto": "4FYRO_fjFKE", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Comments for the weak points.", "comment": "Throughout the rebuttal, we further analyze the proposed approaches, and we wish the additional materials can show the potential of RL methods for solving the multi-worker scheduling problem.\n\n[The numerical experiments are not convincing me that the approach would be useful in practice]\nWe agree that, at first, our numerical results might be seen as weak. We add more experiments and supporting arguments on the updated manuscript for showing the practicality in the perspective of achieving better makespan and shortening the computational times.\nWe want to highlight a few updates as follows:\nOur method is not losing-algorithm. From the gap (makespan of ScheduleNet/ makespan of OR-Tools), distributions, we show our method can discover a better solution than OR-Tools in any size random uniform maps.\nOur method is faster than OR-Tools. Due to the proposed MDP formulation of mTSP, our method can solve the given mTSP instance than OR-tools. The time gap between the two methods becomes severe as the number of cities increase. Please kindly see Section 6.1 and Figure 6 in the appendix.\nOur method becomes \"practical\" when the scheduling decisions are required to be frequently made from two new updated findings, such as job dispatching in automated warehouse facilities.\n\n[To be informative, results in Table 1 should be the average over a number of random instances for each characteristic. Maybe it\u2019s already the case but it is not mentioned. Moreover, the random instances should also follow different distributions to be varied and really helpful to evaluate the method.]\n\nFirst of all, we apologize for the confusion. As you pointed out, the metrics which were on the previous table 1 were computed as you anticipated. \nHowever, we redesign the table 1 to become more statistically valid. We report the makespans of the given method by computing the mean and standard deviation for each pair of $N$ and $m$. The gaps, which show the relative goodness of the proposed model compared to the OR-Tools, are measured per instance.  \n\n[Table 1 and 2: I found the reported gap (fraction of objectives) not so clear to get a precise sense of the performance. In Table 2, using the standard (approximate) optimiality gap would be better (obj_heuristic \u2013 obj_cplex)/obj_cplex.]\n\nWe agree to report the gap compared to the \"optimal solution\" is better than the \"OR-Tools\" gap. However, due to the computational intractability of mTSP problems, it was unavailable to solve the large cases (e.g., $N$=200 $m$=5) within the appropriate time. The premature termination of the optimal solver (CPLEX) would result in too poor outcomes, and all the performance metrics could be optimistically biased.\n\nHence we decided to use the OR-Tools, known for reliably working in general cases with less performance variability across the mTSP instance, as the target. We also found that OR-Tools achieve near-optimal results for some benchmark problems. \n\n[It would be informative to report CPLEX results for the randomly generated instances as well.]\n\n[The authors claim that they propose a new approach for training \u201cClipped REINFORCE, a variant of clipped PPO without the learned value function\u201d. It would be useful to give more explanations for this choice.]\n\nThe combinatorial optimization problems, including mTSP, is notorious for behaving in a highly volatile manner. For instance, the very first action selection of mTSP MDP would change the consequent trajectory in total. And most of the cases similar state cannot be seen again in the given mTSP trajectory. This translates into makespan variability. \n\nFollowing our return definition, if one uses an actor-critic setup, the target of the critic is some (discounted) statistics transformed from the makespan. It naturally leads the hardship in training value functions and results in inaccurate value prediction of the critic.\n\nWe also confirmed that training value function. (i.e., PPO) is not helpful for achieving a better scheduler. Clipped REINFORCE shows better performance than PPO. We update the manuscript to better explains this phenomenon with additional experiments. Please kindly refer to the updated section 6.2 and Figure 4,5.\n\n[In equation (9), I believe there is a missing sum over \\tau. This does not help in understanding]\nThanks for the comments. We update the main body such that the summation over the event index is considered in the objective function."}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "llAkcugWL4U", "original": null, "number": 6, "cdate": 1606232694508, "ddate": null, "tcdate": 1606232694508, "tmdate": 1606260795529, "tddate": null, "forum": "P63SQE0fVa", "replyto": "gOu69LaeVD-", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Response to the reviewer 4", "comment": "we have modified the introduction to further highlight the motivation and novelties of the current study. Please check the updated manuscript\n\n**[Unfortunately, the proposed method has poorer performance than existing works, in particular, OR-Tool. This decreases the merit significantly]**\n\nThe proposed method cannot perform better than OR-tools on small to medium size instances. However, our approach shows a significant chance to win the OR-tools on the uniform random dataset of larger size, as provided in the updated manuscript. We kindly request to look at the revised manuscript. \n\nWe would like to emphasize that the proposed approach tends to perform better for large-size problem instances with many vehicles and customers, as the search space becomes prohibitively larger for the meta-heuristic algorithms. We would like to highlight that the proposed method can find a reasonably good solution with less computational time than OR-tools due to the decentralized decision-making strategy. To prove this, we include the computation time curves (vs OR-tools) in the Appendix.\n\n**[In addition, it is hard to find contribution from proposing new RL method since the stabilizing effect of the clipped REINFORCE is shown in only limited environment (only minimax mTSP).]**\n\nOur main contribution is not on proposing new RL method but rather on proposing a decentralized sequential decision-making scheme to solve mTSP.  Specifically, we formulate mTSP as a semi-MDP and derive a decentralized decision making policy in a multi-agent reinforcement learning framework using only a sparse and delayed episodic reward signal. The major components of the proposed method and their importance are summarized as follows:\n\n-(Formulation)\nDecentralized cooperative decision making strategy: Decentralization of scheduling policy is essential to ensure the learned policy can be employed to schedule any size of mTSP problems in a scalable manner; decentralized policy map local observation of each idle salesman one of feasible individual action while joint policy map the global state to the joint scheduling actions.\n\n-(Forward propagation)\nState representation using type-award graph attention (TGA): the proposed method represents a state (partial solution to mTSP) as a set of graphs, each of which captures specific relationships among works, cities, and a depot. The proposed method then employs type-aware graph attention (TGA) to compute the node embeddings for all nodes (salesman and cities), which are used to assign idle salesman to an unvisited city sequentially. \n\n-(Learning)\nTraining decentralized policy using a single delayed shared reward signal: Training decentralized cooperative strategy using a single sparse and delayed reward is extremely difficult in that we need to distribute credits of a single scalar reward (makespan) over the time and agents. To resolve this, we propose a stable RL training scheme which significantly stabilizes the training and improves the generalization performance.  \nIn short, the clipped Reinforce algorithm is designed specifically to train the ScheduleNet for solving mTSP.\n\n**[Table 2 is not completed.]**\nBoth table 1 and table 2 are updated to compare the results thoroughly. We additionally provide heuristic baselines for further comparison. We kindly request the reviewers to check the revised manuscript.\n\n**[The nature of \"minimax\" mTSP should be more clearly represented and exploited. The proposed RL framework seems not to exploit the nature of minimax.]**\n\nThe \u201cmax\u201d component in the \u201cminimax\u201d problem is used just to compute the performance of a solution (the maximum traveling distance of all the vehicles).  Once this reward is computed as a reward signal, typical RL approach can be used to minimize this reward. In other words, there are no competitive relationships among multi-agents, the situation where a typical \u201cminimax\u201d game is seeking to address. In short, the \u201cminimax\u201d problem can be described as \u201cminimizing the total completion time of all agents.\u201d\n\n**[It might be useful to devise and investigate actor-critic methods sharing the main idea.]**\nRepresenting a function over combinatorial action space using deep NNs is a challenging task because the small change in the combinatorial input space can result in a significant change in the output. Also, the approximated function cannot be differentiated with the input, limiting the trained model to be used for decision making. Fitting the performance (makespan) that is severely nonlinear and (possibly) discontinuous using continuous function approximation will induce performance degradation.  \n\nDue to these challenges, we do not fit the critic and only use the learned policy (actor). To empirically prove this argument, we conducted further extensive experiments showing that Clipped REINFORCE without Critic performs better than PPO  for both sparse and dense reward structures. We kindly request reviewers to look at the revised manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "4FYRO_fjFKE", "original": null, "number": 4, "cdate": 1606231093233, "ddate": null, "tcdate": 1606231093233, "tmdate": 1606247024731, "tddate": null, "forum": "P63SQE0fVa", "replyto": "d0mcg8kxVVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Response to the reviewer 3", "comment": "Thanks for the review. We revised the manuscript such that the questions are well answered. we have modified the introduction to further highlight the motivation and novelties of the current study. Please check the updated manuscript.\n\nAnswers for questions 13: The proposed TGA utilize the directional information severely. We empirically confirmed that (even though it is not discussed in the manuscript) aggregating the messages depending on the source helps the agent each better performance. It is naturally make sense that the edge from \"task\" to \"worker\" has different meaning for the gnn and as well as the RL agent  to the edge from \"worker\" to \"task\".\n\nAnswers for question 14: We consider the unit travel time for all workers.  i.e., If the worker nods $v_i$ is on the way to the assigned city, the worker's position $i$ will be on some point of the line connecting its source city, and the destination (assigned) city. We additionally explains the state update mechanism (which is handled by the simulator) in the appendix.\n\nAnswers for question 15: \"Inactive-worker, Assigned-worker, Unassigned-worker\nInactive-task, Assigned-task, Unassigned-task, and Depot\" are the list of the node types. We also update the appendix to give the full list of the types of nodes. \n\nAnswer for question 16: Thank you for commenting on critical point for enhancing readability.  For the revised manuscript, Any edge related quantities such as $e_{ij}$, $h_{ij}$, $\\alpha_{ij}$, $z_{ij}$ use the unified notation rules such that $j$ is the source node and the $i$ is the destination node. \n\nAnswers for question 17: Yes, we realized that the notation can be misreading. The k-type neighborhood set $\\mathcal{N}_k(i)$ is for enumerating the nodes which are in-neighborhood of node $i$ and simultaneously type $k$ (i.e., $k_j=k$).\n\nAnswer for question 18: We use the greedy version of the current policy as the baseline policy $\\pi_b$. The reason we choose to use the greedy version is two-fold (1) It can be seen as self-improving targets as discussed in [1],[2] (2) We consider practical cases where the baseline policy is not applicable easily. For instance, Job-shop-scheduling problems (JSSP) is known to hard for solving the problems but practically demanding in modern manufacturing facilities such as semi-conductor fabrication systems. Since the problem itself is hard, it is hard to get \"nice\" baseline, especially when considering computational budgets. \n\nAnswer for question 19: The discount factor $\\gamma$ is used to introducing the \"neutrality\" concept while training policy. The normalized makespan and the normalized return become (near) zero whenever the current training policy is neither better nor worse than the baseline policy. The normalized return will be (nearly) zero during the early phase of the mTSP MDP, which is intended since it is hard to know which policy is better during the early phase.\n\nAnswer for question 20: Yes, it should be vice-versa. We revise the manuscript.\n\nAnswer for question 21: Self-organization Map (SOM), Ant Colony Optimization (ACO), and Evolutionary Algorithms (EA) are one of the most well studied and practically favored meta-heuristic algorithms. We choose to such meta-heuristic algorithms as baseline since they often leads nice performances within moderate computational burdens.  We majorly update the manuscripts to give more explanation and comparisons toward the meta-heuristic algorithms.\n\nAnswer for question 22: We agree that, in the field of RL, tasks reflects the objective of RL agents especially in meta-RL cases. However, we hope to keep the terminology \"task\" because, in various scheduling problems, the tasks are often used to describe any quantities to process; for instance, in JSSP, the \"task\" is used to describe the single step of manufacturing process.\n\nAnswer for question 24: We report the upper bounds instead of the average of lower and upper bounds.\n\n[1] Mastering the game of Go with deep neural networks and tree search\n[2] Attention, Learn to Solve Routing Problems!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "fPpcno89ZZX", "original": null, "number": 5, "cdate": 1606231580517, "ddate": null, "tcdate": 1606231580517, "tmdate": 1606245302987, "tddate": null, "forum": "P63SQE0fVa", "replyto": "r0LDSStHGC", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Paper updated with additional numerical results.", "comment": "Thank you for reviewing and our paper. Please let us address your concerns. \n\nwe have modified the introduction to further highlight the motivation and novelties of the current study. Please check the updated manuscript\n\n- ***Multi-agent cooperative RL problem ***\\\nIndeed, our task can be viewed as a multi-agent cooperative RL problem. In this study, we propose a learning-based decentralized and sequential decision-making algorithm for solving Minmax mTSP problem in the MARL framework (centralized training and decentralized execution). The reason why we did not emphasize the MARL aspect is that when all agents share the same decision making policy through parameter sharing, it can be viewed as a single agent RL. However, each agent clearly makes independent action based on its own local observation in a decentralized manner.\n\nThe trained policy, which is a construction heuristic, can be employed to solve mTSP instances with any number of salesman and cities. Learning a transferable mTSP solver in a construction heuristic framework is significantly challenging comparing to its single-agent variants (TSP and CVRP) because (1) we need to use the state representation that is flexible enough to represent any arbitrary number of salesman and cities (2) we need to introduce the coordination among multiple agents to complete the geographically distributed tasks as quickly as possible using a sequential and decentralized decision making strategy and (3) we need to learn such decentralized cooperative policy using only a delayed and sparse reward signal, makespan, that is revealed only at the end of the episode.\n\nTo tackle such a challenging task, we formulate mTSP as a semi-MDP and derive a decentralized decision making policy in a multi-agent reinforcement learning framework using only a sparse and delayed episodic reward signal. The major components of the proposed method and their importance are summarized as follows:\n\n(Formulation) Decentralized cooperative decision-making strategy: Decentralization of scheduling policy is essential to ensure the learned policy can be employed to schedule any size of mTSP problems in a scalable manner; decentralized policy maps local observation of each idle salesman one of feasible individual action while joint policy maps the global state to the joint scheduling actions.\n\n(Forward propagation) State representation using type-award graph attention (TGA): the proposed method represents a state (partial solution to mTSP) as a set of graphs, each of which captures specific relationships among works, cities, and a depot. The proposed method then employs type-aware graph attention (TGA) to compute the node embeddings for all nodes (salesman and cities), which are used to assign idle salesman to an unvisited city sequentially. \n\n(Backward propagation) Training decentralized policy using a single delayed shared reward signal: Training decentralized cooperative strategy using a single sparse and delayed reward is extremely difficult in that we need to distribute credits of a single scalar reward (makespan) over the time and agents. To resolve this, we propose a stable MARL training scheme which significantly stabilizes the training and improves the generalization performance.  \n\n\n\n-***Performance***\\\nWe updated Table 1 with additional \"2phase\" heuristics (K-Means Clustering + TSP Insertion Heuristics). Our results show our competitiveness in regards to the additional baselines. Also, we can observe that ScheduleNet performs comparatively or better than the OR-Tools on problems with a large number of workers.\n\n\n\n-***VRP***\\\nThank you for this suggestion. \n\nmTSP and VRP have different types of constraints. To solve mTSP, we need to consider the coordination among multiple agents who employing decentralized policies. To solve VRP, we need to consider a capacity constraint that requires more strategic routing behavior. Although RL-based approaches have been employed to solve VRP, there are few RL/MARL approaches to solve mTSP. We are planning to expand the mTSP problem to consider the vehicle capacity in the future. This type of problem can be perceived as a multi-agent vehicle routing problem with capacity constraints. \n\nIn a similar direction, we are planning to expand our approach to solve the Flexible Job Shop Scheduling Problem (FJSSP) that also requires multi-robot coordination with joint constraints imposed on them. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2808/Reviewers", "ICLR.cc/2021/Conference/Paper2808/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "juwszY8jV9Y", "original": null, "number": 7, "cdate": 1606245231570, "ddate": null, "tcdate": 1606245231570, "tmdate": 1606245231570, "tddate": null, "forum": "P63SQE0fVa", "replyto": "llAkcugWL4U", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment", "content": {"title": "Response to the reviewer 4  - Continued", "comment": "**[The behavior of ScheduleNet need to be studied further]**\nWe agree with that. We definitely add more detailed qualitative analysis in the revised manuscript. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "P63SQE0fVa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2808/Authors|ICLR.cc/2021/Conference/Paper2808/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844278, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Comment"}}}, {"id": "d0mcg8kxVVa", "original": null, "number": 1, "cdate": 1603886787851, "ddate": null, "tcdate": 1603886787851, "tmdate": 1605024127211, "tddate": null, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review", "content": {"title": "An interesting RL framework for the mTSP problem with limited mild empirical results  ", "review": "Summary\n-------------\nThe paper proposes a reinforcement learning approach to solve the min-max multiple TSP, where there are multiple salesmen and the goal is to minimize the longest subtour while every city is visited by one salesman. The authors propose an architecture, ScheduleNet, that encodes a partial solution or state and outputs a policy, i.e. a probability distribution over the actions. They train the model using a variant of the REINFORCE algorithm. The approach is validated on randomly generated mTSP instances as well as the standard literature benchmark TSPlib.\n\n\nStrong points\n-------------------\n1. The addressed problem, TSP with multiple salesmen is an important combinatorial problem that is more challenging than the standard TSP because of the multi-agent cooperation that it involves. It is true that although there is a lot of literature on learning-based approaches that solve the TSP, only a few very recent papers deal with the multiple agent setting\n2. The MDP formulation with the notion of events is sound and clearly explained. It is more sophisticated than the standard MDPs used in the \u201cone agent\u201d setting\n3. The type-aware embeddings are interesting here to differentiate the interactions between the different types of nodes.\n\n\nWeak points\n-----------------\n4. The numerical experiments are not convincing me that the approach would be useful in practice  \n5. To be informative, results in Table 1 should be the average over a number of random instances for each characteristic. Maybe it\u2019s already the case but it is not mentioned. Moreover, the random instances should also follow different distributions to be varied and really helpful to evaluate the method.\n6. Table 1 and 2: I found the reported gap (fraction of objectives) not so clear to get a precise sense of the performance. In Table 2, using the standard (approximate) optimiality gap would be better (obj_heuristic \u2013 obj_cplex)/obj_cplex. \n7. It would be informative to report CPLEX results for the randomly generated instances as well.\n8. Although the TSP is a natural special case of mTSP, the performance on of the approach on randomly generated TSP instances (cf Table 3) is significantly poorer than that of other learned heuristics. It would be useful to report the results for TSPlib as well.\n9. The authors claim that they propose a new approach for training \u201cClipped REINFORCE, a variant of clipped PPO without the learned value function\u201d. It would be useful to give more explanations for this choice.\n10. In equation (9), I believe there is a missing sum over \\tau. This does not help in understanding \n\n\nRecommendation\n-------------------------\nI would vote for reject. In summary, the proposed approach is an adaptation of known techniques, to a specific interesting problem, that does not lead to a clear gain in performance.\n\nArguments for recommendation\n---------------------------------------------\n11. The MDP framework and type-aware GNNs are interesting and new in this context but not novel \n12. To me, the numerical experiments are very limited and do not demonstrate the added-value of this method, see weak points above\n\n\nQuestions to authors\n-----------------------------\n13. Sec 4.1: It is said that you consider the complete graph and that the edge features are the Euclidian distance which is symmetric. So what is the point of using a *directed* graph?\n14. Sec 4.1: \u201cv_i denotes the node corresponding entity i in mTSP problem\u201d. It sounds like v_i is a node of the graph. But if at \\tau a worker is in between two cities, what would be v_i?\n15. Sec 4.1: what are the types exactly? You give an example \u201cactive-worker\u201d but it would be useful to list them all.\n16. Sec 4: There is a confusion between source and destination indices. \u201ceij denotes the edge between between source node vi and destination node vj\u201d but then for the edge embedding \u201cthe specific type kj of the source node vj\u201d and \u201cthe message from the source node vj to the destination node vi\u201d. Similarly, equation (2), it is confusing to use j as a source index.\n17. Sec 4.2: the definition of Nk(i) = {vj |kj = k, \u2200l \u2208 N (i)} does not make sense. What is the correct one? Because the graph is complete, is N(i) different from the entire V? \n18. Sec 5: \u201c\\pi_b is the evaluation and baseline policy\u201d. What baseline did you use?\n19. Sec 5: equation 8, can you explain the choice of the exponent of gamma?\n20. Sec 6:  \u201cm \u223c U(2, 4) and N \u223c U(10, 20)\u201d. Are m and N switched here? Otherwise there would be more workers than cities.\n21. Table 2: what are SOM, ACO and EA? These baselines should be described (at least named) in the text.\n\nFeedback to help improve the paper\n--------------------------------------------------\n22. \u201cFor the clarity of explanation, we will refer to salesman as a workers, and cities as a tasks.\u201d I actually found it more confusing. Especially because task is standardly used to refer to the entire problem that the RL algorithm is addressing\n23. \u201cWe define the set of m salesmen VT = {1, 2, ..., m}, and the set of N cities VC = {m+1, 2, ..., m+ N}\u201d -> set of m salesmen *indexed by* VT = {1, 2, ..., m}, and the set of N cities *indexed by* VC = {m+1, 2, ..., m+ N}\n24. \u201cCPLEX results are reported as the average of the upper and lower bound\u201d. It would make more sense to report the upper bound, i.e. the value of the best feasible solution found by the solver within the time limit.\n25. To be able to better generalize to the TSP instances, why not include instances with N=1 during training.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088258, "tmdate": 1606915765414, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2808/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review"}}}, {"id": "7tweN75oHy7", "original": null, "number": 2, "cdate": 1603920972929, "ddate": null, "tcdate": 1603920972929, "tmdate": 1605024127147, "tddate": null, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review", "content": {"title": "Good initial results, but not enough at this point.", "review": "Summary of the paper:\nThis paper proposes a deep reinforcement learning (DRL) approach for learning a solution strategy for the minimum-makespan multiple Traveling Salesman Problem (mTSP). The makespan mTSP is a challenging combinatorial optimization problem in which we are given the 2-dimensional locations of a set of customers that must be visited by a (much smaller) set of trucks. The trucks depart from the same depot, and must return to it after their tours. The minimum makespan version of mTSP asks for a set of such tours such that the length of the longest tour is minimized.\n\nThis work is part of a recent interest in using machine learning to design algorithms for hard discrete optimization problems. A number of such methods have been proposed for the standard TSP problem, but the minimum-makespan mTSP brings a number of challenges: the makespan is a sparse reward signal in RL terms, in that it is realized only after a full solution has been constructed (at the end of the episode); unlike the TSP, the mTSP has multiple trucks to be managed at every iteration of a sequential constructive algorithm.\n\nThe authors make two main contributions towards establishing a DRL approach to min-makespan mTSP:\n\n1- They propose a specialized graph neural network architecture which combines known ingredients in a way that is suitable to the structure of the mTSP;\n\n2- They modify the RL training algorithm to take into account the intricate discrete structure of the makespan objective, which stabilizes the training process.\n\nExperimentally, the proposed ScheduleNet method is trained on a single set of random instances with a small number of customers and trucks, then tested on similar and larger random instances, as well as some benchmark mTSP instances from the literature. Compared to some other learned and non-learned algorithms, ScheduleNet seems to be competitive.    \n\nStrengths:\n1- Interesting engineering of the graph network model and of the RL training procedure to take into account mTSP and makespan structure;\n\n2- Generalization from very tiny instances to much larger ones (though not too large in an absolute sense).\n\nWeaknesses:\n1- Experimental evaluation leaves many questions unanswered;\n\n2- Motivation for tackling yet another variant of the TSP is not very strong, in that it is unclear that practitioners solving mTSP in practice would be interested in using the proposed method. No discussion of how the ideas presented here could extend to other variants of TSP or significantly improve performance on some class of instances that are of great interest to the community or an application domain. \n\n3- Submission seems to have been rushed, with missing citations and weird captions in a couple places.\n\nRecommendation: Overall, I have to recommend a rejection, but I do think that the authors are on a good path towards a paper if they strengthen the motivation and experiments. I don't know if that will be possible within the ICLR rebuttal.\n\nQuestions to the authors:\n\n1- Table 1: how many instances are considered for each (N, m) pair here? Please provide standard deviations for the values provided here, without which it's hard to tell how stable the reported average makespan is.\n\n2- Table 1: Why was CPLEX not run on the MTSP Uniform instances as was done in Table 2? This way you can compute the exact approximation ratio.\n\n3- Table 1: Are the results reported for Hu et al. copied from that paper? If so, are you using the exact same set of graphs? If not, a direct comparison such as that claimed in Table 1 is not possible.\n\n4- Table 2: The caption is hard to parse. The following does not make sense: \"CPLEX results are reported as the average of the upper and lower bound.\" Instead, you should report the best solution found by CPLEX (i.e., the best upper bound at termination).\n\n5- Table 2: Please define SOM, ACO and EA, and cite the respective papers as well as any additional implementation details.\n\n6- OR-Tools: You should tune the parameters of OR-Tools (e.g., https://developers.google.com/optimization/routing/routing_options) on the same training set of instances that you train your model on. The tuning can be performed using some kind of grid search or more sophisticated tuning tools such as SMAC (https://github.com/automl/SMAC3).\n\n7- Running time results: There is no mention whatsoever of the running time of ScheduleNet (in training, but more importantly at test time), and how it compares to OR-Tools and the other heuristics of Table 1-2.\n\n8- ScheduleNet hyperparameters: You list the values but no mention of if/how they were tuned.\n\n9- Relation to VRP: mTSP is a special case of VRP in which there are no worker (truck) capacities. Have you considered comparing ScheduleNet to VRP learning approaches from the literature, such as Nazari et al. (which you cite)?\n\nMinor:\n- \"In this study, we formulate (MinMax mTSP as a Markov\" --> remove \"(\"\n- Section 3.1, \"Transition\": I find this paragraph hard to parse.\n- \"is equals to the makespan\" --> \"is equal to the makespan\"\n- \"since the following operations only for the given time step\" --> \"since the following operations only apply to the given time step\"\n- \"instances whose number m of tasks and the number N of workers are sampled\" --> shouldn't this be the opposite?\n- \"benchmark problems in mTSPLib (cite),\" --> please add the appropriate citation: https://profs.info.uaic.ro/~mtsplib/\n- Appendix: \"and produces and produces \u201ctype-aware\u201d\"\n- Appendix: \"The computation steps of equation 11, 12,and ??\"\n- Appendix: \"hyperparameters of SchduleNet\" --> \"hyperparameters of ScheduleNet\"", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088258, "tmdate": 1606915765414, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2808/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review"}}}, {"id": "r0LDSStHGC", "original": null, "number": 4, "cdate": 1604244725491, "ddate": null, "tcdate": 1604244725491, "tmdate": 1605024127014, "tddate": null, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "invitation": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review", "content": {"title": "It proposes an RL algorithm to solve mTSP problem.  ", "review": "The mTSP with the goal of minimizing the longest route is considered, which minimizes the maximum route. This objective results in a balanced-length set of routes, which compared to the sum of routes obtains a more practical result. A graph representation based on the worker and assigned tasks is defined, then a type-aware graph attention (TGA) embedding procedure is proposed in which it obtains an embedding for node and edge representation. The state for each entity involves the 2D coordinates of the entity and the boolean indicator of idleness and assigned task of the worker. The action is worker-to-task assignment, and the reward is the makespan of finishing the problem, which is a sparse function. In type-aware graph attention (TGA) embedding, the embedding of node and edge are obtained and using the attention mechanism, an important weight for the embedding of each edge is obtained. The embedding functions are type-dependent which means the embedding considers the source node-type. The final message value for each node is obtained by multiplying the weight and edge embedding value of its neighbors. Then, an MLP is used to obtain a value for each source and possible target node, which takes the final value of the source, target, and edge between those as input. Then, the output of the MLP is used to get the final probability of choosing the next node.\n\nMajor comment: \nThe proposed algorithm looks interesting. Specifically, this problem can be modeled as multi-agent cooperative RL problem, and this paper suggests a model to handles it as a single RL problem. (For example, see paper [1] which suggests a multi-agent approach for a similar problem). However, the numerical results do not suggest a competitive algorithm yet and there are several baselines which obtain smaller objective in a smaller time, and yet it does not make sense to solve mTSP with a RL method. Note that this is not the case in VRP, since the current non-learning/non-commercial algorithms are not powerful in solving even medium-size problems with 50 nodes.\n\n[1] Zhang, Ke, et al. \"Multi-Vehicle Routing Problems with Soft Time Windows: A Multi-Agent Reinforcement Learning Approach.\" arXiv preprint arXiv:2002.05513 (2020).\n\n+\n\nminor comment:\nThe citation of mTSPLib is missing. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2808/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2808/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ScheduleNet: Learn to Solve MinMax mTSP Using Reinforcement Learning with Delayed Reward", "authorids": ["~Junyoung_Park1", "~Sanzhar_Bakhtiyarov1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Sanzhar Bakhtiyarov", "Jinkyoo Park"], "keywords": [], "abstract": "Combinatorial Optimization (CO) problems are theoretically challenging yet crucial in practice. Numerous works used Reinforcement Learning (RL) to tackle these CO problems. As current approaches mainly focus on single-worker CO problems such as the famous Travelling Salesman Problem (TSP), we focus on more practical extension of TSP to multi-worker (salesmen) setting, specifically MinMax mTSP. From the RL perspective, Minmax mTSP raises several significant challenges, such as the cooperation of multiple workers and the need for a well-engineered reward function. In this paper, we present the RL framework with (1) worker-task heterograph and type-aware Graph Neural Network, and (2) the RL training method that is stable, has fast convergence speed, and directly optimizes the objective of MinMax mTSP in a delayed reward setting. We achieve comparable performance to a highly optimized meta-heuristic baseline, OR-Tools, and outperforms it in 10% of the cases, both on in-training and out-of-training problem distributions. Moreover, our problem formulation enables us to solve problems with any number of salesmen (workers) and cities.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|schedulenet_learn_to_solve_minmax_mtsp_using_reinforcement_learning_with_delayed_reward", "one-sentence_summary": "The study introduces RL based approach for constructing the solution to mTSP, and demonstrates the potential that the learned policy can be effectively used to schedule multiple vehicles for solving large-scale, practical, real-world applications", "pdf": "/pdf/a619cd10648a92075bd78de06051db96c41a230e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WKfOH7cMA", "_bibtex": "@misc{\npark2021schedulenet,\ntitle={ScheduleNet: Learn to Solve MinMax m{\\{}TSP{\\}} Using Reinforcement Learning with Delayed Reward},\nauthor={Junyoung Park and Sanzhar Bakhtiyarov and Jinkyoo Park},\nyear={2021},\nurl={https://openreview.net/forum?id=P63SQE0fVa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "P63SQE0fVa", "replyto": "P63SQE0fVa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2808/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088258, "tmdate": 1606915765414, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2808/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2808/-/Official_Review"}}}], "count": 13}