{"notes": [{"id": "Nieyh8anDiR", "original": null, "number": 3, "cdate": 1589528871339, "ddate": null, "tcdate": 1589528871339, "tmdate": 1589528871339, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment", "content": {"title": "Hope to see comparison with some existing energy-based text generation methods, especially MCMC-based methods.", "comment": "Dear Authors,\n\nCongratulations on the accepted paper!\n\nIn this paper, you mentioned MCMC methods for energy-based text generation. Actually, we proposed such a method called CGMH (<CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling>) in last year's AAAI. \n\nWhile your proposed method generates samples by importance sampling, CGMH performs lexical-level jumps between sentences to generate samples from an unnormalized distribution (such as P_{\\theta} in your paper), in the spirit of Metropolis-Hastings sampling. \n\nCGMH is also a highly efficient energy-based text generation model, which achieves remarkable results on several constrained-text-generation tasks such as keyword-to-sentence generation and unsupervised paraphrase. Hope we will have the opportunity to discuss and compare these two models.\n\nThank you."}, "signatures": ["~Ning_Miao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ning_Miao1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182541, "tmdate": 1576860572194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment"}}}, {"id": "41j7UJu8aC0", "original": null, "number": 2, "cdate": 1588286257652, "ddate": null, "tcdate": 1588286257652, "tmdate": 1588286257652, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "vELKwAGC4o", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment", "content": {"title": "Thanks!", "comment": "Thank you so much! Wish you all a successful and healthy 2020!"}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianwen_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182541, "tmdate": 1576860572194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment"}}}, {"id": "B1l4SgHKDH", "original": "ByxnHuxFwS", "number": 2281, "cdate": 1569439803887, "ddate": null, "tcdate": 1569439803887, "tmdate": 1587605905437, "tddate": null, "forum": "B1l4SgHKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "vELKwAGC4o", "original": null, "number": 10, "cdate": 1581743353911, "ddate": null, "tcdate": 1581743353911, "tmdate": 1581743353911, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "d8jezQgru7", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment", "content": {"title": "References Updated", "comment": "Thanks for your comments! Compared to images, a challgenge in text modeling is that the inputs are discrete, such that we cannot directly apply Langevin dynamics (our initial attempts at working in a relaxed space were not very successful). That being said, these prior works on EBMs for images/videos are indeed relevant, and we've updated our references accordingly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2281/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2281/Authors|ICLR.cc/2020/Conference/Paper2281/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143675, "tmdate": 1576860538514, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment"}}}, {"id": "d8jezQgru7", "original": null, "number": 1, "cdate": 1577997790438, "ddate": null, "tcdate": 1577997790438, "tmdate": 1577997790438, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment", "content": {"title": "Missing related reference about un-normalized energy-based models parameterized by neural nets for image/video/3D shape generation", "comment": "Dear Authors, \n\nCongratulations on your nice accepted paper about text generation by un-normalized energy-based models. \n\nI would like to point out some papers that are highly related to your current one, and hope you can cite them in your final version.  Similar to your current paper, all of them are about un-normalized energy-based models parameterized by modern neural nets for image/video/3D shape generation.  The learning is based on MLE. The sampling is based on Langevin dynamics.  \n\nMore specifically,  \n\nThe first paper that proposes an energy-based model parameterized by modern deep neural network and learned it by Langevin based MLE is in (Xie. ICML 2016) [1].  The model is called generative ConvNet, because it can be derived from the discriminative ConvNet.  (Xie. ICML 2016) [1] originally studied such an EBM model on image generation  theoretically and practically in 2016.   \n\n(Xie. CVPR 2017) [2] (Xie. PAMI 2019) [3] proposed to use Spatial-Temporal ConvNet as the energy function for video generation.  The model is called Spatial-Temporal generative ConvNet. \n\n(Xie. CVPR 2018) [4] proposed to use volumetric 3D ConvNet as the energy function for 3D shape pattern generation. It is called 3D descriptor Net. \n\n(Gao. CVPR 2018) [5] proposed multi-grid MCMC to learn EBM with ConvNet as energy function for image generation. \n\n(Nijkamp. NeurIPS 2019) [6] proposed short-run MCMC to learn EBM with ConvNet as energy function for image generation. \n\nThank you :)\n\nReference\n[1] A Theory of Generative ConvNet. \nJianwen Xie *, Yang Lu *, Song-Chun Zhu, Ying Nian Wu (ICML 2016)\n\n[2] Synthesizing Dynamic Pattern by Spatial-Temporal Generative ConvNet\nJianwen Xie, Song-Chun Zhu, Ying Nian Wu (CVPR 2017)\n\n[3] Learning Energy-based Spatial-Temporal Generative ConvNet for Dynamic Patterns\nJianwen Xie, Song-Chun Zhu, Ying Nian Wu\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2019\n\n[4] Learning Descriptor Networks for 3D Shape Synthesis and Analysis\nJianwen Xie *, Zilong Zheng *, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, Ying Nian Wu (CVPR) 2018 \n\n[5]  Learning generative ConvNets via multigrid modeling and sampling. \nR Gao*, Y Lu*, J Zhou, Song-Chun Zhu, Ying Nian Wu (CVPR 2018).  \n\n[6] On learning non-convergent non-persistent short-run MCMC toward energy-based model. \nE Nijkamp, M Hill, Song-Chun Zhu, Ying Nian Wu (NeurIPS 2019)\n"}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianwen_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182541, "tmdate": 1576860572194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Public_Comment"}}}, {"id": "ZrhQoS-k2", "original": null, "number": 1, "cdate": 1576798745150, "ddate": null, "tcdate": 1576798745150, "tmdate": 1576800890988, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a Residual Energy-based Model for text generation.\n\nAfter rebuttal and discussion, the reviewers all converged on a vote to accept, citing novelty and interestingness of the approach.\n\nAuthors are encouraged to revise to address reviewer comments.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713997, "tmdate": 1576800263744, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Decision"}}}, {"id": "HJggab7ccS", "original": null, "number": 3, "cdate": 1572643255871, "ddate": null, "tcdate": 1572643255871, "tmdate": 1574433652964, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This work is an interesting extension of Gutmann and Hyvarinen (2010), where the parametric model is the combination of a noise model (language model) and an energy function (residual energy), so the difference of parametric model and the noise model cancels out the noise model. Therefore optimizing (3) under some conditions converges to a set of parameters of the parametric model (P\\theta(x) here) that best describes the data.\n\nOne important assumption of Gutmann and Hyvarinen (2010) is that there exists a set of optimum parameters for the parametric model such that the probability of data and the parametric models match for these optimum parameters. This should be mentioned in Theorem-1.  \n\nDoes Theorem-1 need extra parameters to act as a normalization constant in order for the theorem to hold at the optimum?\nlog P_lm(x) - E(x) + const = log p_data\n\nTo sample from the model, the authors first sample from the language model and re-sample it with respect to the energy values of the residual model.\n\n\nTo compute the perplexity, they have given an upperbound and lowerboud for the partition function based on number samples in Theorem 2, but I haven't checked the correction of the bounds.  They also factorize the joint model in auto-regressive factorization to compute the perplexity by approximate marginalizing. \n\n\nAs mentioned in Section 5, this approach heavily depends on a strong pretrained language model. \n\nHave you considered improving the language model during training?\n\nThe described idea is simple and effective and I really liked it.\n\n\n--- Based on other reviews and the authors' response (especially review #3), I reduced my rating to 'Weak accept'. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575867163290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Reviewers"], "noninvitees": [], "tcdate": 1570237725073, "tmdate": 1575867163307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review"}}}, {"id": "Hyl9u-SX5H", "original": null, "number": 2, "cdate": 1572192625702, "ddate": null, "tcdate": 1572192625702, "tmdate": 1574429548278, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The authors make good points, starting from the exposure bias and label bias suffered by the mainstream neural auto-regressive models.\nResidual EBMs are defined and trained using NCE. Experiments on two large language modeling datasets show that residual EBMs yield lower perplexity and generation via importance sampling is of higher quality, compared to locally normalized baselines.\n\nIn generally, the paper is well motivated and interesting. But I have some concerns.\n\n1. Missing important relevant references.\n\nEBMs (a.k.a. un-normalized models, random fields) have been successfully developed in language modeling in recent years. A large body of this paper has been studied in [5,6], including the model and the NCE estimation method. The model Eq.(2) is exactly the model in [5], defining the model in the form of exponential tilting of a reference distribution.\nConnecting and comparing to these previous works are needed.\n\n[1] R. Rosenfeld, S. F. Chen, and X. Zhu, \u201cWhole-sentence exponential language models: a vehicle for linguistic-statistical integration,\u201d Computer Speech & Language,  2001.\n[2] B. Wang, Z. Ou, and Z. Tan, \u201cTrans-dimensional random fields for language modeling,\u201d ACL, 2015.\n[3] B. Wang, Z. Ou, and Z. Tan, \u201cLearning transdimensional random fields with applications to language modeling,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.\n[4] B. Wang and Z. Ou, \u201cLanguage modeling with neural trans-dimensional random fields,\u201d IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.\n[5] B. Wang and Z. Ou, \u201cLearning neural trans-dimensional random field language models with noise-contrastive estimation,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n[6] B. Wang and Z. Ou, \u201cImproved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation,\u201d IEEE Spoken Language Technology Workshop (SLT), 2018.\n\n2. I am a little bit concerned that the theoretical contribution seems weak. \nThough Eq. (4) and (5) seem to be novel, I am not sure whether such a contribution is substantial enough to motivate acceptance.\n\nI'm happy to adjust the score if the paper can be better placed in the literature and the authors take efforts to improve the paper.\n\n--------update after reading the response-----------\nBeing well-placed in the literature and properly claiming contribution with respect to prior work is one of the key questions in reviewing a paper. The first version of the paper clearly lacks in this respect. That's the main concern when I gave a 1.\n\nI appreciate the authors' response. The updated paper has been improved to address my main concern, although the added discussions presented in the updated paper is not as clear as the authors' clarifications in the response. I suggest to polish the main text incorporating these clarifications.\n\nGenerally, it is nice to see the successful application of energy-based/random-field-based models in text generation, besides in speech recognition. I update the score to 6 (Weak Accept).\n\nIt would have been better that the following can be further clarified.\n\n\"the partition function estimated via importance sampling would lead to bias favoring the random field language model\" --- this comment is not clear to me. \n\nBoth Eq.4 and Eq.5 give estimates for perplexity. It would be better to clarify different uses of the two equations. If the perplexities are estimated using Eq.4 (as in Table 1), then what is the purpose of developing Eq.5?\n\nHow to calculate the lower and upper bounds of the step-wise perplexity gain at each position in Figure 1?\n\nUnder Figure 1, \"At each position the lower and upper bounds (see Eq. 4) are estimated using 20,000 samples.\" But in the main text, it is said that \"We therefore break down perplexity per position in the generated sequences as in Eq. 5\" at page 8. It is confusing.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575867163290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Reviewers"], "noninvitees": [], "tcdate": 1570237725073, "tmdate": 1575867163307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review"}}}, {"id": "rylLgGNojS", "original": null, "number": 4, "cdate": 1573761518408, "ddate": null, "tcdate": 1573761518408, "tmdate": 1573761518408, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "HJggab7ccS", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you so much for your helpful comments!\n\n- Missing assumption on the existence of the solution:\nThe reviewer is correct. The revised version of the paper now makes this assumption explicit in Theorem 1.\n\n- Partition function needs extra parameter:\nAs proven in Ma & Collins and reported in the paper, the assumption is that the amount of training data goes to infinity and that the model has sufficient capacity. There is no need for additional parameters under this assumption since a powerful energy function would learn this normalizer.\nIn practice, we observed that the score produced by the transformer energy function after training with the NCE objective is well calibrated as we vary the conditioning prefix. No additional bias parameter is required.\n\n- Alternating between training the residual energy function and the generator: \nThis is possible. In this work, we focused on improving the original generator by only training the residual energy function which is the simplest setting. However, iterative training like in GANs might further improve the overall model. \nSince the transformer models we use in this work are big and slow to train, and training sequence GANs requires policy gradients and heavy tuning to control gradient variance, this direction requires significant engineering efforts. Alternatively, a knowledge-distillation like procedure where we use the generated samples from the joint model to fine-tune the generator might also work. We leave this as future work."}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2281/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2281/Authors|ICLR.cc/2020/Conference/Paper2281/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143675, "tmdate": 1576860538514, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment"}}}, {"id": "BylUcWNiiS", "original": null, "number": 3, "cdate": 1573761421991, "ddate": null, "tcdate": 1573761421991, "tmdate": 1573761421991, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "Hyl9u-SX5H", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for the useful feedback, and especially for the pointers to prior work.  This paper is improved due to your care!\n\n- Missing references and unclear contribution:\nWe thank the reviewer for pointing this out. The revised version of the paper has now a whole new paragraph discussing the relation to prior works on energy-based models for sequence generation [1, 2, 3, 4, 5, 6]. In particular, the residual modeling form and the training algorithm is the same as in [5], of course with different choice of generator (transformer in our case VS LSTM in [5]) and energy function (BERT in our case VS CNN-LSTM-based model in [5]). Therefore, the modeling form and loss function should not be considered our contribution.\n\nOur theoretical contributions are: a) new lower and upper bounds for the log-probability of the joint model which allows us to show that these models have better perplexity compared to autoregressive approaches (since otherwise the partition function estimated via importance sampling would lead to bias favoring the random field language model), b) the importance weighting sampling scheme used at generation time, and c) the setting which is focused on conditional generation as opposed to rescoring for speech recognition. \n\nIn particular, (a) is important because it allows comparing the EBMs (with bi-directional models) against auto-regressive models in the standard metric by which these methods are judged; and we do indeed show that the residual EBMs get good results even compared against large SOTA LMs. We also show this with human evaluations.  In our opinion, improving upon these modern language models is an exciting accomplishment.  \n\nPlease, let us know if our revised discussion of prior work needs further clarifications. Thank you."}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2281/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2281/Authors|ICLR.cc/2020/Conference/Paper2281/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143675, "tmdate": 1576860538514, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment"}}}, {"id": "rJe1DgVjjr", "original": null, "number": 2, "cdate": 1573761111455, "ddate": null, "tcdate": 1573761111455, "tmdate": 1573761111455, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "Syg4BwvptB", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you so much for your helpful comments!\n\n- Comparison to SeqGAN:\nCompared to SeqGAN, the difference is that our goal is to use the \"discriminator\" (energy network) to improve the generator at test time whereas SeqGAN would throw away the discriminator after training and use the improved generator. SeqGAN requires policy gradients, which is usually unstable, and a too-powerful discriminator would make the generator gradients vanish (Arjovsky et al 2017). GANs \"have proven difficult to train in the domain of natural language\", \"most models yield worse results than a simple Language Model\", \"can be extremely sensitive to the random initialization and small deviations from the best hyperparameter choice\", and \"extensive evaluation has shown that existing language GANs do not improve over maximum likelihood-trained models\" (d\u2019Autume et al 2019, Semeniuta et al 2018, Caccia et al 2018). \nTo the best of our knowledge, there's no existing work that used a discriminator as powerful as BERT to successfully train language GANs, and in practice we observed that BERT is able to distinguish real text from LM samples over 95% of the time, hence language GAN learning with such a powerful discriminator seems implausible, not to mention that due to our large datasets and models, it is hard to do hyper-parameter tuning to stabilize policy gradients based training.\n\n- PPL: \nPerplexity is the standard metric for language modeling. It is true that other metrics could be considered, like diversity. We don't think that our approach would improve diversity compared to the original language model (our generator). \nThe main reason people worry about generation diversity in GANs is because of the mode collapsing problem (e.g., a generator always generating \"I don't know\" might fool the discriminator). However, unlike GANs which only use the generator at test time, we use the energy function to adjust the original language model to better approximate data distribution, so we don't see mode collapsing problems in our approach. One empirical evidence is that we found the adjusted per-step probabilities are largely similar to the original language model probabilities (Appendix A.1), and empirically language models do not have as severe mode collapsing problems as GANs. \nOne could improve diversity by sampling hypotheses sequentially from the joint model adding an additional constraint on diversity with respect to the previously drawn samples, or use temperatured sampling. We believe this is an interesting avenue of future research.\n\n- Qualitative analysis: \nThank you for the suggestion. We have added some examples to the supplementary material (A.5).\n\n- Conclusions are expected: \nResidual EBMs provide a very natural way of leveraging BERT for language modeling, and we believe that providing a simple working recipe to use unnormalized sequence-level generative modeling to improve very large state-of-the-art language models is an important contribution.\n\nReferences:\nArjovsky et al 2017: Towards Principled Methods for Training Generative Adversarial Networks\nd\u2019Autume et al 2019: Training language GANs from Scratch\nCaccia et al 2018: Language GANs falling short\nSemeniuta et al 2018: On accurate evaluation of GANs for language generation\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1l4SgHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2281/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2281/Authors|ICLR.cc/2020/Conference/Paper2281/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143675, "tmdate": 1576860538514, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Authors", "ICLR.cc/2020/Conference/Paper2281/Reviewers", "ICLR.cc/2020/Conference/Paper2281/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Comment"}}}, {"id": "Syg4BwvptB", "original": null, "number": 1, "cdate": 1571809084390, "ddate": null, "tcdate": 1571809084390, "tmdate": 1572972359258, "tddate": null, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nContributions:\n\nThe main contribution of this paper lies in the proposed Residual Energy-based Model (EBM) for text generation. Traditional normalized models operate at the token level with MLE training, while the proposed EBM operates at the sentence level. Therefore, BERT, or RoBERTa, can be leveraged for EBM design. The residual energy function is trained via conditional NCE, which reduces to training a binary classifier to discriminate between real text and text generated by an auto-regressive language model. After model training, text can be generated by top-k joint sampling. Experiments are conducted on two large language modeling datasets. The proposed model achieves lower perplexity, and preferred by human evaluation.  \n\nStrengths:\n\n(1) Writing & Clarity: This paper is well written, easy to follow, and clearly presented. I enjoyed reading this paper. \n\n(2) Novelty: The proposed model contains some novelty inside. It is framed in a residual EBM framework, though by the end, the residual energy function reduces to training a binary classifier to discriminate real and fake text. Though simple, this idea is wrapped up in a nice framework. It is also interesting to observe that this sequence-level EBM regularization can be considered as a way to fine-tune BERT for the text generation task.\n\n(3) Experiments: Generally, the experiments are comprehensive. Detailed analysis, and human evaluation is also provided.\n\nWeaknesses:\n\n(1) Clarity: I have some concerns regarding the selection of baselines, with details shown below.\n\nThis paper is basically about using BERT as a binary classifier, which serves as a residual energy function to regularize a pre-trained language model and provides sequence-level supervision. The experiments are comprehensive, but on the other hand, it is also quite expected that the proposed model should work better than an MLE baseline, since sequence-level supervision is provided. \n\nI think if the authors want to make a stronger paper, they should also compare with other possible ways to inject sequence-level supervision. For example, a simple solution is to use GAN, like in a SeqGAN setup. And the discriminator in the GAN will be the same BERT-based binary classifier. In this GAN setup, sequence-level supervision is also provided. \n\nThen the difference is that in the GAN setup, the BERT-based binary classifier is a discriminator, but in this paper's setup, it is a residual energy function. It would be interesting to discuss and conduct experiments to see which way is better. \n\n(2) Experiments: I have some concerns regarding the experimental setup. \n\na) One of the main results is Table 1, which reports all the PPL numbers. However, reporting PPL results is less interesting, because we also care about the diversity of generated samples. Lower PPL does not necessarily mean higher-quality text. Though Figure 2 provides some analysis on the diversity, a more comprehensive evaluation on this will be appreciated. \n\nb) It will be good if the authors can also provide some generated samples for qualitative analysis. \n\nOverall, I think this paper is well executed. The paper is well written, and experiments are carefully conducted. However, on the other hand,  I also think the conclusion in this paper is expected, it only shows that the proposed model is better than an MLE baseline. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2281/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dengyuntian@seas.harvard.edu", "yolo@fb.com", "aszlam@fb.com", "ranzato@fb.com"], "title": "Residual Energy-Based Models for Text Generation", "authors": ["Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc'Aurelio Ranzato"], "pdf": "/pdf/a52949443b29f47fd3298eec9c93f3b5d25eb545.pdf", "TL;DR": "We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. ", "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.", "keywords": ["energy-based models", "text generation"], "paperhash": "deng|residual_energybased_models_for_text_generation", "_bibtex": "@inproceedings{\nDeng2020Residual,\ntitle={Residual Energy-Based Models for Text Generation},\nauthor={Yuntian Deng and Anton Bakhtin and Myle Ott and Arthur Szlam and Marc'Aurelio Ranzato},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l4SgHKDH}\n}", "original_pdf": "/attachment/06c202af4b0bd1738dea7afbb100bbb3cbe14e06.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l4SgHKDH", "replyto": "B1l4SgHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2281/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575867163290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2281/Reviewers"], "noninvitees": [], "tcdate": 1570237725073, "tmdate": 1575867163307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2281/-/Official_Review"}}}], "count": 12}