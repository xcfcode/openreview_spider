{"notes": [{"id": "B1xJAsA5F7", "original": "rJlM0fFYKX", "number": 856, "cdate": 1538087878834, "ddate": null, "tcdate": 1538087878834, "tmdate": 1548717628730, "tddate": null, "forum": "B1xJAsA5F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bkgjce41xE", "original": null, "number": 1, "cdate": 1544663187400, "ddate": null, "tcdate": 1544663187400, "tmdate": 1545354504549, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Meta_Review", "content": {"metareview": "The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "after revisions the reviewers reached a consensus on accepting the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper856/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353060275, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353060275}}}, {"id": "HylNHt4h0X", "original": null, "number": 11, "cdate": 1543420220032, "ddate": null, "tcdate": 1543420220032, "tmdate": 1543420220032, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "r1gHailcAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Thank you!", "comment": "Thank you for your insightful comments again! They are very helpful!"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "r1gHailcAQ", "original": null, "number": 10, "cdate": 1543273405339, "ddate": null, "tcdate": 1543273405339, "tmdate": 1543273405339, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "B1lysCy5CX", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "score updated", "comment": "Thank you for updating the paper. I've updated the score as well."}, "signatures": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "SyeFxjBc37", "original": null, "number": 2, "cdate": 1541196529354, "ddate": null, "tcdate": 1541196529354, "tmdate": 1543273105205, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "content": {"title": "Interesting idea, issues in the execution", "review": "Update:\nThe score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now.\n\n\nOld Review Below:\n\nThe paper describes a graph-to-graph translation model for molecule optimization inspired from matched molecular pair analysis, which is an established approach for optimizing the properties of molecules. The model extends a chemistry-specific variational autoencoder architecture, and is assessed on a set of three benchmark tasks.\n\n\nWhile the idea of manuscript is interesting and promising for bioinformatics, there are several outstanding problems, which have to be addressed before it can be considered to be an acceptable submission. This referee is willing to adjust their rating if the raised points are addressed. Overall, the paper might also be more suited at a domain-specific bioinformatics conference.\n\n\nMost importantly, the paper makes several claims that are currently not backed up by experiments and/or data. \n\nFirst, the authors claim that MMPs \u201conly covers the most simple and common transformation patterns\u201d. This is not correct, since these MMP patterns can be as complex as desired. Also, it is claimed that the presented model is able to \u201clearn far more complex transformations than hard-coded rules\u201d. The authors will need to provide compelling evidence to back up these claims. At least, a comparison with a traditional MMPA method needs to be performed, and added as a baseline. Also, it has to be kept in mind that the reason MMPA was introduced was to provide an easily interpretable method, which performs only local transformations at one part of the molecule. \u201cFar more complex transformations\u201d may thus not be desirable in the context of MMPA. Can the authors comment on that?\n\nSecond, the authors state that they \u201csidestep\u201d the problem of non-generalizing property predictors in reinforcement learning, by \u201cunifying graph generation and property estimation in one model\u201d. How does the authors\u2019 model not suffer from the same problem? Can they provide evidence that their model is better in property estimation than other models?\n\n\nIn the first benchmark (logP) the GCPN baseline is shown, but in the second benchmark table, the GCPN baseline is missing. Why? The GCPN baseline will need to be added there. Can the authors also comment on how they ensure the comparison to the GPCN and VSeq2Seq is fair? Also, can the authors comment on why they think the penalized logP task is a good benchmark?\n\nAlso, the authors write that Jin et al ICML 2018 (JTVAE) is a state of the model. However, also Liu et al NIPS 2018 (CGVAE) state that their model is state of the art. Unfortunately, both JTVAE and CGVAE were never compared against the strongest literature method so far, by Popova et al, which was evaluated on a much more challenging set of tasks than JT-VAE and CGVAE. The authors cite this paper but do not compare against it, which should to be rectified. This referee understands it is more compelling to invent new models, but currently, the literature of generative models for molecules is in a state of anarchy due to lack of solid comparison studies, which is not doing the community a great service.\n\n\nFurthermore, the training details are not described in enough detail. \nHow exactly are the pairs selected? Where do the properties for the molecules come from? Were they calculated using the logP, QED and DRD2 models? How many molecules are used in total in each of these tasks?\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "cdate": 1542234361365, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817851, "tmdate": 1552335817851, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lysCy5CX", "original": null, "number": 9, "cdate": 1543270039346, "ddate": null, "tcdate": 1543270039346, "tmdate": 1543270039346, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "r1xXOkQO07", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Paper updated based on your feedbacks", "comment": "Thank you very much for your insightful comments. We have removed claims about practical drug discovery, as well as claims that are not well supported by our current manuscript. For instance, we have modified the related work section (see point 3) and removed statements in the ablation study paragraph in the appendix (see point 5). We also updated statements in the experiment section since we have added MMPA and GCPN baselines.\n\n1a) How did the authors optimize the hyperparameters of the mmpdb algorithm?\nThe current mmpdb program is very expensive to run. It takes about 4-5 hours to perform MMPA on 1000 molecules due to large number of extracted rules. Therefore, we performed limited amount of hyperparameter tuning on the validation set to find good hyperparameters. Moreover, some hyperparameters (e.g., the size of environment fingerprints) are hard-coded in the source code, and we couldn\u2019t investigate how these hyperparameters will affect the model performance.\n\n1b) Why the authors need to translate each molecule 50 times? MMPA is deterministic, so one should just need to translate once and then pick the top 50 translated/transformed molecules with the highest expected improvement ...\nWe did exactly what you describe here. Each test set molecule is translated \u201conce\u201d, but in this \u201cone-time\u201d translation, multiple matching transformation rules are applied to this compound. And we simply picked the top 50 transformed molecules within the similarity constraint. We defined \u201cone\u201d translation in MMPA as applying \u201cone\u201d transformation rule.\n\n2) Popova\u2019s work, GCPN or any other comparable RL framework can be applied in straightforward way to lead optimization as well: One would just plugin a reward function of f(mol) = min( sim(startmol, mol),threshold ) + Property(mol) [...] Wouldn\u2019t this be even more flexible & general compared to the method presented here?\nWe agree that RL framework could be extended to our conditional translation scenario. However, adding similarity into the reward itself is not enough, unless you also feed the \u201cstartmol\u201d into the RL model so that it knows what the starting molecule looks like. Otherwise the RL model will get confused since the reward function will keep changing as the starting molecule changes during training. Therefore a successful extension of this algorithm would be a contribution in its own right.\n\n3) Re II 2: This reviewer remains unconvinced. This paragraph needs to be fixed in the manuscript because also implicit estimation is estimation.\nWe suppose that you are referring to our response Part II 3 (not II 2, which is about MMPA instead of implicit property estimation). We agree that current manuscript does not provide enough evidence regarding this point. Therefore we have changed the paragraph in related work section. We removed statements involving \u201csuboptimal property estimator\u201d.\n\n4) The authors have scored all 250k/350k molecules using logP/QED/the DRD SVM, which are exactly the \u201csuboptimal property predictors\u201d that BayesOpt/RL would use for scoring, and then created pairs from them? Doesn\u2019t this imply the same suboptimal estimation is now baked into the translation model, but implicitly?\nWe agree that the suboptimal property estimator can implicitly affect our model, given the way we created the training data. Therefore, we have removed these claims (see point 3). However, our graph-to-graph translation model can be trained on molecular pairs constructed based on their measured properties without any property estimation models. We couldn\u2019t do this experiment as such datasets are not publicly available, but they often exist in pharma companies. In contrast, prior models require property predictor to be an integral part of the model.\n\n5) The authors state that \u201cIn a real-world drug discovery setting, there is usually a budget on how many drug candidates can be tested in the laboratory [\u2026] This is beneficial as it requires fewer experiments in the real scenario.\u201d, but then require 250k/350k samples to train the model. Isn\u2019t this a contradiction?\nWe have removed these sentences as they can be misleading and they are irrelevant to the ablation comparison. Please note that the goal of this ablation study is to investigate the importance of the adversarial learning component.\nRegarding your \u201ccontradiction\u201d concern, we used 250k/350k samples as they were readily available. The question of data efficiency applies to all neural models, including RL models for drug discovery and neural models for property prediction. To investigate this, we trained VJTNN on the logP task (delta=0.4) using only 3k molecular pairs, as compared to 120k pairs extracted from the full dataset. The test set result is 1.26 +/- 1.53 (full dataset performance was 3.3 +/- 1.8). Indeed, learning graph translation is challenging under low-resource scenario, and we leave this issue for future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "r1xXOkQO07", "original": null, "number": 8, "cdate": 1543151466697, "ddate": null, "tcdate": 1543151466697, "tmdate": 1543151466697, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "H1xuZ76rTm", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "reply", "comment": "First, thanks a lot for the authors efforts, this is much appreciated! \nNevertheless, this reviewer thinks the paper is still overselling the results, and hides limitations, which is unfortunate and unnecessary, since the modeling idea is actually promising.\n\n\nComments:\n\nIn terms of modeling, there is indeed a distinction between mapping from molecules to better molecules over other generative models, e.g. variational autoencoders or graph-convolutional policy networks.\n\nHowever, in practice, there is no distinction, since *in effect* both models perform the optimization of molecular properties with respect to the molecules. In fact, the same scoring functions that are used in this paper here could be used by a VAE+Bayesian optimization or an RL model as the reward, and are applied in practice to hit/lead optimization as well as library generation. The former application is even more frequent in practice than the latter.\n\nComments to the authors comments:\n\nRe: I 1) \nThanks for running the mmpdb baseline! A few questions on that:\n\na) How did the authors optimize the hyperparameters of the mmpdb algorithm?\nb) This reviewer does not fully understand why the authors need to translate each molecule 50 times? MMPA is determistic, so one should just need to translate once and then pick the top 50 translated/transformed molecules with the highest expected improvement that are within the similarity constraint. Can the authors comment on that in more detail? \n\n\nRe I 2: Thank you for running the GCPN baseline!\nPlease note that Popova\u2019s work, GCPN or any other comparable RL framework can be applied in straightforward way to lead optimization as well: One would just plugin a reward function of f(mol) = min( sim(startmol, mol),threshold ) + Property(mol), and wouldn\u2019t actually have to worry about pretraining, Wouldn\u2019t this be even more flexible & general compared to the method presented here?\n\nRe II 2: This reviewer remains unconvinced. This paragraph needs to be fixed in the manuscript because also implicit estimation is estimation.\n\nRe II 6:\nSo, if this reviewer understands correctly, the authors have scored all 250k/350k molecules using logP/QED/the DRD SVM, which are exactly the \u201csuboptimal property predictors\u201d that BayesOpt/RL would use for scoring, and then created pairs from them? Doesn\u2019t this imply the same suboptimal estimation is now baked into the translation model, but implicitly?\n\n\nAlso in the (commendable) ablation study in the appendix, the authors state that \u201cIn a real-world drug discovery setting, there is usually a budget on how many drug candidates can be tested in the laboratory, as biological experiments are time-consuming in general. [\u2026] This is beneficial as it requires fewer experiments in the real scenario.\u201d, but then require 250k/350k samples to train the model. Isn\u2019t this a contradiction?\n\n\nOverall:\nSo to be crystal-clear: The authors will need to remove any claims to practical drug discovery, and position their paper more realistically, then this reviewer will recommend acceptance. But in the current form, there are still too many unsupported and misleading claims.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "Syl_q2s_67", "original": null, "number": 3, "cdate": 1542139023896, "ddate": null, "tcdate": 1542139023896, "tmdate": 1542211526819, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "HkgKBKlq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Response to Reviewer 1 (Part II): Clarifications with paper updated to elaborate Section 3.3", "comment": "Thank you very much for your insightful comments. Our response to the issues you mentioned is the following:\n\n1) Please provide an explanation of why using a larger value for delta gives worse performance than a smaller value.\nA larger delta implies a tighter similarity constraint. For instance, setting delta to 0.6 means the generated compounds Y have to be very similar to the input molecule X (sim(X,Y) > 0.6). When delta decreases to 0.4, the generated structures are allowed to deviate more from the starting point X (sim(X,Y) > 0.4). Therefore, one would naturally expect the model to perform better (find higher scoring molecules) when delta is smaller since the structures can be chosen from a larger set. \n\n2) Diversity could be influenced by the cardinality of the sample. Please discuss why diversity is (not) biased versus larger sets.\nWe agree that the diversity depends on the sample size. Therefore, all the models are evaluated with the same sample size (K=50) for fair comparison. That is, for each molecule in the test set, we randomly sample 50 times from each model to compute the resulting diversity score.\n\n3) Tree and graph encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph.\nWe agree that a number of iterations (T) is required for proper propagation of information across the input graph. However, T does not need to be larger than the diameter since we adopted an attention mechanism in the decoder. It can dynamically read the information across the input graph in different decoding steps. In fact, a large T (e.g., the diameter) may potentially lead to overfitting.\n\n4) Clarification of tree decoding step (Section 3.2)\nFirst, the tree decoding process stops when it choose to backtrack at the root node. Second, we agree that this probability should depend on the number of nodes having been generated. This is implicitly captured by the neural message passing procedure. As noted in Eq. (4), the model makes this decision (expanding a new node or not) based on all the incoming messages at the current node. The messages carry information about the current (partial) tree structure, including potentially the number of nodes generated so far though not explicitly. \n\n5) Explanation of graph decoding step (Section 3.3)\nWe added Figure 2 to illustrate why the graph decoding step is not deterministic and how one junction tree can be decoded into different molecular graphs. Regarding the likelihood of ground truth subgraphs, we applied teacher forcing, i.e., we feed the graph decoder with ground truth junction trees as input. Section 3.3 has been updated correspondingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "SyghQTiupQ", "original": null, "number": 4, "cdate": 1542139172473, "ddate": null, "tcdate": 1542139172473, "tmdate": 1542211401106, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "HkgKBKlq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Response to Reviewer 1 (Part I): Probabilistic modeling of the involved components", "comment": "Thank you very much for your insightful comments. We want to provide more explanations on the probabilistic modeling of different involved components.\n\n1) Explicit probabilistic modeling of junction tree encoder-decoder (Section 3).\nPrior work (Jin et al. 2018) found that it is beneficial to adopt a coarse-to-fine approach to generate molecular graphs: first generate the backbone structure (i.e., junction tree T) and then assemble the sub-graphs in the tree into a complete molecular graph Y. Thus\n                                                  p(Y | X) =  \\sum_T p(Y | T, X) p(T | X)\nwhere p(Y | T, X) is the graph decoder and p(T | X) is the tree decoder. As the junction tree T of any graph is constructed through a deterministic tree decomposition algorithm, T does not function as a latent variable during training but is rather an intermediate object that can be predicted via supervised learning. Therefore, \n                                              p(Y | X) \\approx p(Y | T_y, X) * p(T_y | X)\nwhere T_y is the junction tree underlying the target graph Y.\n\nThe tree decoder generates a tree in an autoregressive manner, based on a specific sequentialization of the tree structure. A tree T is laid out as a sequence of edges {(i_1, j_1), \u2026, (i_m, j_m)} visited in the depth-first traversal over the tree. The probability of generating T is thus\n                          p(T | X) = \\prod_t  p( (i_t, j_t) | (i_1, j_1), \u2026, (i_t-1, j_t-1), X )\nwhere j_t always equals i_{t+1}. Probability of (i_t, j_t) depends on two factors: 1) whether j_t is a new node; 2) If j_t is a new node, what is its label; These two factors are modeled by the topological predictor (Eq. 4-6) and the label predictor (Eq. 8-9). The message passing procedure (Eq. 3) embeds the current partial tree realized by {(i_1, j_1), \u2026, (i_t-1, j_t-1)} into a continuous representation. Beyond the above architecture, in this paper we introduced an attention mechanism to capture how the decoded tree unravels step-by-step in an input graph X dependent manner. \n\nThe graph decoder models the conditional probability p(Y | T_y, X). This is a structured prediction task since Y is a graph. The variables in this structured prediction problem are node assembling decisions between neighboring nodes in the tree. For efficiency reasons, the assembling decisions are solved locally, starting from the root and its direct neighbors. In other words, p(Y | T_y, X) is a product of probabilities of choosing the right graph attachments with each node\u2019s neighbors, resulting in Eq. (10) (after taking log).\n\n2) Probabilistic modeling of multi-modal translation model (Section 4) \nIn this paper, we aim to learn diverse multi-modal mappings between two molecular domains, as there are many different ways to improve a given molecule. This diversity is introduced via latent variables z:\n                                           p(Y | X) = \\int_z p(Y | X, z) p(z) dz\nwhere prior p(z) models diverse strategies of improvement, independent of X, and is taken to be a standard Gaussian distribution. The overall model resembles a conditional variational autoencoder, learnable through reparameterization (Section 4.1). The approximate posterior Q(z | Y) only depends on the target Y so as to force z to capture resulting type of molecule, inferable from Y alone. \n\nThe proposed adversarial training technique (Section 4.2) is an additional regularization trying to discourage the model from generating undesirable outputs (e.g. molecules outside of the defined target domain). As a side note, p(Y | X, z) can be expanded as \n                                       p(Y | X, z) = p(Y | T_y, X, z) p(T_y | X, z)\nwhere latent variable z is concatenated with the encoded representation of X (Eq. 11)."}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "H1xuZ76rTm", "original": null, "number": 1, "cdate": 1541948159824, "ddate": null, "tcdate": 1541948159824, "tmdate": 1542211372602, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "SyeFxjBc37", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part II): Response to your other comments and questions", "comment": "Thank you very much for your insightful comments. Regarding your other comments and questions, our response is the following:\n\n1) \u201cThe authors claim that MMPs \u201conly covers the most simple and common transformation patterns\u201d. This is not correct, since these MMP patterns can be as complex as desired.\u201d\nWe agree that MMP patterns can be as complex as desired. However, allowing the patterns to be arbitrarily complex will result in a huge number of transformation rules. For instance, we have extracted 12 million rules in total on the logP and QED tasks when no constraints are imposed. Therefore, we have updated this claim in the paper with the following statement: \u201cMMPA's main drawback is that large numbers of rules have to be realized (e.g. millions) to cover all the complex transformation patterns.\u201d\n\n2) \u201cthe reason MMPA was introduced was to provide an easily interpretable method, which performs only local transformations at one part of the molecule. \u2018Far more complex transformations\u2019 may thus not be desirable in the context of MMPA.\u201d\nYes, we agree that there is always a trade-off between simple and understandable rules vs performance, and that the same trade-off is present in other machine learning applications (e.g., shallow decision trees vs  neural networks). Our focus in this paper is on demonstrating the performance gains we can obtain by reformulating the task as a translation problem. Deriving interpretable explanations for the predictions is clearly an important future direction, but is orthogonal to our current effort.\n\n3) \u201cThe authors state that they \u201csidestep\u201d the problem of non-generalizing property predictors in reinforcement learning \u2026  How does the authors\u2019 model not suffer from the same problem? Can they provide evidence that their model is better in property estimation than other models?\u201d\nWe want to clarify that our model does not explicitly estimate the properties. As a result, we can only provide indirect evidence showing that our model can nevertheless outperform other models in mapping precursor molecules into the target set of molecules with better properties. \n\n4) \u201cCan the authors also comment on how they ensure the comparison to the GCPN and VSeq2Seq is fair?\u201d\nWhen comparing to VSeq2Seq, we ensure that all models have about the same number of parameters (3.8~3.9 million), trained on the same dataset with the same optimizer and the same number of epochs. Both models are evaluated with K=50 translation attempts for each test compound.\nRegarding GCPN, their exact setup is not provided. As described in their paper [4], GCPN was trained in an environment whose initial state is one of the test set molecule of the logP task. They kept all the molecules generated during training and reported the molecule with the best logP improvement. We think this may bring more advantage to GCPN in our comparison, as our models do not have access to the test set.\n\n5) \u201cCan the authors comment on why they think the penalized logP task is a good benchmark?\u201d\nWe evaluated on this task because some prior work (e.g. JT-VAE, GCPN) has been tested on this benchmark, and their results are readily available for comparison. Indeed, this benchmark itself is not comprehensive enough. We therefore tested on two more tasks (QED and DRD2) aiming to provide a more thorough evaluation.\n\n6) \u201cHow exactly are the pairs selected? Where do the properties for the molecules come from? Were they calculated using the logP, QED and DRD2 models? How many molecules are used \u2026?\u201d\nThose details have been discussed in the Appendix B. We updated the relevant paragraphs to make it more clear. To summarize, logP and QED scores are calculated with RDKit built-in functions. For DRD2 activity prediction, we directly used the pre-trained model in Olivecrona et al. [3].\nOn the QED and DRD2 tasks, a molecular pair (X,Y) is selected if the Tanimoto similarity sim(X,Y) >= 0.4 and both X and Y fall into the source and target property range. On the logP task, we select molecular pairs when similarity sim(X,Y) >= delta and property improvement is greater than 0.5 (if delta=0.6) and 2.5 (if delta=0.4). In total 250K molecules are used for constructing the training pairs in the logP and QED tasks, and 350K molecules in the DRD2 task.\n\nReferences\n[1]  A. Dalke, J. Hert, C. Kramer. mmpdb: An Open-Source Matched Molecular Pair Platform for Large Multiproperty Data Sets. J. Chem. Inf. Model., 2018, 58 (5), pp 902\u2013910.\n[2]  M. Popova, O. Isayev, and A. Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018.\n[3]  M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):48, 2017.\n[4]  J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "Skl19Q6Hpm", "original": null, "number": 2, "cdate": 1541948294789, "ddate": null, "tcdate": 1541948294789, "tmdate": 1542211355397, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "SyeFxjBc37", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part I): Required experiments added and paper updated ", "comment": "Thank you very much for your insightful comments. We\u2019d like to clarify first that our model is a conditional graph-to-graph translation model which maps a given precursor compound to another with more desirable properties. Our translation approach is therefore NOT equivalent to a generative model over molecular structures (i.e., for chemical library design). This conditional translation model is useful and important for hit/lead compound optimization.\n\nIn response to your suggestions, we added two additional experiments:\n1) MMPA baseline: We utilized the open source tool \u201cmmpdb\u201d [1] to perform MMPA. For each task, we constructed a database of transformation rules extracted from the ZINC and Olivecrona et al. [3]\u2019s dataset. Same as our methods, each test set molecule is translated 50 times using the matching rules found in the database. When there are more than 50 matching rules, we choose those having higher average property improvement in the database. This statistic is calculated during the database construction. More details can be found in the Appendix B.\n\nThe results are shown in Tables 1 and 2 in the updated paper. On the QED and DRD2 tasks, our model outperforms MMPA with significant margin in terms of translation success rate (56.9% vs 20.8% on QED and 81.0% vs 35.6% on DRD2). On the logP task, our model also outperforms MMPA in terms of average property improvement (3.37 vs 2.00 when delta=0.4 and 1.53 vs 1.41 when delta=0.6).\n\n2) GCPN baseline: We used You et al [4]\u2019s open source implementation to train GCPN on the QED and DRD2 tasks. As stated in their paper [4], GCPN was trained in an environment whose initial state is one of the test set molecules. They kept all the molecules generated during training and reported the molecule with the best property improvement. For consistency, we adopted the same strategy in training and evaluation of GCPN (i.e., training on the test set of QED and DRD2). The performance is reported in Table 2. Our model greatly outperforms GCPN (56.9% vs 9.4% on QED and 81.0% vs 4.4% on DRD2).\n\nRegarding Popova et al.\u2019s method [2], we have carefully read the paper and studied its open-sourced code. The model described in [2] is not directly applicable to our setting as it targets chemical library design while our focus is on lead optimization starting from a given precursor compound. Their model architecture would have to be modified so as to take a precursor compound as an input to be optimized / translated. In fact, Popova et al. list this task as a future work.\n\nDue to limited length, our response to your other questions is posted in another post.\n\nReferences\n[1]  A. Dalke, J. Hert, C. Kramer. mmpdb: An Open-Source Matched Molecular Pair Platform for Large Multiproperty Data Sets. J. Chem. Inf. Model., 2018, 58 (5), pp 902\u2013910.\n[2]  M. Popova, O. Isayev, and A. Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018.\n[3]  M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):48, 2017.\n[4]  J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "SkgWrRs_Tm", "original": null, "number": 5, "cdate": 1542139449432, "ddate": null, "tcdate": 1542139449432, "tmdate": 1542211316760, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "Hkl9bvO52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "content": {"title": "Response to Reviewer 2: Explanation to your questions", "comment": "Thank you very much for your insightful comments. \n\n1) Why VSeq2Seq is better than JT-VAE and GCPN?\nThe main reason is that VSeq2Seq is trained with direct translation pairs through supervised learning, while JT-VAE and GCPN have to learn to discover these pairs in a weakly supervised manner. For instance, GCPN iteratively modifies a given molecule to maximize the predicted property score, where the translation pairs are discovered through reinforcement learning. JT-VAE optimizes a molecule by first mapping it into its latent representation and then performing gradient ascent in the latent space. In this case, translation pairs are discovered through the gradient signal given by the property predictor, which is trained on molecules with labeled properties. As the models are evaluated by translation quality, training the model directly with translation pairs is advantageous. \n\n2) Suppose we keep translating the molecule X1 -> X2 -> X3 ...  using the learned translation model, would the model still get improvement after X2? When would it get maxed out?\nOn the logP task, the model may still get improvements after X2, but we suspect this process will get maxed out after several steps because in general it is harder to optimize a molecule with high property scores. The QED and DRD2 tasks are different from logP task, as the target domain now becomes a closed set defined by the property range. As long as X2 belongs to the target domain (e.g., QED >= 0.9, DRD2 >= 0.5), this process will get maxed out since the model is trained only to improve molecules outside of the target domain.\n\n3) If we train with \u2018path\u2019 translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? \nIn general, it is harder to collect \u2018path\u2019 translation data than translation pairs due to data sparsity. For instance, to find a translation path X1 -> X2 -> X3, we need (X1,X2) and (X2,X3) to be valid translation pairs (i.e., both pairs satisfying property improvement and similarity constraints). Nonetheless, we believe that training the model with path translation will be helpful for global optimization -- finding molecules with the best property scores in the entire molecular space."}, "signatures": ["ICLR.cc/2019/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610107, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xJAsA5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper856/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper856/Authors|ICLR.cc/2019/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers", "ICLR.cc/2019/Conference/Paper856/Authors", "ICLR.cc/2019/Conference/Paper856/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610107}}}, {"id": "Hkl9bvO52Q", "original": null, "number": 3, "cdate": 1541207810028, "ddate": null, "tcdate": 1541207810028, "tmdate": 1541533633153, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "content": {"title": "review on \"Learning Multimodal Graph-to-Graph Translation for Molecule Optimization\"", "review": "This paper proposed an extension of JT-VAE [1] into the graph to graph translation scenario. To help make the translation model predicting diverse and valid outcomes, the author added the latent variable to capture the multi-modality, and an adversarial regularization in the latent space. Experiment on molecule translation tasks show significant improvement over existing methods.\n\nThe paper is well written. The author explains the GNN, JT-VAE and GAN in a very organized way. The idea of modeling the molecule optimization as translation problem is interesting, and sounds more promising (and could be easier) than finding promising molecule from scratch. \n\nTechnically I think it is reasonable to use latent variable model to handle the multi-modality. Using GAN to align the distribution is also a well adapted method recently. Thus overall the method is not too surprising to me, but the paper executes it nicely. Given the significant empirical improvement, I think this paper has made a valid contribution to the area.\n\nRegarding the results in Table 1, I\u2019m curious why the VSeq2Seq is better than JT-VAE and GCPN (given the latter two are the current state-of-the-art)? \n\nAnother thing I\u2019m curious about is the \u2018stacking\u2019 of this translation model. Suppose we keep translating the molecule X1 -> X2 -> X3 ...  using the learned translation model, would the model still gets improvement after X2? When would it get maxed out?\nOr if we train with \u2018path\u2019 translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? I\u2019m not asking for more experiments, but some discussion might be useful.\n\n[1] Jin et.al, Junction tree variational autoencoder for molecular graph generation, ICML 2018\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "cdate": 1542234361365, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817851, "tmdate": 1552335817851, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgKBKlq2m", "original": null, "number": 1, "cdate": 1541175617508, "ddate": null, "tcdate": 1541175617508, "tmdate": 1541533632694, "tddate": null, "forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "content": {"title": "A paper proposing a quite complex system (with no explicit probabilistic factorisation) which seems to obtain good experimental results.", "review": "As a reviewer I am expert in learning in structured data domains. \nThe paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. \nOverall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to \"how many\" nodes have been generated before, i.e. we do not want to have a high probability to \"go back\" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. \nThe complexity of the proposed system is actually an issue since the author(s) do not attempt (except for  the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). \nReference to previous relevant work seems to be complete.\nI think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.\n\nMinor issues:\n- Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed.\n- eq.(6): \\mathbb{u}^d is not defined.\n- Section 3.3:\n   - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles.\n  - the definition of f(G_i) involves  \\mathbb{x}_u. I guess they should be  \\mathbb{x}_u^G.\n  - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ?\n- Table I: please provide an explanation of why using a larger value for \\delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal.\n- diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper856/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n", "keywords": ["graph-to-graph translation", "graph generation", "molecular optimization"], "authorids": ["wengong@csail.mit.edu", "yangk@mit.edu", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Wengong Jin", "Kevin Yang", "Regina Barzilay", "Tommi Jaakkola"], "TL;DR": "We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.", "pdf": "/pdf/dee24460691863813212530280fc3d4335b0caac.pdf", "paperhash": "jin|learning_multimodal_graphtograph_translation_for_molecule_optimization", "_bibtex": "@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper856/Official_Review", "cdate": 1542234361365, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xJAsA5F7", "replyto": "B1xJAsA5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper856/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335817851, "tmdate": 1552335817851, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper856/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}