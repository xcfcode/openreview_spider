{"notes": [{"id": "Hye-p0VFPB", "original": "BJed0at_vr", "number": 1383, "cdate": 1569439417057, "ddate": null, "tcdate": 1569439417057, "tmdate": 1577168265606, "tddate": null, "forum": "Hye-p0VFPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RXHopZ3Yp", "original": null, "number": 1, "cdate": 1576798722061, "ddate": null, "tcdate": 1576798722061, "tmdate": 1576800914531, "tddate": null, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "invitation": "ICLR.cc/2020/Conference/Paper1383/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an energy-efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, I\u2019d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper. \n\n[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727002, "tmdate": 1576800279202, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1383/-/Decision"}}}, {"id": "HJxryQQy5r", "original": null, "number": 1, "cdate": 1571922652710, "ddate": null, "tcdate": 1571922652710, "tmdate": 1572972475838, "tddate": null, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "invitation": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to shorten the shift-addition operations in the straightforward configurable MACs (Sharma et al., 2018), to an addition-shift style. The authors claim that the new design is able to lower the energy consumption in the matrix multiplication. In the experimental analysis, the authors demonstrate the effectiveness of the proposed method.\n\nThis paper should be rejected since it proposes exactly the same architecture with the following published work:\nBitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation\n\nAlso, the authors do not provide a valid approach for the auto-selection of quantization bits, which is more significant in my opinion."}, "signatures": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575551612824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1383/Reviewers"], "noninvitees": [], "tcdate": 1570237738187, "tmdate": 1575551612840, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review"}}}, {"id": "SyeLKScX5H", "original": null, "number": 2, "cdate": 1572214141626, "ddate": null, "tcdate": 1572214141626, "tmdate": 1572972475793, "tddate": null, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "invitation": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a decomposable MAC unit for low-power computation for matrix-multiplication operation for neural networks.\n\nAlthough I believe I understood the idea of this technique, I strongly believe this paper should be submitted to an architecture of design automation conference instead of ICLR. I am also not in the position to assess the experiments, which were conducted with synthesis of Synopsis 28nm library. The paper discussed systolic array, MAC in detail, without too many algorithmic elements inside, therefore may be a paper not toward the audience of this conference."}, "signatures": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575551612824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1383/Reviewers"], "noninvitees": [], "tcdate": 1570237738187, "tmdate": 1575551612840, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review"}}}, {"id": "Skg4roPYcr", "original": null, "number": 3, "cdate": 1572596540191, "ddate": null, "tcdate": 1572596540191, "tmdate": 1572972475749, "tddate": null, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "invitation": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: \n\nThis paper proposes a novel decomposition strategy for matrix multiplication to aim for less energy consumption. They demonstrate that energy consumption is reduced on several CNNs. It is an interesting work but it is not sure that there is a difference in contribution when compared to previous work.\n\nThe main argument to impact to the score:\n\n    1. A similar idea is addressed in [1]. It is highly recommended to show the difference in contribution.\n    2. In Figure 7,  the performance and energy consumption of systolic arrays of decomposable MAC  is shown. However, the only energy consumption is shown in section 4.2 when the method is applied to DNNs. It is better to show both performance and energy consumption.\n    3. It is inappropriate to quantize VGG16, VGG19, DenseNet-121, DenseNet-169 and MobileNet based on CIFAR10 dataset since all models are designed for ImageNet dataset. It is appropriate to quantize these models on ImageNet dataset.\n\n\n[1]     Sungju Ryu, Hyungjun Kim, Wooseok Yi and Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC. 2019.\n\nMinor comments not to impact the score: \n    1. \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" has been accepted by ICLR 2014. It is better not to use arXiv preprint on citations unless there is a reason.\n    2. It is recommended to put citations on CIFAR-10 dataset."}, "signatures": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1383/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575551612824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1383/Reviewers"], "noninvitees": [], "tcdate": 1570237738187, "tmdate": 1575551612840, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1383/-/Official_Review"}}}, {"id": "HkgSF3sRKr", "original": null, "number": 1, "cdate": 1571892348615, "ddate": null, "tcdate": 1571892348615, "tmdate": 1571892482103, "tddate": null, "forum": "Hye-p0VFPB", "replyto": "Hye-p0VFPB", "invitation": "ICLR.cc/2020/Conference/Paper1383/-/Public_Comment", "content": {"title": "Overlap with previous work", "comment": "This paper proposes decomposable MAC and a systolic array-based accelerator for varible bitwidth of weights/activations in CNN. The core insight is that the partial results of a multiplication which have the same shift distance can be composed together before shifted so as to reduce area and energy consumption.\n\nHowever, exactly the same idea has been seen in previous work BitBlade [1], which was also an incremental work based on Bit Fusion [2]. The difference is that BitBlade adopted a 2D-mesh architecture instead of systolic array. \n\nI think the authors should cite BitBlade in the paper and reconsider the contributions. \n\nThanks.\n\n\n\n[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019\nhttps://dl.acm.org/citation.cfm?id=3317784\n\n[2] Hardik Sharma et al. Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. ISCA'2018"}, "signatures": ["~gang_li1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~gang_li1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nchuang@cs.nctu.edu.tw", "kulugu2.cs07g@nctu.edu.tw", "kcw@cs.nctu.edu.tw"], "title": "Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks", "authors": ["Ning-Chi Huang", "Huan-Jan Chou", "Kai-Chiang Wu"], "pdf": "/pdf/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "abstract": "Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "keywords": [], "paperhash": "huang|efficient_systolic_array_based_on_decomposable_mac_for_quantized_deep_neural_networks", "original_pdf": "/attachment/80a01c3414adae564e6acf3e5296346bdd07a3a1.pdf", "_bibtex": "@misc{\nhuang2020efficient,\ntitle={Efficient Systolic Array Based on Decomposable {\\{}MAC{\\}} for Quantized Deep Neural Networks},\nauthor={Ning-Chi Huang and Huan-Jan Chou and Kai-Chiang Wu},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye-p0VFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye-p0VFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504195520, "tmdate": 1576860574369, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1383/Authors", "ICLR.cc/2020/Conference/Paper1383/Reviewers", "ICLR.cc/2020/Conference/Paper1383/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1383/-/Public_Comment"}}}], "count": 6}