{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028565370, "tcdate": 1490028565370, "number": 1, "id": "Hy6MuY6se", "invitation": "ICLR.cc/2017/workshop/-/paper38/acceptance", "forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.  Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/f7c04962254ec90bbfcab65a807c868dabe9ccde.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028565959, "id": "ICLR.cc/2017/workshop/-/paper38/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028565959}}}, {"tddate": null, "tmdate": 1489595385123, "tcdate": 1489595385123, "number": 1, "id": "SJZ-nJDie", "invitation": "ICLR.cc/2017/workshop/-/paper38/public/comment", "forum": "Hk6dkJQFx", "replyto": "r1U87aQje", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Response to Reviewer2", "comment": "Thanks a lot for your comments and suggestions of our work. Actually, we have already updated the paper in this workshop submission to address your comments. Our responses are shown as follows (some of them are the same as our previous comment):\n\n1. About section 3.1 (section 2.1 in this version)\nWe have updated our writing. However, we still think aligning the distribution of training data is not just a side effect. It is the key way to achieve the purpose of BN which is to avoid the problem of vanishing gradients and help optimization. In original BN paper, the authors\u2019 motivation is to address the problem of \u201cinternal covariate shift\u201d, which means \u201cthe change in the distributions of layers\u2019 inputs\u201d. Thus, BN is proposed to \u201creduce internal covariate shift\u201d and make \u201cthe distribution of nonlinearity inputs more stable\u201d.\n\n(2) We also directly visualize the intermediate features with Inception-BN network instead of our BN features. The figure can be seen at this link (https://s30.postimg.org/fdamc2l1t/a2d_feature_tsne.png). Red circles are features of samples from training domain (Amazon) while blue ones are testing features (DSLR). It blends much more than that in Figure 1. This demonstrates the statistics of BN layer indeed contain the traits of the data domain. The features of intermediate features of CNN cannot be separated directly in terms of different domains.\n\n2. About section 3.3, section 4.3.1\nWe have revised section 3.3 to make it clearer and we have removed the previous section 4.3.1. \n\n3. About section 4.3.2 (section 5.3.3 in this version)\nWe have updated additional experimental results in the workshop submission. \n(1) We have experiments with smaller number of samples and found that the performance will drop more (e.g. 0.652 with 16 samples, 0.661 with 32 samples.) We have updated the results in the section \u201cSensitivity to target domain size\u201d.\n(2) In the section \u201cAdaptation Effect for Different BN Layers\u201d (section 5.3.4), we add the detailed analysis of adaptation effect for different BN layers of our AdaBN method.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.  Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/f7c04962254ec90bbfcab65a807c868dabe9ccde.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487232885837, "tcdate": 1487232885837, "id": "ICLR.cc/2017/workshop/-/paper38/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper38/reviewers"], "reply": {"forum": "Hk6dkJQFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487232885837}}}, {"tddate": null, "tmdate": 1489388366007, "tcdate": 1489388366007, "number": 2, "id": "r1U87aQje", "invitation": "ICLR.cc/2017/workshop/-/paper38/official/review", "forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "signatures": ["ICLR.cc/2017/workshop/paper38/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper38/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "I previously reviewed the paper and am attaching my review below.\n\nI still have concern regarding the incorrect arguments raised in the paper (such as the nature of matching the source/target domain distributions), and it seems that the authors simply reorganized the sections down to the appendix section. As a result I am keeping my original recommendation, but will not argue if the AC decides to accept the paper.\n\n*** Original review ***\nOverall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.\n\nDetailed comments:\n\nSection 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.\n\nSection 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)\n\nExperiments:\n\n- section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.\n\n- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.  Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/f7c04962254ec90bbfcab65a807c868dabe9ccde.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489388366807, "id": "ICLR.cc/2017/workshop/-/paper38/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper38/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper38/AnonReviewer1", "ICLR.cc/2017/workshop/paper38/AnonReviewer2"], "reply": {"forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper38/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper38/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489388366807}}}, {"tddate": null, "tmdate": 1489103792909, "tcdate": 1489103792909, "number": 1, "id": "HkKhiPysx", "invitation": "ICLR.cc/2017/workshop/-/paper38/official/review", "forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "signatures": ["ICLR.cc/2017/workshop/paper38/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper38/AnonReviewer1"], "content": {"title": "simple but effective domain adaptation approach", "rating": "7: Good paper, accept", "review": "(I previously reviewed this paper for the main conference.  I will copy most of my comments here, removing criticisms that have since been addressed.)\n\n\nPros:\n\nThe method is very simple and easy to understand and apply.\n\nThe experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.\n\nThe analysis in section 5.3.3 shows that only a small number of target domain samples are needed for adaptation of the network.\n\nGood results for remote sensing domain adaptation included in appendix.\n\n\nCons:\n\nThere is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  (The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me.)\n\n\nOverall, there\u2019s not much novelty here, but the paper includes sufficient experimentation and interesting analysis, and it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.  Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/f7c04962254ec90bbfcab65a807c868dabe9ccde.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489388366807, "id": "ICLR.cc/2017/workshop/-/paper38/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper38/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper38/AnonReviewer1", "ICLR.cc/2017/workshop/paper38/AnonReviewer2"], "reply": {"forum": "Hk6dkJQFx", "replyto": "Hk6dkJQFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper38/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper38/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489388366807}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487578636475, "tcdate": 1487232885121, "number": 38, "id": "Hk6dkJQFx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "Hk6dkJQFx", "original": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "content": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.  Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/f7c04962254ec90bbfcab65a807c868dabe9ccde.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1482816698663, "tcdate": 1478240992583, "number": 126, "id": "BJuysoFeg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "content": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}