{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573610126, "tcdate": 1521573610126, "number": 283, "cdate": 1521573609785, "id": "ByGe1Jk9z", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HytbCQG8z", "replyto": "HytbCQG8z", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning from Imperfect Demonstrations", "abstract": "Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm, Normalized Actor-Critic (NAC), that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data. NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.", "pdf": "/pdf/1cbe623f3942b3ffb3de2de934d8df09c729fcc6.pdf", "paperhash": "gao|reinforcement_learning_from_imperfect_demonstrations", "_bibtex": "@misc{\ngao2018reinforcement,\ntitle={Reinforcement Learning from Imperfect Demonstrations},\nauthor={Yang Gao and Huazhe(Harry) Xu and Ji Lin and Fisher Yu and Sergey Levine and Trevor Darrell},\nyear={2018},\nurl={https://openreview.net/forum?id=BJJ9bz-0-},\n}", "keywords": [], "authors": ["Yang Gao", "Huazhe(Harry) Xu", "Ji Lin", "Fisher Yu", "Sergey Levine", "Trevor Darrell"], "authorids": ["yg@eecs.berkeley.edu", "huazhe_xu@eecs.berkeley.edu", "lin-j14@mails.tsinghua.edu.cn", "fy@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730165519, "tcdate": 1517596160635, "number": 15, "cdate": 1517596160635, "id": "HytbCQG8z", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HytbCQG8z", "original": "BJJ9bz-0-", "signatures": ["~Yang_Gao1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Reinforcement Learning from Imperfect Demonstrations", "abstract": "Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm, Normalized Actor-Critic (NAC), that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data. NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.", "pdf": "/pdf/1cbe623f3942b3ffb3de2de934d8df09c729fcc6.pdf", "paperhash": "gao|reinforcement_learning_from_imperfect_demonstrations", "_bibtex": "@misc{\ngao2018reinforcement,\ntitle={Reinforcement Learning from Imperfect Demonstrations},\nauthor={Yang Gao and Huazhe(Harry) Xu and Ji Lin and Fisher Yu and Sergey Levine and Trevor Darrell},\nyear={2018},\nurl={https://openreview.net/forum?id=BJJ9bz-0-},\n}", "keywords": [], "authors": ["Yang Gao", "Huazhe(Harry) Xu", "Ji Lin", "Fisher Yu", "Sergey Levine", "Trevor Darrell"], "authorids": ["yg@eecs.berkeley.edu", "huazhe_xu@eecs.berkeley.edu", "lin-j14@mails.tsinghua.edu.cn", "fy@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730165519, "tcdate": 1509134727209, "number": 778, "cdate": 1518730165510, "id": "BJJ9bz-0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJJ9bz-0-", "original": "SkRKWz-Ab", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Reinforcement Learning from Imperfect Demonstrations", "abstract": "Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.", "pdf": "/pdf/472dfbe7059af65461c84e0e210c27f8245c9421.pdf", "paperhash": "gao|reinforcement_learning_from_imperfect_demonstrations", "_bibtex": "@misc{\ngao2018reinforcement,\ntitle={Reinforcement Learning from Imperfect Demonstrations},\nauthor={Yang Gao and Huazhe(Harry) Xu and Ji Lin and Fisher Yu and Sergey Levine and Trevor Darrell},\nyear={2018},\nurl={https://openreview.net/forum?id=BJJ9bz-0-},\n}", "authors": ["Yang Gao", "Huazhe(Harry) Xu", "Ji Lin", "Fisher Yu", "Sergey Levine", "Trevor Darrell"], "keywords": ["learning from demonstration", "reinforcement learning", "maximum entropy learning"], "authorids": ["yg@eecs.berkeley.edu", "huazhe_xu@eecs.berkeley.edu", "lin-j14@mails.tsinghua.edu.cn", "fy@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}