{"notes": [{"id": "xMnVDv3ZIm", "original": null, "number": 1, "cdate": 1615119697822, "ddate": null, "tcdate": 1615119697822, "tmdate": 1615266075292, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Comment", "content": {"title": "A new survey on domain generalization", "comment": "For readers who are interested in the topic of domain generalization (DG), we would like to share our recently released survey on DG at https://arxiv.org/abs/2103.02503.\n\n**Why writing this survey?**\n\nDG has undergone a decade progress since its first introduction in 2011, leading to a plethora of methodologies developed from different angles, as well as covering various applications like object recognition, medical image segmentation, person re-identification, action recognition, and so on. DG is of great importance to practical applications where out-of-distribution (OOD) generalization is key to successfully deploying machine learning systems.\n\nHowever, there is no such a survey paper to provide the research community with a clear picture on how DG has developed so far. We think it is time to write a paper to summarize the ten-year development in DG.\n\n**What you can learn from this survey?**\n\nThis survey basically answers the following questions:\n- What is the definition of DG?\n- Why do we study DG (*the motivation*)?\n- How does DG compare to related problems such as domain adaptation and transfer learning?\n- How is a DG method typically evaluated?\n- What are the common datasets used for benchmarking DG methods?\n- What methodologies have been developed to tackle DG? Can we categorize them?\n- What are missing in the current DG research? And how can we possibly address them to further this field?"}, "signatures": ["~Kaiyang_Zhou1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Kaiyang_Zhou1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6xHJ37MVxxp", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper214/Authors|ICLR.cc/2021/Conference/Paper214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649463312, "tmdate": 1610649463312, "id": "ICLR.cc/2021/Conference/Paper214/-/Comment"}}}, {"id": "6xHJ37MVxxp", "original": "RffVpO-bnAR5", "number": 214, "cdate": 1601308032417, "ddate": null, "tcdate": 1601308032417, "tmdate": 1614946594040, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "q2raPE_2e8e", "original": null, "number": 1, "cdate": 1610040481152, "ddate": null, "tcdate": 1610040481152, "tmdate": 1610474086143, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All three reviewers recommend acceptance after the rebuttal stage, and the AC found no reason to disagree with them. The proposed method is simple and effective, and the concerns raised about experimental validation and novelty seem well addressed in the rebuttal. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040481139, "tmdate": 1610474086127, "id": "ICLR.cc/2021/Conference/Paper214/-/Decision"}}}, {"id": "S94HuhmWRwh", "original": null, "number": 2, "cdate": 1603886133098, "ddate": null, "tcdate": 1603886133098, "tmdate": 1607357932804, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Review", "content": {"title": "Insufficient technical novelty and experimental validation", "review": "This work proposes a technique for domain generalization by mixing style of images from different domains. This work adopts a mix up style approach [A] for domain generalization. Different from [A], the paper proposes to conduct mix-up in the intermediate layers, in particular, instance normalization layers. The proposed approach diversifies the data implicitly and the experimental results show that the mix-style can improve domain generalization.\n\nOverall the paper is well-written with plenty of details. I also appreciate the experimental analysis in Sec 3.4 and the variance reported in Table 1. However, I have several concerns regarding the paper:\n- The technical novelty seems rather incremental. This method is an extension of [A] to the instance normalization layer. Similar strategies have been discussed in other works such [B] and [C]. However, these works are not discussed in terms of main similarities/differences.\n- I also found the experimental validation not fully sufficient to grant publication. Currently the validation is only conducted on PACS, the improvement also seems limited. I believe validation on more datasets(such as Digits, Office-Home as used in L2A-OT) can further confirm the effectiveness of the proposed method.\n- I suspect that  interpolating the style parameter might cause performance drop on the domains that have been seen during training. Would it be possible to report performance on the domains that have been seen in the training?\n\n[A] Vikas Verma et al. Manifold Mixup: Better Representations by Interpolating Hidden States. In ICML 2019.\n[B] Rui Gong et al. DLOW: Domain Flow for Adaptation and Generalization. In CVPR 2019.\n[C] Seonguk Seo. Learning to Optimize Domain Specific Normalization for Domain Generalization. In ECCV 2020.\n\n---\nI have read authors' response and other reviews. Some of my concerns are addressed in the response. Especially the added discussion with related work is helpful. Thus I would increase my rating to 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147985, "tmdate": 1606915795070, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Review"}}}, {"id": "ZBtzm7IqoV", "original": null, "number": 2, "cdate": 1605734818872, "ddate": null, "tcdate": 1605734818872, "tmdate": 1605735453357, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "g2YJDUhcz-b", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "**Q1. More analysis on where to apply MixStyle**\n\nThanks. We have now followed the suggestion and conducted the ablation study on the re-ID datasets in Table 3.  We also included more combinations as suggested. The notation has been improved (using \u201cres1\u201d to denote the 1st residual block, \u201cres12\u201d to denote both the 1st and 2nd residual blocks, and so forth). Indeed, where best to add MixStyle is application dependent to some extent, but the conclusion is similar: 2-3 blocks at the very bottom are the best place to add MixStyle and anywhere near the top (classification layer) must be avoided. Please see the updated discussion in the \u201cWhere to apply MixStyle\u201d part (in Sec.3.4) for details.\n\n**Q2. Clarification on Fig.4.**\n\nEach column corresponds to a residual block (from res1 to res4). The top shows the t-sne plot of feature maps, say from res1, while the bottom shows the t-sne plot of res1\u2019s style statistics (i.e. concatenation of mean and std). We use these two plots together to show that low- and mid-level features encode style information while high-level features focus more on semantics, and therefore, MixStyle should be applied to the low- and mid-level features.\n\nThe legend in Fig.4(d) is different from those in (a-c) because we want to show that, when we reach the top of the CNN (res4), the domain information is gone, and the clusters in (d) are related to class rather than domain. So adding MixStyle at res4 is not meaningful.\n\n**Q3. Discussion on why MixStyle w/ random shuffle is better on some domains.**\n\nGood point. The reason might be because there exist sub-domains in a source domain. For example in Fig. 1 and  Fig.4(a-c), it is clear that in each domain, there are distinct clusters in the style statistics distribution which correspond to sub-domains.  So these sub-domains, random shuffling could produce more diverse \u201cnew\u201d domains that lead to a more domain-generalizable model. This discussion has been added to Sec.3.1 in the last sentence in the \u201ccomparison with general-purpose\u201d part.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6xHJ37MVxxp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper214/Authors|ICLR.cc/2021/Conference/Paper214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873433, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment"}}}, {"id": "eaBLacKTvC_", "original": null, "number": 4, "cdate": 1605735287880, "ddate": null, "tcdate": 1605735287880, "tmdate": 1605735287880, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "--ls-mdu-U-", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "**Q1. Relation to Mixup and an additional baseline.**\n\nThanks. We have now reworded some of the claims to clarify the relation to Mixup. We have added the suggested baseline to Table 1 (i.e. Mixup w/o label interpolation, following Sohn et al. NeurIPS\u201920) and updated the discussion in the \u201ccomparison with general-purpose regularization methods\u201d part (in Sec.3.1).\n\nWe also include the main results below. Mixup w/o label interpolation is 1% worse than Mixup. The results seem to support our claim that mixing style statistics at the feature level via MixStyle is more useful for the DG problem.\n\nModel | Art | Cartoon | Photo | Sketch | Avg\n:--- | :---: | :---: | :---: | :---: | :---:\nResNet18 | 77.0 | 75.9 | 96.0 | 69.2 | 79.5\nMixup w/o label interpolation | 74.7 | 72.3 | 93.0 | 69.2 | 77.3\nMixup | 76.8 | 74.9 | 95.8 | 66.6 | 78.5\nMixStyle w/ random shuffle | 82.3 | 79.0 | 96.3 | 73.8 | 82.8\nMixStyle w/ domain label | 84.1 | 78.8 | 96.1 | 75.9 | 83.7\n\n**Q2. Notation in Table 3.**\n\nSorry for confusion. MixStyle was applied after a residual block rather than every convolution layer in that block. We have improved the notation in Table 3 by changing \u201cconv2/3/4/5_x\u201d to \u201cres1/2/3/4\u201d for clarity. \u201cres123\u201d (originally \u201cconv234_x\u201d) means MixStyle is applied after the 1st, 2nd and 3rd residual blocks.\n\n**Q4. Clarification on the setting for the re-ID experiments.**\n\nThanks for the suggestion. We have updated the text in Sec.3.2 (in the \u201cdataset and implementation details\u201d part) and the caption in Table 2 to highlight the cross-dataset setting.\n\n**Q5. Use of words.**\n\nThanks. We have removed \u201cclearly\u201d and \u201cslightly\u201d in those sentences to avoid overstatement.\n\n**Q6. MixStyle on SSL.**\n\nInteresting suggestion. MixStyle was specifically designed for the DG problem, but it may have potential in solving other problems as well. The experiments on the SSL task are currently beyond the scope of this paper. We will leave this investigation for future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6xHJ37MVxxp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper214/Authors|ICLR.cc/2021/Conference/Paper214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873433, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment"}}}, {"id": "DE3f18co2hV", "original": null, "number": 3, "cdate": 1605734956931, "ddate": null, "tcdate": 1605734956931, "tmdate": 1605735171666, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "S94HuhmWRwh", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "**Q1. Novelty.**\n\nWe would like to highlight that mixing style statistics at the feature level has never been introduced in the context of DG before, and we have demonstrated its effectiveness on a wide range of tasks in this paper. We also showed that MixStyle *largely* outperforms Mixup (Zhang et al. ICLR\u201918) and its extension ManifoldMixup (Verma et al. ICML\u201919) in the DG setting, as well as outperforming other general-purpose regularization methods.\n\nCompared with the two referenced works [B, C], MixStyle is novel. Specifically, [B] tackles domain adaptation by modeling intermediate domains between source and target via a domainness factor and learning a CycleGAN model for image translation. This is very different from mixing the style statistics at the feature level done in MixStyle, which is much simpler in implementation. Furthermore, compared with L2A-OT (Table 1), which shares a similar spirit with [B] in terms of generating novel images at the pixel level, MixStyle achieves highly comparable performance, despite consuming much less computing resources. Therefore, we think it is worth sharing our idea of MixStyle to the DG community and to encourage more research in the direction of exploiting feature-level style statistics. [C] learns domain-specific BN layers, so it has a much different motivation than that of MixStyle. We have now added the discussion on these two works in the related work section.\n\n**Q2. Experimental evaluation.**\n\nWe would like to emphasize that our main goal is to evaluate MixStyle on a wide range of domain generalization tasks beyond the traditional image classification problem. Due to the space constraint, we were not able to present more extensive results on each of the three tasks. The results on the three different applications (image classification on PACS, instance retrieval on the re-ID datasets and RL on Coinrun) have demonstrated that MixStyle achieves significant improvements over the baselines. It is noteworthy that none of the existing domain generalization methods has demonstrated effectiveness on these datasets jointly.\n\nAs requested, we have now conducted experiments on Digits and Office-Home, and have added the results to Table 8 and Table 9 respectively in the Appendix (A.2). The conclusion is similar to that on PACS: a) MixStyle brings clear improvements over the baseline CNN model; b) MixStyle outperforms all general-purpose regularization methods; c) MixStyle\u2019s performance is comparable to more sophisticated DG methods such as L2A-OT, despite being much simpler to train and consuming much less computing resources.\n\n**Q3. Report performance on seen domains.**\n\nWe have added these results in Table 7 in the Appendix (A.2). The results suggest that MixStyle does not sacrifice the performance on seen domains in exchange for gains on unseen domains. In fact, the source domains\u2019 recognition accuracy is also improved very slightly. This is because for the source domain, MixStyle can be considered as a conventional data augmentation strategy. It thus is also able to improve the model\u2019s generalization to test data, even when the domains are unchanged.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6xHJ37MVxxp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper214/Authors|ICLR.cc/2021/Conference/Paper214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873433, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Comment"}}}, {"id": "--ls-mdu-U-", "original": null, "number": 1, "cdate": 1603747743154, "ddate": null, "tcdate": 1603747743154, "tmdate": 1605024738901, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Review", "content": {"title": "Simple but effective contribution to domain generalization", "review": "**Summary:** The paper proposes a simple method for domain generalization where multiple source domains are given for a certain task (like image classification) and testing happens on an unseen domain. The authors are inspired by normalization-based style-transfer techniques (Adaptive InstanceNorm) and propose to mix the styles of different source domains to effectively increase diversity of domains during training.\n\n**Pros:**\n\n- Overall, this is a well written paper with a clear idea that is simple but intuitive.\n- The idea is well described, put into context of prior work and empirically validated to improve results over various baselines.\n- It is good to see experiments outside of plain image classification to validate the proposed idea.\n- The analysis where to apply MixStyle is good and makes intuitive sense.\n\n\n**Cons:**\n\n- The relation to MixUp needs to be explained in more details. While related to the proposed MixStyle, MixUp creates a convex combination of both input and output spaces. I can believe that MixUp as a standard data augmentation gives worse results than a vanilla CNN (Table 1) but I would not fully agree with the statement \"... which demonstrates the advantage of mixing style statistics at the feature level over mixing images at the pixel level\" from page 4. MixUp also interpolates the output label space, so the advantage cannot be only attributed the placement of the mixing within the network instead of at the pixel level.\n- As an additional baseline, one could use MixUp with a sampled lambda that is larger than 0.5 in all cases (like in [FixMatch. Sohn et al. NeurIPS'20]) but keeping the label from sample $x$ rather than interpolating with $\\hat{x}$.\n\n- I do not understand why the suffix \"_x\" is added to the analysis in Table 3. Is MixStyle applied after each convolutional layer or after each block in a ResNet architecture? Specifically, for \"conv234_x\", how often is the MixStyle layer added? (3 times or 3 * num_convs_in_block times?)\n\n- For the ReID experiments, I think it should be better highlighted that the cross-dataset setup is the key difference to evaluations in prior work. This somehow gets almost unnoticed because the default setting of ReID is already considered a valid domain generalization task due to the new label space and camera views. This left me a bit confused about how RandomErase can be a widely used data augmentation technique for ReID when it gives worse results in the experiments from Table 2. This became clear to me only after reading the discussion in the last paragraph of Section 3.2.\n\n- I would not make the statement that \"... mixing is CLEARLY better than replacing\" on page 7 (see Table 4) while also stating that \"... with alpha increasing from 0.1 to 0.4, the accuracy SLIGHTLY slides from 82.8% to 81.7%\". That \"slight\" change is larger than the \"clear\" gap before.\n\n\n**Other notes and open questions:**\n- MixUp was used successfully as regularization for semi-supervised learning (SSL) [MixMatch. Berthelot et al. NIPS'19]. Can MixStyle also be used for SSL?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147985, "tmdate": 1606915795070, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Review"}}}, {"id": "g2YJDUhcz-b", "original": null, "number": 3, "cdate": 1604121263406, "ddate": null, "tcdate": 1604121263406, "tmdate": 1605024738778, "tddate": null, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "invitation": "ICLR.cc/2021/Conference/Paper214/-/Official_Review", "content": {"title": "Review for \"domain generalization with MixStyle\" ", "review": "** Paper Summary **\n\nThis paper proposed a simple regularization technique for domain generalization tasks, termed MixStyle, based on the observation that domains are determined by image styles. By mixing styles of different instances, which generates synthesized domain samples while preserving the content features, the proposed method achieves the generalizability of the trained model. The MixStyle was applied to numerous applications, such as category classification, instance retrieval, and reinforcement learning, and attained the state-of-the arts. The MixStyle is relatively simple to implement, but effective. \n  \n** Paper Strength **\n+ Simple methodological design, so it is easy to implement.\n+ Understanding the domain shift problems as a style variation makes sense.\n+ Randomizing the styles might be the solution to alleviate the domain generalization problems, but searching all the possible styles and applying them would be challenging and not feasible. So, using different instance samples to extract the styles was nice.\n+ It makes sense that introducing the \\lambda to mix the styles itself and ones of different instances.\n+ The paper is well organized and written.\n\n** Paper Weakness **\n\nI have no major comments on this paper, but minor comments as follows:\n- Even though the authors have shown the ablation study to analyze the levels where the MixStyle should be applied, it is not clear for me yet. The authors applied the MixStyle after 1st, 2nd, and 3rd residual blocks for category classification problems, but applied the MixStyle after 1st and 2nd residual blocks for category classification problems for instance retrieval task. In 3.4 analysis, they only showed the ablation studies on the category classification. Thus, one think the optimal combinations may vary according to the applications. In addition, another combination, e.g., conv34, conv25, would be more interesting.\n- Fig 4 is hard to understand; what do the corresponding style statistics mean? Why does (d) only represent different legends? \n- In Table 1, some experimental settings, e.g., Cartoon or Photo, have shown that MixStyle w/ random shuffle was better? The discussion on this might be interesting. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper214/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper214/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Generalization with MixStyle", "authorids": ["~Kaiyang_Zhou1", "~Yongxin_Yang1", "~Yu_Qiao1", "~Tao_Xiang1"], "authors": ["Kaiyang Zhou", "Yongxin Yang", "Yu Qiao", "Tao Xiang"], "keywords": ["Domain Generalization", "Style Mixing"], "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "one-sentence_summary": "MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.", "pdf": "/pdf/e3135e8b22333ad40de9d8f855c3fb4253a2090b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|domain_generalization_with_mixstyle", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021domain,\ntitle={Domain Generalization with MixStyle},\nauthor={Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6xHJ37MVxxp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6xHJ37MVxxp", "replyto": "6xHJ37MVxxp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147985, "tmdate": 1606915795070, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper214/-/Official_Review"}}}], "count": 9}