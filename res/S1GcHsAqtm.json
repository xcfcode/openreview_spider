{"notes": [{"id": "S1GcHsAqtm", "original": "Hylz7FIYY7", "number": 112, "cdate": 1538087746141, "ddate": null, "tcdate": 1538087746141, "tmdate": 1545355419644, "tddate": null, "forum": "S1GcHsAqtm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skleryyz1V", "original": null, "number": 1, "cdate": 1543790392297, "ddate": null, "tcdate": 1543790392297, "tmdate": 1545354495519, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Meta_Review", "content": {"metareview": "The area chair agrees with the authors and the reviewers that the topic of this work is relevant and important. The area chair however shares the concerns of the reviewers about the setup and the empirical evaluation:\n- Having one model that can be pruned to varying sizes at run-time is convenient, but in practice it is likely to be OK to do the pruning at training time. In light of this, the empirical results are not so impressive.\n- Without quantization, distillation and fused ops, the value of the empirical results seems questionable as these are important and well-known techniques that are often used in practice. A more thorough evaluation that includes these techniques would make the paper much stronger.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper112/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353332802, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper112/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353332802}}}, {"id": "rkgJIIoPn7", "original": null, "number": 1, "cdate": 1541023303373, "ddate": null, "tcdate": 1541023303373, "tmdate": 1543955373435, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "content": {"title": "Torn About This Work", "review": "In this paper, the authors investigate the accuracy-efficiency tradeoff for neural language models. In particular, they explore how different compression strategies impact the accuracy (and flops), and more interestingly, also how it impacts the power use for a RaspberryPi. The authors consider the QRNNs and SRUs for this purpose and use standard datasets for their analysis. I am torn about this paper. On one hand, I feel that the analysis is interesting, thoughtful and detailed; the power usage statistics bring a different perspective to the compression community. The section on inference time pruning was especially interesting to read. On the other hand however, there is limited novelty in the setup. The authors use standard, well known, compression algorithms on common neural language modeling architectures and datasets and use out-of-the-box tools for their ultimate analysis. Further, the paper needs additional work before it can be accepted in my opinion. I detail my arguments below:\n\n- The authors begin by discussing SwiftKey and similar apps but I'm not sure if its clear that they use neural language modeling as the backend. Do the authors have a source to validate this claim?\n- Knowledge distillation is another algorithm that has been found to be quite competitive in compressing models into smaller versions of themselves. Have the authors experimented with that? \n- The R@3 is an good metric but I suggest that the authors look at mean reciprocal rank (MRR) instead. This removes the arbitrary-ness of \"3\" while ensuring that the metric of interest is the accuracy and not probability of being correct (perplexity). \n- Can you comment on the sensitivity of the results to the RPi frameworks? For instance, the RPi deployment tools, architecture, and variance in the predictions? \n- Along the same line, I'm curious how generalizable the RPi results are for other computing architectures. For those of us who are not experts on hardware, it would be nice to read about whether similar tradeoffs will exist in other architectures such as mobile phones, GPUs or CPUs. \n- Could the authors add some meta-analysis about the results? If the perplexity goes up as a consequence of compression, what kinds of tokens it that localized to? Is it primarily rare words that the model is less confident about, or are the probabilities for most words getting skewed?\n- Finally, I feel that such an exploration will catch on only if the tools are open-sourced and made easy to replicate/use. If there were a blog or article summarizing the steps needed to replicate the power measurement (including sources from where to buy the necessary hardware), more people would be inclined on adding such an analysis to future neural language modeling work. \n\nI am willing to revisit my rating, as necessary, once I read through the rebuttal. \n\n\nUPDATE:\n\nAfter reading the rebuttal, I am increasing my score to 6. The authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "cdate": 1542234535138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335651988, "tmdate": 1552335651988, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygJ8-PKhX", "original": null, "number": 2, "cdate": 1541136711079, "ddate": null, "tcdate": 1541136711079, "tmdate": 1543558402550, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "content": {"title": "Nice work but I think another baseline is needed.", "review": "This paper proposes to evaluate the accuracy-efficiency trade off in QRNN language model though pruning the filters using four different methods. During evaluation, it uses energy consumption on a Raspberry Pi as an efficiency metric. Directly dropping filters make the accuracy of the models worse. Then the paper proposes single-rank update(SRU) method that uses negligible amount of parameters to recover some perplexity. I like this paper focuses on model's performance on real world machines.\n\n1) The proposed approaches just work for QRNN, but not for many other neural language models such as LSTM, vanilla RNN language models, the title could be misleading. \n\n2) In the experiment section, I think one baseline is needed for comparison: the QRNN language model with a smaller number of filters trained from scratch. With this baseline, we can see if the large number of filters are needed even before pruning.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "cdate": 1542234535138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335651988, "tmdate": 1552335651988, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xKdMa_A7", "original": null, "number": 4, "cdate": 1543193200936, "ddate": null, "tcdate": 1543193200936, "tmdate": 1543193200936, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "HylJTQ3b6X", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "content": {"title": "Re: Recent related work ", "comment": "Thanks for sharing this paper with us!"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612412, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1GcHsAqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper112/Authors|ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612412}}}, {"id": "Hkxkrb6_07", "original": null, "number": 3, "cdate": 1543192886681, "ddate": null, "tcdate": 1543192886681, "tmdate": 1543192886681, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "Byeurdx0hm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "content": {"title": "Re: Much-needed exploration of efficiency tradeoffs in neural language model deployment", "comment": "We thank the reviewer for the helpful comments:\n\n- The QRNN architecture contains two components: convolutions alternate with a recurrent pooling operation. The fact that the authors report using a PyTorch QRNN implementation (which runs on the Arm architecture but doesn't contain a fused recurrent pooling kernel for any hardware other than NVIDIA GPUs) makes me afraid that they used a non-fused, op-by-op, approach for the pooling step [...]\n\nWe acknowledge that this is a limitation of our study -- a fused kernel would indeed provide performance gains. Nevertheless, we successfully capture the trends in the accuracy-efficiency tradeoffs of different inference-time pruning methods. To make this limitation aware to readers and practitioners, we have included this as part of a \"study limitations\" subsection in the revision (see last subsection in the discussion).\n\n- Although the engineering effort would be much higher, it's worth considering block-sparse weight matrices (as described in Narang et al. (Baidu) and Gray et al. (OpenAI)). While this remains an underexplored area, it's conceivable that block-sparse kernels (which should be efficient on Arm NEON with block sizes as low as 4x4 or so) and blockwise pruning could give more than a 50% speedup in convolution/matmul efficiency.\n- In a real-world application, you would probably also want to explore quantization and distillation approaches to see if they have additional efficiency gains. Overall results of 10x or more wall clock time reduction with <5% loss in accuracy are typical for domains that have seen more optimization for mobile deployment (especially mobile-optimized CNNs like MobileNet), so I think that's entirely possible for your application.\n\nOther compression approaches are quite interesting. We have mentioned vector quantization and distillation as potential future work in the revised conclusion.\n\nWe thank the reviewer again for the insightful feedback.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612412, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1GcHsAqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper112/Authors|ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612412}}}, {"id": "HkgAwXOvAX", "original": null, "number": 2, "cdate": 1543107429717, "ddate": null, "tcdate": 1543107429717, "tmdate": 1543107429717, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "BygJ8-PKhX", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "content": {"title": "Re: Nice work but I think another baseline is needed. ", "comment": "We thank the reviewer for the feedback and interest in our work. We would like to address your comments:\n\n1) The proposed approaches just work for QRNN, but not for many other neural language models such as LSTM, vanilla RNN language models, the title could be misleading. \n\nWe believe the title correctly reflects the work, since we perform adaptive pruning of NLMs on mobile devices. In addition to being efficient to train and deploy, QRNNs represent state of the art in NLM, with comprehensive analysis done in the literature (see https://arxiv.org/pdf/1803.08240.pdf).\n\n2) In the experiment section, I think one baseline is needed for comparison: the QRNN language model with a smaller number of filters trained from scratch. With this baseline, we can see if the large number of filters are needed even before pruning.\n\nWe agree; we have added the requested baselines to the results tables in the revision. We find that the smaller models indeed perform worse than the original models do.\n\nWe thank the reviewer again for the insightful feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612412, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1GcHsAqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper112/Authors|ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612412}}}, {"id": "BkxxgxuPAm", "original": null, "number": 1, "cdate": 1543106536133, "ddate": null, "tcdate": 1543106536133, "tmdate": 1543106536133, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "rkgJIIoPn7", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "content": {"title": "Re: Torn About This Work", "comment": "We would like to thank the reviewer for their thorough feedback and interest in our work. We have taken the time to address your comments in the revision:\n\n- The authors begin by discussing SwiftKey and similar apps but I'm not sure if its clear that they use neural language modeling as the backend. Do the authors have a source to validate this claim?\n\nYes, the official blog contains a post about using neural networks: https://blog.swiftkey.com/swiftkey-debuts-worlds-first-smartphone-keyboard-powered-by-neural-networks/. We have added this link as a footnote in the introduction.\n\n- Knowledge distillation is another algorithm that has been found to be quite competitive in compressing models into smaller versions of themselves. Have the authors experimented with that? \n\nKnowledge distillation is indeed interesting; it could be a potential extension of this work. We have mentioned it as future work in the conclusion.\n\n- The R@3 is an good metric but I suggest that the authors look at mean reciprocal rank (MRR) instead. This removes the arbitrary-ness of \"3\" while ensuring that the metric of interest is the accuracy and not probability of being correct (perplexity). \n\nOur justification for using R@3 is that most mobile keyboards provide three side-by-side predictions, since screen space is quite limited. We have added this reason to the revision.\n\n- Can you comment on the sensitivity of the results to the RPi frameworks? For instance, the RPi deployment tools, architecture, and variance in the predictions? \n- Along the same line, I'm curious how generalizable the RPi results are for other computing architectures. For those of us who are not experts on hardware, it would be nice to read about whether similar tradeoffs will exist in other architectures such as mobile phones, GPUs or CPUs. \n\nIt would be interesting to extend this work to GPUs, whose operating characteristics are quite different from those of CPUs. For example, host-device data transfer and parallelism are much bigger issues on GPUs. An evaluation on GPUs is important due to the usage of NLMs in large-scale vocabulary speech recognition. We have added this in the conclusion and future work section of the revision.\n\n- Could the authors add some meta-analysis about the results? If the perplexity goes up as a consequence of compression, what kinds of tokens it that localized to? Is it primarily rare words that the model is less confident about, or are the probabilities for most words getting skewed?\n\nWe have added a meta-analysis to the revision (see last subsection in the revised discussion). We find that the pruned model is less confident about rare words, and we hypothesize that the effects are due to common words being easier to model.\n\n- Finally, I feel that such an exploration will catch on only if the tools are open-sourced and made easy to replicate/use. If there were a blog or article summarizing the steps needed to replicate the power measurement (including sources from where to buy the necessary hardware), more people would be inclined on adding such an analysis to future neural language modeling work. \n\nWe agree and plan to release our power measurement software upon the unblinding of this paper.\n\nWe thank the reviewer again for the insightful comments and feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612412, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1GcHsAqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper112/Authors|ICLR.cc/2019/Conference/Paper112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612412}}}, {"id": "HylJTQ3b6X", "original": null, "number": 1, "cdate": 1541682103055, "ddate": null, "tcdate": 1541682103055, "tmdate": 1541682103055, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Public_Comment", "content": {"comment": "Hey,  I agree nlm is powerful but sometimes too slow. You may be interested in a recent work also on language model pruning (as a contextualized representation model instead of a nlm).\n\nLiyuan Liu, et, al.  \"Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling\", EMNLP'18\n\nThanks", "title": "Recent related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311915972, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1GcHsAqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper112/Authors", "ICLR.cc/2019/Conference/Paper112/Reviewers", "ICLR.cc/2019/Conference/Paper112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311915972}}}, {"id": "Byeurdx0hm", "original": null, "number": 3, "cdate": 1541437504359, "ddate": null, "tcdate": 1541437504359, "tmdate": 1541534272044, "tddate": null, "forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "content": {"title": "Much-needed exploration of efficiency tradeoffs in neural language model deployment", "review": "This paper presents an investigation of perplexity-efficiency tradeoffs in deploying a QRNN neural language model to mobile devices, exploring several kinds of weight pruning for memory and compute savings. While their primary effort to evaluate pruning options and compare points along the resulting tradeoff curves doesn't result in a model that would be small and fast enough to serve, the authors also introduce a clever method (single-rank weight updates) that recovers significant perplexity after pruning.\n\nBut there are many other things the authors could have tried that might have given significantly better results, or significantly improved the results they did get (the top-line 40% savings for 17% perplexity increase seems fairly weak to me). In particular:\n\n- The QRNN architecture contains two components: convolutions alternate with a recurrent pooling operation. The fact that the authors report using a PyTorch QRNN implementation (which runs on the Arm architecture but doesn't contain a fused recurrent pooling kernel for any hardware other than NVIDIA GPUs) makes me afraid that they used a non-fused, op-by-op, approach for the pooling step, which would leave potentially 10 or 20 percentage points of free performance on the table. The QRNN architecture is designed for a situation where you already have optimized matrix multiply/convolution kernels, but where you're willing to write a simple kernel for the pooling step yourself; at the end of the day, pooling represents a tiny fraction of the QRNN's FLOPs and does not need to take more than 1 or 2 percent of total runtime on any hardware. (If you demonstrate that your implementation doesn't spend a significant amount of time on pooling, I'm happy to bump up my rating; I think this is a central point that's critical to motivating QRNN use and deployment).\n\n- Once pooling is reduced to <2% of runtime, improvements in the convolution/matmul efficiency will have increased effect on overall performance. Perhaps your pruning mechanisms improved matmul efficiency by 50%, but the fact that you're spending more time on pooling than you need to has effectively reduced that to 40%.\n\n- Although the engineering effort would be much higher, it's worth considering block-sparse weight matrices (as described in Narang et al. (Baidu) and Gray et al. (OpenAI)). While this remains an underexplored area, it's conceivable that block-sparse kernels (which should be efficient on Arm NEON with block sizes as low as 4x4 or so) and blockwise pruning could give more than a 50% speedup in convolution/matmul efficiency.\n\n- In a real-world application, you would probably also want to explore quantization and distillation approaches to see if they have additional efficiency gains. Overall results of 10x or more wall clock time reduction with <5% loss in accuracy are typical for domains that have seen more optimization for mobile deployment (especially mobile-optimized CNNs like MobileNet), so I think that's entirely possible for your application.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper112/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.", "keywords": ["Inference-time pruning", "Neural Language Models"], "authorids": ["r33tang@uwaterloo.ca", "jimmylin@uwaterloo.ca"], "authors": ["Raphael Tang", "Jimmy Lin"], "pdf": "/pdf/a713b9c004963c36d77ba4d80f0444ad0de3a461.pdf", "paperhash": "tang|adaptive_pruning_of_neural_language_models_for_mobile_devices", "_bibtex": "@misc{\ntang2019adaptive,\ntitle={Adaptive Pruning of Neural Language Models for Mobile Devices},\nauthor={Raphael Tang and Jimmy Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1GcHsAqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper112/Official_Review", "cdate": 1542234535138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1GcHsAqtm", "replyto": "S1GcHsAqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335651988, "tmdate": 1552335651988, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}