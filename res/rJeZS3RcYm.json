{"notes": [{"id": "rJeZS3RcYm", "original": "SkgPDdact7", "number": 1514, "cdate": 1538087992716, "ddate": null, "tcdate": 1538087992716, "tmdate": 1545355427452, "tddate": null, "forum": "rJeZS3RcYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1l1Pn-VlN", "original": null, "number": 1, "cdate": 1544981591138, "ddate": null, "tcdate": 1544981591138, "tmdate": 1545354488530, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Meta_Review", "content": {"metareview": "The paper considers a procedure for the generation of adversarial examples under a black box setting. The authors claim simplicity as one of the main selling points, with which reviewers agreed, while also noting that the results were impressive or \"promising\". There were concerns over novelty and some confusion over the contribution compared to Guo et al, which I believe has been clarified.\n\nThe highest confidence reviewer (AnonReviewer2), a researcher with significant expertise in adversarial examples, raised issues of inconsistent threat models (and therefore unfair comparisons regarding query efficiency), missing baselines. A misunderstanding about comparison against a concurrent submission to ICLR 2019 was resolved on the basis that the relevant results are mentioned but not originally presented in the concurrent submission. \n\nWhile I disagree with AnonReviewer2 that results on attacking a particular image from previous work (when run against the Google Cloud Vision API) would be informative, the reviewer has remaining unaddressed concerns about the fairness of comparison (comparing against results reported in previous work rather than re-run in the same setting), and rightly points out that as many variables should be controlled for as possible when making comparisons. Running all methods under the same experimental setting with the same *collection* of query images is therefore appropriate. \n\nThe authors have not responded to AnonReviewer2's updated post-rebuttal review, and with the remaining sticking point of fairness of comparison with respect to query efficiency I must recommend rejection at this point in time, while noting that all reviewers considered the method promising; I thus would expect to see the method successfully published in the near future once issues of the experimental protocol have been solidified.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "A method that is appealing for its simplicity, but reviewer concerns regarding fairness of comparison persist."}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1514/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352810546, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352810546}}}, {"id": "SkesQhRO2Q", "original": null, "number": 1, "cdate": 1541102626573, "ddate": null, "tcdate": 1541102626573, "tmdate": 1543288110048, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "content": {"title": "interesting black-box adversarial attack using DCT basis; performance evaluation on targeted attack is insufficient and threat model is inconsistent (unfair comparison)", "review": "This paper proposed a simple query-efficient \"score-based\" black-box attack based on iteratively perturbing an input image with a direction randomly sampled (w/o replacement) from a set of orthonormal bases. In particular, the authors proposed the use of low-frequency parts of DCT (discrete cosine transformation) as in (Guo 2018) to perform this task. Experimental results on ImageNet and three different classification models demonstrate the query efficiency of the proposed method -- able to achieve high attack success rate within fewer query budgets, where the visual distortion has an L2 norm threshold set to be 10. The authors also demonstrate an untargeted score-based black-box attack on Google CloudVision API.\n\nWhile the results seem promising, there are several issues that may potentially weaken the query-efficient claims made in this paper, especially due to the lack of sufficient attack comparisons (on smaller datasets) and inconsistent threat models when compared to existing works. My main concerns are summarized as follows.\n\n1. Unfair comparison due to inconsistent threat models (knowledge known to an attacker): the proposed method (simBA) is a \"score-based\" black-box attack, not a \"decision-based\" black-box attack. The proposed method assumes knowing the prediction likelihood (or prediction score) as the model output when performing black-box attacks, whereas the compared methods in black-box settings, such as Opt Attack and Boundary Attack, are \"decision-based\" attack that assumes only knowing the top-1 prediction label. Therefore, the query count comparison is meaningless and unfair, since these two methods require far less information from the model.\n\nOn the other hand, ZOO/AutoZOOM is a score-based attack. But ZOO can achieve a very low L2 distortion due to its coordinate descent nature. A fair comparison is to set the same L2 distortion for all score-based methods, and compare the median/avg query counts of each image to reach the same L2 distortion. The comparison to Opt-Attack / Boundary attack makes sense only if the proposed method (simBA) can also perform decision-based attack. Nonetheless, the query count to same-distortion comparison argument still holds. The authors should specify whether simBA can apply to the decision-only attack scenario. If so, how to implement and what is the performance?\n\nLastly, the QL attack (Ilyas 2018) can perform both score-based and decision-based attacks. So the authors should make the query comparison (to same L2 distortion) as well. According to a recent report (Table-1, NES column) in https://arxiv.org/pdf/1807.07978.pdf, the QL-attack has a comparable performance in terms of query counts as reported in this paper.\n\n2. More experiments on targeted black-box attacks: While untargeted attacks on Imagenet is a relatively easy task, I was a bit skeptical on the attacking performance of simBA in targeted attacks - since the selection of low-frequency bases directly limits the search space of adversarial examples, as opposed to arbitrary random directions adopted in QL-attack, Boundary-attack, and Opt-attack. It is also not clear how the target label is chosen in the targeted attack experiment.\nI suggest including two more experiments to validate the function of simBA: (i) compare the performance of least-likely targeted attack (ii) show results on smaller datasets such as Cifar-10. As pointed out by the authors, Imagenet has too many image dimensions and make it more vulnerable to attack. Showing attacking results on smaller datasets can properly justify the value of the proposed attack, rather than the benefit from high dimensionality.\n\n\n3. Novelty relative to LFBA (Guo) should be better differentiated: The idea of using DCT is originated from the LFBA paper. Since in that paper the authors also leveraged low-frequency DCT to perform black-box attacks, it is not clear to me what makes the proposed method perform better than the LFBA paper. The novelty and difference between this paper and the LFBA paper should be addressed.\n\n4. The Google Cloud Vision API attack is not too appealing - the tree label is still there and the trees are obviously present in the picture, while I appreciate the effect of removing the original top-3 labels. Can the authors show another set of non-trivial (more surprising) and targeted-attack experiments? Or simply do the same experiment using the same image (men snowing -> dog)  as in the QL-attack.\n\n----\nPost-rebuttal review\n\nI appreciate the authors' efforts in clarifying some of my concerns. However, I am still not convinced the comparison has been made fair. Many numbers from Table 1, such as ZOO, Opt-attack, QL-attack and AutoZOOM seem to be directly adapted from the papers rather than implemented and reproduced based on the same setting as the proposed attack. In particular, given that QL-attack is a published work, one of the state-of-the-art method and its codes has been released, I would really love to see a direct comparison using the same data samples and threat model. I would also like to emphasize that implementing all attacks under the same setting is crucial, since different attack methods may have a different criterion to determine attack successfulness. For example, QL-attack has some pre-defined distortion (L2 or Linfinity) for determining an adversarial example is successful, in addition to a different predicted class.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "cdate": 1542234213548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962695, "tmdate": 1552335962695, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJeaN4LihX", "original": null, "number": 2, "cdate": 1541264436568, "ddate": null, "tcdate": 1541264436568, "tmdate": 1542893135365, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "content": {"title": "simple and effective blackbox attack based on random directions", "review": "This paper presents a simple and effective black box adversarial attack on sota deep nets for image classification tasks. It is based on randomly picking a low frequency component of the DC Transform.  It is claimed to be most efficient when compared to the sota methods in terms of number of queries required for the attack. It is shown that a median of 600 queries for resnet-50 for imagenet dataset, and 2500 for google cloud vision. Due to its simplicity, it is also claimed that the attack is quite simple to implement in code. The paper presents a detailed analysis of their attack in pixel and DCT space, targeted vs untargeted attack, comparison over different architecture such as Densenet, resnet, and inception.\n\nThough the work is quite important and presents a simple and effective baseline black box attack. My concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of Guo etal 2018, which this paper says is the motivation behind their work. So, it is not clear what is the contribution of this paper, as a similar study seems to have been carried out in that paper as well. The authors do not clearly give the relative comparison wrt Guo etal 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "cdate": 1542234213548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962695, "tmdate": 1552335962695, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeUWhL3TX", "original": null, "number": 7, "cdate": 1542380542061, "ddate": null, "tcdate": 1542380542061, "tmdate": 1542380556983, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "ryxurJRsTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Re: Clarification on NES attack performance (aka QL attack)", "comment": "We understand that the NES attack is the same as the QL-attack evaluated in our paper. We would like to point out that the evaluation in Ilyas et al. 2018 (https://arxiv.org/pdf/1807.07978.pdf) does not compromise our claim of unprecedented efficiency. Although QL-attack can achieve average query count close to that of our method, its failure rate is extremely high -- 41.7% -- whereas SimBA and SimBA-DCT achieve failure rate of <2% within around the same number of queries."}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "ryxurJRsTX", "original": null, "number": 6, "cdate": 1542344512246, "ddate": null, "tcdate": 1542344512246, "tmdate": 1542344512246, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "Bklu_v6spQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Clarification on NES attack performance (aka QL attack)", "comment": "I appreciate the authors' prompt response. But the authors apparently misunderstood my comment 1. I am NOT asking to compare with the new method proposed in an unpublished work (Ilyas 2018 - https://arxiv.org/pdf/1807.07978.pdf). Rather, I am asking the performance comparison with NES attack (it is called QL attack in this paper), which is a published paper at ICML 2018 (https://arxiv.org/pdf/1804.08598.pdf). I did explicitly point out the \"NES column\" in the table of the unpublished work because it's basically the same setting to be compared with the results presented in this paper. I am very surprised to see the response that \"we think it is generally inappropriate to ask for a comparison with a concurrent submission to the same conference\", as QL attack was already compared (but in a different setting) in this paper.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "HygHF_6spX", "original": null, "number": 4, "cdate": 1542342781106, "ddate": null, "tcdate": 1542342781106, "tmdate": 1542342781106, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Revision", "comment": "We have uploaded a revision with the following changes:\n\n- Significantly improved attack when using the standard basis (SimBA). In particular, Table 1 and Figures 2-4 were updated.\n- Supplementary material containing 10 additional sample images for Google Cloud Vision attack."}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "Bklu_v6spQ", "original": null, "number": 3, "cdate": 1542342512240, "ddate": null, "tcdate": 1542342512240, "tmdate": 1542342512240, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "SkesQhRO2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Re: interesting black-box adversarial attack using DCT basis; performance evaluation on targeted attack is insufficient and threat model is inconsistent (unfair comparison)", "comment": "Our detailed response is below. We are happy to include the additional results and comparisons that you ask for, however we do want to emphasize that we think it is generally inappropriate to ask for a comparison with a concurrent submission to the same conference [Ilyas et al. 2018]. \n\nDetailed response:\nWe believe there may have been several possible misunderstandings that resulted in R2\u2019s concerns.\n\n1. We agree that the threat model varies across the various baselines. However, our inclusion of boundary attack and Opt attack are not meant to diminish their results, but rather to be comprehensive in our evaluation of prior work. As for other score-based attacks, although it is true that ZOO can sometimes achieve a very low L2 distortion, it also suffers from an order of magnitude higher failure rate (11.1% rather than <2% of SimBA) and requires several orders of magnitude more queries (192,000!!! rather than 1,232 for SimBA-DCT or 1,665 SimBA). In the preprint https://arxiv.org/pdf/1807.07978.pdf, the QL-attack is comparable to our attack in terms of query efficiency but at a high failure rate of 41.7%!! We believe that our evaluation is as fair to the other baselines as possible by achieving a lower L2 distortion than all methods other than ZOO and maintaining a success rate close to 100%, while requiring far fewer queries. \n\n2. We performed targeted attack against random classes, similar to Tu et al. 2018 (https://arxiv.org/abs/1805.11770) and Cheng et al. 2018 (https://arxiv.org/pdf/1807.04457.pdf). We tested our attack against the least likely class as well and found that our method is less efficient but remains very competitive. More precisely, the average query count for SimBA increases from 7,899 to 12,256, while the average query count for SimBA-DCT increases from 9,275 to 17,272.\n\nWe also tested our method on CIFAR-10 (targeted attack against the least likely class) and found that restricting to the low frequency basis does not affect the attack\u2019s efficiency. SimBA achieves an average query count of 522 with average L2 norm = 1.41, while SimBA-DCT achieves an average query count of 606 with average L2 norm = 1.60. Both attacks are successful 100% of the time and are very competitive with state-of-the-art attack algorithms such as AutoZOOM. While using low frequency perturbations does not improve the attack for CIFAR-10, it does not hinder the attack\u2019s efficiency either. As for the comment regarding limiting search space, Guo et al. have found that restricting to the low frequency subspace does not hinder adversarial optimality, which is empirically demonstrated in both theirs and our work.\n\n3. The basic SimBA attack uses axis-aligned directions rather than the DCT basis when picking random directions of descent, and provides the majority of the query efficiency compared to other methods (see Table 1). The choice of the DCT basis further improves our attack in the untargeted case and demonstrates that it can generalize to other orthonormal bases, but is not crucial. In this regard, our paper differs significantly from the work of Guo et al.\n\n4. Since our attack does not begin with an image of the adversarial class, it is not designed for targeted attacks on GCV. However, as GCV is the most widely used real world image classification platform for black-box adversarial attacks, we would like to demonstrate the efficacy of our method despite this limitation. Thus, we chose removing the top 3 original classes as a reasonable objective. In comparison with the QL-attack, the query efficiency of our method allows substantially more adversarial images to be created within the same cost budget, and our work is the first to show aggregate statistics for attacking a deployed machine learning service. We included 10 additional random samples in the Supplementary Material. For a non-trivial example, the second image in the Supplementary Material shows a case where a set of camera instruments is misclassified as a weapon after perturbation."}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "SkxPHvTiaQ", "original": null, "number": 2, "cdate": 1542342463173, "ddate": null, "tcdate": 1542342463173, "tmdate": 1542342463173, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "SJeaN4LihX", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Re: simple and effective blackbox attack based on random directions", "comment": "Comparison with Guo et al. 18: \nThere may have been a misunderstanding. We do compare directly to the exact method by Guo et al. It is algorithm \u201cLFBA\u201d in Figure 2 and Table 1. LFBA stands for \u201cLow Frequency Boundary Attack\u201d, which is the terminology used by Guo et al.\n\nNovelty over Guo et al. 18: \nWhile black-box adversarial examples in DCT space has certainly been studied in Guo et al.,\nthe core component that makes SimBA drastically more efficient than other black-box attacks is that we take numerous small steps in random orthonormal directions, whether that be random pixel perturbations or the DCT basis. The fact that SimBA (without DCT) is already very competitive compared to all previous untargeted and targeted black-box attacks supports this claim. We consider this insight important to be shared with the community, because it shows that the problem of adversarial attacks may be much simpler than most of us had assumed. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "BklIEDTs6X", "original": null, "number": 1, "cdate": 1542342445866, "ddate": null, "tcdate": 1542342445866, "tmdate": 1542342445866, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rkxkTpcJTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "content": {"title": "Re: simple algorithm, intriguing message", "comment": "Thank you for your review. We are equally surprised that the SimBA attack has not been discovered earlier - and that it can even outperform far more sophisticated approaches. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619218, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJeZS3RcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1514/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1514/Authors|ICLR.cc/2019/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers", "ICLR.cc/2019/Conference/Paper1514/Authors", "ICLR.cc/2019/Conference/Paper1514/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619218}}}, {"id": "rkxkTpcJTQ", "original": null, "number": 3, "cdate": 1541545399105, "ddate": null, "tcdate": 1541545399105, "tmdate": 1541545399105, "tddate": null, "forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "content": {"title": "simple algorithm, intriguing message", "review": "This paper demonstrates that a simple greedy random search algorithm in DCT space based on score feedback is able to synthesize adversarial examples with quite good query efficiency.  The algorithm is demonstrated on ImageNet  with three common architectures, showing much higher efficiency when sampling from the DCT basis. The algorithm is also shown to outperform state of the art attacks in terms of query count. Finally, a successful attack is demonstrated on Google Cloud Vision.\n\nWhile not particularly heavy on technicalities, this work does make a couple intriguing points, namely that adversarial attacks can potentially be quite easy to perform due to the inherent nature of high-dimensional classification, and that the space is in which the search is perform might be more important than the sophistication of the search itself. I interpret the proposal not so much as a claim to a state-of-the-art algorithm (even though the results are impressive) but as a very reasonable baseline in the evaluation of attack efficiency -- one might even wonder why it has not been common practice thus far to evaluate against such kinds of algorithms by default. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1514/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple Black-box Adversarial Attacks", "abstract": "The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. ", "keywords": [], "authorids": ["cg563@cornell.edu", "jrg365@cornell.edu", "yy785@cornell.edu", "andrew@cornell.edu", "kqw4@cornell.edu"], "authors": ["Chuan Guo", "Jacob R. Gardner", "Yurong You", "Andrew G. Wilson", "Kilian Q. Weinberger"], "pdf": "/pdf/dfedb03996426d491479278b797615cb2b0de79f.pdf", "paperhash": "guo|simple_blackbox_adversarial_attacks", "_bibtex": "@misc{\nguo2019simple,\ntitle={Simple Black-box Adversarial Attacks},\nauthor={Chuan Guo and Jacob R. Gardner and Yurong You and Andrew G. Wilson and Kilian Q. Weinberger},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeZS3RcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1514/Official_Review", "cdate": 1542234213548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJeZS3RcYm", "replyto": "rJeZS3RcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1514/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962695, "tmdate": 1552335962695, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1514/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}