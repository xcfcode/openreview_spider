{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1491259299603, "tcdate": 1478241826996, "number": 129, "id": "S1jmAotxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1jmAotxg", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "content": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396373637, "tcdate": 1486396373637, "number": 1, "id": "BkC0sG8ug", "invitation": "ICLR.cc/2017/conference/-/paper129/acceptance", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396374127, "id": "ICLR.cc/2017/conference/-/paper129/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396374127}}}, {"tddate": null, "tmdate": 1484004014907, "tcdate": 1484004014907, "number": 13, "id": "B1wn9cb8e", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "HyVvqMqEl", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Response to Response", "comment": "Thanks for your responses and elaborations, Reviewer #2.  Here are our responses and comments...\n\n> Is the simplex in question the Stick-Breaking Weights ?\n\nYes, the joint space.\n\n\n> I don't know if that's because it's obvious in hindsight, but I do not see the issue.... As to sample from a constrained set, it is indeed challenging but the constrained set in question ]0, 1[ can be easily solved.\n\nAt the risk of being redundant, I\u2019ll attempt to state our position in other words.  Perhaps this is just a point of fundamental disagreement...\n\nThere are two approaches to generating differentiable samples on the unit interval, as we see it, and one is easier than the other.  The easier approach is to appropriately transform an unbounded random variable, such as applying the logistic function to a Gaussian sample.  This is our Gauss-Logit baseline, which is also used in the ICLR submission \u201cNeural Variational Inference for Topic Models\u201d (https://openreview.net/forum?id=BybtVK9lg).  If this is all we used to sample the latent variables, your assessment would be fair and accurate.\n\nThe harder approach is to use a proper continuous distribution on the unit interval.  This too would be easy if the Beta had a non-centered parameterization---but it doesn\u2019t.  At least two other recent papers have addressed this problem: \u201dThe Generalized Reparametrization Gradient\u201d (https://arxiv.org/abs/1610.02287) (GRG) and \u201cRejection Sampling Variational Inference\u201d (https://arxiv.org/abs/1610.05683) (RSVI).  Both can produce differentiable Beta samples, but each has a drawback: the former is not a true DNCP (since there is still weak dependence on the variational parameters) and the latter, obviously, might require multiple attempts to produce a valid sample.  \n\nOur use of the Kumaraswamy is the only approach we are aware of that [1] has a variational approximation with the natural support (unlike the easier transformation-based approach) and [2] can be sampled via a true non-centered parametrization (unlike GRG and RSVI).  While our proposal is certainly not groundbreaking, having these two qualities makes it a notable contribution supplementary to the GRG and RSVI.  \n\n\n> Does this generative process put zero values on the decoder weights corresponding to \\pi_{k}s that were not used during training ? The generative model from (Blei and Jordan, 2004) is not truncated despite using a truncated variational approximation.\n\nOur generative model is not truncated: the decoder weight matrix is theoretically infinitely wide, but the columns at indices higher than the truncation threshold are censored (and hence, don\u2019t need implemented) because they are multiplied by latent variables with \\pi_k>T = 0 and thus never influence downstream computation.  \n\n\n> With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight.  For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\n\nWe agree that the SB-VAE\u2019s latent variables at higher indices will have small values, and as a result, their corresponding decoder weights will receive small updates.  This is not to say those indices are \u2018collapsed\u2019 in the same manner as the Gauss-VAE\u2019s decoder though.  The difference is that, by zeroing-out decoder weights, no data instance can ever use those latent dimensions of the Gaussian VAE.  However, in the SB-VAE, nothing is preventing an instance from using those higher indices if it needs to at any time (even though, in aggregate, those dimensions may look effectively inactive).\n\nThanks,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1482463835717, "tcdate": 1482463835717, "number": 3, "id": "HyVvqMqEl", "invitation": "ICLR.cc/2017/conference/-/paper129/official/comment", "forum": "S1jmAotxg", "replyto": "Hke7o1wNe", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "content": {"title": "Response to Authors", "comment": "\"Are you referring to how the stick segments are recursively constructed? \"\nYes.\n\n\"While you are correct that the stick segments lie on the simplex and therefore look as if they could be produced by a soft-gating function, this view neglects the underlying probabilistic model and the need for sampling from the simplex (for ELBO optimization).  Generating differentiable samples from the simplex can not be done via existing methods and was an open problem---for example, see the discussion of the NVI Topic Model in my response to Review 1.\"\nIs the simplex in question the Stick-Breaking Weights ? I don't know if that's because it's obvious in hindsight, but I do not see the issue. You are generating differentiable sample v_{k}s from a ]0, 1[ interval and this very problem can be addressed using inverse transform sampling as you did using functions that only have values in ]0, 1[. The KL divergence are between variables that are in ]0, 1[ (with the Beta distribution) and the construction of the stick segment are part of the decoding process and not the inference process. \nUsing inverse transform sampling to generate differentiable samples for VAEs is not novel: it was mentioned in one of the original VAE papers by Kingma and Welling and was further explored in the Normalizing Flow paper by Rezende et al. As to sample from a constrained set, it is indeed challenging but the constrained set in question ]0, 1[ can be easily solved.\n\n\"Their generative process is still a formal Bayes nonparametric model, just as ours is.  As we discuss in Section 4, our model does work with an adaptive number of latent variables; it\u2019s just that optimization is slower.  \"\nDoes this generative process put zero values on the decoder weights corresponding to \\pi_{k}s that were not used during training ? The generative model from (Blei and Jordan, 2004) is not truncated despite using a truncated variational approximation. \n I understand that there is a Bayesian nonparametric view behind it but I fail how this view has practical implication on the model if the number of latent variables is not adaptive. Even in the case of adaptive number of variables, I'm not sure I see a strong difference between the adaptation in the number of latent variables and component collapsing. I'll repeat my remark: \"With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\"\n\nI am not convinced to which extent this paper opens the path to link Bayesian nonparametric with VAEs yet and about the pruning comparison with the standard VAE. I do agree that the simplex representation seems to be more discriminative than a gaussian feature space."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715597, "id": "ICLR.cc/2017/conference/-/paper129/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715597}}}, {"tddate": null, "tmdate": 1482427269960, "tcdate": 1482427269960, "number": 12, "id": "S1RKsFF4g", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "HkNEFmt4x", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Response to Response to Response", "comment": "Thanks, Reviewer #3.  I understand now and agree.  I will edit the draft to make a point of this.\n\nBest,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1482402092159, "tcdate": 1482402092159, "number": 2, "id": "HkNEFmt4x", "invitation": "ICLR.cc/2017/conference/-/paper129/official/comment", "forum": "S1jmAotxg", "replyto": "B1Bhiyv4l", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer3"], "content": {"title": "Response to Response", "comment": "When talking about 'the prior latent space', one usually talks about the joint distribution over all the latents: p(z), where 'z' is the concatenation of the individual components of latent space: p(z) = p(z1,z2,z3,...) = p(z1)p(z2|z1)p(z3|z1,z2)...\n\nRegular DRAW indeed has spherical Gaussian priors since the latent space factorizes as independent Gaussians: p(z)=p(z1)p(z2)p(z3).. However, convolutional DRAW (https://arxiv.org/pdf/1604.08772.pdf), ladder VAE and IAF VAE have non-factorized priors: each mean and variance of each level of the latent hierarchy depends non-linearly on the values their ancestors, such that their joint distribution becomes non-Gaussian. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715597, "id": "ICLR.cc/2017/conference/-/paper129/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715597}}}, {"tddate": null, "tmdate": 1482255325633, "tcdate": 1482255128374, "number": 10, "id": "Hke7o1wNe", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "HkqgbsZEl", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Responses to Reviewer #2", "comment": "Thanks for your comments, Reviewer #2.  Our responses are below.\n\n1. \"After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\"\n\nWe are not sure what you mean by \u201cthe latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity\u201d.  Are you referring to how the stick segments are recursively constructed?  \n\nWhile you are correct that the stick segments lie on the simplex and therefore look as if they could be produced by a soft-gating function, this view neglects the underlying probabilistic model and the need for sampling from the simplex (for ELBO optimization).  Generating differentiable samples from the simplex can not be done via existing methods and was an open problem---for example, see the discussion of the NVI Topic Model in my response to Review 1.\n\nAnd yes, all the latent variables are used during every forward propagation, but this is commonly the case in Bayesian nonparametric models with posterior truncation.  For instance, Blei and Jordan, in their seminal work on VI for Infinite Mixtures (http://www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf), use a truncated posterior, just as we do, and therefore the number of components is fixed and all are \u2018used\u2019 in the sense that they have non-zero weight.  Their generative process is still a formal Bayes nonparametric model, just as ours is.  As we discuss in Section 4, our model does work with an adaptive number of latent variables; it\u2019s just that optimization is slower.   \n\n2. \"The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.\"\n\nWe agree that the paper can only be improved by making the semi-supervised results more comparable to Kingma et al.\u2019s.  We will try to add these before the discussion period ends.  Yet, we believe the absence of M1+M2 experiments is not a crucial flaw or deficiency.  Kingma et al\u2019s results are not SOTA any longer, and the point of our paper is not semi-supervised learning (as Kingma et al.\u2019s is).  Rather, our aim is to compare SB vs Gauss latent variables in a semi-supervised setting, and we believe our experiment accurately does this (while still having results partially comparable to Kingma et al\u2019s, as explained in a pre-review response).\n\nThanks again,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1482255314084, "tcdate": 1482255003958, "number": 9, "id": "H1EiqkwEe", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "BkgfmZf4l", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Responses to Reviewer #1", "comment": "Thanks for your thoughts, Reviewer #1.  Our responses are below, paired with the corresponding segment of your review.\n\n1. \"I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.\"\n\nThanks, this is a good point.  We\u2019ll make different subsections for the generative and inference processes in Section 4 and include and/or elaborate on our answers to your pre-review questions.\n\n2. \"Firstly, conditioning the model directly on the stick-breaking weights seems a little odd\u2026.To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0.  The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention\u2026.\nIt seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated.\" \n\nYes, we agree that choosing the simplex for the latent space is one of multiple options.  Other formulations---such as using the remaining length of the stick---are possible.  Ultimately, we made the choice to use the simplex for the following reasons: (1) if the decoder is sufficiently powerful, the simplex should have as much representational capacity as Euclidean space does for the Gaussian VAE, (2) using the weights keeps the model closest to its natural extension to a mixture VAE, and lastly, (3) we found the formulation had some interesting properties, such as being better at classification, that seemed worthy of reporting.  We believe the experiments generating MNIST samples from increasingly higher dimensional latent spaces and examining the number of latent variables used for MNIST+rot provided insight into the model\u2019s nonparametric qualities to some degree.  \n\nAs for the paper\u2019s high level motivation: our original goal was to connect SGVB and Bayesian Nonparametrics.  Finding a differentiable non-centered parametrization (DNCP) for the Stick-Breaking process was the central challenge to this, and once we found the Kumaraswamy, we formulated the SB-VAE to test if optimization of this somewhat unusual computation graph (i.e. the recurrent dependence across the hidden layer) and this little known distribution was possible.  It turned out that it was, and in addition, we saw some interesting properties that seemed useful for people to know.  \n\nYet, we agree that, while the SB-VAE is a VAE alternative one should try if unhappy with the Gauss-VAE\u2019s performance, the use of the Kumaraswamy as a DNCP for Beta-like or Dirichlet-like random variables is the paper\u2019s most widely applicable lesson.  In fact, in light of recently proposed VAE extensions, the usefulness of the Kumaraswamy is more apparent now than when we began this work.  For instance, current ICLR submissions such as \u201cDeep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders\u201d and \u201cNeural Variational Inference for Topic Models\u201d both workaround the difficulty of priors on the simplex by either, in the case of the former, just not placing priors on the mixture weights, and in the case of the latter, using a Laplace approximation.  The latter work even acknowledges the problem that our paper solves: \u201cBut it is difficult to handle the Dirichlet within NVI because it is difficult to develop an effective reparameterization function for the [Reparametrization Trick].\u201d  Moreover, the recent work from the Blei lab still does not describe a true DNCP for the Beta, their solutions being either weakly dependent on the approx. posterior parameters (\u201cThe Generalized Reparametrization Gradient, NIPS 2016) or requiring rejection sampling (\u201cRejection Sampling Variational Inference\u201d, ArXiv).  \n\n3. \"Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.\"\n\nThis is a good idea to improve motivation; we can elaborate on the connection to Nested Dropout, perhaps moving it out of the \u2018Related Work\u2019 section.  However, for experimental comparisons, we found optimization under Nested Dropout very difficult, even with unit sweeping.  The models were not competitive and thus left out of experiments.  Comparing against the Infinite RBM is another option, but to make comparison fair, we\u2019d have to tie the encoder and decoder weights (since RBMs have only one weight matrix), and it is not totally clear how to do this in a VAE since the networks are asymmetric.  \n\nThanks again,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1482255277255, "tcdate": 1482255277255, "number": 11, "id": "B1Bhiyv4l", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "Hy3_VU-Eg", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Responses to Reviewer #3", "comment": "Thank you for your review.  Our responses / comments are below.\n\n1. \"The fact that they do report this 'negative' result suggests good scientific taste.\"\t\n\nWe appreciate you recognizing this.  \n\n2. \"sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.\"\n\nDo you mean non-factorized Gaussians?  In the case of DRAW and Ladder VAEs, we still think they are using factorized Gaussians.  The DRAW paper says, \u201cIn our experiments the latent distribution is a diagonal Gaussian\u201d (Section 2.1), and the Ladder VAE paper states, \u201cConditioned on the stochastic layer below each stochastic layer is specified as a fully factorized gaussian distribution\u201d (Section 2.1).  The Ladder VAE paper says this when describing the regular multi-layer VAE but the same notation is used for their model.\n\n3.  \"Figure 3(f). Interesting that k-NN works so well on raw pixels.\"\n\nYes, this was a bit surprising to us too.  This blog post reports similar results: http://acgrama.blogspot.com/2012/09/knn-with-euclidean-distance-on-mnist.html\n\nThanks again,\nEric\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481938299326, "tcdate": 1481908466152, "number": 2, "id": "HkqgbsZEl", "invitation": "ICLR.cc/2017/conference/-/paper129/official/review", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.\nAfter reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\nWith respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\nAdding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.\nThe semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512687415, "id": "ICLR.cc/2017/conference/-/paper129/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper129/AnonReviewer3", "ICLR.cc/2017/conference/paper129/AnonReviewer2", "ICLR.cc/2017/conference/paper129/AnonReviewer1"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512687415}}}, {"tddate": null, "tmdate": 1481933575741, "tcdate": 1481933575741, "number": 3, "id": "BkgfmZf4l", "invitation": "ICLR.cc/2017/conference/-/paper129/official/review", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "content": {"title": "Motivation is not very clear but good paper overall", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.\n\nThere's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \"black-box\" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.\n\nAlthough the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.\n\nI have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.\n\nThe second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.\n\nOverall, I found this to be an interesting paper, it would be a good fit for ICLR.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512687415, "id": "ICLR.cc/2017/conference/-/paper129/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper129/AnonReviewer3", "ICLR.cc/2017/conference/paper129/AnonReviewer2", "ICLR.cc/2017/conference/paper129/AnonReviewer1"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512687415}}}, {"tddate": null, "tmdate": 1481916026094, "tcdate": 1481916026094, "number": 8, "id": "r1GYChbVx", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "Skd6Q_ZEx", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "SB Motivation", "comment": "Hi Zhenwen,\n\nThanks for your comment.\n\nThe motivation for keeping the latent variables on the simplex was mostly to keep consistent with the formulation of GEM random variables.  Of course one could use other constructions, such as using the remaining length of the stick as the latent variable.  This may work better in practice, but in theory, the representational power of the simplex vs unconstrained real space should be the same when using a sufficiently powerful decoder (such as an MLP or GP).  Moreover, we see this work as the first necessary stepping stone to integrating inference networks and Bayesian Nonparametric processes and have started working on implementations with base distributions: http://bayesiandeeplearning.org/papers/BDL_20.pdf \n\nThanks for the pointer to your work.  It was already on my \"to-read\" list.  It looks very interesting.\n\nBest,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481896896035, "tcdate": 1481896896035, "number": 7, "id": "Skd6Q_ZEx", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["~Zhenwen_Dai1"], "readers": ["everyone"], "writers": ["~Zhenwen_Dai1"], "content": {"title": "Motivation for Stick-Breaking Process", "comment": "Very Interesting work for extending VAE towards Bayesian non-parametric! \n\nI wonder what is the motivation of constraining \\pi to be positive and summed up to one (via taking the stick-breaking process)? \n\nI would suggest to have a comparison with our work \"Variational Auto-encoded Deep Gaussian Processes\". It is another Bayesian non-parametric VAE via using Gaussian process as the decoder, "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481888914319, "tcdate": 1481888883999, "number": 1, "id": "Hy3_VU-Eg", "invitation": "ICLR.cc/2017/conference/-/paper129/official/review", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer3"], "content": {"title": "Good paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.\n\nIn experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.\n\nComments:\n- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.\n- sec 2.2: two comma's\n- text flow eq 6: please refer to appendix with the closed-form KL divergence\n- \"The v's are sampled via\" => \"In the posterior, the v's are sampled via\". It's not clear you're talking about the posterior here, instead of the prior.\n- The last paragraph of section 4 is great.\n- Sec 7.1: \"Density estimation\" => Technically you're also doing mass estimation.\n- Sec 7.1: 100 IS samples is a bit on the low side. \n- Figure 3(f). Interesting that k-NN works so well on raw pixels.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512687415, "id": "ICLR.cc/2017/conference/-/paper129/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper129/AnonReviewer3", "ICLR.cc/2017/conference/paper129/AnonReviewer2", "ICLR.cc/2017/conference/paper129/AnonReviewer1"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512687415}}}, {"tddate": null, "tmdate": 1481773626034, "tcdate": 1481670874151, "number": 6, "id": "HJfJW-AXl", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "HJZ3uq67x", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Likelihood and Base Measure", "comment": "Hi again.\n\n1.  p_\\theta: The likelihood's form depends on the data, and thus it will be Gaussian when the data is over the real line and Bernoulli when the data is binary.  In either case, the neural network decoder parametrizes the mean.  The neural network itself consists of one or more hidden layers, and it's input will be the vector of stick-breaking weights.\n\n2.  G_0: There is no base distribution.  The latent variables are drawn only from the GEM component of the Dirichlet Process (DP).  Adding a base distribution will result in a mixture latent representation.  This can be done---we are currently investigating it, in fact (http://bayesiandeeplearning.org/papers/BDL_20.pdf)---but it is impractical as the decoder network needs run K^S times, where K is the number of mixture components (truncation size, in the DP case) and S is the number of stochastic layers.\n\nBest,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481644200963, "tcdate": 1481644200954, "number": 1, "id": "HJZ3uq67x", "invitation": "ICLR.cc/2017/conference/-/paper129/official/comment", "forum": "S1jmAotxg", "replyto": "ryA3POn7x", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "content": {"title": "Eq. 8 and G_0", "comment": "Can you please elaborate on the form of the distribution p_\\theta (x | \\pi_i) in equation (8)?\n\nAlso, what is the base distribution G_0 in your case? And what is the dimensionality of this distribution?\n\nThanks "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715597, "id": "ICLR.cc/2017/conference/-/paper129/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715597}}}, {"tddate": null, "tmdate": 1481570552281, "tcdate": 1481570230132, "number": 5, "id": "ryA3POn7x", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "rJ85Yv37g", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Kumaraswamy, Cross-Validation, and Marginal LL", "comment": "Hi, thanks for your questions.  \n\n1.  Our use of the Kumaraswamy is an approximation in the variational sense---i.e. we don't know the true posterior so we need to pick a parametric family---but it is NOT a numerical approximation like Knowles' work is.  In fact, even if the Beta had a parametrization amenable to SGVB, there's no reason to think the true posterior is more accurately approximated by a Beta than a Kumaraswamy.  The reason the Beta has, historically, been the default approximation for Stick-Breaking models is because it (usually) gives closed-form expectations, which are not available here in any case.\n\n2.  It's only crucial to set a_0 large enough.  If set too small, the Kumaraswamy parameters can become very big or very small, resulting in numerical instabilities.  The only reason not to set a_0 very large is for training-time considerations.  Runtime aside, I haven't seen a problem with setting it 'too' large.\n\n3.  I think the marginal likelihood is worse than the Gauss-VAE because of the restricted form of stick-breaking latent variables (ie decreasing in expectation, summing to 1).  In theory, since MLPs are universal approximators, these constraints shouldn't be an issue (compared to Gaussian space), but in practice, I think the recursive nature of the stick-breaking weights makes optimization harder, which is supported in the expected reconstruction error plots by the slower learning curve.\n\n4.  By 'decay of the associated decoder weight', I meant that the Gauss-VAE turns off weights in the decoder, setting them to zero.  The SB-VAE doesn't do this and leaves the decoder weights without any systematic sparsity.  'Decay' might be a poor word choice; perhaps 'pruning' is better.\n\nThanks,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481566605615, "tcdate": 1481566605606, "number": 2, "id": "rJ85Yv37g", "invitation": "ICLR.cc/2017/conference/-/paper129/pre-review/question", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer1"], "content": {"title": "Questions", "question": "1. You mention that Knowles' (18) approximation becomes poor for increasing \\alpha. Do you have an analysis about what happens to Kumaraswamy's distribution (wrt the approximation) as you change the parameters?\n\n2. The parameter a_0 is cross-validated in the experiments. I was wondering how sensitive is the overall performance to changes in a_0 (i.e. how crucial is for the cross-validation to be done well)?\n\n3. Any insight about why the marginal likelihood is smaller for SB-VAE? Do you think it might be because of the approximation, or because of the actual probabilistic model having more parameters?\n\n4. Minor question, but I couldn't understand what you mean by  \"decay of the associated decoder weights\" in the legend of Fig. 2.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481566606179, "id": "ICLR.cc/2017/conference/-/paper129/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper129/AnonReviewer2", "ICLR.cc/2017/conference/paper129/AnonReviewer1"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481566606179}}}, {"tddate": null, "tmdate": 1481565080841, "tcdate": 1481565080835, "number": 4, "id": "rJWiXPn7x", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "BJCj7rnme", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Comparison with (Kingma et al, NIPS 2014)", "comment": "Hi, thanks for your question.  Notice that Kingma et al. reports SVHN results only for stacked models (M2+M1, M1+TSVM, etc.).  In order to isolate the effects of using S.B. vs Gauss. latent variables, we chose to experiment with just the (non-stacked) M2 model, for which they report results on just MNIST.  Thus their results are not directly comparable for SVHN.  Yet, notice that with just the M2 model, we get ~37% error at 5% labeled; whereas with M1+M2, Kingma et al. get 36% at roughly 2% labeled.  Of course their result is better, but they are using two VAEs.\n\nFor MNIST, the results are somewhat comparable.  I say 'somewhat' because we use much larger encoders and decoders (3-layer ResNets), which is what allows us to get respectable performance on SVHN with one model.  Still, our MNIST results are roughly indistinguishable for theirs.  For example, on MNIST, Kingma et al. reports 3.92% (plus/minus .63) for 3,000 labeled examples; our Gauss-M2 implementation gets 4.74% (plus/minus .43) for 2,250 labeled examples.  Note there is statistical overlap in the results despite us using less labels.  Even though we have a higher-capacity model, I think we don't get better results because the extra encoder/decoder layers make us more prone to overfitting on MNIST.      "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1481556902068, "tcdate": 1481556902064, "number": 1, "id": "BJCj7rnme", "invitation": "ICLR.cc/2017/conference/-/paper129/pre-review/question", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper129/AnonReviewer2"], "content": {"title": "Comparison with \"Semi-supervised Learning with Deep Generative Models\" ?", "question": "The technique is potentially an interesting step towards fusing Black Box Variational Inference and Bayesian Nonparametrics.\nHowever, I'm wondering why the model was not compared with the results in \"Semi-supervised Learning with Deep Generative Models\" for the semi-supervised learning experiments, they seem to have stronger results than the ones shown Table 2. Why would these results be not comparable ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481566606179, "id": "ICLR.cc/2017/conference/-/paper129/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper129/AnonReviewer2", "ICLR.cc/2017/conference/paper129/AnonReviewer1"], "reply": {"forum": "S1jmAotxg", "replyto": "S1jmAotxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481566606179}}}, {"tddate": null, "tmdate": 1478562249268, "tcdate": 1478562249263, "number": 3, "id": "BkWAW5Rlx", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "BJe5WwRgg", "signatures": ["~Christian_A_Naesseth1"], "readers": ["everyone"], "writers": ["~Christian_A_Naesseth1"], "content": {"title": "Hi Eric", "comment": "Thanks Eric! Yes, I most certainly did not expect you to have implemented it already :) Just wanted to make you aware of it in case you missed it. I would suggest using the method described in the rejection sampling-based paper if you want to compare. I will have a look at your code but I can't promise anything. What I can easily do is share some useful functions written in Python for implementing it and answer questions, just write me an e-mail.\n\nBest,\nChristian"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1478549896481, "tcdate": 1478549896474, "number": 2, "id": "BJe5WwRgg", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "BJYi3LRle", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Thanks for the pointer", "comment": "Hi Christian, \n\nThanks for the pointer to your work; it's very interesting.  Yes, the Blei Lab's recent work on the 'Generalized Reparametrization Gradient' and 'Rejection Sampling Variational Inference' are both applicable to Beta r.v.s and thus could be used for our Stick-Breaking VAE.  I mention the former in Footnote #1 in the paper.  Unfortunately, this work was released on 10/7/16 and 10/18/16, respectively, and I haven't yet had a chance to implement the methods.  Hopefully I'll get around to it in the next few weeks.  I would also gladly accept a pull request: https://github.com/enalisnick/stick-breaking_dgms  \n\nBest, \nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}, {"tddate": null, "tmdate": 1478548640803, "tcdate": 1478548640797, "number": 1, "id": "BJYi3LRle", "invitation": "ICLR.cc/2017/conference/-/paper129/public/comment", "forum": "S1jmAotxg", "replyto": "S1jmAotxg", "signatures": ["~Christian_A_Naesseth1"], "readers": ["everyone"], "writers": ["~Christian_A_Naesseth1"], "content": {"title": "Reparameterization for Beta", "comment": "Very interesting work! \n\nShameless plug: I would suggest to look at our recent paper on reparameterization for random variables drawn using rejection sampling https://arxiv.org/abs/1610.05683 (incl. gamma, beta, Dirichlet, ...)\nThis should work much better than numerical approximations of the inverse CDF."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stick-Breaking Variational Autoencoders", "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "pdf": "/pdf/7763c4fd20502cd6801d446a1d63c59eb9adab1d.pdf", "TL;DR": "We define a variational autoencoder variant with stick-breaking latent variables thereby giving it adaptive width.", "paperhash": "nalisnick|stickbreaking_variational_autoencoders", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": ["Deep learning", "Unsupervised Learning", "Semi-Supervised Learning"], "conflicts": ["uci.edu", "lehigh.edu", "twitter.com"], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287715797, "id": "ICLR.cc/2017/conference/-/paper129/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jmAotxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper129/reviewers", "ICLR.cc/2017/conference/paper129/areachairs"], "cdate": 1485287715797}}}], "count": 23}