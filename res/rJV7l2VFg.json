{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528908126209, "tcdate": 1487351835777, "number": 99, "id": "rJV7l2VFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJV7l2VFg", "original": "HysBZSqlx", "signatures": ["~Nadav_Bhonker1"], "readers": ["everyone"], "content": {"title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to trainon various Atari 2600 games.  In many games the state-of-the-art algorithms out-perform  humans.   In  this  paper  we  introduce  a  new  learning  environment,  the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles.The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining a simple unified interface. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. A more extensive paper describing our work is available on arXiv", "pdf": "/pdf/0a5d718073b7bd9c08e9e36411585def4d9917e4.pdf", "TL;DR": "Investigating Deep Reinforcement Learning algorithms in a new framework based on the SNES gaming console", "paperhash": "bhonker|playing_snes_in_the_retro_learning_environment", "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "conflicts": ["ibm.com", "technion.ac.il"], "authors": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "authorids": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484328453553, "tcdate": 1478279491495, "number": 238, "id": "HysBZSqlx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HysBZSqlx", "signatures": ["~Nadav_Bhonker1"], "readers": ["everyone"], "content": {"title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carriedout in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.", "pdf": "/pdf/ff00969a1ba58edc121628fdd88d330efe6077b3.pdf", "TL;DR": "Investigating Deep Reinforcement Learning algorithms in a new framework based on the SNES gaming console", "paperhash": "bhonker|playing_snes_in_the_retro_learning_environment", "keywords": ["Reinforcement Learning", "Deep learning", "Games"], "conflicts": ["technion.ac.il", "mellanox.com", "ibm.com"], "authors": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "authorids": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028598155, "tcdate": 1490028598155, "number": 1, "id": "B1A4_F6sx", "invitation": "ICLR.cc/2017/workshop/-/paper99/acceptance", "forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to trainon various Atari 2600 games.  In many games the state-of-the-art algorithms out-perform  humans.   In  this  paper  we  introduce  a  new  learning  environment,  the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles.The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining a simple unified interface. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. A more extensive paper describing our work is available on arXiv", "pdf": "/pdf/0a5d718073b7bd9c08e9e36411585def4d9917e4.pdf", "TL;DR": "Investigating Deep Reinforcement Learning algorithms in a new framework based on the SNES gaming console", "paperhash": "bhonker|playing_snes_in_the_retro_learning_environment", "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "conflicts": ["ibm.com", "technion.ac.il"], "authors": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "authorids": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028598682, "id": "ICLR.cc/2017/workshop/-/paper99/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028598682}}}, {"tddate": null, "tmdate": 1489549596071, "tcdate": 1489549596071, "number": 2, "id": "Sk4XFVUsx", "invitation": "ICLR.cc/2017/workshop/-/paper99/official/review", "forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "signatures": ["ICLR.cc/2017/workshop/paper99/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper99/AnonReviewer1"], "content": {"title": "Good workshop candidate, though limited scientific contribution", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a new environment similar to the Arcade Learning Environment, but able to support more consoles and games. It also reports performance on four SNES games, with three standard deep reinforcement learning algorithms, showing that reaching human-level performance is more challenging than on Atari games.\n\nI believe this is a good fit for a workshop: although it has limited scientific contribution in its current form, having a new environment with the ability to play more complex games may eventually lead to significant advances in the field, so it is good to promote it. That being said, although the potential is there, it is hard to tell whether this environmnent will \"take off\" in the community, since the \"competitors\" (ALE, OpenAI Universe, DeepMind Lab...) currently enjoy higher popularity. In my opinion the only way to know is to wait and see... and giving this environment a chance to get known can't hurt!", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to trainon various Atari 2600 games.  In many games the state-of-the-art algorithms out-perform  humans.   In  this  paper  we  introduce  a  new  learning  environment,  the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles.The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining a simple unified interface. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. A more extensive paper describing our work is available on arXiv", "pdf": "/pdf/0a5d718073b7bd9c08e9e36411585def4d9917e4.pdf", "TL;DR": "Investigating Deep Reinforcement Learning algorithms in a new framework based on the SNES gaming console", "paperhash": "bhonker|playing_snes_in_the_retro_learning_environment", "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "conflicts": ["ibm.com", "technion.ac.il"], "authors": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "authorids": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489549596950, "id": "ICLR.cc/2017/workshop/-/paper99/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper99/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper99/AnonReviewer2", "ICLR.cc/2017/workshop/paper99/AnonReviewer1"], "reply": {"forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper99/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper99/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489549596950}}}, {"tddate": null, "tmdate": 1489166241227, "tcdate": 1489166241227, "number": 1, "id": "BJFj1Plje", "invitation": "ICLR.cc/2017/workshop/-/paper99/official/review", "forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "signatures": ["ICLR.cc/2017/workshop/paper99/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper99/AnonReviewer2"], "content": {"title": "An interface to many games without established guidelines and protocols", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents the Retro Learning Environment (RLE), which provides a unified interface to Atari, SNES, Sega Genesis and other consoles. Good interfaces to interesting reinforcement learning tasks are potentially very useful. One good example is the Arcade Learning Environment (ALE) of Bellemare et al. This project could go on to have a similar effect on the reinforcement learning community, but not in its current form. ALE succeeded partly because it provided a thorough set of benchmarks and suggested evaluation protocols. In order to encourage wide adoption of RLE the paper should provide guidelines for sets of interesting task, how to deal with practical issues like selection of difficulty levels, more benchmarks for interesting games, etc.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to trainon various Atari 2600 games.  In many games the state-of-the-art algorithms out-perform  humans.   In  this  paper  we  introduce  a  new  learning  environment,  the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles.The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining a simple unified interface. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. A more extensive paper describing our work is available on arXiv", "pdf": "/pdf/0a5d718073b7bd9c08e9e36411585def4d9917e4.pdf", "TL;DR": "Investigating Deep Reinforcement Learning algorithms in a new framework based on the SNES gaming console", "paperhash": "bhonker|playing_snes_in_the_retro_learning_environment", "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "conflicts": ["ibm.com", "technion.ac.il"], "authors": ["Nadav Bhonker", "Shai Rozenberg", "Itay Hubara"], "authorids": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489549596950, "id": "ICLR.cc/2017/workshop/-/paper99/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper99/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper99/AnonReviewer2", "ICLR.cc/2017/workshop/paper99/AnonReviewer1"], "reply": {"forum": "rJV7l2VFg", "replyto": "rJV7l2VFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper99/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper99/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489549596950}}}], "count": 4}