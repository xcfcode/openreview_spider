{"notes": [{"id": "Hkl1iRNFwS", "original": "rJxdukYuDS", "number": 1303, "cdate": 1569439382851, "ddate": null, "tcdate": 1569439382851, "tmdate": 1583912040521, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Y6qohXMNn", "original": null, "number": 1, "cdate": 1576798719865, "ddate": null, "tcdate": 1576798719865, "tmdate": 1576800916690, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper studies numerous ways in which the statistics of network weights evolve during network training.  Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process.  Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723516, "tmdate": 1576800275006, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Decision"}}}, {"id": "SJlvCqBatr", "original": null, "number": 2, "cdate": 1571801806937, "ddate": null, "tcdate": 1571801806937, "tmdate": 1574566147385, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper aims at exploring the properties of neural network training during the early phase. By some studies on the lottery ticket hypothesis, something important happens during the early phase of training so rewinding the network should go to these early phases instead of the initial phase. So, what is important during training? The paper explores this problem from four aspects through empirical studies: \n1. By showing the various statistics through different training iterations, especially the gradient magnitude, the training phase is divided into three parts, and each part has different behaviors.\n2. The paper explores what is more important for the early phase of training: signs of the weights or magnitude of the weights.\n3. The paper explores what is more important for the early phase of training if we redistribute the weights, signs or magnitudes.\n4. The paper explores how training depends on data. Giving weak information such as self-supervised information may work but giving wrong information such as random labels will hurt the performance.\n\nThis paper studies the properties of deep neural networks. Through a series of carefully designed experiments, the paper shows what is important for the weights (magnitude or signs), and what is important for the data (weak information or random information). It enables a deep understanding of neural networks and may motivate new neural network compression methods to be proposed. Generally, the paper is well-written, although some parts can be improved. I would vote for acceptance of the paper.\n\nSome questions/suggestions to make the paper clearer:\n1. It is better to have a table summarizing various results of the paper, to give readers an overall impression after going through so many detailed experimental results.\n2. The results of Fig. 4 and Fig.6 can be inconsistent. In Figure 4, it says that signs are less important than magnitudes. In Figure 6, it says that signs are more important than magnitudes if shuffling filters and keep signs. Any explanation on the inconsistency?\n3. There is no explanation of the \u201cweight trace\u201d in Figure 3.\n4. It is not clear what is the difference between \u201cInit\u201d and \u201cFinal\u201d in \u201cL2 Dist\u201d and \u201cCosine Similarity\u201d in Figure 3. \n\nFinally,  it is said in the \u201ccall for papers\u201d, \u201cReviewers will be instructed to apply a higher standard to papers in excess of 8 pages\u201d. This paper is nearly nine pages, and higher standards should be applied. \n\n--------------------------------------\nI am satisfied with the rebuttal. Since the paper is now within the 8 pages limit, I would not apply a high standard and increase my score. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574996748473, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Reviewers"], "noninvitees": [], "tcdate": 1570237739345, "tmdate": 1574996748486, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review"}}}, {"id": "rke5xYS7iB", "original": null, "number": 3, "cdate": 1573243121774, "ddate": null, "tcdate": 1573243121774, "tmdate": 1573243121774, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "H1ggWiM15r", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment", "content": {"title": "Author Response to Reviewer #1", "comment": "We thank the reviewer for their comments. Our response to their concerns is below:\n\nWhile the reviewer is correct that our work does not propose any strictly new metrics, we would like to emphasize that discovering new metrics was not the goal of this work. Rather, our aim was to rigorously and exhaustively measure a number of network properties as they change over the initial portion of training in order to better understand why this period of learning is so critical and to elucidate the precise factors responsible for these changes. This is best done using existing metrics that are already meaningful to the reader.\n\nWhile novel algorithms and metrics are of course important, careful empirical work aiming to better understand these metrics and algorithms are equally important. This style of work is increasingly being recognized and sought by the community, as evidenced by several recent workshops and talks emphasizing rigorous empirical studies [1, 2, 3]. \n\nMoreover, we note that we have developed an entirely new scientific application for the lottery ticket framework as a means to investigate the early phase of training, and our experiments have several clear implications for a number of downstream investigations, including better pruning strategies and initializations. First, we found that, in contrast to previous work on smaller networks [4], reinitializing networks with random weights but maintained signs is not in fact sufficient for more commonly used deeper networks, such as ResNets. Second, we found that even after only a small number of iterations, the weight distributions are highly non-i.i.d., suggesting that approximating such a distribution to sample from for initialization, as was implied by initial work into the lottery ticket hypothesis, is unlikely to succeed (at least without jointly modeling the weights). Finally, we demonstrated that the changes which take place over the initial course of training *can* in fact be approximated by unsupervised pretraining, which suggests several avenues for improving training of compressed networks. We believe that all of these conclusions will likely be important for future investigations into ways to improve training. Reviewer 2 agreed that these conclusions will likely have impact on future network improvements, stating that \u201c...[our study] may motivate new neural network compression methods to be proposed.\u201d\n\nWhile the reviewer is correct to note that our experiments were only performed on CIFAR-10, primarily due to compute limitations (each line on each plot represents approximately 100 individual model trainings, making equally rigorous experiments on larger datasets prohibitively expensive), we emphasize that our conclusions were robust across five distinct network architectures: ResNet-20, ResNet-56, ResNet-18, Wide-ResNet-16-8, and VGG-13. While we were only able to include ResNet-20 in the main text due to space limitations, we have included plots for the other four networks for all experiments in the appendix section C. We were glad to see all three reviewers recognize the breadth of experiments, calling them \u201cextensive and detailed\u201d (R1), \u201ccarefully designed\u201d (R2), and \u201cexhaustive\u201d (R3). Therefore, while we agree with the reviewer that further work for other task types would be interesting, we believe it is beyond the scope of the present work.\n\n[1] Identifying and Understanding Deep Learning Phenomena, ICML 2019 workshop, http://deep-phenomena.org/\n\n[2] Science meets Engineering of Deep Learning, NeurIPS 2019 workshop, https://sites.google.com/view/sedl-neurips-2019/main\n\n[3] Ali Rahimi, Test of Time award talk, NeurIPS 2017, https://www.youtube.com/watch?v=Qi1Yry33TQE\n\n[4] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. NeurIPS, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkl1iRNFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1303/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1303/Authors|ICLR.cc/2020/Conference/Paper1303/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158072, "tmdate": 1576860553448, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment"}}}, {"id": "HkeEPdBXoS", "original": null, "number": 2, "cdate": 1573242972249, "ddate": null, "tcdate": 1573242972249, "tmdate": 1573242972249, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "SJlvCqBatr", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment", "content": {"title": "Author Response to Reviewer #2", "comment": "We thank the reviewer for the helpful suggestions to improve the clarity of our paper. We have addressed all stated concerns except for the table suggestion in the present revision. We intend to add a table summarizing results before the end of the discussion period. Below, we describe the changes we have made to the paper in detail.\n\n1. We are working on adding a summary table of results as \u201cAppendix A\u201d that will summarize each of the experiments we ran, each of the dependent variables, and the key takeaways. We agree that this will be very helpful for readers. We are still determining how exactly to structure this table as there are many experiments, and we will update the paper again once it is ready.\n\n2. While we understand why Figures 4 and 6 may seem at odds with one another, their relationship to one another is more nuanced based on which rewinding iteration is analyzed. We clarify the relationship below, and we have updated the second paragraph of Section 5.2 to make clear the differences.\n\nFor rewinding iteration 500, the magnitudes from iteration 500 are more important than the signs. In Figure 4 (left), having the magnitudes alone leads to improved performance (purple line), while having the signs alone leads to bad performance (green and red lines). In every experiment in Figure 6 (left), the magnitudes have been shuffled and performance correspondingly drops, even when fixing the signs. The sole exception is \u201cShuffle Filters,\u201d which, as Figure 8 shows, is a relatively weak perturbation.\n\nFor rewinding iteration 2000, the magnitudes from iteration 2000 are sufficient for improved performance, but the signs only lead to improved performance in limited settings. Namely, we only see this behavior when the signs are paired with the original initialization (red line in Figure 4 right) or with magnitudes from iteration 2000 (red and green lines in Figure 6 right). When paired with a random initialization (green line in Figure 4 right), performance is poor. \n\nOur interpretation is that the signs at iteration 2000 only lead to improved performance when the magnitudes are similar to their values in 2000, whether distributionally (the shuffle experiments) or in the values themselves (using the original initialization, whose values will be more similar than random values).\n\n3. The \u201cweight trace\u201d graph plots the values of ten randomly-selected weights over the first ten epochs of training. Some weights remain roughly constant while others vary wildly; since there was no general behavior, we did not analyze this graph in the text. We have updated the caption of Figure 3 to clarify the data that is presented in this graph.\n\n4. The \u201cinit\u201d and \u201cfinal\u201d referred to the L2 and cosine distance from the weights at each iteration to the initialization and to the final weights at the end of training. In other words, these metrics show how far the network has moved from its starting point (\u201cinit\u201d) and how far it still has to move to reach its converged point (\u201cfinal\u201d). We have updated the caption of Figure 3 to clarify the meaning of these terms.\n\nFinally, we have reduced the length of our paper so that it fits within 8 pages by moving the  experiments where we pretrain already-sparse networks from Section 6.4 to the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkl1iRNFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1303/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1303/Authors|ICLR.cc/2020/Conference/Paper1303/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158072, "tmdate": 1576860553448, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment"}}}, {"id": "HJlSyuSQjB", "original": null, "number": 1, "cdate": 1573242844997, "ddate": null, "tcdate": 1573242844997, "tmdate": 1573242844997, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "HJx_-RnntS", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment", "content": {"title": "Author Response to Reviewer #3", "comment": "We thank the reviewer for their helpful suggestions, and have updated our paper in response to their feedback and questions. We answer specific questions below:\n\nRegarding how our results can be used to find better lottery tickets, we argue that our results do in fact provide useful and efficient guidance for the lottery ticket hypothesis. However, this guidance tells us where *not* to look, rather than how to directly improve lottery tickets. Notably, our finding that weight distributions are highly non-i.i.d. after a small number of iterations suggests that approaches to approximate rewound winning tickets through sampling from an i.i.d. distribution are unlikely to bear fruit, and that future explorations of improvements to the lottery ticket hypothesis should focus on early pruning approaches. Furthermore, our finding that the early phase of training can be approximated by training without supervision by training on rotation prediction tasks (though at the cost of longer training times) suggests that a strategy to derive winning tickets using pretraining with less supervision may be possible. We included a paragraph discussing these implications at the end of Section 7. \n\nRegarding the legend in Figure 9, we thank the reviewer for catching this error! We have added legends to all graphs in Figure 9 (and in Figures A8-A11, which contain the same data for our other networks). The legend was indeed the same for all of the graphs, but we agree that including the legend on each plot improves readability.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkl1iRNFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1303/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1303/Authors|ICLR.cc/2020/Conference/Paper1303/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158072, "tmdate": 1576860553448, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Authors", "ICLR.cc/2020/Conference/Paper1303/Reviewers", "ICLR.cc/2020/Conference/Paper1303/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Comment"}}}, {"id": "HJx_-RnntS", "original": null, "number": 1, "cdate": 1571765759752, "ddate": null, "tcdate": 1571765759752, "tmdate": 1572972486460, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\n\nThis paper is dedicated to examining the changes that networks undergo during the early phase of the network training. The author conducts extensive measurements of the network state and its updates during the early iterations of training. Based on the observations, they find that: i) deep network is not robust to reinitialization with random weights, but maintained signs; ii) the distribution is highly non-i.i.d after the early phase of network training. This is why permuting weight dramatically harms performance. iii) the changes in the supervised networks are label-agnostic. The author claims these results can play an important role in explaining the network changes in the initial critical period.\n\nStrength Bullets:\n\n1. The paper performs exhaustive experiments in the early phase of network training. And it has some interesting implications for lottery tickets. i.e. to some extent, then signs from the rewinding iteration are sufficient to recover the damage caused by permutation.\n2. I am very like the analysis of whether weight distributions are i.i.d.. The results in the paper are aligned with my intuition that the weights in the early stage are highly dependent and they share some similarities in the distribution level. And weight in different training stages supposes in a different distribution. Networks aren't robust to these permutations. They also show that the perturbations can be roughly be approximated by adding Gaussian noise to network weights.s\n3. The author offers detailed results to analysis the data-dependence of the early phase of network training. The experiment organization is complete and convincing.\n\nWeakness Bullets:\n\n1. Although the paper gives extensive measurements and observations about the early phase about network training, it doesn't provide useful and efficient guidance for the lottery tickets hypothesis. In other words, the observation is interesting but the novelty is arguable. How can we use these \"implications\" to find better tickets? I mean it should be efficient. It is not reasonable cost more the find the best weight magnitude point or sign point for a better initialization.\n2. [Minor] The legend in figure 9 is missing. I guess it may be the same as the upper left. But it is still confusing and hard to read.\n\nRecommendation:\n\nI think this paper provides plenty of insightful observations. I still hope the author can solve the above weakness bullets. This is a weak accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574996748473, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Reviewers"], "noninvitees": [], "tcdate": 1570237739345, "tmdate": 1574996748486, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review"}}}, {"id": "H1ggWiM15r", "original": null, "number": 3, "cdate": 1571920632045, "ddate": null, "tcdate": 1571920632045, "tmdate": 1572972486369, "tddate": null, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "invitation": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Pros\n+ This work provided a good summary of observations and network properties that worth studying during the early stage of network\u2019s training.\n+ The authors conducted extensive and detailed experiments to study the statistics of weights and their gradients. Ablation studies also considered network\u2019s accuracy under perturbation of weight signs, weight shuffling, and different standard deviations.\n+ The authors also verified the effectiveness of weak labels used in self-supervised learning.\n\nCons\n- The work itself did not propose any new network properties or any new metric to measure. Most experiments are designed for previous observations and mostly for verification purpose. I am concern about the core motivation of this work, like to identify or solve any new problems, in addition to experimentally verify the observations during network\u2019s training.\n- The conclusion in this work is still very empirical: it remains uncertain whether in other vision tasks and with more complex networks (e.g. multi-branch network) these conclusions would hold."}, "signatures": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1303/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Early Phase of Neural Network Training", "authors": ["Jonathan Frankle", "David J. Schwab", "Ari S. Morcos"], "authorids": ["jfrankle@mit.edu", "dschwab@gc.cuny.edu", "arimorcos@gmail.com"], "keywords": ["empirical", "learning dynamics", "lottery tickets", "critical periods", "early"], "TL;DR": "We thoroughly investigate neural network learning dynamics over the early phase of training, finding that these changes are crucial and difficult to approximate, though extended pretraining can recover them.", "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.", "pdf": "/pdf/974ac3614f6b53c7b83ae7fc0141a2e6e24118b4.pdf", "paperhash": "frankle|the_early_phase_of_neural_network_training", "_bibtex": "@inproceedings{\nFrankle2020The,\ntitle={The Early Phase of Neural Network Training},\nauthor={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkl1iRNFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c28e45ecc1bf66a40e0ac7b00f6d0b2646353d03.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkl1iRNFwS", "replyto": "Hkl1iRNFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1303/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574996748473, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1303/Reviewers"], "noninvitees": [], "tcdate": 1570237739345, "tmdate": 1574996748486, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1303/-/Official_Review"}}}], "count": 8}