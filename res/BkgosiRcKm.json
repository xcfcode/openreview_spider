{"notes": [{"id": "BkgosiRcKm", "original": "HylXU3jqK7", "number": 650, "cdate": 1538087842577, "ddate": null, "tcdate": 1538087842577, "tmdate": 1545355396460, "tddate": null, "forum": "BkgosiRcKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxEA2PleV", "original": null, "number": 1, "cdate": 1544744140155, "ddate": null, "tcdate": 1544744140155, "tmdate": 1545354515309, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Meta_Review", "content": {"metareview": "This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond \u201cfilling the gap\u201d in the literature. \n\nAll reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. \n\nOverall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "New model but presentation and insights need to improve."}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper650/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353140077, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353140077}}}, {"id": "SJgR7X0th7", "original": null, "number": 1, "cdate": 1541165862474, "ddate": null, "tcdate": 1541165862474, "tmdate": 1544004935469, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "content": {"title": "DEEP RECURRENT GAUSSIAN PROCESS WITH VARIATIONAL SPARSE SPECTRUM APPROXIMATION", "review": "Overall Score: 7/10.\nConfidence Score: 7/10.\n\nDetailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations.\n\nRelated to: RNN models and Sparse Nystrom approximation.\n\nStrengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.\n\nWeaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists.\n\nDoes this submission add value to the ICLR community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies.\n\nQuality:\nIs this submission technically sound?: Yes it is.\nAre claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way.\nIs this a complete piece of work or work in progress?: Complete piece of work.\nAre the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest.\n\nClarity:\nIs the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before.\nIs it well organized?: Yes it is.\nDoes it adequately inform the reader?: Yes it is.\n\nOriginality:\nAre the tasks or methods new?: Yes, they are sound.\nIs the work a novel combination of well-known techniques?: Yes it is.\nIs it clear how this work differs from previous contributions?: Yes.\nIs related work adequately cited?: Yes, being a strength of the paper.\n\nSignificance:\nAre the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems.\nAre others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes.\nDoes the submission address a difficult task in a better way than previous work?: Yes I think.\nDoes it advance the state of the art in a demonstrable way?: Yes, empirically.\n\nArguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done\n\nArguments against acceptance: Clarity of the paper.\n\nMinor issues and typos:\n-> (V)SS not defined before being used.\n-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions.\n-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.\n-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.\n-> Q is not defined in section 3.1 paragraph 1.\n-> A valid covariance function must produce a PSD matrix, put that in section 3.1. \n-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).\n-> Section 3.4 statistics should be explained.\n\nReading thread and authors response rebuttal decision:\n=================================================\n\nI consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "cdate": 1542234411249, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335772229, "tmdate": 1552335772229, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJg6mhJi07", "original": null, "number": 9, "cdate": 1543334948816, "ddate": null, "tcdate": 1543334948816, "tmdate": 1543334948816, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "content": {"title": "Response to All Reviewer", "comment": "We are sorry for a 'typo' in the lower bound in Equation (15) regarding the expectation:\n\nActually one has to replace\n\nE_p_l[  sum_l   ...  ]\n\nwith\n\nlog(  prod_l  E_p_l[  ...  ]  )\n\nWe are sorry for this and will fix this in the next upload phase.\n\nThank you. Sincerely,\n\nThe authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612538, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper650/Authors|ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612538}}}, {"id": "ryeukb2_07", "original": null, "number": 8, "cdate": 1543188704225, "ddate": null, "tcdate": 1543188704225, "tmdate": 1543188704225, "tddate": null, "forum": "BkgosiRcKm", "replyto": "SJgR7X0th7", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We greatly appreciate your insightful feedback. We would like to respond to your comments and explain how we improved the manuscript.\n\nFirst of all, we recognized that you changed your rating from\n\n<<Rating: 7, Confidence: 3>>          to           <<Rating: 5: Confidence: 2>> ,\n\nwithout any further explanations and without waiting for our responses.\nWe do not know, what your concerns are, but we hope, we can address these with our answers now.\n\n<<Arguments against acceptance: Clarity of the paper.\n-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.>>\n\nThanks, we addressed this issue as good as possible.\n\n<<-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions. >>\n\nThank you for the advice. We reformulated the abstract. We added:\n\u201cOur approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases.\u201d \n\n\u201cFor the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists.\u201d\n\nAnd deleted\n\n\u201cThis case naturally collapses to a tractable expression by calculating the integrals. For the simple\nextension of the (V)SS approximation an optimal variational distribution exists. \u201c\n\n \u201cTraining is realized through optimizing the variational lower bounds.\u201d\n\n<<-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.\nMinor issues and typos: -> (V)SS not defined before being used. \n-> Q is not defined in section 3.1 paragraph 1.\n-> A valid covariance function must produce a PSD matrix, put that in section 3.1. >>\n\nThank you for pointing out these issues.\nWe see your point with the time horizon and simplified it to H, similar to our experiments.\nWe addressed the issue with (V)SS and also defined in the abstract the GP.\nWe added in Section 3.1: \n\u201c \u2026,Q \u2208 N the input-dimension,\u2026\u201d\n\u201cBe aware of that a valid covariance function must produce a positive definite matrix K NN\ndef= (k(x i ,x j )) N i,j=1 \u2208 R N\u00d7N , when filling in combinations of data-input points x i , i = 1,...,N.\u201d\n\n<<-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).>>\n\nYou are correct that the LHS depends on U and we have to make this point clear to the reader. Additionally, the LHS depends on further variables like L, p, b. etc. Nevertheless, we decided to highlight U just in p(y|a,Z,U,X) in the integral. On the one hand, our approximations are built on U in the following sections (c.f. Section 3.3.) and on the other hand we want to be notationally conform with Gal (2016), Section 3. Having said this, for clarity we added: \n\u201c\u2026highlighting U just in the integral, to be notationally conform to Gal & Turner (2015), Section 3.\u201d\n\n <<-> Section 3.4 statistics should be explained.>>\n\nThank you for the advice. We added: \n\u201cThese statistics are essentially the given matrices \u03a6, \u03a6 T \u03a6, K_MN K_NM from the beginning, but every input h_i and every spectral point z_m are now replaced by a mean \u00b5_i , \u03b1_m and a variance \u03bb_i , \u03b2_m resulting in matrices of the same size. The property of positive definiteness is preserved.\u201d\n\nThank you. Sincerely,\n\nThe authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612538, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper650/Authors|ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612538}}}, {"id": "Hyxwdyn_C7", "original": null, "number": 7, "cdate": 1543188334670, "ddate": null, "tcdate": 1543188334670, "tmdate": 1543188334670, "tddate": null, "forum": "BkgosiRcKm", "replyto": "rJgI340tn7", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We greatly appreciate your insightful feedback. We would like to respond to your comments and explain how we improved the manuscript.\n\n<<For the first contribution stated by the authors, what are the theoretical and practical implications of the different regularization terms/properties \u2026>>\n\nThank you for asking for details. The practical implications are, that the GP is regularized during optimization when optimizating over U. These parameters U have, following Gal & Turner (2015), the same properties as in the Nystr\u00f6m case, but in the lower bound in (8) they are simply used without being linked to the weights a.\nWhat we can show, is, that we can further marginalize the integral in (7) to a Gaussian with mean 0 and covariance matrix\nK = K_NN + (\u03a6 (K_MM)^-1 \u03a6^T - K_NM (K_MM)^-1 K_MN) + \u03c3_noise^2.\nWe see that we have the true covariance matrix plus a discrepancy of the sparse approximations of the Sparse Spectrum and the Nystr\u00f6m covariance matrix plus noise. Therefore, our approximation can be seen as a trade-off between these two sparse approximations.\nFor now we included these insights in Section 3.3. \nWe further fixed a typo in the prior assumption and added the noise. We also made a distinction for the IP case in Section 4.2.\n\n<<Can the authors provide a detailed derivation of DVI for equation 13 as well as for the predictive distributions in Section 6.3.5? >>\n\nPlease refer to Section 6.3.6. and to the end of Section 6.3.5. for the detailed derivation.\n\n<<Can the authors provide a time complexity analysis of all the tested deep recurrent GPs? >>\n\nSince the different implementations have different evolution states with regard to optimization, we decided against a time complexity analysis, since we think it might be misleading. But we agree, that your question is of interest and should be done in an extra work.\n\n<<Would the authors' proposed approach be able to extend the framework of Hoang et al. (2017) (see below) \u2026 >>\n\nThank you for making us aware of this paper. We do not see any problems. We added a sentence in Section 5.3.\n\n<<Minor issues: Just below equation 6, equation 9, \u2026 need to decide whether to italicize their notations in bold or not. >>\n\nThank you for the advice. We consciously decided to use italic and bold. We have to make this point clear to the reader.  To make this clearer to the reader, we added an explanation in beginning of Section 3. We hope this will meet your concerns. We fixed an issue for the expectation in Section 3.1, write it in bold as in the entire paper and also write f_x instead of f(x) for the random variable at x.\n\n<<Equations are not properly referenced in a number of instances. >>\n<<The authors have used their commas too sparingly, which makes some sentences very hard to parse. >>\nThanks for your comment. We tried to address these issues as good as possible.\n\n<<What is the difference between REVARB-(V)SS(-IP), DRGP-(V)SS(-IP), and DRGP-VSS-IP?\nPage 5: DRGP-SSGP, -VSSGP, -SSGP-IP, -VSSG-IP>>\n\nREVARB-(V)SS(-IP) is the name of the lower bounds, overall four cases.\nDRGP-(V)SS(-IP) is the name of the corresponding models/methods.\n\nWe changed beginning of Section 4. and beginning of Section 4.2. to be more clear.\n\n<<Equation 7: LHS should be conditioned on U. >>\n\nYou are correct that the LHS depends on U and we have to make this point clear to the reader. Additionally, the LHS depends on further variables like L, p, b. etc. Nevertheless, we decided to highlight U just in p(y|a,Z,U,X) in the integral. On the one hand, our approximations are built on U in the following sections (c.f. Section 3.3.) and on the other hand we want to be notationally conform with Gal (2016), Section 3. Having said this, for clarity we added:\n\u201c\u2026, highlighting U just in the integral, to be notationally conform to Gal & Turner (2015), Section 3.\u201d\n\n<<Page 4: (V)SSGP does not have the same...\nEquation 8: q_a and q_Z should be placed next to the expectation.\nPage 4: choosen? \nPage 5: will makes it possible? >>\n\nThank you for pointing out all those issues, we addressed all of them.\n\n<<Page 5: to simplify notation, we write h^{L+1}_{Hx+1:} = y_{Hx+1:}?\nSuch a notation does not look simplified. >>\n\nThe simplification comes from the special role of the last layer, where h^(L+1) corresponds with y. In order to avoid these notational issues throughout the paper, we introduced the respective notation (see the joint density and Equations 14, 15). Anyhow, we changed the position of the simplification right below the joint density.\n\n<<Equation after equation 12: On LHS, should U^(l) be a random variable? >>\n\nWe did not define any prior or variational distribution on these parameters, but the standard notation is, to assume it is. We also write f_X |X, where e.g. over X no distribution is defined.\n\n<<Page 17: Should the expressions begin with >=?>>\n\nYes, in order to highlight that they are lower bounds.\n\nThank you. Sincerely,\n\nThe authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612538, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper650/Authors|ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612538}}}, {"id": "S1lc23s_Am", "original": null, "number": 6, "cdate": 1543187633854, "ddate": null, "tcdate": 1543187633854, "tmdate": 1543187633854, "tddate": null, "forum": "BkgosiRcKm", "replyto": "HJeVgHjqh7", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We greatly appreciate your insightful feedback. We would like to respond to your comments and explain how we improved the manuscript.\n\n<<Most (if not all) of the technical developments in the paper are straightforward applications \u2026 technical contribution of the paper is largely incremental.>>\n\nYou are correct that many of the steps we take are incremental and we face the same difficulties as in Titsias & Lawrence (2010), Gal & Turner (2015) and Mattos et al. (2016). Nevertheless, we think that the combination of these methods is new, as well the integration of the input space for spectral covariance function, which is not straightforward. Also, the combination of (V)SSGP with the regularization property of Titsias & Lawrence (2009);(2010) is not clear from the start. All in all, we hope that the sum of all these steps gives a valuable contribution to the community.\n\n<<Furthermore, while it is sensible to use random-feature approximation approaches (such as SS and VSS), \u2026 and from my perspective, having a prior conditioned on the inducing variables lacks any theoretical motivation. >>\n\nThese parameters U have, following Gal & Turner (2015), the same properties as in the Nystr\u00f6m case, but in the lower bound in (8) they are simply used without being linked to the weights a.\nWhat we can show, is, that we can further marginalize the integral in (7) to a Gaussian with mean 0 and covariance matrix\nK = K_NN + (\u03a6 (K_MM)^-1 \u03a6^T - K_NM (K_MM)^-1 K_MN) + \u03c3_noise^2.\nWe see that we have the true covariance matrix plus a discrepancy of the sparse approximations of the Sparse Spectrum and the Nystr\u00f6m covariance matrix plus noise. Therefore, our approximation can be seen as a trade-off between these two sparse approximations.\nFor now we included these insights in Section 3.3. \nWe further fixed a typo in the prior assumption and added the noise. We also made a distinction for the IP case in Section 4.2.\n\n<<The empirical results are a bit of a mixed bag, \u2026, it will be good to have some insights into when the proposed methods are expected to be better than their competitors. >>\n\nOf course you are correct. Our approach does not beat all corresponding benchmarks. The question, what method should be used in what setting is of great interest and we want to address this topic in future work. Currently we cannot characterize situations, when the new methods are better. Nevertheless, we hope that our work shows the capabilities of our approach and therefore is of interest.\n\n<<While the proposed method is motivated from an uncertainty propagation perspective, \u2026 predictive posterior distributions. What is the point of using GPs otherwise? >>\n\nWe compared our predictive posterior distribution with the one of Mattos et al. (2016). We implemented his version by ourselves and did not have the same problems with variance predictions as in his paper Mattos et al. (2016) Figure 2, (l) (we therefore think that it might have been an implementation issue). \nAll in all we think, that the variance predictions are equally good and therefore we concentrated on the RSME comparison. We added two more sentences in Section 5.2 regarding this.\n\n<<It is unnecessary to cite Bishop to explain how one obtains a marginal distribution. >>\n\nYes, it is straightforward. We deleted the reference before Equation 7, but we think that it might be of interest for a reader unfamiliar with the topic, and so we kept the reference in Section 3.3.\n\n<<Would it be possible, to use the work of Cutajar et al (2017), \u2026 If so, why aren\u2019t the authors comparing to this? >>\n\nThank you for the input. We added the experiments for this DGP with NARX structure for the first layer to our paper. \n\n<<I recommend the authors use the notation p(v) = \u2026 and q(v) = \u2026 everywhere rather than v ~ \u2026>>\n\nThank you for your advice. We agree that the notation p(v)=... is widely used. Nevertheless, we prefer to be mathematically more precise and differentiate between the random variable v (italic) and the realizations/samples v (upright) and therefore we use the notation p_v, where v ~ N(m,v). We agree, that our paper was not very specific in highlighting that point. To make this clearer to the reader, we added an explanation in beginning of Section 3. We hope this will meet your concerns.\n\n<<The analysis of Figure 1 needs expanding. >>\n\nThank you for the advice. We added the following explanation and hope that we meet your concerns: \n\u201dWe initialize the states with the output training-data for all layers with minor noise (first column) and after training we obtain a trained state (second column).\u201d\n\n<<What are the performance values obtained with a standard recurrent neural net / LSTM? >>\n\nWe agree, that a comparison with those methods is of interest and we therefore added the results of Al-Shedivat et al. (2017) and our own results for standard RNN and LSTM. We further added also the results, where we deleted the auto-regressive part in the first layer for GP-LSTM.\n\nThank you. Sincerely,\n\nThe authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612538, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper650/Authors|ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612538}}}, {"id": "BJgt7uoO07", "original": null, "number": 5, "cdate": 1543186464761, "ddate": null, "tcdate": 1543186464761, "tmdate": 1543186464761, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "content": {"title": "Response to All Reviewer", "comment": "We greatly appreciate all your insightful feedback. We would like to respond to your comments in general and explain how we improved the manuscript.\nIn particular, we included three more references:  Eleftheriadis et al. (2017) for SSM models, (Rahimi & Recht, 2008) for the case of Random Fourier Features, Hoang et al. (2017) for a generalization of the (V)SSGP.\nMore methods in the experiments are included, the RNN, the LSTM and the DGP-RFF from Cutajar et al. (2016).\nWe improved the appendix.\nFurthermore, we added the detailed derivation of DVI in the appendix.\nWe further changed the appearance of the density for all expectations, bringing it to the front, for more readability.\nThere was a discussion about our notation (upright, bold and italic), therefore we added at the beginning of Section 3. an explanation.\nIn Section 3.3 we explain more in detail how the combination with the IP case makes sense from the theoretical point and hope this will meet your concerns.\nIn Section 3.4 we explain the upcoming statistics more in detail.\nWe further improved many language issues, punctuation problems and hope the paper improved its clarity.\n\nDetails are explained in the individual responses to each referee report.\n\nThank you. Sincerely,\n\nThe authors.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612538, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgosiRcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper650/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper650/Authors|ICLR.cc/2019/Conference/Paper650/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers", "ICLR.cc/2019/Conference/Paper650/Authors", "ICLR.cc/2019/Conference/Paper650/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612538}}}, {"id": "rJgI340tn7", "original": null, "number": 2, "cdate": 1541166254234, "ddate": null, "tcdate": 1541166254234, "tmdate": 1541533807540, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "content": {"title": "Combination of known ideas, hard to read", "review": "This paper proposes deep recurrent GP models based on the existing DRGP framework, two works on sparse spectrum approximation as well as that of inducing points. In these models, uncertainty is propagated by marginalizing out the hidden inputs at every layer.\n\nThe authors have combined a series of known ideas in the proposed work. There is a serious lack of discussion or technical insights from the authors for their technical formulations: in particular, what are the non-trivial technical challenges addressed in the proposed work? Furthermore, the authors are quite sloppy in referencing equations and inconsistent in the use of their defined notations and acronyms. I also find it hard to read and understand the main text due to awkward sentence structures.\n\nHave the authors revealed their identity on page 2 of the paper? I quote: \"We refer to the report Foll et al. (2017) for a detailed but preliminary formulation of our models and experiments.\" and \"DRGP-(V)SS code available from http://github.com/RomanFoell/DRGP-VSS.\"\n\n\n\nDetailed comments are provided below:\n\nFor the first contribution stated by the authors, what are the theoretical and practical implications of the different regularization terms/properties between the lower bounds in equations 10 vs. 8? These are not described in the paper.\n\nCan the authors provide a detailed derivation of DVI for equation 13 as well as for the predictive distributions in Sectio 6.3.5?\n\nCan the authors provide a time complexity analysis of all the tested deep recurrent GPs?\n\n\nWould the authors' proposed approach be able to extend the framework of Hoang et al. (2017) (see below) that has generalized the SS approximation of Lazaro-Gredilla et al. (2010) and the improved VSS approximation of Gal & Turner (2015)?\n\nHoang, Q. M.; Hoang, T. N.; and Low, K. H. 2017. A generalized stochastic variational Bayesian hyperparameter learning framework for sparse spectrum Gaussian process regression. In Proc. AAAI, 2007\u20132014.\n\n\n\nMinor issues:\nJust below equation 6, equation 9, and throughout the entire paper, the authors need to decide whether to italicize their notations in bold or not.\n\nEquations are not properly referenced in a number of instances.\n\nThe authors have used their commas too sparingly, which makes some sentences very hard to parse.\n\nWhat is the difference between REVARB-(V)SS(-IP), DRGP-(V)SS(-IP), and DRGP-VSS-IP?\n\nEquation 7: LHS should be conditioned on U.\nPage 4:  (V)SSGP does not have the same...\nEquation 8: q_a and q_Z should be placed next to the expectation.\nPage 4: choosen?\nPage 5: will makes it possible?\nPage 5: DRGP-SSGP, -VSSGP, -SSGP-IP, -VSSG-IP?\nPage 5: to simplify notation, we write h^{L+1}_{Hx+1:} = y_{Hx+1:}? Such a notation does not look simplified.\n\nEquation after equation 12: On LHS, should U^(l) be a random variable?\n\nPage 17: Should the expressions begin with >=?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "cdate": 1542234411249, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335772229, "tmdate": 1552335772229, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJeVgHjqh7", "original": null, "number": 3, "cdate": 1541219564059, "ddate": null, "tcdate": 1541219564059, "tmdate": 1541533807334, "tddate": null, "forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "content": {"title": "A combination of existing sparse-spectrum techniques and deep recurrent Gaussian processes but not properly justified", "review": "This paper addresses the problem of modeling sequential data based on one of the deep recurrent Gaussian process (DRGP) structures proposed by Mattos et al (2016). This structure acts like a recurrent neural net where every layer is defined as a GP. One of the main limitations of the original method proposed by Mattos et al (2016) is that it is limited to a small set of covariance functions, as the variational expectations over these have to be analytically tractable.\n\nThe main contributions of this paper are the use of previously proposed inference, namely (i) the sparse spectrum (SS) of Lazaro-Gredilla et al (2010); its variational improvement by Gal and Turnner (2015) (VSS);  and the inducing-point (IP) framework of Titsias and Lawrence (2010) into the recurrent setting of Mattos et al (2016). Most (if not all) of the technical developments in the paper are straightforward applications of the results in the papers above. Therefore, the technical contribution of the paper is largely incremental. Furthermore, while it is sensible to use random-feature approximation approaches (such as SS and VSS) in GP models, it is very unclear why combining the IP framework with SS approaches makes any sense at all. Indeed, the original IP framework was motivated as a way to deal with the scalability issue in GP models, and the corresponding variational formulation yielded a nice property of an additional regularization term in the variational bound. However, making the prior over a (Equation 9) conditioned on the inducing variables U is rather artificial and lacks any theoretical justification. To elaborate on this, in the IP framework both the latent functions (f in the original paper) and the inducing inputs come from the same GP prior, hence having a joint distribution over these comes naturally. However, in the approach proposed in this paper, a is a simple prior over the weights in a linear-in-the-parameters model, and from my perspective, having a prior conditioned on the inducing variables lacks any theoretical motivation. \n\nThe empirical results are a bit of a mixed bag, as the methods proposed beat (by a small margin) the corresponding benchmarks on 6 out of 10 problems. While one would not expect a proposed method to win on all possible problems (no free lunch), it will be good to have some insights into when the proposed methods are expected to be better than their competitors. \n\nWhile the proposed method is motivated from an uncertainty propagation perspective, only point-error metrics (RMSE) are reported. The paper needs to do a proper evaluation of the full predictive posterior distributions. What is the point of using GPs otherwise?\n\nOther comments:\nI recommend the authors use the notation p(v) = \u2026 and q(v) = \u2026 everywhere rather than v ~ \u2026 as the latter may lead to confusion on how the priors and the variational distributions are defined. \nIt is unnecessary to cite Bishop to explain how one obtains a marginal distribution\nWould it be possible to use the work of Cutajar et al (2017), who use random feature expansions for deep GPs,  in the sequential setting? If so, why aren\u2019t the authors comparing to this?\nThe analysis of Figure 1 needs expanding \nWhat are the performance values obtained with a standard recurrent neural net / LSTM?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper650/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation", "abstract": "Modeling sequential data has become more and more important in practice. Some applications are autonomous driving, virtual sensors and weather forecasting. To model such systems, so called recurrent models are frequently used. In this paper we introduce several new Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) and the improved version, called Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the recurrent structure given by an existing DRGP based on a specific variational sparse Nystr\u00f6m approximation, the recurrent Gaussian Process (RGP). Similar to previous work, we also variationally integrate out the input-space and hence can propagate uncertainty through the Gaussian Process (GP) layers. Our approach can deal with a larger class of covariance functions than the RGP, because its spectral nature allows variational integration in all stationary cases. Furthermore, we combine the (Variational) Sparse Spectrum ((V)SS) approximations with a well known inducing-input regularization framework. For the DRGP extension of these combined approximations and the simple (V)SS approximations an optimal variational distribution exists. We improve over current state of the art methods in prediction accuracy for experimental data-sets used for their evaluation and introduce a new data-set for engine control, named Emission.", "keywords": ["Deep Gaussian Process Model", "Recurrent Model", "State-Space Model", "Nonlinear system identification", "Dynamical modeling"], "authorids": ["foell@mathematik.uni-stuttgart.de", "haasdonk@mathematik.uni-stuttgart.de", "markus.hanselmann@etas.com", "holger.ulmer@etas.com"], "authors": ["Roman F\u00f6ll", "Bernard Haasdonk", "Markus Hanselmann", "Holger Ulmer"], "TL;DR": "Modeling time-series with several Gaussian Processes in a row via a specific Variational Sparse Spectrum Approximation", "pdf": "/pdf/37b4cfef6eb4b737fdfdc6707ba01fa59cfe44be.pdf", "paperhash": "f\u00f6ll|deep_recurrent_gaussian_process_with_variational_sparse_spectrum_approximation", "_bibtex": "@misc{\nf\u00f6ll2019deep,\ntitle={Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation},\nauthor={Roman F\u00f6ll and Bernard Haasdonk and Markus Hanselmann and Holger Ulmer},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgosiRcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper650/Official_Review", "cdate": 1542234411249, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgosiRcKm", "replyto": "BkgosiRcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper650/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335772229, "tmdate": 1552335772229, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper650/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}