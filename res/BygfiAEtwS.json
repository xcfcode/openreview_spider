{"notes": [{"id": "BygfiAEtwS", "original": "BygbgWKOPH", "number": 1311, "cdate": 1569439386160, "ddate": null, "tcdate": 1569439386160, "tmdate": 1577168227318, "tddate": null, "forum": "BygfiAEtwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5yjKtkaCxW", "original": null, "number": 1, "cdate": 1576798720115, "ddate": null, "tcdate": 1576798720115, "tmdate": 1576800916448, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network.  The reviewers were unanimous in their opinion that the paper should not be accepted to ICLR in its current form.  A main concern is that the proposed method shows improvement over a relatively weak base system.  Although the author response proposed to include additional analysis, but the reviewers felt that without the additional analysis already included it was not possible to change the overall review score.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713972, "tmdate": 1576800263713, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Decision"}}}, {"id": "r1lK0BNojB", "original": null, "number": 5, "cdate": 1573762512717, "ddate": null, "tcdate": 1573762512717, "tmdate": 1573762512717, "tddate": null, "forum": "BygfiAEtwS", "replyto": "B1xmPmNjir", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment", "content": {"title": "The paper needs a revision", "comment": "Thank you for your response. It seems like we mostly agree, but without seeing the additional analyses you promised I cannot change my assessment. So I would suggest revising the paper accordingly and re-submitting the revision in the future."}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfiAEtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1311/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1311/Authors|ICLR.cc/2020/Conference/Paper1311/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157946, "tmdate": 1576860556331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment"}}}, {"id": "BkxA2mEoir", "original": null, "number": 4, "cdate": 1573761974392, "ddate": null, "tcdate": 1573761974392, "tmdate": 1573761974392, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Response to Reviewer #2\n\nThank you for the detailed feedback and careful review. Below we address your specific comments:\n\n\u201cHowever, the proposed solution of just integrating an additional detection task branch within the Siamese tracking architecture is naive. The main idea of integrating instance driven detection as an auxiliary task is borrowed from [1]\u201d\n\nJust to clarify, we do not claim that this is the first work to propose conditioning a full-frame detector on an exemplar image (as in [1], which we cite in our work). Our claim is that this is the first work to incorporate it in a tracking-specific architecture, trained end-to-end alongside a traditional center-crop Siamese tracking network. We would also like to highlight that the addition of the detector branch is simply to improve and regularize the latent representations used by the Siamese tracker. In this sense, the motivation for instance conditioned image detection is very different to those of [1] (which propose doing so to improve detector performance).\n\n\u201cSome recent works, such as [2, 3] have also investigated a similar problem of richer object representations for deep visual tracking. These approaches are desired to be discussed and empirically compared in order to fully validate the strength of the proposed approach.\u201d\n\n\nThank you for the additional related work. We will cite these papers in the next version of the manuscript. While, discriminative (non Siamese cross-correlation based) trackers were outside the scope of the initial manuscript, it appears that inference code for DiMP has recently been released. As such, we will include perturbation analysis results for subsequent versions.\n\n\n\u201cTracking datasets, such as VOT and OTB, provide additional analysis tools (i.e., attribute analysis) to thoroughly evaluate visual trackers. ... How does the proposed approach fare, compared to SOTA, on the subset of VOT image sequences that are labeled with occlusion?\u201d\n\nWe thank the reviewer for this suggestion. We will include analysis correlating performance and characteristic to the included labels of VOT and OTB.\n\n\u201cThe reviewer does not fully agree with this statement. A comprehensive empirical evaluation is crucial to fully access the merits of the contributions. State-of-the-art visual object trackers [2, 3, 4] achieve competitive tracking performance while being computationally efficient and fast. Therefore, a proper state-of-the-art comparison is desired to compare the proposed tracker with SOTA methods.\u201d \n\nAs per our comment to Reviewer #3 and as we described in the paper, we choose specifically to not focus on SoTA results for VOT2018 and OTB but rather instead focus on analysis of Siamese trackers and the tracker performance when including full-frame image detection as an auxiliary loss. Note that the absolute number of failures between SiamRPN++ and this work is 7 (50 vs 57 failures out of 21356 frames). We believe this places our work sufficiently close to SoTA to make our experimental results relevant and interesting to the community.\n\n\u201cThe reviewer recommends to perform additional experiments on other large-scale datasets, such as TrackingNet [5] and Lasot [6]\u201d\n\nWe will include results on these datasets in the camera ready version of the paper if accepted. Note that analysis of tracker dynamics will be performed on the validation set of TrackingNet."}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfiAEtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1311/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1311/Authors|ICLR.cc/2020/Conference/Paper1311/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157946, "tmdate": 1576860556331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment"}}}, {"id": "B1xmPmNjir", "original": null, "number": 3, "cdate": 1573761882864, "ddate": null, "tcdate": 1573761882864, "tmdate": 1573761882864, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Response to Reviewer #1\n\nThank you for your insightful review. We address your concerns below:\n\n\"- Makes claims about Siamese trackers in general but only experiments with authors\u2019 architecture\u201d.\n\nSiamRPN++ was open sourced only recently and therefore we could not run analysis directly on their architecture. We will do so for follow-up revision of the paper. Our architecture is similar to theirs, and initial experiments using SiamRPN++ suggests that the conclusions will transfer. The main difference between the two architectures is that we do not use multiple scales/aspect ratios anchors to generate our bounding box proposals as we found that it did not help performance. Indeed, we tried to reproduce their results exactly but could not get the stated results on OTB or VOT. \n\nWe agree with the reviewer that the hypothesis needs to be tested across other Siamese trackers as well. \n\n\"- Analyses in Figs. 6\u20138 not very helpful/conclusive\u201d\n\nPerhaps the results in Figs. 6-8 should be stated more clearly in the text. The aim of those figures was to show that the performance gained by adding the detector is attributed to learning more robust target representations. The aim of Figs. 6-7 is to show that the response to spatial perturbations between the models trained with and without the detector are virtually the same. Figure 8 shows a small, but stable improvement in the IoU when more abstract (temporally older) targets are fed into the network. \n\n\u201c- No test case where capturing appearance is important for tracking and authors\u2019 approach helps\u201d\n\nThanks for the helpful suggestion. We will attempt to perform such analysis for the final manuscript if accepted. \n\n\u201c- No comparison of tracker trained without detection objective on real vs. random targets\u201d\n\nWe agree with the reviewer that such a model would be a useful baseline. A model trained end-to-end with random targets would learn to track as well as possible without making use of the target. Nevertheless, some of our experiments show that even models trained with standard target schemes learn to rely on the target in an \u201cadaptive\u201d fashion. Specifically, Figure 7 shows the IoU performance increases when the target is moved completely outside the receptive field of the network. \n\nThank you for the edit suggestions listed under \u201cminor comments\u201d. We will address these in the next version of the manuscript."}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfiAEtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1311/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1311/Authors|ICLR.cc/2020/Conference/Paper1311/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157946, "tmdate": 1576860556331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment"}}}, {"id": "HJx5eQNjiB", "original": null, "number": 2, "cdate": 1573761778246, "ddate": null, "tcdate": 1573761778246, "tmdate": 1573761778246, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Response to Reviewer #3\nThank you for your insightful review. \nWe would like to first clarify a point: Our claim is not that Siamese trackers \u201c...blindly predict the center of a search window as target location\u2026\u201d but rather they have a tendency to predict the bounding box corresponding to the most salient object in the search window with some center bias. Indeed the strategy learned by the network of when to rely on the template and when to rely on saliency can be quite complicated as depicted by the experiment in Figure 7. \n\n1 - The performance of a given tracker is not only dependent on the network output but also on post-processing. Namely the proposals generated by the network are selected according to hand-crafted heuristics. The representations learned by Siamese trackers are typically independent of the way the proposals are selected (certainly the case for SiamRPN++).The focus of this work is on analyzing the representations learned by the Siamese networks in these trackers. We will make this point clearer in the text. Our tracker results in only 7 more failures than SiamRPN++ (57 vs 50) on a total of 21356 frames. We believe this result to be sufficiently close to SoTA for our results to be considered relevant.\n\n2 - Perhaps we should have made this clearer in the paper, but the purpose of Figures 6 & 7 is precisely to show that the detection task has little to no effect on reducing spatial bias. Where it does have an effect is on promoting more abstract learned target representations (e.g. temporally stale targets) as depicted in Figure 8. A small gain in average IoU can lead to a considerable gain in robustness (Figure 5). Please note that the legend was swapped in the original version of the paper, see our comment on OpenReview. \n\n3 - We agree with the reviewer that additional analysis may provide further insights. Nevertheless, please note that the results illustrating the effect of the detector robustness (e.g. Figure 5) is already more statistically significant than what is typically reported in tracking literature. This is particularly important on datasets without standard train/validation splits such as VOT.  Specifically, we show robustness with error bars throughout training and not merely a final robustness value. What type of additional analysis would the reviewer suggest? In subsequent versions of the paper, we will also include additional perturbation analysis results for the open source version of the SiamRPN++ tracker in the final manuscript (early experimental results show SiamRPN++ has similar characteristics to our tracker - which is unsurprising given that the architectures of both trackers are both Siamese Correlation-based networks). We will also include a similar analysis on for a curated set of discrete events (e.g. saliency bias for frames labeled to contain occlusion)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfiAEtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1311/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1311/Authors|ICLR.cc/2020/Conference/Paper1311/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157946, "tmdate": 1576860556331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment"}}}, {"id": "S1lT_rjdtr", "original": null, "number": 1, "cdate": 1571497333326, "ddate": null, "tcdate": 1571497333326, "tmdate": 1572972485399, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper investigates representations learned by Siamese trackers. The paper argues that existing trackers rely on saliency detection despite being designed to track by template matching in feature space. An auxiliary detection task is proposed to induce stronger target representations in order to improve tracking performance. Experiments are performed on VOT2018 tracking dataset.\n\nThe paper investigates an interesting and active research problem of stronger object representations for deep visual object tracking. However, the proposed solution of just integrating an additional detection task branch within the Siamese tracking architecture is naive. The main idea of integrating instance driven detection as an auxiliary task is borrowed from [1].  [1] also utilizes a Siamese architecture that is similar to the ones generally used in visual object tracking to localize particular instances of objects. Therefore, the novelty of the proposed tracking approach is limited.\n\nSome recent works, such as [2, 3] have also investigated a similar problem of richer object representations for deep visual tracking. These approaches are desired to be discussed and empirically compared in order to fully validate the strength of the proposed approach. \n\nThe paper shows some qualitative analysis. However, most of it is limited to just few frames of an image sequence. Tracking datasets, such as VOT and OTB, provide additional analysis tools (i.e., attribute analysis) to thoroughly evaluate visual trackers. Such analysis is missing in the paper. For instance, the main argument of this paper is that current approaches rely on center saliency and likely struggle in the presence of occlusion. How does the proposed approach fare, compared to SOTA, on the subset of VOT image sequences that are labeled with occlusion?\n\nOn page 3, it is stated that \"Our model uses a lightweight backbone network (MobileNetV2), and is somewhat simpler than recent state-of-the-art models  .....................  Although the model doesn\u2019t outperform state-of-the-art, it attains competitive performance.\" The reviewer does not fully agree with this statement. A comprehensive empirical evaluation is crucial to fully access the merits of the contributions. State-of-the-art visual object trackers [2, 3, 4] achieve competitive tracking performance while being computationally efficient and fast. Therefore, a proper state-of-the-art comparison is desired to compare the proposed tracker with SOTA methods. Further, currently experiments are only performed on the VOT2018 dataset. The reviewer recommends to perform additional experiments on other large-scale datasets, such as TrackingNet [5] and Lasot [6] and compare the performance with SOTA methods that are also investigating the problem of richer object representations for tracking. \n\n[1] Phil Ammirato, Cheng-Yang Fu, Mykhailo Shvets, Jana Kosecka, Alexander C. Berg: Target Driven Instance Detection. CoRR abs/1803.04610 (2018).\n[2] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg: ATOM: Accurate Tracking by Overlap Maximization. CVPR 2019.\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte: Learning Discriminative Model Prediction for Tracking. CoRR abs/1904.07220 (2019).\n[4] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan: SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks. CVPR 2019.\n[5] Matthias M\u00fcller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi, Bernard Ghanem: TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild. ECCV  2018.\n[6] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling:\nLaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking. CVPR 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834332313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Reviewers"], "noninvitees": [], "tcdate": 1570237739229, "tmdate": 1575834332325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review"}}}, {"id": "rJlGv0Watr", "original": null, "number": 2, "cdate": 1571786329950, "ddate": null, "tcdate": 1571786329950, "tmdate": 1572972485363, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper examines the performance of Siamese single-object trackers. The authors claim that state-of-the-art Siamese trackers mostly rely on saliency detection in the center of the search window while ignoring the target instance representation and propose an additional object detection branch during training to mitigate this effect.\n\nStrengths:\n+ Analysis how perturbation target and search image influence tracking performance\n+ Demonstrates that including detection objective during training improves performance\n\nWeaknesses:\n- Makes claims about Siamese trackers in general but only experiments with authors\u2019 architecture\n- Analyses in Figs. 6\u20138 not very helpful/conclusive\n- No test case where capturing appearance is important for tracking and authors\u2019 approach helps\n- No comparison of tracker trained without detection objective on real vs. random targets\n\nThe paper is well motivated and has a clear hypothesis. However, its execution unfortunately leaves a lot of room for improvement and getting it into acceptable shape would require a major revision. Some details on my main criticisms:\n\nClaim is way too general. The authors state that Siamese trackers in general suffer from the center bias problem. However, the authors do not analyze any other trackers than their own one. I understand that training a state-of-the-art tracker with the additional object detection branch could possibly require resources beyond what\u2019s available to the authors. However, it\u2019s not clear to me why the authors do not use the pre-trained version of at least one or two existing trackers to demonstrate their shortcomings (like Fig. 4)\n\nAnalyses in Fig. 6\u20138 are not very helpful. The only real effect the detector shows in Figs. 6\u20138 is a tiny improvement in IoU in Fig. 8 \u2013 otherwise the analyses neither support the claim nor do they reveal why detection as an additional objective actually helps. To establish that there is a center bias and trackers ignore the appearance term, the following two analyses could be done:\n\n- Identify a set of test cases where capturing appearance is important (e.g. temporary occlusion) and demonstrate that your approach improves performance on these cases.\n\n- For the table in Fig. 5, also show how a conventional tracker (i.e without object detection branch) performs on random targets. If the hypothesis is correct that the object detection objective during training reduces center bias and increases reliance on appearance, then a conventional tracker should show a smaller reduction due to random targets than your improved tracker does. The same quantitative analysis could also be done for existing state-of-the-art trackers, as it does not require training. \n \nMinor Comments:\n- Caption of Fig. 4: is instead of if?\n- Figure 5\n\t- y-axis label: lower is better despite \u201crobustness\u201d suggesting the opposite\n- Why do the numbers in the figure not match those in the table?\n- What is the center bias baseline, i.e. what is the performance if center of search image is predicted without any network?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834332313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Reviewers"], "noninvitees": [], "tcdate": 1570237739229, "tmdate": 1575834332325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review"}}}, {"id": "rJe0cj3z9B", "original": null, "number": 3, "cdate": 1572158358235, "ddate": null, "tcdate": 1572158358235, "tmdate": 1572972485319, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n= Summary\nThis paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network. The main motivation is that, current state-of-the-art models based on the Siamese architecture often blindly predict the center of a search window as target location due to the bias in datasets and training strategies. This issue is alleviated in this paper by introducing an auxiliary task, target detection in the entire image space. The auxiliary task is conducted by another branch on top of the visual feature shared with the tracking branch. By learning to detect object in the entire image space, the shared feature extractor will be trained to capture discriminative and unique appearance features of target.\n\n\n= Decision\nAlthough the main motivation is convincing and the manuscript is well written, I would recommend to reject this submission mainly due to its limited contribution and weakness in experimental analysis. \n\n(1) In the experiments, the practical benefit of adding the auxiliary detection task is demonstrated, but the final scores of the proposed model are clearly below those of current state of the art in terms of both reliability and accuracy. Further, it is not explained why the proposed model is worse than the other models in performance and what can be claimed as an advantage of the proposed method even in this situation. Also, I do not understand why the proposed model is not based on the current state of the art like SiamRPN++ but is built upon a manually designed/low-performance model. \n\n(2) The experiments in Section 5 do not demonstrate the advantage of the proposed model at all. In Figure 6 and 7, the difference between the proposed model and its reduced version without the auxiliary task looks quite subtle, and it is hard to say which one is better than the others. In Figure 8, adding the auxiliary detection task results in even worse tracking performance. \n\n(3) More qualitatively and quantitatively analysis should be done on the other tracking benchmarks and be compared with other tracking models recently proposed too."}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834332313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Reviewers"], "noninvitees": [], "tcdate": 1570237739229, "tmdate": 1575834332325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Review"}}}, {"id": "rJg5RTRVuS", "original": null, "number": 1, "cdate": 1570201042300, "ddate": null, "tcdate": 1570201042300, "tmdate": 1570201042300, "tddate": null, "forum": "BygfiAEtwS", "replyto": "BygfiAEtwS", "invitation": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment", "content": {"comment": "We noticed that the legend in Figure 8 is swapped. The blue curve should be labeled as \"without detector\" and orange curve should be labeled as \"with detector\". We will upload a revised version of the paper as soon as we are able to. ", "title": "Swapped legend in Figure 8. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1311/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Stronger Object Representations in Deep Visual Trackers", "authors": ["Ross Goroshin", "Jonathan Tompson", "Debidatta Dwibedi"], "authorids": ["goroshin@google.com", "tompson@google.com", "debidatta@google.com"], "keywords": ["Object Tracking", "Computer Vision", "Deep Learning"], "abstract": "Fully convolutional deep correlation networks are integral components of state-of-\nthe-art approaches to single object visual tracking. It is commonly assumed that\nthese networks perform tracking by detection by matching features of the object\ninstance with features of the entire frame. Strong architectural priors and conditioning\non the object representation is thought to encourage this tracking strategy.\nDespite these strong priors, we show that deep trackers often default to \u201ctracking-\nby-saliency\u201d detection \u2013 without relying on the object instance representation. Our\nanalysis shows that despite being a useful prior, salience detection can prevent the\nemergence of more robust tracking strategies in deep networks. This leads us to\nintroduce an auxiliary detection task that encourages more discriminative object\nrepresentations that improve tracking performance.", "pdf": "/pdf/512bebc7cdcc248f17bf64e4e587aebdfbae07e3.pdf", "paperhash": "goroshin|inducing_stronger_object_representations_in_deep_visual_trackers", "original_pdf": "/attachment/e41acdc3a5afb38dfb4fd510e363cd43cfc51d64.pdf", "_bibtex": "@misc{\ngoroshin2020inducing,\ntitle={Inducing Stronger Object Representations in Deep Visual Trackers},\nauthor={Ross Goroshin and Jonathan Tompson and Debidatta Dwibedi},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfiAEtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfiAEtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1311/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1311/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1311/Authors|ICLR.cc/2020/Conference/Paper1311/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157946, "tmdate": 1576860556331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1311/Authors", "ICLR.cc/2020/Conference/Paper1311/Reviewers", "ICLR.cc/2020/Conference/Paper1311/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1311/-/Official_Comment"}}}], "count": 10}