{"notes": [{"id": "dGuOP5iXjRo", "original": "LTZtQsjsB00", "number": 19, "cdate": 1615844208515, "ddate": null, "tcdate": 1615844208515, "tmdate": 1615844208596, "tddate": null, "forum": "dGuOP5iXjRo", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/Learning_to_Learn/-/Blind_Submission", "content": {"title": "Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment", "authorids": ["ICLR.cc/2021/Workshop/Learning_to_Learn/Paper19/Authors"], "authors": ["Anonymous"], "keywords": ["Causality", "Reinforcement Learning", "Modularity", "Credit Assignment", "Societal Decision Making"], "TL;DR": "We define and derive a criterion for modularity in RL methods as a constraint on the algorithmic independence among feedback signals in credit assignment and show that temporal difference methods satisfy this criterion, unlike policy gradient methods", "abstract": "Many transfer problems require re-using previously optimal decisions for solving new tasks, which suggests the need for learning algorithms that can modify the mechanisms for choosing certain actions independently of those for choosing others. However, there is currently no formalism nor theory for how to achieve this kind of modular credit assignment. To answer this question, we define modular credit assignment as a constraint on minimizing the algorithmic mutual information among feedback signals for different decisions. We introduce what we call the modularity criterion for testing whether a learning algorithm satisfies this constraint by performing causal analysis on the algorithm itself. We generalize the recently proposed societal decision-making framework as a more granular formalism than the Markov decision process to prove that for decision sequences that do not contain cycles, certain single-step temporal difference action-value methods meet this criterion while all policy-gradient methods do not. Empirical evidence suggests that such action-value methods are more sample efficient than policy-gradient methods on transfer problems that require only sparse changes to a sequence of previously optimal decisions. ", "pdf": "/pdf/a513d62cbb56a08c4544bff76387364558c5e4b8.pdf", "proposed_reviewers": "", "paperhash": "anonymous|modularity_in_reinforcement_learning_via_algorithmic_independence_in_credit_assignment", "_bibtex": "@inproceedings{\nanonymous2021modularity,\ntitle={Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment},\nauthor={Anonymous},\nbooktitle={Submitted to Learning to Learn - Workshop at ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=dGuOP5iXjRo},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/Learning_to_Learn"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/Learning_to_Learn"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/Learning_to_Learn"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/Learning_to_Learn"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "proposed_reviewers": {"value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/Learning_to_Learn"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/Learning_to_Learn"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615595596483, "tmdate": 1615844205826, "id": "ICLR.cc/2021/Workshop/Learning_to_Learn/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}