{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488382994921, "tcdate": 1478296023489, "number": 472, "id": "rJY0-Kcll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJY0-Kcll", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "content": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396603293, "tcdate": 1486396603293, "number": 1, "id": "SkQp3MLOl", "invitation": "ICLR.cc/2017/conference/-/paper472/acceptance", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors propose a meta-learner to address the problem of few-shot learning. The algorithm is interesting, and results are convincing. It's a very timely paper that will receive attention in the community. All three reviewers recommend an accept, with two being particularly enthusiastic. The authors also addressed some issues raised by the more negative reviewer. The AC also agrees that the writing needs a little more work to improve clarity. Overall, this is a clear accept.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396603882, "id": "ICLR.cc/2017/conference/-/paper472/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396603882}}}, {"tddate": null, "tmdate": 1485291601726, "tcdate": 1485291601726, "number": 4, "id": "SJqUerSvl", "invitation": "ICLR.cc/2017/conference/-/paper472/official/comment", "forum": "rJY0-Kcll", "replyto": "BJPokH_Vg", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "content": {"title": "post rebuttal comment", "comment": "This is the best paper of my batch. \nI strongly recommend acceptance. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562718, "id": "ICLR.cc/2017/conference/-/paper472/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562718}}}, {"tddate": null, "tmdate": 1485042454221, "tcdate": 1485042454221, "number": 1, "id": "HyCGm_Wwg", "invitation": "ICLR.cc/2017/conference/-/paper472/official/comment", "forum": "rJY0-Kcll", "replyto": "SyiRxi7El", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "content": {"title": "Updated score to 8", "comment": "Just wanted to call this to the attention of the meta-reviewer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562718, "id": "ICLR.cc/2017/conference/-/paper472/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562718}}}, {"tddate": null, "tmdate": 1485040890257, "tcdate": 1482039506704, "number": 1, "id": "SyiRxi7El", "invitation": "ICLR.cc/2017/conference/-/paper472/official/review", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "content": {"title": "Strong paper but presentation unclear at times", "rating": "8: Top 50% of accepted papers, clear accept", "review": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575145, "id": "ICLR.cc/2017/conference/-/paper472/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper472/AnonReviewer1", "ICLR.cc/2017/conference/paper472/AnonReviewer2", "ICLR.cc/2017/conference/paper472/AnonReviewer3"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575145}}}, {"tddate": null, "tmdate": 1483844145857, "tcdate": 1483844145857, "number": 9, "id": "ryq49XyLg", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "BJPokH_Vg", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Response to AnonReviewer3", "comment": "We appreciate your feedback!\n\n1. \"Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\u201d\n\nWe have not yet performed studies to study which inputs are most useful. Though there could be redundancy, by removing some inputs, we expect to gain only in efficiency as the performance with less inputs would be similar or worse.\n\n2. \"It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\u201d\n\nThis would be the ideal as the meta-learner could control the structure of the learner more carefully for the task at hand. Allowing the meta-learner to also control the architecture of the learner would give the meta-learner another way to control overfitting on a few-shot task. Because optimizing those parameters means we would be operating in a discrete space, learning would be a bit complicated and would likely require reinforcement learning or approximations using continuous relaxations. This would definitely be interesting to pursue in future work.\n\n3. \"The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\u201d\n\nWe apologize for missing some previous work. We have added the references you mentioned and added some discussion about older work in meta-learning in the updated version of the submission."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1483844080537, "tcdate": 1483844080537, "number": 8, "id": "BJdgqmkLg", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "r1bVaaUNx", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for your thoughts! \n\n1. \"The analogy would be closer to GRUs than LSTMs\"\n\nYes, there is a similarity to the GRU in that meta-learning LSTM uses only the cell state and does not have an additional hidden state. We have added a note about this in the new draft of the paper.\n\n2. \"The description of the data separation in meta sets is hard to follow and could be visualized\u201d\n\nWe have added a figure that gives an example with concrete data to help understand the notation. Any additional feedback is welcome.\n\n3. \"The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest\u201d\n\"Fig 2 doesn't have much value\u201d\n\nThe plots were mainly to show two points:\n(1) Different update rules (with regard to i_t and f_t) were used for different layers, which benefits training the learner.\n(2) Different update rules were used across episodes, meaning that the meta-learner was adjusting for the data in each episode.\nUnfortunately, it is hard to derive a concrete learning strategy used by the meta-learner. That said, for completeness and transparency\u2019s sake, we felt it was important to show something like Figure 2. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1483844008946, "tcdate": 1483844008946, "number": 7, "id": "r1W3tXkIx", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "SJ-NzjmNg", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Response to question", "comment": "We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes. This is true for the 16 and 20 classes picked for meta-validation and meta-testing, respectively.\nWe consider 1-shot, 5-class and 5-shot, 5-class classification, where for 5-shot, 5-class classification (for example), to create a training set for each dataset, we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes. These 5 classes are randomly assigned labels 1-5 for this episode. Thus, the softmax layer will have 5 outputs, indicating predictions for each of the 5 classes."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1483843953910, "tcdate": 1483843953910, "number": 6, "id": "r1qOYX1Ie", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "SyiRxi7El", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your feedback!\n\n1. \u201cThe writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\u201d\n\nThose are valid points. We added a concrete example to Section 2 so that it is clear what task we will be solving in the experimental section. Thanks for this great suggestion!\n\nWe also slightly reworded some of the text, in the hope to clarify it. Please do not hesitate to request other changes to the text that you think would improve its clarity. Meta-learning isn\u2019t an easy topic to discuss with perfect clarity, so any additional suggestion will be appreciated! Or if you wish to point out specific paragraphs or sentences that you find particularly hard to digest, we\u2019ll be happy to clarify them ASAP in revisions of the draft.\n\n2. \"Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\u201d\n\nSorry for the confusion. We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes. This is true for the 16 and 20 classes picked for meta-validation and meta-testing, respectively.\nWe consider 1-shot, 5-class and 5-shot, 5-class classification, where for 5-shot, 5-class classification (for example), to create a training set for each dataset, we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes. These 5 classes are randomly assigned labels 1-5 for this episode. Thus, the softmax layer will have 5 outputs, indicating predictions for each of the 5 classes.\n\n3. \"The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\u201d\n\nThe plots were mainly to show two points:\n(1) Different update rules were used for different layers, which benefits training the learner.\n(2) Different update rules were used across episodes, meaning that the meta-learner was adjusting for the data in each episode.\nUnfortunately, it is hard to derive a concrete learning strategy used by the meta-learner. That said, for completeness and transparency\u2019s sake, we felt it was important to show something like Figure 2. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1483843855301, "tcdate": 1483843855301, "number": 5, "id": "ryvMtQyUg", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Revision in response to reviews uploaded", "comment": "We have uploaded a new revision with changes suggested by reviews:\n\n1. Added figure with concrete example of meta-learning framework\n2. Reworded some text in order to improve explanation of meta-learning\n3. Added citations suggested by reviewers"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1482342303069, "tcdate": 1482342303069, "number": 3, "id": "BJPokH_Vg", "invitation": "ICLR.cc/2017/conference/-/paper472/official/review", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "content": {"title": "nice paper", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).\nThe paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. \n\nSeveral tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. \nThe experiments are convincing. This is a strong paper. My only concerns/questions are the following:\n\n1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\n2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\n3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\n     - Samy Bengio PhD thesis (1989) is all about this ;-)\n     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)\n     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  \n\nOverall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575145, "id": "ICLR.cc/2017/conference/-/paper472/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper472/AnonReviewer1", "ICLR.cc/2017/conference/paper472/AnonReviewer2", "ICLR.cc/2017/conference/paper472/AnonReviewer3"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575145}}}, {"tddate": null, "tmdate": 1482247464625, "tcdate": 1482247464625, "number": 2, "id": "r1bVaaUNx", "invitation": "ICLR.cc/2017/conference/-/paper472/official/review", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer2"], "content": {"title": "An interesting work to understand gradient descent as recurrent process", "rating": "6: Marginally above acceptance threshold", "review": "This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.\n\nPros:\n\n- An interesting and feasible approach to meta-learning\n- Competitive results and proper comparison to state-of-the-art\n- Good recommendations for practical systems\n\nCons:\n\n- The analogy would be closer to GRUs than LSTMs\n- The description of the data separation in meta sets is hard to follow and could be visualized\n- The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest\n- Fig 2 doesn't have much value\n\nRemarks:\n\n- Small typo in 3.2: \"This means each coordinate has it\" -> its\n\n> We plan on releasing the code used in our evaluation experiments.\n\nThis would certainly be a major plus.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575145, "id": "ICLR.cc/2017/conference/-/paper472/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper472/AnonReviewer1", "ICLR.cc/2017/conference/paper472/AnonReviewer2", "ICLR.cc/2017/conference/paper472/AnonReviewer3"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575145}}}, {"tddate": null, "tmdate": 1482039849465, "tcdate": 1482039849465, "number": 2, "id": "SJ-NzjmNg", "invitation": "ICLR.cc/2017/conference/-/paper472/pre-review/question", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer1"], "content": {"title": "Clarify details of sequential N-class, few-shot learning task", "question": "Can the authors clarify the details of the N-class, few-shot learning problem formulation here. To make things concrete: the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed at all during meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482039849983, "id": "ICLR.cc/2017/conference/-/paper472/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper472/AnonReviewer3", "ICLR.cc/2017/conference/paper472/AnonReviewer1"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482039849983}}}, {"tddate": null, "tmdate": 1481569752075, "tcdate": 1481569752070, "number": 4, "id": "rJeJ8On7l", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "rkW31cP7e", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "nice paper: reply", "comment": "You are right in that the performance is dependent on the classes and examples picked for the mini-Imagenet dataset. We will release our code and splits we used for mini-Imagenet upon acceptance to allow for comparison."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1481248681126, "tcdate": 1481248681121, "number": 3, "id": "rkW31cP7e", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "nice paper", "comment": "Nice paper! I have one orthogonal question for the mini-imageNet dataset:\n\n\"we create our own version of the Mini-Imagenet dataset by selecting a random 100 classes from ImageNet and picking 600 examples of each class\". \n\nI am curious how sensitive with random selected classes and examples? It would be great if the exact split or the whole dataset is shared publicly, so others can repeat experiments and make comparison on a fixed benchmark dataset.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1481078783025, "tcdate": 1481078783019, "number": 2, "id": "S1P-_grQx", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "SyiRPcJ7g", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Answers to AnonReview3 ", "comment": "We are not 100% sure what you mean by \"learning the hyperparameters with models having increasing number of parameters\" but if it means learning using the meta-learner, for example, the number of hidden units for the model, we have not tried something like that and it is a slightly less straightforward problem to solve because we'd then be optimizing over a discrete space (e.g. integer number of hidden units). We could perhaps use a continuous relaxation or reinforcement-learning to obtain gradients for the meta-learner. Please correct us if we misinterpreted your question!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}, {"tddate": null, "tmdate": 1480726482667, "tcdate": 1480726482663, "number": 1, "id": "SyiRPcJ7g", "invitation": "ICLR.cc/2017/conference/-/paper472/pre-review/question", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper472/AnonReviewer3"], "content": {"title": "sample vs model complexity", "question": "Have you assessed the importance of learning the hyperparameters using the meta-learner with models having increasing number of parameters?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482039849983, "id": "ICLR.cc/2017/conference/-/paper472/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper472/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper472/AnonReviewer3", "ICLR.cc/2017/conference/paper472/AnonReviewer1"], "reply": {"forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper472/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482039849983}}}, {"tddate": null, "tmdate": 1480653249287, "tcdate": 1480653249284, "number": 1, "id": "H1taFu0zl", "invitation": "ICLR.cc/2017/conference/-/paper472/public/comment", "forum": "rJY0-Kcll", "replyto": "rJY0-Kcll", "signatures": ["~Sachin_Ravi1"], "readers": ["everyone"], "writers": ["~Sachin_Ravi1"], "content": {"title": "Typo for equation 2 fixed", "comment": "In the 3rd revision, we fixed a minor typo in description of equation 2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimization as a Model for Few-Shot Learning", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "pdf": "/pdf/191257b538cdf09db8808fe926c1ffb2f51db178.pdf", "TL;DR": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "paperhash": "ravi|optimization_as_a_model_for_fewshot_learning", "keywords": [], "conflicts": ["princeton.edu", "twitter.com", "umontreal.ca", "google.com"], "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562846, "id": "ICLR.cc/2017/conference/-/paper472/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJY0-Kcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper472/reviewers", "ICLR.cc/2017/conference/paper472/areachairs"], "cdate": 1485287562846}}}], "count": 18}