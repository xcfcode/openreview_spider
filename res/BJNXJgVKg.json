{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028574938, "tcdate": 1490028574938, "number": 1, "id": "ByvQdKTix", "invitation": "ICLR.cc/2017/workshop/-/paper60/acceptance", "forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Similarity preserving compressions of high dimensional sparse data", "abstract": "The rise of internet has resulted in an explosion of data consisting of millions of\narticles, images, songs, and videos. Most of this data is high dimensional and\nsparse, where the standard compression schemes, such as LSH, become in-\nefficient due to at least one of the following reasons: 1. Compression length is\nnearly linear in the dimension and grows inversely with the sparsity 2. Randomness \nused grows linearly with the product of dimension and compression length.\nWe propose an efficient compression scheme mapping binary vectors into binary\nvectors and simultaneously preserving Hamming distance and Inner Product. Our\nschemes avoid all the above mentioned drawbacks for high dimensional sparse\ndata. The length of our compression depends only on the sparsity and is indepenent\n of the dimension of the data, and our schemes work in the streaming setting\nas well. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and k-way Inner Product.", "pdf": "/pdf/7de3ccd7d528db78e9f10d0621c97f2d97e38737.pdf", "TL;DR": "In this work we propose an efficient compression scheme for sparse high dimensional datasets. ", "paperhash": "kulkarni|similarity_preserving_compressions_of_high_dimensional_sparse_data", "keywords": ["Theory"], "conflicts": ["NA"], "authors": ["Raghav Kulkarni", "Rameshwar Pratap"], "authorids": ["kulraghav@gmail.com", "rameshwar.pratap@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028575509, "id": "ICLR.cc/2017/workshop/-/paper60/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028575509}}}, {"tddate": null, "tmdate": 1489517815704, "tcdate": 1489517815704, "number": 2, "id": "Syxba3Sjx", "invitation": "ICLR.cc/2017/workshop/-/paper60/official/review", "forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "signatures": ["ICLR.cc/2017/workshop/paper60/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper60/AnonReviewer1"], "content": {"title": "Clearly written but unsure of novelty", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a technique to compress high-dimensional sparse vectors while preserving Hamming distance and inner products. The approach amounts to multiplying the sparse (say, column) vector by a sparse matrix with mutually orthogonal binary or +/-1-valued rows. \n\nThe work reads well and is clearly presented. However, the work fails to mention directly related approaches such as the Sparse JL transform or the Fast  JL transform. From my understanding these approaches share most (all?) of the benefits of the proposed approach, so I have concerns about the novelty. At minimum a discussion about the differences/tradeoffs compared to these prior techniques is required.\n\nI must say though that I am not very familiar with this area or the mentioned approaches, so it is difficult for me to fully evaluate novelty.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Similarity preserving compressions of high dimensional sparse data", "abstract": "The rise of internet has resulted in an explosion of data consisting of millions of\narticles, images, songs, and videos. Most of this data is high dimensional and\nsparse, where the standard compression schemes, such as LSH, become in-\nefficient due to at least one of the following reasons: 1. Compression length is\nnearly linear in the dimension and grows inversely with the sparsity 2. Randomness \nused grows linearly with the product of dimension and compression length.\nWe propose an efficient compression scheme mapping binary vectors into binary\nvectors and simultaneously preserving Hamming distance and Inner Product. Our\nschemes avoid all the above mentioned drawbacks for high dimensional sparse\ndata. The length of our compression depends only on the sparsity and is indepenent\n of the dimension of the data, and our schemes work in the streaming setting\nas well. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and k-way Inner Product.", "pdf": "/pdf/7de3ccd7d528db78e9f10d0621c97f2d97e38737.pdf", "TL;DR": "In this work we propose an efficient compression scheme for sparse high dimensional datasets. ", "paperhash": "kulkarni|similarity_preserving_compressions_of_high_dimensional_sparse_data", "keywords": ["Theory"], "conflicts": ["NA"], "authors": ["Raghav Kulkarni", "Rameshwar Pratap"], "authorids": ["kulraghav@gmail.com", "rameshwar.pratap@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489517816557, "id": "ICLR.cc/2017/workshop/-/paper60/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper60/AnonReviewer2", "ICLR.cc/2017/workshop/paper60/AnonReviewer1"], "reply": {"forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper60/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489517816557}}}, {"tddate": null, "tmdate": 1489458947025, "tcdate": 1489458947025, "number": 1, "id": "B1sbDREix", "invitation": "ICLR.cc/2017/workshop/-/paper60/official/review", "forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "signatures": ["ICLR.cc/2017/workshop/paper60/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper60/AnonReviewer2"], "content": {"title": "Lack of novelty", "rating": "3: Clear rejection", "review": "I feel the proposed approach is just a special case of the well-known FJLT, where a sparse +-1 random matrix is used to multiply a signal efficiently while preserving inner products of signal vectors.\n\nThe only difference is that in the proposed approach the sampling is without replacement (i.e., one entry can only contribute to one bucket). I don't think it is an important difference. The theoretical results don't show why without replacement sampling matters either.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Similarity preserving compressions of high dimensional sparse data", "abstract": "The rise of internet has resulted in an explosion of data consisting of millions of\narticles, images, songs, and videos. Most of this data is high dimensional and\nsparse, where the standard compression schemes, such as LSH, become in-\nefficient due to at least one of the following reasons: 1. Compression length is\nnearly linear in the dimension and grows inversely with the sparsity 2. Randomness \nused grows linearly with the product of dimension and compression length.\nWe propose an efficient compression scheme mapping binary vectors into binary\nvectors and simultaneously preserving Hamming distance and Inner Product. Our\nschemes avoid all the above mentioned drawbacks for high dimensional sparse\ndata. The length of our compression depends only on the sparsity and is indepenent\n of the dimension of the data, and our schemes work in the streaming setting\nas well. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and k-way Inner Product.", "pdf": "/pdf/7de3ccd7d528db78e9f10d0621c97f2d97e38737.pdf", "TL;DR": "In this work we propose an efficient compression scheme for sparse high dimensional datasets. ", "paperhash": "kulkarni|similarity_preserving_compressions_of_high_dimensional_sparse_data", "keywords": ["Theory"], "conflicts": ["NA"], "authors": ["Raghav Kulkarni", "Rameshwar Pratap"], "authorids": ["kulraghav@gmail.com", "rameshwar.pratap@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489517816557, "id": "ICLR.cc/2017/workshop/-/paper60/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper60/AnonReviewer2", "ICLR.cc/2017/workshop/paper60/AnonReviewer1"], "reply": {"forum": "BJNXJgVKg", "replyto": "BJNXJgVKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper60/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489517816557}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487302501090, "tcdate": 1487302427767, "number": 60, "id": "BJNXJgVKg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BJNXJgVKg", "signatures": ["~Rameshwar_Pratap1"], "readers": ["everyone"], "content": {"title": "Similarity preserving compressions of high dimensional sparse data", "abstract": "The rise of internet has resulted in an explosion of data consisting of millions of\narticles, images, songs, and videos. Most of this data is high dimensional and\nsparse, where the standard compression schemes, such as LSH, become in-\nefficient due to at least one of the following reasons: 1. Compression length is\nnearly linear in the dimension and grows inversely with the sparsity 2. Randomness \nused grows linearly with the product of dimension and compression length.\nWe propose an efficient compression scheme mapping binary vectors into binary\nvectors and simultaneously preserving Hamming distance and Inner Product. Our\nschemes avoid all the above mentioned drawbacks for high dimensional sparse\ndata. The length of our compression depends only on the sparsity and is indepenent\n of the dimension of the data, and our schemes work in the streaming setting\nas well. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and k-way Inner Product.", "pdf": "/pdf/7de3ccd7d528db78e9f10d0621c97f2d97e38737.pdf", "TL;DR": "In this work we propose an efficient compression scheme for sparse high dimensional datasets. ", "paperhash": "kulkarni|similarity_preserving_compressions_of_high_dimensional_sparse_data", "keywords": ["Theory"], "conflicts": ["NA"], "authors": ["Raghav Kulkarni", "Rameshwar Pratap"], "authorids": ["kulraghav@gmail.com", "rameshwar.pratap@gmail.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}