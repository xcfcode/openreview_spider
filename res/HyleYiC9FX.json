{"notes": [{"id": "HyleYiC9FX", "original": "Hkg1QuqqKX", "number": 411, "cdate": 1538087799597, "ddate": null, "tcdate": 1538087799597, "tmdate": 1545355406828, "tddate": null, "forum": "HyleYiC9FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Text Embeddings for Retrieval from a Large Knowledge Base", "abstract": "Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models specifically for retrieval purposes can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 recall of the system by 14% in terms of the question side, and by 8% in terms of the paragraph side.", "keywords": ["Text Embeddings", "Document Ranking", "Improving Retrieval", "Question-Answering", "Learning to Rank"], "authorids": ["txcakaloglu@ualr.edu", "szegedy@google.com", "xwxu@ualr.edu"], "authors": ["Tolgahan Cakaloglu", "Christian Szegedy", "Xiaowei Xu"], "TL;DR": "The new attempt for creating semantically meaningful text embeddings via improved language modeling and utilizing an extra knowledge base", "pdf": "/pdf/6c873d89e0a8eb1c2a37c6b6607377462fd65063.pdf", "paperhash": "cakaloglu|text_embeddings_for_retrieval_from_a_large_knowledge_base", "_bibtex": "@misc{\ncakaloglu2019text,\ntitle={Text Embeddings for Retrieval from a Large Knowledge Base},\nauthor={Tolgahan Cakaloglu and Christian Szegedy and Xiaowei Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HyleYiC9FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syebneh1l4", "original": null, "number": 1, "cdate": 1544695977436, "ddate": null, "tcdate": 1544695977436, "tmdate": 1545354506854, "tddate": null, "forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper411/Meta_Review", "content": {"metareview": "I have to agree with the reviewers here and unfortunately recommend a rejection.\n\nThe methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Serious evaluation issues"}, "signatures": ["ICLR.cc/2019/Conference/Paper411/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper411/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Embeddings for Retrieval from a Large Knowledge Base", "abstract": "Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models specifically for retrieval purposes can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 recall of the system by 14% in terms of the question side, and by 8% in terms of the paragraph side.", "keywords": ["Text Embeddings", "Document Ranking", "Improving Retrieval", "Question-Answering", "Learning to Rank"], "authorids": ["txcakaloglu@ualr.edu", "szegedy@google.com", "xwxu@ualr.edu"], "authors": ["Tolgahan Cakaloglu", "Christian Szegedy", "Xiaowei Xu"], "TL;DR": "The new attempt for creating semantically meaningful text embeddings via improved language modeling and utilizing an extra knowledge base", "pdf": "/pdf/6c873d89e0a8eb1c2a37c6b6607377462fd65063.pdf", "paperhash": "cakaloglu|text_embeddings_for_retrieval_from_a_large_knowledge_base", "_bibtex": "@misc{\ncakaloglu2019text,\ntitle={Text Embeddings for Retrieval from a Large Knowledge Base},\nauthor={Tolgahan Cakaloglu and Christian Szegedy and Xiaowei Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HyleYiC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper411/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353225640, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper411/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper411/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper411/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353225640}}}, {"id": "SkghxJq9nQ", "original": null, "number": 3, "cdate": 1541213939839, "ddate": null, "tcdate": 1541213939839, "tmdate": 1541534018367, "tddate": null, "forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "content": {"title": "Good motivation, weak organization, unclear results", "review": "Summary: \nThis paper proposes to reformulate the QA task in SQUAD as a retrieval task, i.e., using question as query and paragraphs as candidate results to be ranked.  Authors makes some modifications to elmo model to create better word embedding for the ranking task. Authors have mentioned and are aware of open domain QA methodologies (e.g., DrQA).\n\nPros:\n- The general idea is interesting, to reformulate any QA task as a ranking task\n\nCons:\n- The methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.\n- The novelty: The novelty is not significant. Although modifications to ELMO are interesting. \n- Results: Why authors have not compared their work with DrQA? ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper411/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Embeddings for Retrieval from a Large Knowledge Base", "abstract": "Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models specifically for retrieval purposes can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 recall of the system by 14% in terms of the question side, and by 8% in terms of the paragraph side.", "keywords": ["Text Embeddings", "Document Ranking", "Improving Retrieval", "Question-Answering", "Learning to Rank"], "authorids": ["txcakaloglu@ualr.edu", "szegedy@google.com", "xwxu@ualr.edu"], "authors": ["Tolgahan Cakaloglu", "Christian Szegedy", "Xiaowei Xu"], "TL;DR": "The new attempt for creating semantically meaningful text embeddings via improved language modeling and utilizing an extra knowledge base", "pdf": "/pdf/6c873d89e0a8eb1c2a37c6b6607377462fd65063.pdf", "paperhash": "cakaloglu|text_embeddings_for_retrieval_from_a_large_knowledge_base", "_bibtex": "@misc{\ncakaloglu2019text,\ntitle={Text Embeddings for Retrieval from a Large Knowledge Base},\nauthor={Tolgahan Cakaloglu and Christian Szegedy and Xiaowei Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HyleYiC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "cdate": 1542234467416, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335718202, "tmdate": 1552335718202, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlCpmx92Q", "original": null, "number": 1, "cdate": 1541174213814, "ddate": null, "tcdate": 1541174213814, "tmdate": 1541534018161, "tddate": null, "forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "content": {"title": "Interesting results but need more experiments and clearer explanation", "review": "This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residual network on top of the word embedding. \n\nPros:\n* This work set a strong baseline for the retrieving target paragraph for question answering on the SQuAD dataset.\n* The experiments were sounds and leverage interesting points -- the use of word embedding itself as the feature representation didn't have as much impact to retrieval performance as the distance function.\n* The studied problem -- retrieval for answering question rather than getting the most relevant document worth more attention.\n\nCons:\n* The motivation of using the word embedding and contextual word embedding over the TF-IDF feature wasn't clear. Results on using simple feature like TF-IDF maybe useful to give readers better judgement of the use of word embedding.\n* The choice of dataset, SQuAD over more retrieval based QA like TrivialQA also wasn't strongly motivated. Also, it would be nice to see how the QA result would be improve with better retrieval model. \n* Another use of TF-IDF/IDF and embedding is to use TF-IDF/IDF to identify the related document and then use word embedding to resolve semantic ambiguity. Do you have theoretical/empirical reason why this shouldn\u2019t be considered?\n\nComment on writing:\n    - In Section 3.1: the dimension of the tensor should reflect the meaning (vocab size, embedding size or the number of documents) rather than numbers.\n    - In Section 3.1: since the weighting for each document is not shared, it would be clearer to just use M and W for each document instead of M\u2019, W'\n    - In Section 3.1: Evaluation metrics, e.g., recall@k, ROC, AUC; technical details, for example, tensor dimension, optimizer hyperparameters should be moved to the experiment section", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper411/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Embeddings for Retrieval from a Large Knowledge Base", "abstract": "Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models specifically for retrieval purposes can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 recall of the system by 14% in terms of the question side, and by 8% in terms of the paragraph side.", "keywords": ["Text Embeddings", "Document Ranking", "Improving Retrieval", "Question-Answering", "Learning to Rank"], "authorids": ["txcakaloglu@ualr.edu", "szegedy@google.com", "xwxu@ualr.edu"], "authors": ["Tolgahan Cakaloglu", "Christian Szegedy", "Xiaowei Xu"], "TL;DR": "The new attempt for creating semantically meaningful text embeddings via improved language modeling and utilizing an extra knowledge base", "pdf": "/pdf/6c873d89e0a8eb1c2a37c6b6607377462fd65063.pdf", "paperhash": "cakaloglu|text_embeddings_for_retrieval_from_a_large_knowledge_base", "_bibtex": "@misc{\ncakaloglu2019text,\ntitle={Text Embeddings for Retrieval from a Large Knowledge Base},\nauthor={Tolgahan Cakaloglu and Christian Szegedy and Xiaowei Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HyleYiC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "cdate": 1542234467416, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335718202, "tmdate": 1552335718202, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxuqQXc3Q", "original": null, "number": 2, "cdate": 1541186448476, "ddate": null, "tcdate": 1541186448476, "tmdate": 1541534017955, "tddate": null, "forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "content": {"title": "Not ready for publication", "review": "This paper tries to study retrieval methods for multi-paragraph / multi-document reading comprehension.  The basic approach is to embed the question and the paragraph and train a system to put the correct paragraph close to the question.  I had a very hard time following the details of the proposed approach for this, however, and I still don't really understand what the authors are proposing.\n\nThis paper is not ready for publication.  The exposition is not at all clear and needs substantial rewriting.  Additionally, the evaluation done in the paper is not well-justified.  I do not know what \"paragraph-side\" means, but I assume that means you are trying to retrieve the question given the paragraph.  Why?  There were no standard baselines compared against, like a simple IR system (Lucene).  And I expected to see actual impact of the retrieved results on downstream QA performance of a system like Chen et al.'s, or Clark and Gardner 2018.  Even if you have a slightly better ranking of the retrieved paragraphs, it's not clear to me that this will improve performance, if the downstream method is properly calibrated to handle multiple paragraphs (see Clark and Gardner 2018).\n\nA few writing suggestions for the authors, for next time:\n\nThis paper does not follow the typical flow of an academic paper.  It reads too much like a logbook of what you did, presented chronologically, instead of presenting the ideas in a coherent sequence.  Part of this is just simple wording fixes (e.g., avoid things like \"it was time to compute ELMo representations\" - this isn't a logbook).  Also, all of the shape comments and numerical details at the top of page 4 are out of place.  Describe your method first in general terms, then give experimental details (like corpus size, etc.) later.  I suggest reading the award-winning papers at various conferences to get a sense of how these papers are typically structured and phrased.\n\nSection 2: A full page dedicated to the history of word embeddings is entirely unnecessary for this paper.  This is not a survey on word embeddings.  It's much more useful to the reader to give pointers to multiple connection points between your work and the rest of the literature.  You could have given a paragraph to the most relevant embedding techniques, a paragraph to the most relevant retrieval / multi-paragraph techniques (e.g., Clark and Gardner 2018, which is very relevant, along with Chen et al., TriviaQA, others), and a paragraph to distance metric learning.                                                                                                                                            \n       ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper411/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Embeddings for Retrieval from a Large Knowledge Base", "abstract": "Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models specifically for retrieval purposes can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 recall of the system by 14% in terms of the question side, and by 8% in terms of the paragraph side.", "keywords": ["Text Embeddings", "Document Ranking", "Improving Retrieval", "Question-Answering", "Learning to Rank"], "authorids": ["txcakaloglu@ualr.edu", "szegedy@google.com", "xwxu@ualr.edu"], "authors": ["Tolgahan Cakaloglu", "Christian Szegedy", "Xiaowei Xu"], "TL;DR": "The new attempt for creating semantically meaningful text embeddings via improved language modeling and utilizing an extra knowledge base", "pdf": "/pdf/6c873d89e0a8eb1c2a37c6b6607377462fd65063.pdf", "paperhash": "cakaloglu|text_embeddings_for_retrieval_from_a_large_knowledge_base", "_bibtex": "@misc{\ncakaloglu2019text,\ntitle={Text Embeddings for Retrieval from a Large Knowledge Base},\nauthor={Tolgahan Cakaloglu and Christian Szegedy and Xiaowei Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HyleYiC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper411/Official_Review", "cdate": 1542234467416, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyleYiC9FX", "replyto": "HyleYiC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335718202, "tmdate": 1552335718202, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}