{"notes": [{"id": "Qpik5XBv_1-", "original": "xxwiGC_sxAL", "number": 3429, "cdate": 1601308380586, "ddate": null, "tcdate": 1601308380586, "tmdate": 1614985777676, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "cfJJ7QDY7aa", "original": null, "number": 1, "cdate": 1610040353031, "ddate": null, "tcdate": 1610040353031, "tmdate": 1610473942319, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes to improve image segmentation from referring expression  by integrating visual and language features using an UNet architecture and experimenting with top-down, bottom-up, and combined (dual) modulation.  \n\nReview Summary: The submission received divergent reviews with scores spanning from 2 (R2) to 5 (R3,R4) to 10 (R1).  The author response failed to address the reviewer concerns with some reviewer (R4) lowering their score tto 4 after the rebuttal.  It also became clear that some relevant work (Mei et al, 2018) was used for the baseline but not cited.  The author response also did not recognize the importance of significance tests.\n\nAs there is considerable work in the area of image segmentation from referring expression, and the proposed model is very similar to the LingUNet model of Misra 2018, the originality and significance of the work is fairly low.  The main contributions appears to be experimental comparisons of the three types of modulation (top-down, bottom-up, dual).\n\nPros:\n- Investigation of a important problem of grounding language to visual regions\n- Experimental study of whether dual modulation improves image segmentation from referring expression\n\nCons\n- Relatively minor novelty with limited analyses (R3,R2)\n- Missing citations (see R3's comments).  Relevant work (Mei et al, 2018) which was the basis for the top-down baseline model, was used but not cited or properly compared against\n- Relatively weak experimental results (R2,R4).  As R4 noted, while validation results are good, test results are weak compared to existing work, indicating potential overtuning.  \n- No qualitative comparisons against baselines.  \n- Cognitive claims not backed up and limited discussion/analysis (R2)\n\nRecommendation:\nThe AC concurs with R2, R3, and R4 that the work is limited in novelty and not ready for publication at ICLR.   Despite R1's high score, referring expression for image segmentation is a well studied task, and it is unclear what are the key innovations of the proposed model over LingUNet.  Due to the limited novelty, relatively weak test results, as well as other flaws pointed out by the reviewers, the AC recommends rejection.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040353015, "tmdate": 1610473942299, "id": "ICLR.cc/2021/Conference/Paper3429/-/Decision"}}}, {"id": "bxS1-55Cv35", "original": null, "number": 1, "cdate": 1603901115914, "ddate": null, "tcdate": 1603901115914, "tmdate": 1606753302677, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review", "content": {"title": "Possibly worthwhile idea, but poorly motivated and evaluated", "review": "This paper presents a model for image segmentation from referring expressions which integrates linguistic representations of the referring expressions both at low-level and high-level stages of visual processing. They argue that this model is both more cognitively plausible and more successful than models which only use linguistic representations to modulate attention over high-level visual features.  \n\nI vote for rejection, mainly on grounds of significance and quality, expanded below. To change my vote I would require a substantial improvement on one or both of the quality/significance issues listed below, either presenting the model with a clear conceptual motivation, or doing satisfactory model analysis to understand the contribution of the paper.  \n\nPros: The presented model shows some moderate quantitative improvement over other recent work.\n\nCons: Poor conceptual motivation and error analysis, with little evident understanding of the actual effect of the linguistic representations within the model.  \n\nQuality/Significance\n\n1. The paper does not provide a clear motivation for their model. Here are some arguments I looked for but did not find:\n  1. Cognitively: There are some references to relevant cognitive science papers, but there seems to be little concrete inspiration taken from this or other cognitive work in the particular model design.\n  2. A priori based on the task: What would we expect to gain from using language in early-stage visual representations? What sort of correlations might exist between particular types of linguistic input and low-level visual representations? This might be another way to motivate the model, but I can't find any such discussion in the introduction or anywhere else.\n2. The evaluations don't convince me that this paper has made a significant conceptual contribution.\n  1. The quantitative results don't seem to constitute an enormous improvement over past work. The variability in table 2 across evaluation sets makes me doubt the statistical significance of the claims. No statistical significance tests (across resamples of test data or across random training restarts) are provided.\n  2. There is no satisfactory analysis of the actual cause of the model's success. What are the contents of the linguistic representations, and how exactly do they modulate low-level visual features? For reference, Hu et al. (2020, Figure 4) [1] and Hui et al. (2020, Figure 5) [2] both do some of what I'm looking for here, showing the influence of language on the behavior of the model. While the more complex representations used in this model make it more difficult to provide e.g. an easy heatmap, we absolutely need to see an error analysis that helps us believe your claim that language ought to play a role in low-level visual processing.  \n\nOriginality\n\nI don't closely follow the relevant literature and can't speak confidently on the originality of the model. I did have trouble understanding the innovation over Step-ConvRNN, however -- these models seemed within tweaking-distance of one another based on the presentation in this paper.  \n\n[1]: https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.pdf#page=5\n[2]: https://arxiv.org/pdf/2010.00515.pdf#page=14\n\n## Post-rebuttal update\n\nI have read the other reviews and the authors' rebuttals, and do not wish to change my review.\n\nI strongly believe that numerical task improvements are not in themselves a conceptual contribution. I look forward to the results of the analyses the authors mention in response to R3-Q2, to better understand what exact interaction between language and low-level visual input is being modeled.\n\nAlong with R4 I remain unconvinced of the strength of the empirical results. The authors' response is not helpful here. I can't understand where the numbers (mean 60.74 IoU, std 0.06) come from -- taking stats across table 2 and table 1, I get very different results, so I must be misunderstanding where they come from.\n\nSignificance tests would not take too much time -- it's not absolutely critical that you retrain the models for this. You can use data resampling methods instead. For example, on each individual dataset, run bootstrap tests comparing the predictions of your model and others on random resamples of the evaluation data and corresponding predictions.\nPooling IoU results across datasets within model and then comparing between models can yield misleading results and should be avoided. ", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075939, "tmdate": 1606915759260, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3429/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review"}}}, {"id": "YpGeltReJ7D", "original": null, "number": 3, "cdate": 1604040233817, "ddate": null, "tcdate": 1604040233817, "tmdate": 1606546756051, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review", "content": {"title": "Interesting idea; empirical results are ok", "review": "This paper concerns the problem of image segmentation from referring expressions. Given an image and a query phrase about a particular object in the image, the goal is to locate the target object as a mask at the pixel level. The basic framework is U-Net, which consists of two branches: an image encoder and segmentation map decoder (connected at the bottom in a U-shape). The paper proposes to use language to modulate the image encoding and decoding process intensively, by applying auxiliary convolutional connections between the two branches and further condition the convolution kernel on the language embedding. Overall, the paper is easy to follow and has done a good literature review.\n\nThe major concern of the paper lies in the empirical results. Despite that the introduction of top-down and bottom-up language modulation significantly boost the baseline performance (Tab. 1), the full model struggles to match existing works on certain metrics such as UNC testA/testB, UNC+ testB, and ReferIt, which put a question mark on the effectiveness of the work. The results on the validation set are promising but not as good on the test set, which indicates a possible over-tuning of the model.\n\nA minor comment on the model part. In the text above Eq. 1, the paper mentions \"[...] ,we split the textual representation [...]\". However, what is the rationale for splitting the representation since each split does not attach to any particular abstract of the image feature (low-level, mid-level, and high-level)?\n\nBesides, some numbers from Tab. 1 do not match those from Tab. 2. For instance, the IoU on LSCM and Step-ConvRNN. Please double check.\n\n============== Post-Rebuttal ==============\n\nThe authors' responses to point 1 & 2 do not sound (reflecting a question to another paper does not solve the problem). The authors mentioned \"We made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach)\", where the reference of Mei et al (2018) cannot be found in the paper, as a critical baseline. This raises a flag on the novelty of the work and completeness of the related work. Therefore, I am lowering my rating to 4.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075939, "tmdate": 1606915759260, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3429/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review"}}}, {"id": "OgJh8j0_RbH", "original": null, "number": 6, "cdate": 1606305559226, "ddate": null, "tcdate": 1606305559226, "tmdate": 1606305559226, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "bxS1-55Cv35", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment", "content": {"title": "Thank you for your constructive comments", "comment": "See the end of this section for an answer to 1-3.\n\n> The evaluations don't convince me that this paper has made a significant conceptual contribution.\n\nOur ablation analysis shows that modulating both bottom-up and top-down visual processing with language improves the performance over bottom-up-only (-1.22) IoU and top-down-only (-5.82 IoU) baselines. This is our main contribution in this study. We also showed that our model generalizes well enough on four different datasets achieving SOTA or near SOTA results. \n\n> The quantitative results don't seem to constitute an enormous improvement over past work. The variability in table 2 across evaluation sets makes me doubt the statistical significance of the claims. No statistical significance tests (across resamples of test data or across random training restarts) are provided.\n\nSee the previous comment also. We agree with you but we didn't perform any signifiance tests because of two reasons: (i) it'd take too much time, (ii) no previous work presented these results. We spent our time performing comprehensive ablation studies to show our main contribution. We also performed several experiments for \u201cDual Modulation w/ 1x1 filters\u201d model in ablation studies. Our model achieves 60.74 mean IoU for all the experiments with 0.06 standard deviation.\n\n> I don't closely follow the relevant literature and can't speak confidently on the originality of the model. I did have trouble understanding the innovation over Step-ConvRNN, however -- these models seemed within tweaking-distance of one another based on the presentation in this paper.\n\nThe cognitive science studies we cited show that language has an important role in visual processing and has an effect on the early stages of the visual processing. Those studies measure this effect by looking at the response times to the inputs that trigger the visual cortex that are known to correspond to the early visual process. However, they do not discuss how language affects the visual perception in detail (nobody yet knows). We also do not possess the answer to this question. We were inspired by the idea of language having an effect on the low-level visual processing and tried this idea on a related task with a model that we can modulate both top-down and bottom-up visual processing with language explicitly.\nOn the other hand, the Step-ConvRNN work does not show the effect of top-down and bottom-up visual processing individually. They only performed experiments with both  top-down and bottom-up visual processing at the same time. Their architecture is far more different/complicated than ours and it is also not suitable for this kind of ablation study."}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qpik5XBv_1-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3429/Authors|ICLR.cc/2021/Conference/Paper3429/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837601, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment"}}}, {"id": "nd9FRVs5_w1", "original": null, "number": 5, "cdate": 1606305481439, "ddate": null, "tcdate": 1606305481439, "tmdate": 1606305481439, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "YpGeltReJ7D", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment", "content": {"title": "Thank you for your constructive comments", "comment": "> The major concern of the paper lies in the empirical results. Despite that the introduction of top-down and bottom-up language modulation significantly boost the baseline performance (Tab. 1), the full model struggles to match existing works on certain metrics such as UNC testA/testB, UNC+ testB, and ReferIt, which put a question mark on the effectiveness of the work. The results on the validation set are promising but not as good on the test set, which indicates a possible over-tuning of the model.\n\nThe main point of our paper is to argue for the effectiveness of modulating both top-down and bottom-up visual streams with language, which our results support. We do not believe there was any over-tuning: we only tuned the model (e.g., the number of layers, the number of filters, the size of the LSTM) looking at the results obtained on the UNC validation set. We use the other validation sets only for early stopping. This will be made more clear in the camera-ready version.\n\n> A minor comment on the model part. In the text above Eq. 1, the paper mentions \"[...] ,we split the textual representation [...]\". However, what is the rationale for splitting the representation since each split does not attach to any particular abstract of the image feature (low-level, mid-level, and high-level)?\n\nWe made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach). We tried to use the final hidden state as a whole for language kernel generation in our preliminary experiments, however, we observed slight declines in the performance.\n\n> Besides, some numbers from Tab. 1 do not match those from Tab. 2. For instance, the IoU on LSCM and Step-ConvRNN. Please double check.\n\nWe have obtained those numbers from the corresponding studies. Step-ConvRNN presents results of different models (step=4 and step=5) for the ablation study and the SOTA comparison. We checked it again and we couldn\u2019t find a reason for why the authors present different numbers for LSCM.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qpik5XBv_1-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3429/Authors|ICLR.cc/2021/Conference/Paper3429/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837601, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment"}}}, {"id": "D0WF1fjAEIM", "original": null, "number": 4, "cdate": 1606305373607, "ddate": null, "tcdate": 1606305373607, "tmdate": 1606305373607, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "CiQc9XzOPRR", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment", "content": {"title": "Thank You", "comment": "Thank you for your time and review. We believe that our work demonstrates the importance of modulating both bottom-up and top-down processing with language for vision-language studies."}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qpik5XBv_1-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3429/Authors|ICLR.cc/2021/Conference/Paper3429/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837601, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment"}}}, {"id": "X02BGSPzzR3", "original": null, "number": 3, "cdate": 1606305319404, "ddate": null, "tcdate": 1606305319404, "tmdate": 1606305319404, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "vr4cjmbkui", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment", "content": {"title": "Thank you for your suggestions", "comment": "Thank you for your suggestions. We implemented S2, S3 and S5 for right now.\n\nAnswer to Q1: We obtained results for the number of layers 2, 3 and 4. The depth=4 improves the performance slightly (1 IoU) over depth=3. In this architecture, the contradicting branch halves the input on each layer, which limits the number of layers that can be used in the model. Due to the size limit of the GPU, we haven\u2019t experimented with larger number of layers.\n\nAnswer to Q2: One of the possible ways of interpreting the interaction between language and visual processing is clustering a specific layer\u2019s language filters  obtained for each phrase. Possibly, obtained clusters would give information about which language component (adjectives, prepositions, nouns etc.) has a role on which part of the architecture. Another possible way of inspecting the effect of language on the visual processing could be a word removing/masking experiment. In this experiment, checking the segmentation performance of the model after removing/masking a word would give some insights whether the model actually uses the words given in the phrase or not. Currently, we are working on both analyses. We also added an incremental analysis in the appendix.\n\nAnswer to Q3: We have experimented with bi-directional LSTM and self-attention over embeddings obtained from the last layer of a Bert model. In our preliminary experiments, these approaches didn\u2019t improve the current model. Due to the memory constraints, we continued with the basic LSTM model. As suggested by the question, introducing an inductive bias by aligning feature maps and word tokens using the parse tree of the expression could improve the performance of the model or help the learning process. However, it also requires an understanding of how language works in visual processing (which part of the expression affects which part of the visual processing). In this study, we proposed an end-to-end approach where the model itself learns the connection between language and visual processing."}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qpik5XBv_1-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3429/Authors|ICLR.cc/2021/Conference/Paper3429/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837601, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Comment"}}}, {"id": "CiQc9XzOPRR", "original": null, "number": 2, "cdate": 1603989876187, "ddate": null, "tcdate": 1603989876187, "tmdate": 1605024001836, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review", "content": {"title": "Very interesting paper", "review": "This article proposes a novel approach integrating language throughout the visual pathway for segmenting objects according to referring expressions. \n\nThe article is well written, and poses an important question about how best to integrate linguistic and visual information. The limitations of the currently dominant top-down approach are well argued. The answer proposed by the authors is to integrate linguistic information throughout the visual hierarchy. The task of segmenting by referring expression is important and well chosen. \n\nThe proposed model is sound, and well described in the article, and the experimental results demonstrate that the model outperforms clearly the state-of-the art in all metrics. The qualitative examples provided are quite impressive and demonstrate the success of the approach. \n\nIn sum, I feel this is a well written paper addressing a very timely and important problem in computer vision and AI research and should be of broad interest in the community. ", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075939, "tmdate": 1606915759260, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3429/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review"}}}, {"id": "vr4cjmbkui", "original": null, "number": 4, "cdate": 1604416709926, "ddate": null, "tcdate": 1604416709926, "tmdate": 1605024001720, "tddate": null, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "invitation": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review", "content": {"title": "good experimental setup, lacks the depth of novelty and analyses", "review": "This paper proposes to integrate visual and linguistic features in both top-down and bottom-up modulation of the visual input. This is done by fusing two modalities while doing convolution and deconvolution operations over the visual input. Experiments on image segmentation from referring expressions in standard datasets show that the proposed approach achieves state-of-the-art or competitive results. Ablation studies show that both top-down and bottom-up is essential. I believe the novelty and contribution are rather thin because many ways of the modeling language are not explored at all. \n\nBelow I list suggestions (S) and questions (q) for authors:\n\nS1 Second paragraph of introductions: please add a figure to explain the concepts of top-down, bottom-up processing, high-level, low-level effects etc.\n\nS2 Section 2.2.: Please cite the below papers [1,2] for referring expression comprehension. For Section 2.4 please add [3]\n\nS3 Figure1: following this figure is not intuitive. I recommend adding two arrows for top-down and bottom-up processing and adding more space between two branches. \n\nS4 Section 4.2: It is not clear how each of these ablations was performed. For instance, I'm not 100% sure whether two modalities are fused at different levels of top-down or bottom-up processing.\n\n[1] Nagaraja et. al. \"Modeling context between objects for referring expression understanding.\" \n[2] Cirik et. al \"Using syntax to ground referring expressions in natural images.\"\n[3] Chen et. al. \"Touchdown: Natural language navigation and spatial reasoning in visual street environments.\" \n\nS4: Section 4.3: the claim of modeling the long-range dependencies is a bit speculative. I would rephrase that.\n\nS5: Figure2: Failure cases are more informative than successful ones. Please either bring the figure from A.1 or add a comparison with a model from the literature where the other model is successful where yours is not to do a contrastive analysis on how your model can be improved.\n\nQ1: Section3: What's the effect of the number of layers for the model? Why stop at 3? Do you have results for the number of layers 0,1,2?\n\nQ2: Is there a way to interpret the interaction between language and visual input?\n\nQ3: Have you experimented with different ways of fusing or processing language input? Examples: gating the language representation, attention over tokens, using different fusion methods, bi-directional LSTM, BERT-like contextual representations, adding inductive bias with parse tree for referring expressions, alignment between feature maps and word tokens or phrases?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3429/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3429/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions", "authorids": ["~Ozan_Arkan_Can1", "~Ilker_Kesen1", "~Deniz_Yuret1"], "authors": ["Ozan Arkan Can", "Ilker Kesen", "Deniz Yuret"], "keywords": ["Referring Expression Understanding", "Language-Vision Problems", "Grounded Language Understanding"], "abstract": "How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "can|language_controls_more_than_topdown_attention_modulating_bottomup_visual_processing_with_referring_expressions", "one-sentence_summary": "We modulate both top-down and bottom-up visual processing with referring expressions.", "pdf": "/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HCnZWJgNTb", "_bibtex": "@misc{\ncan2021language,\ntitle={Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions},\nauthor={Ozan Arkan Can and Ilker Kesen and Deniz Yuret},\nyear={2021},\nurl={https://openreview.net/forum?id=Qpik5XBv_1-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qpik5XBv_1-", "replyto": "Qpik5XBv_1-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3429/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075939, "tmdate": 1606915759260, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3429/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3429/-/Official_Review"}}}], "count": 10}