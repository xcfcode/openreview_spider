{"notes": [{"id": "B1eRGEa1tV", "original": null, "number": 1, "cdate": 1554138134193, "ddate": null, "tcdate": 1554138134193, "tmdate": 1554627187889, "tddate": null, "forum": "S1xkr2LTIN", "replyto": "S1xkr2LTIN", "invitation": "ICLR.cc/2019/Workshop/RML/-/Paper5/Official_Review", "content": {"title": "Interesting Framework", "review": "Summary: This paper discusses some features of simple_rl, a framework for RL in Python that emphasizes simplicity and tools for reproducibility.  This is a nice workshop paper but would benefit from a clearer discussion of the related work.  \n\nNotes: \n  -SimpleRL is an algorithm for RL experiments in Python.  \n  -After creating agents and MDPs, an experiment log as a json is produced.  \n  -Practitioners can share a copy of the experiment file to ensure reproducibility.  However with docker or containers this should always be achievable, unless there\u2019s some guarantee that all of the seeds are in the experiment file?  \n  -Consists of MDP objects and agent objects.  \n  -Main design goal is simplicity.  \n  -MDP has \u201ctransition function\u201d and \u201creward function\u201d objects.  I wonder how well the structure generalizes to model-based RL?  \n  -Some utilities for reproducing results from the json files.  \n  -Plotting utilities are included.  \n\nComments: \n  -Section 2 could do a better job of making it clearer how the simplicity of simple_rl isn\u2019t achieved by the other libraries.  Nonetheless, it\u2019s still a nice overview.  \n", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/RML/Paper5/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/RML"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "simple_rl: Reproducible Reinforcement Learning in Python", "authors": ["Anonymous"], "authorids": ["ICLR.cc/2019/Workshop/RML/Paper5/Authors"], "keywords": ["reinforcement learning", "python", "experiments", "new library", "open source"], "TL;DR": "This paper introduces and motivates simple_rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity.", "abstract": "Conducting reinforcement-learning experiments can be a complex and timely process. A full experimental pipeline will typically consist of a simulation of an environment, an implementation of one or many learning algorithms, a variety of additional components designed to facilitate the agent-environment interplay, and any requisite analysis, plotting, and logging thereof. In light of this complexity, this paper introduces simple rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity. The goal of simple_rl is to support seamless, reproducible methods for running reinforcement learning experiments. This paper gives an overview  of the core design philosophy of the package, how it differs from existing libraries, and showcases its central features.", "pdf": "/pdf/300d6f7bc30c40fecb6e5e6c680a49d2d5d4331f.pdf", "paperhash": "anonymous|simple_rl_reproducible_reinforcement_learning_in_python", "_bibtex": null}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/RML/-/Paper5/Official_Review", "cdate": 1554124269658, "expdate": 1556236800000, "duedate": 1555372800000, "reply": {"forum": "S1xkr2LTIN", "replyto": "S1xkr2LTIN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values": ["ICLR.cc/2019/Workshop/RML"]}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/RML/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554124269658, "tmdate": 1554627184421, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/RML"], "invitees": ["ICLR.cc/2019/Workshop/RML/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/RML"]}}}, {"id": "B1ejPHbrKN", "original": null, "number": 1, "cdate": 1554482531431, "ddate": null, "tcdate": 1554482531431, "tmdate": 1554482531431, "tddate": null, "forum": "S1xkr2LTIN", "replyto": "S1xkr2LTIN", "invitation": "ICLR.cc/2019/Workshop/RML/-/Paper5/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/RML/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/RML/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "simple_rl: Reproducible Reinforcement Learning in Python", "authors": ["Anonymous"], "authorids": ["ICLR.cc/2019/Workshop/RML/Paper5/Authors"], "keywords": ["reinforcement learning", "python", "experiments", "new library", "open source"], "TL;DR": "This paper introduces and motivates simple_rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity.", "abstract": "Conducting reinforcement-learning experiments can be a complex and timely process. A full experimental pipeline will typically consist of a simulation of an environment, an implementation of one or many learning algorithms, a variety of additional components designed to facilitate the agent-environment interplay, and any requisite analysis, plotting, and logging thereof. In light of this complexity, this paper introduces simple rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity. The goal of simple_rl is to support seamless, reproducible methods for running reinforcement learning experiments. This paper gives an overview  of the core design philosophy of the package, how it differs from existing libraries, and showcases its central features.", "pdf": "/pdf/300d6f7bc30c40fecb6e5e6c680a49d2d5d4331f.pdf", "paperhash": "anonymous|simple_rl_reproducible_reinforcement_learning_in_python", "_bibtex": null}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/RML/-/Paper5/Decision", "cdate": 1554482375668, "reply": {"forum": "S1xkr2LTIN", "replyto": "S1xkr2LTIN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/RML/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/RML/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554482375668, "tmdate": 1554482377447, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/RML"], "invitees": ["ICLR.cc/2019/Workshop/RML/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/RML"]}}}, {"id": "S1xkr2LTIN", "original": "HklCb9K3L4", "number": 5, "cdate": 1551883319289, "ddate": null, "tcdate": 1551883319289, "tmdate": 1551886663206, "tddate": null, "forum": "S1xkr2LTIN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/RML/-/Blind_Submission", "content": {"title": "simple_rl: Reproducible Reinforcement Learning in Python", "authors": ["Anonymous"], "authorids": ["ICLR.cc/2019/Workshop/RML/Paper5/Authors"], "keywords": ["reinforcement learning", "python", "experiments", "new library", "open source"], "TL;DR": "This paper introduces and motivates simple_rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity.", "abstract": "Conducting reinforcement-learning experiments can be a complex and timely process. A full experimental pipeline will typically consist of a simulation of an environment, an implementation of one or many learning algorithms, a variety of additional components designed to facilitate the agent-environment interplay, and any requisite analysis, plotting, and logging thereof. In light of this complexity, this paper introduces simple rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity. The goal of simple_rl is to support seamless, reproducible methods for running reinforcement learning experiments. This paper gives an overview  of the core design philosophy of the package, how it differs from existing libraries, and showcases its central features.", "pdf": "/pdf/300d6f7bc30c40fecb6e5e6c680a49d2d5d4331f.pdf", "paperhash": "anonymous|simple_rl_reproducible_reinforcement_learning_in_python", "_bibtex": null}, "signatures": ["ICLR.cc/2019/Workshop/RML"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/RML"], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/RML/-/Blind_Submission", "cdate": 1551883316747, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/RML"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/RML"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1551883316747, "tmdate": 1551883316747, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/RML"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/RML"]}}, "tauthor": "OpenReview.net"}], "count": 3}