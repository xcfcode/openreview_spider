{"notes": [{"id": "ryxepo0cFX", "original": "r1gtQhZ5tX", "number": 768, "cdate": 1538087863562, "ddate": null, "tcdate": 1538087863562, "tmdate": 1550615400797, "tddate": null, "forum": "ryxepo0cFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Sklrd8Drx4", "original": null, "number": 1, "cdate": 1545070189291, "ddate": null, "tcdate": 1545070189291, "tmdate": 1545354481678, "tddate": null, "forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Meta_Review", "content": {"metareview": "The paper presents a novel idea with a compelling experimental study. Good paper, accept.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper768/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353093279, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper768/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper768/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper768/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353093279}}}, {"id": "BJl23nE5h7", "original": null, "number": 3, "cdate": 1541192883724, "ddate": null, "tcdate": 1541192883724, "tmdate": 1543340033315, "tddate": null, "forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "content": {"title": "A novel RNN architecture", "review": "This paper introduces antisymmetric RNN, a novel RNNs architecture that is motivated through ordinary differential equation (ODE) framework. Authors consider a first order ODE and the RNN that results from the discretization of this ODE. They show how the stability criteria with respect to perturbation of  the initial state results in an ODE  in a trainability criteria for the corresponding  RNN. This criteria ensures that there are  no exploding/vanishing gradients. Authors then propose a specific parametrization, relying on antisymmetric matrix to ensure that the stability/trainability criteria is respected. They also propose a gated-variant of their architecture.  Authors evaluate their proposal on pixel-by-pixel MNIST and CIFAR10 where they show they can outperforms an LSTM.\n\nThe paper is well-written and pleasant to read. However, while the authors argue that their architecture allows to mitigate vanishing/exploding gradient, there is no empirically verification of this claim. In particular, it would be nice to visualize how the gradient norm changes as the gradient is  backpropagated in time, compare the gradient flows of Antisymmetric RNN with a LSTM or report the top eigenvalue of the jacobian for the different models.\n\nIn addition,  the analysis for the antisymmetric RNN assumes no input is given to the model. It is not clear to me how having an input at each timestep affects those results?\n\nA few more specific questions/remarks:\n-\tExperimentally, authors find that the gated antisymmetric RNN sometime outperforms its non-gated counterpart. However, one motivation for the gate mechanism is to better control the gradients flow. It is unclear to me what is the motivation of using gate for the antisymmetric RNN ?\n-\tas the proposed RNN relies on a antisymmetric matrix to represent the hidden-to-hidden transition matrix, which has less degree of liberty, can we expect the antisymmetric RNN to have same expressivity as a standard RNN. In particular, how easily can an antisymmetric RNN forgets information ?\n-\tOn the pixel-by-pixel MNIST, authors report the Arjosky results for the LSTM baseline.\nNote that some papers reported better performance for the LSTM baseline such as Recurrent Batch Norm (Cooijman et al., 2016) .\n\nAntisymmetric RNN appears to be well-motived architecture and seems to outperforms previous RNN variants that also aims at solving exploding/vanishing gradient problem. Overall I lean toward acceptance, although I do think that adding an experiment explicitly showing that the gradient does not explode/vanish would strengthen the paper. \n\n\n* Revision\n\nThanks for your response,  the paper new  version address my main concerns, I appreciate the new experiment looking at the eigenvalues of the  end-to-end Jacobian which clearly shows the advantage of the AntisymmetricRNN.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "cdate": 1542234381044, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335798489, "tmdate": 1552335798489, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgebfDiTm", "original": null, "number": 3, "cdate": 1542316535731, "ddate": null, "tcdate": 1542316535731, "tmdate": 1542316535731, "tddate": null, "forum": "ryxepo0cFX", "replyto": "BJexpMXwh7", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your comments and feedback.\n\n\u201cConnection to prior work on orthogonal/unitary weights\u201d? Thanks to the reviewer for bringing up another angle to connect this work to the prior work on orthogonal/unitary weights. While prior work reaches unitary Jacobian by constraining the weight matrices to be orthogonal/unitary with linear activation (the condition breaks if nonlinear activation is used), unitary Jacobian is reached in AntisymmetricRNN with the residual connection and constraining f\u2019 to have imaginary eigenvalues. Unitary/orthogonal matrices have eigenvalues that lie on the unit circle. Antisymmetric matrices have eigenvalues of the form i\\lambda where \\lambda is arbitrary. This implies that the dimension of the possible transformation is much larger (the whole imaginary axis). Therefore, antisymmetric networks are more expressive than unitary ones. There are three advantages of our approach: 1) our condition can be easily achieved with the antisymmetric weight parameterization, with no computational overhead; 2) our condition takes nonlinear activations into consideration; 3) we empirically demonstrate that our formulation is more expressive than constraining the weight matrix to be orthogonal/unitary, as shown in Table 1. Moreover, we expect the connections between RNNs and the ODE theory to serve as a framework to inspire new RNN architectures in the future.\n\n\u201cstore information in 'cycles'\u201d. The behavior of the network in phase space is not repetitive. Similar manifolds are obtained when one looks at Lorenz systems for example, which is a simplification of the weather system. The phase diagrams suggest that the network never blows or decays but it is important to note that it does not repeat itself and samples different points in space.\n\nWe thank the reviewer for suggesting the tasks with more categories and varying sequence lengths. It is definitely worth studying the performance of AntisymmetricRNN on tasks such as copy and addition in future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619496, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxepo0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper768/Authors|ICLR.cc/2019/Conference/Paper768/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619496}}}, {"id": "H1lcTWPj6m", "original": null, "number": 2, "cdate": 1542316481859, "ddate": null, "tcdate": 1542316481859, "tmdate": 1542316481859, "tddate": null, "forum": "ryxepo0cFX", "replyto": "BklTkn-9hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for the detailed comments and for pointing us to the latest work on the Spectral RNN and Fourier Recurrent Units. We have updated the paper accordingly. \n\n\u201cdiffusion breaks the critical criterion\u201d? We want to emphasize that the critical criterion describes a condition of stability of the underlying ODE w.r.t. initial values, while the diffusion term is necessary to stabilize the forward Euler discretization of the ODE. The AntisymmetricRNN does run into issues of vanishing gradient when the diffusion factor is set to large values, with order (1-c\\epsilon\\gamma)^t. Here \\epsilon is the step size of Euler discretization, \\gamma is the diffusion factor and c captures the derivatives of the hidden activation. Due to the small step size and the bounded derivatives, we find AntisymmetricRNN can tolerate a broad range of diffusion factors, as shown in the new Figure 2 added on the eigenvalues. \n\n\u201cbegin the analysis with advanced RNN architectures that fit in this form\u201d. We have added discussion of more advanced recurrent architectures that fit in the \u201cresidual connection\u201d form.\n\n\u201cWhy sharing the weight matrix of gated units and recurrent units\u201d? The weight matrix is shared between the gated units and recurrent units to satisfy the critical criterion. When the weight matrix is shared, the Jacobian matrix has the form of (D_1 + D_2) M, where D_1 and D_2 are diagonal matrices and M is an antisymmetric matrix. On the other hand, if the gated units and recurrent units use different weights, then the Jacobian matrix has the form of D_1 M_1 + D_2 M_2. Even if both M_1 and M_2 are antisymmetric, the eigenvalues of the Jacobian matrix can have real parts, thus breaking the criticality.\n\n\u201cConduct experiment on language models and machine translation\u201d? We conducted experiments on the pixel-by-pixel image tasks as the benchmark datasets for studying long-range dependence to demonstrate the effectiveness of the proposed method. We would like to study the performance of AntisymmetricRNN on language models and machine translation in future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper768/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619496, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxepo0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper768/Authors|ICLR.cc/2019/Conference/Paper768/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619496}}}, {"id": "ryghF-viT7", "original": null, "number": 1, "cdate": 1542316419783, "ddate": null, "tcdate": 1542316419783, "tmdate": 1542316419783, "tddate": null, "forum": "ryxepo0cFX", "replyto": "BJl23nE5h7", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your constructive feedback. The paper is updated with the suggested changes. In particular, a new Figure 2 visualizing the eigenvalues of the end-to-end Jacobian is included.\n\n\u201cempirical verification of mitigation of vanishing/exploding gradient\u201d? We included a new Figure 2 on the mean and standard deviation of the eigenvalues of the end-to-end Jacobian matrices for LSTMs and AntisymmetricRNNs with different diffusion constants. They are computed on the networks trained for the padded CIFAR10 dataset with time steps T in {100, 200, 400, 800}. As a quick summary, the eigenvalues for LSTMs quickly approaches zero as time steps increase, indicating vanishing gradients as they back-propagate in time. This explains why LSTMs fail to train at all on this task. AntisymmetricRNNs with a broad range of diffusion, on the other hand, have eigenvalues centered around 1. It is worth noting though as the diffusion constant increases to large values, AntisymmetricRNNs run into vanishing gradients as well. The diffusion constant plays an important role in striking a balance between the stability of discretization and non-vanishing gradients.\n\n\u201chow do inputs affects the analysis\u201d? Our analysis in Section 3 on the stability of an ODE is valid with inputs. In Equation 9 where we calculate the Jacobian matrix, the inputs only affect the diagonal matrix. As long as the diagonal matrix is bounded, which is true for derivatives of most activation functions, the Jacobian matrix still satisfies the critical criterion with inputs. Figure 5 in Appendix D shows the simulation with independent standard Gaussian input. Although the dynamics become slightly noisier comparing with those in Figure 1, the trend remains the same.\n\n\u201cthe motivation of using gate for the antisymmetric RNN\u201d? We see AntisymmetricRNN and AntisymmetricRNN w/ gating as discretizations of two different ODEs under the same theoretical framework. Gating provides a mechanism for the underlying ODE to have more degrees of freedom and to capture more complex dynamics. Experimental results show that AntisymmetricRNN performs better on pMNIST while AntisymmetricRNN w/ gating works well on the other tasks. \n\n\u201cexpressivity of AntisymmetricRNN\u201d? Structural constraint on the weight matrix could limit the expressivity of AntisymmetricRNN. However, we do not observe performance degradation in our empirical studies. We hypothesize it is due to over-parametrization in these networks. An AntisymmetricRNN can outperform other RNN models with fewer model parameters.\n\n\u201chow easily can an antisymmetric RNN forgets information\u201d. The diffusion term can be regarded as a mechanism for AntisymmetricRNNs to forget inputs in the past. As shown in the newly added Figure 2, when the diffusion constant increases, the eigenvalues of the end-to-end Jacobian decreases, resulting in shrinking gradient w.r.t. inputs in the past. In our current formulation, the diffusion factor is a constant across all the time steps and dimensions, but we could extend it to be time-dependent and/or data-dependent in future work. \n\n\u201cbetter baseline in Cooijman et al., (2016)\u201d? Thanks for the pointer. We added that in the footnote. We decide to keep the LSTM baseline reported by Arjovsky et al., (2016) because it has a higher accuracy on the more challenging pMNIST task than that in Cooijman et al., (2016) (92.6% vs 90.2%). We added the 92.6% accuracy in the footnote. Cooijman et al., (2016) is very relevant to our paper and we have added it to the related work section. It would be interesting to compare a \u201cbatch-normalized AntisymmetricRNN\u201d with the batch-normalized LSTM in future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619496, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxepo0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper768/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper768/Authors|ICLR.cc/2019/Conference/Paper768/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers", "ICLR.cc/2019/Conference/Paper768/Authors", "ICLR.cc/2019/Conference/Paper768/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619496}}}, {"id": "BklTkn-9hQ", "original": null, "number": 2, "cdate": 1541180389069, "ddate": null, "tcdate": 1541180389069, "tmdate": 1541533705770, "tddate": null, "forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "content": {"title": "Good paper with original work, experiments could be improved", "review": "In this paper, the authors provide a new approach to analyze the behavior of\nRNNs by relating the RNNs with ODE numerical schemes. They provide analysis on\nthe stability of forward Euler scheme and proposed an RNN architecture called\nAntisymmetricRNN to solve the gradient exploding/vanishing problem. \n\nThe paper is well presented although more recent works in this direction\nshould be cited and discussed. Also, some important issues are omitted and not\nexplained. \nFor example, the analysis begins with \"RNNs with feedback\" rather than vanilla\nRNN, since vanilla RNN does not have the residual structure as eq(3). The\nauthors should note that clearly in the paper. \n\nAlthough there are previous works relating ResNets with ODEs, such as [1],\nthis paper is original as it is the first work that relates the stability of\nODE numerical scheme with the gradient vanishing/exploding issues in RNNs. \n\nIn general, this paper provides a novel approach to analyze the gradient\nvanishing/exploding issue in RNNs and provides applicable solutions, thus I\nrecommend to accept it. \n\n\nDetailed comments:\n\nThe gradient exploding/vanishing issue has been extensively studied these\nyears and more recent results should be discussed in related works.\nAuthor mentioned that existing methods \"come with significant computational\noverhead and reportedly hinder representation power of these models\". However\nthis is not true for [2] which achieves full expressive power with\nno-overhead. \nIt is true that \"orthogonal weight matrices alone does not prevent exploding\nand vanishing gradients\", thus there are architectural approaches that can\nbound the gradient norm by constants [3]. \n\nThe authors argued that the critical criterion is important in preserving the\ngradient norm. However, later on added a diffusion term to maintain the\nstability of forward Euler method. Thus the gradient will vanish\nexponentially w.r.t. time step t as: (1-\\gamma)^t. Could the authors provide\nmore detailed analysis on this issue? \n\nSince eq(3) cannot be regarded as vanilla RNN, it would be better begin the\nanalysis with advanced RNN architectures that fit in this form, such as\nResidual RNN, Statistical Recurrent Units and Fourier Recurrent Units. \n\nWhy sharing the weight matrix of gated units and recurrent units? Is there any\nother reason to do this other than reducing the number of parameters?\n\nMore experiment should be conducted on real applications of RNN, such as\nlanguage model or machine translation. \n\n\n[1] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer\nneural networks: Bridging deep architectures and numerical differential\nequations. In ICML, pp. 3276\u20133285, 2018.  \n\n[2] Zhang, Jiong, Qi Lei, and Inderjit S. Dhillon. \"Stabilizing Gradients for\nDeep Neural Networks via Efficient SVD Parameterization.\" In ICML, pp.\n5806-5814, 2018.\n\n[3] Zhang, Jiong, Yibo Lin, Zhao Song, and Inderjit S. Dhillon. \"Learning Long\nTerm Dependencies via Fourier Recurrent Units.\" In ICML, pp. 5815-5823, 2018. \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "cdate": 1542234381044, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335798489, "tmdate": 1552335798489, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJexpMXwh7", "original": null, "number": 1, "cdate": 1540989624039, "ddate": null, "tcdate": 1540989624039, "tmdate": 1541533705531, "tddate": null, "forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "content": {"title": "capacity of long-term storage?", "review": "This is an interesting paper which proposes a novel angle on the problem of learning long-term dependencies in recurrent nets. The authors argue that most of the action should be in the imaginary part of the eigenvalues of the Jacobian J=F' of the new_state = old_state + epsilon F(old_state, input) incremental type of recurrence, while the real part should be slightly negative. If they were 0 the discrete time updates would still not be stable, so slightly negative (which leads to exponential loss of information) leads to stability while making it possible for the information decay to be pretty slow. They also propose a gated variant which sometimes works better. \n\nThis is similar to earlier work based on orthogonal or unitary Jacobians of new_state = H(old_state,input) updates, since the Jacobian of H(old_state,input) = old_state + epsilon F( old_state,input) is I + epsilon F'. In this light, it is not clear why the proposed architecture would be better than the partially orthogonal / unitary variants previously proposed. My general concern with this this type of architecture is that they can store information in 'cycles' (like in fig 1g, 1h) but this is a pretty strong constraint. For example, in the experiments, the authors did not apparently vary the length of the sequences (which would break the trick of using periodic attractors to store information). In practical applications this is very important. Also, all of the experiments are with classification tasks with few categories (10), i.e., requiring only storing 4 bits of information. Memorization tasks requiring to store many more bits, and with randomly varying sequence lengths, would better test the abilities of the proposed architecture.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper768/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks", "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.", "keywords": [], "authorids": ["bchang@stat.ubc.ca", "minminc@google.com", "haber@math.ubc.ca", "edchi@google.com"], "authors": ["Bo Chang", "Minmin Chen", "Eldad Haber", "Ed H. Chi"], "pdf": "/pdf/bb1ee1fd82aac529efcebca0ddd48ad14d3d4209.pdf", "paperhash": "chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks", "_bibtex": "@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper768/Official_Review", "cdate": 1542234381044, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxepo0cFX", "replyto": "ryxepo0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper768/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335798489, "tmdate": 1552335798489, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper768/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}