{"notes": [{"id": "o2N6AYOp31", "original": "zHH8yjJSNC4", "number": 2469, "cdate": 1601308272739, "ddate": null, "tcdate": 1601308272739, "tmdate": 1614985667513, "tddate": null, "forum": "o2N6AYOp31", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VrdKnjUj_Fk", "original": null, "number": 1, "cdate": 1610040491592, "ddate": null, "tcdate": 1610040491592, "tmdate": 1610474097518, "tddate": null, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This meta-review is written after considering the reviews, the authors\u2019 responses, the discussion, and the paper itself.\n\nThe paper proposes a training scheme for autoencoders, involving data augmentation and interpolation, that results in autoencoders for which interpolations in the latent space lead to meaningful interpolations in the image space. The paper notices that this property carries over reasonably well to datasets different from the training one.\n\nThe reviewers point out that the idea is interesting (R1, R2, R4) and simple (R2), but the experiments are substandard (R2, R4) and presentation is at times suboptimal (R1). Overall consensus is towards rejection. Authors addressed some of the concerns in their responses, but failed to convince the reviewers to change their evaluations.\n\nI agree with the reviewers and recommend rejection at this point. The idea is indeed interesting and could be publishable if presented and evaluated well, but in the current manuscript the presentation is at times unclear or somewhat misleading (e.g. presenting the method as a general image generation method, not an interpolation method) and the experiments are reasonable, but not quite convincing, mainly because the architectures and the baselines are outdated (as also pointed out by R1 and R4). I encourage the authors to further improve the paper and resubmit to a different venue. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491579, "tmdate": 1610474097503, "id": "ICLR.cc/2021/Conference/Paper2469/-/Decision"}}}, {"id": "xGePHk3XOWe", "original": null, "number": 3, "cdate": 1603919683039, "ddate": null, "tcdate": 1603919683039, "tmdate": 1606499643017, "tddate": null, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review", "content": {"title": "Interesting and simple (which is positive) method - but experiments are lacking", "review": "The general idea of the paper is interesting: when using an AE one can use the constraint that \"interpolated images\" should also correspond to \"interpolated latent codes\". While the idea is interesting, the experimental results are not really that compelling. \n\nThe proposed approach in section 4 is an interesting and well motivated extension to Interpolative AEs. The idea is quite simple and the proposed setting to use synthesized and augmented images for the above mentioned interpolation constraint seems interesting to explore. \n\nThe main weakness of the paper are the experimental results in my view. \n\nWhile qualitatively (and potentially hand-picked) examples seem to show that the proposed approach is working well, the experiments are not sufficient to convince me as a reviewer about the power of the approach. Let me be more specific\n\n- In general, quantitative results are rare and thus it is close to impossible to assess the performance of the proposed method. In essence mostly qualitative results are shown that are obviously anecdotal only. An exception is table 2, where FID and an error is shown. While I understand the FID score, I am lacking comparisons to FID scores for these models trained to the \"same\" domain. Otherwise it is unclear how good the numbers really are. Also, the error numbers where not entirely clear to me what they correspond to.\n\n- An important ingredient and component of the approach is the way the synthesized images are obtained via augmentation. While the reader get a vague idea about what kind of augmentation is used, there is no experiment that shows which kind of augmentation is necessary and which kind of augmentation will break the system. In fact, without such an \"ablation-type\" experiment the paper is not particularly insightful. To me some experiments around this essential component the paper is incomplete and should not be accepted. \n\n- Finally, somewhat linked to the previous comment, the paper does not really show failure modes (with the somewhat too obvious failure mode given in fig 3 right) to understand the limitations of the proposed method\n\n\nSo overall the proposed method is interesting and simple (which is positive) - but the experimental results are not convincing and complete enough to justify acceptance at ICLR. \n\nUpdate after the rebuttal:\n\nThanks for the responses. Given that the other reviewers also raise serious issues I will stick with my initial rating. The paper seems not to be ready for publication at ICLR", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095652, "tmdate": 1606915797878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2469/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review"}}}, {"id": "Bg639sw0R39", "original": null, "number": 5, "cdate": 1605627995040, "ddate": null, "tcdate": 1605627995040, "tmdate": 1605638412538, "tddate": null, "forum": "o2N6AYOp31", "replyto": "ZRmhj5MhwtC", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment", "content": {"title": "Point by point responses for Reviewer 1:", "comment": "Thank you for your comments and feedback! We address concerns in order:\n1. It is almost certainly the case that a semantic relationship of some sort must exist between X and X\u2019. However, understanding how to choose a \u201cproper\u201d dataset X remains an open area of research. The fields of few-shot learning and transfer learning have yet to solve or even propose a rigorous solution to this problem for classifiers. We believe that even demonstrating that image generators can be made to generalize is a valuable contribution in and of itself. \n2. In this work \u201cgeneralization\u201d refers to generalizing to new domains, not to generalizing to new images from the same domain. We are not aware of claims that VAEs generalize better than AEs in this sense (and have cited work suggesting that they don\u2019t). We have emphasized this distinction in the paper. \n3. We use z=mu(x), no sampling is involved. Added clarification. \n4. Our models send all latent representations to the unit ball, but there is no other prior, so direct comparison is difficult. We find that AugIntAE uses a smaller portion of the spherical surface (feature values range from ~-.4 to .4, as opposed to -.6 to .6 for AE) but this could just indicate that the latent space exists at a higher resolution only with less curvature. Our results indicate that the AugIntAE latent space is significantly smoother than those from prior methods, but beyond that we have found no significant differences. \n5. The FID of 54.3 comes from a CIFAR10 model evaluated on CIFAR100, so a bad score is expected. Our label-free CIFAR100 WGAN-GP scored on CIFAR100 (the standard evaluation setup) gets 42.0, which is comparable to the class-conditioned DCGAN (whose neural architecture we share) given in [1]. \n6. We train only on the larger dataset. Our main finding is that our generator can interpolate out-of-the-box on the smaller dataset without any adaptation. We have added clarification in the paper. \n7. Each \u201cfew-shot\u201d run that we evaluate or visualize uses 2-6 images. For the quantitative scores in Table 2, interpolations are synthesized from pairs of images drawn randomly from the testing dataset.  \n8. You\u2019re correct, this is overstated. We meant only that the method can sample novel images given a set of seeds and no other information. We did not mean to imply that the method works automatically for *any* arbitrary seed set (see point 1). We have clarified the language in the paper. \n9. We have added a subsection in results that explains this better. A separate classifier is trained to distinguish train from test images. If the image generator generalizes, then images synthesized from test seeds should be classified as test images. We report the predicted rate of test-set membership. \n10. Thank you, added the reference \n11. We use the same set of all the labeled images to train the classifier. Because we have class labels in this setting, we interpolate only between images of the same class, and preserve the label. We have clarified this in the paper. \n12. FID and classification scores for Fig.8 and 9 images can be found in table 2. We view GAN-adaptation and sparse-data methods as largely orthogonal to our work: our model can be used as a pretrained initialization for these methods, incorporating domain knowledge from a larger dataset. Added [2] to related work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o2N6AYOp31", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2469/Authors|ICLR.cc/2021/Conference/Paper2469/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment"}}}, {"id": "FdSJVF4Tbji", "original": null, "number": 6, "cdate": 1605628140181, "ddate": null, "tcdate": 1605628140181, "tmdate": 1605628140181, "tddate": null, "forum": "o2N6AYOp31", "replyto": "Xc0Hj7Ed1i", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment", "content": {"title": "Point by point responses for Reviewer 4: ", "comment": "Thank you for your comments and feedback! We address concerns in order:\n- First, we believe our contribution is different from the perception given in this review. Our proposed approach (AugIntAE) is not merely data augmentation; instead, it is a novel way of training generative models to interpolate. Second, we agree that our proposed approach is simple: we (and Reviewer 2) view this as a feature, not a bug. As reviewers have no doubt seen, our field has a veritable graveyard of complex, uninterpretable models that yield minor performance improvements; if we find a simple, easily understood insight with a consistent and disproportionate impact, this is to be celebrated. Third, note that our results show that our model interpolates successfully in completely unseen domains. Generative quality on seen domains, which reviewers in this field are probably more used to, will of course be higher. The fact that we can get non-trivial generation to unseen domains at all is in itself a big result. \n- The proposed trivial solution of memorizing the training set does not minimize the interpolative training objective. The recovered image is a midpoint of two different sampled augmentations, not the original unaugmented image. As such, any augmentation of the image could conceivably be sampled at some point during training as the target reconstruction. In any case, our generalization results to entirely new domains should indicate that the network is not simply memorizing the training images. \n- Figure 1 is intended only as an illustrative example. [1] does reproduce images from a novel domain. However, they do so by optimizing in an 18*512=9216 dimensional latent space, which is sufficient to recover the image even from a random, untrained network ([1] Figure 5, row e). While certainly useful practically, this does not disprove our claim that a trained GAN does not generalize to new classes by default (see for example [1] Figure 5 rows c and d). We chose PGAN more or less arbitrarily as a generic representative for large pretrained GANs. \n- Omniglot train and test are not similar; they contain disjoint classes from different alphabets. For example, the train set may include letters from English, while the test set may include characters from Arabic. The difference between Omniglot train and Omniglot test is roughly comparable to the difference between letters and digits. \n- There is very little work that attempts to generalize image generation to new domains. Neural statistician and DAGAN were the two latest baselines whose results we were able to reproduce (alongside IntAE). Other than the approaches that fall under the IntAE umbrella, we are not aware of any more recent advances in few-shot image generation. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o2N6AYOp31", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2469/Authors|ICLR.cc/2021/Conference/Paper2469/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment"}}}, {"id": "ChASrY-ZwQi", "original": null, "number": 4, "cdate": 1605627773064, "ddate": null, "tcdate": 1605627773064, "tmdate": 1605627773064, "tddate": null, "forum": "o2N6AYOp31", "replyto": "xGePHk3XOWe", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment", "content": {"title": "Response to Reviewer 2: clarified quantitative results, added ablations", "comment": "Thank you for your comments and feedback! We agree that qualitative evaluation is difficult and thus quantitative scores are very important. We have added a new subsection devoted to discussing/explaining the quantitative results and have expanded and clarified table 2. We believe that the clarified language/tables address your concerns, in particular:\n- FID scores for models trained in the same domain are in fact included in table 2. The right-hand column \"WGAN_GP (test)\" contains scores for a generative model trained and evaluated on the same test dataset. Our models approach but do not exceed these oracle scores. \n- We have added an ablation study on the MNIST/EMNIST setting. We find that no one augmentation makes or breaks the model - they act synergistically, hence the kitchen sink approach adopted in the remaining experiments. \n- The additional results in supplementary present non-handpicked results and discuss failure modes, eg. figs 17-21. We find that the network tends to simplify complex shapes (omniglot), blur out small details (celeba/cifar), and completely fail when semantic interpolation is inherently difficult (cifar)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o2N6AYOp31", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2469/Authors|ICLR.cc/2021/Conference/Paper2469/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Comment"}}}, {"id": "Xc0Hj7Ed1i", "original": null, "number": 1, "cdate": 1603813787743, "ddate": null, "tcdate": 1603813787743, "tmdate": 1605024203790, "tddate": null, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review", "content": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "review": "#######################################\n\nSummary: \n\nThe paper investigates the generative model which generalizes to new domain with limited samples.  Authors firstly explore the current hot generative models:  VAEs and GANs, and experimentally find that both VAEs and GANs fail to learn a model which generalizes  well to novel domain.  Interestingly, AutoEncoders exhibits effective performance of the generalizability to new domain. With the encouraging insight, authors further approach Augmentation-Interpolative AutoEncoders. Specially,  the paper firstly augments the training sample to get the  input pair, and extracts the latent feature by the sharing Encoder. A weighted sum of both features is conducted to form the mixed feature, which further is taken as input for the decoder to synthesize the output sample. Differently   the paper performs the reconstruction loss between the output and the mixed input which sum the input pair with the same from to the one of the latent space.    \n\n#######################################\n\nPros:\n\n+To my background, mixing the latent space from the  input pair is new.\n\n+Unlike most papers which explore VAES and GANs, this paper focuses on AEs, and experimentally find its generalizability to new domain.\n\n+This paper is well-written, and easy to follow. The organization is clear, which firstly introduces the drawback of current hot generative models, then indicates the insight that AEs is able to generalize well to novel domain, and finally propose augmentation-Interpolative AutoEncoders.  \n\n+The framework is clear to introduce the proposed method.\n\n+Authors show quantitative and qualitative results to support the proposed method which generalizes well to new domain. \n\n#######################################\n\nCons:\n\n-The idea is little simple, and lack of enough novel. To my best knowledge, the data augmentation is able to increase the generalizability of model, and reduce the domain gap between the train and the test. In this paper, authors indicate AEs has good generalizability, which is interesting, but the proposed method is simple. In fact, for complex dataset, the visualized result is not convincing. \n\n-I am thinking the proposed techniques push the decoder remember the sum of input features.   The same form of both mixing  feature and input pair makes the decoder just remember which feature is from the input and which one form the augmented input.\n\n-Figure 1 is not convincing. Current paper[1] has shown great result on styleGAN, even target domain is from far domaon.  why authors utilize PGAN?\n\n-In this paper, Omniglot train and test are used to evaluate the generalizability. I am wondering why authors consider train and test which extremely close. Please collect me if I am wrong.\n\n-In part of experiment, authors leverage the baseline (Neural Statistician and DAGAN ) which is old. Are there latest paper?\n  \n\n[1] Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space? ICCV2019", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095652, "tmdate": 1606915797878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2469/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review"}}}, {"id": "ZRmhj5MhwtC", "original": null, "number": 2, "cdate": 1603889116264, "ddate": null, "tcdate": 1603889116264, "tmdate": 1605024203727, "tddate": null, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "invitation": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review", "content": {"title": "Image generation for dataset with few examples", "review": "PROS:\n\n1. The work is well motivated. The topic about doing augmentation for few shot learning has potential value in real application.\n2. The authors made interesting discovery on the generalization of AE.\n3. The authors also made a good point in utilizing AE to do augmentation for few shot learning \n\nCONS:\n\nThe contributions of this work are a bit weak and not enough for ICLR. The work needs to be polished more seriously and it is not ready for publishing. The assumption/settings about the method is not clear/correct, some statements are too strong, the implementation is not clear, the settings for the experiments are omitted. Detailed concerns are listed as follows. \n\n1. What\u2019s the relationship between the large available X and the few shot X\u2019? Given a few shot X\u2019, will it work if we use an arbitrarily selected large X? if no, dose there have any constrain for choosing a proper X, should be seriously analyzed. \n2. The definition of a models\u2019 generalization in this paper seems in contrary to the existing literatures (where the VAE are claimed to generalize better than AE), a discussion on this topic is important in understanding the contribution of this work.\n3. How to obtain the reconstruction for VAE method in Figure2? Since there is a sampling process in obtaining the latent code, is the latent code obtained via z=mu(x)+sigma(x)*eps, eps~N(0,I)? If yes, what about the reconstruction with z=mu(x)? \n4. I am curious about the range of the value for latent code learned by AE, also what's the difference of the latent space between AE and AugIntAE?  \n5. The WGAN-GP on CIFAR100 as shown in Table 2 seems not trained well, by referring to [1], the FID can be as small as 15.6, however the reported performance here is 54.3, which is much higher than that in previous literature.  In section 5, the authors used a shallow network for illustration, which makes it difficult for the readers to evaluate the reliability of the experiments, it is easier if an existing model is employed, i.e. WGAN-GP.\n6. The training process for the method is very cryptic, since two datasets are mentioned in this work (one in large scale and another with few shot), which one is the case used in this work, trained jointly on these two datasets, pretrained on the large scale one and transfer to the few shot case, or only trained on the large scale dataset?\n7. How many images are used for the few shot dataset?\n8. I doubt the statement on few-shot generation is correct, once the AugIntAE is trained on some dataset, can we really apply it to \u201cany\u201d set of seeds (images)? This statement is really strong and the experiments didn\u2019t support this statement properly, i.e. the model is trained  and tested on relatively related datasets.\n9. The classification error in Table 2 is confusing, e.g. how to obtain this value?\n10. Table 3 is not referred.\n11. About Table 3. How many real images (except the augmentations) are used to train the classifier? Since the augmentation is obtained from two randomly selected images, how to define the label for this augmentation? e.g. what is the label for the augmentation obtained from 3 and 4?\n12. A quantitative evaluation for the generated images in experiments of Fig.8 and Fig.9 is necessary, e.g. FID or Kernel Maximum Mean Discrepancy (KMMD). What\u2019s the performance when compared to some recent GAN based method[2]? what\u2019s the pros and cons by comparison? \n\n\n[1] Shmelkov K, Schmid C, Alahari K. How good is my GAN?[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 213-229.\n[2] Zhao S, Liu Z, Lin J, et al. Differentiable augmentation for data-efficient gan training[J]. arXiv preprint arXiv:2006.10738, 2020.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2469/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2469/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "authorids": ["~Davis_Wertheimer1", "~Omid_Poursaeed2", "~Bharath_Hariharan3"], "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "keywords": ["Interpolation", "autoencoder", "reconstruction", "few-shot learning", "few-shot image generation", "generalization", "augmentation"], "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "one-sentence_summary": "A reconstruction-based method with strong generalization capability for synthesizing images beyond the training domain", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wertheimer|augmentationinterpolative_autoencoders_for_unsupervised_fewshot_image_generation", "pdf": "/pdf/7d56765de4510fd060003161377a7bfc4cb62fb8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Nu6wH8oB6U", "_bibtex": "@misc{\nwertheimer2021augmentationinterpolative,\ntitle={Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation},\nauthor={Davis Wertheimer and Omid Poursaeed and Bharath Hariharan},\nyear={2021},\nurl={https://openreview.net/forum?id=o2N6AYOp31}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o2N6AYOp31", "replyto": "o2N6AYOp31", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095652, "tmdate": 1606915797878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2469/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2469/-/Official_Review"}}}], "count": 8}