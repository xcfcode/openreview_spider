{"notes": [{"id": "Bklzkh0qFm", "original": "S1xxzkCcFX", "number": 964, "cdate": 1538087897637, "ddate": null, "tcdate": 1538087897637, "tmdate": 1545355405184, "tddate": null, "forum": "Bklzkh0qFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryeyq5lxeE", "original": null, "number": 1, "cdate": 1544714886754, "ddate": null, "tcdate": 1544714886754, "tmdate": 1545354508182, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Meta_Review", "content": {"metareview": "The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn't have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper964/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353017892, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353017892}}}, {"id": "S1eX1lnFCm", "original": null, "number": 11, "cdate": 1543253978734, "ddate": null, "tcdate": 1543253978734, "tmdate": 1543254008964, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Summary of modifications", "comment": "Here is a summary of the modifications we have made to the paper: \n- We have performed additional experiments using multiplicative attention. On the RDF tasks, we find that ARGAT is best paired with multiplicative attention, whereas WIRGAT is best paired with additive attention. On the Tox21 task we observe a much more significant difference coming from multiplicate attention when compared with its constant attention mode.\n- We have evaluated RGCN under the same setup as RGAT. We have found that this narrows the gap between RGCN and RGAT, causing us to modify our conclusions and reframe the paper slightly - it is now presented as \u201can investigation into relational attention mechanisms\u201d, rather than the presentation of a specific model we are advocating.\n- The discussion in the introduction is more graph focussed.\n- CDFs for the model are presented in appendix D, to aid insight into characteristic model behaviour, enabling future investigation. We provide some commentary around these results.\n- Hypothesis tests are performed in appendix E, to aid discussion of whether any results in terms of model performance are significant. These tests are referred to in the new results section, which now compares additive and multiplicative attention against the retrained RGCN, rather than the performance of existing RGCN benchmarks.\n\nMany thanks,\nThe authors.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "B1lK2M8jnX", "original": null, "number": 2, "cdate": 1541264048643, "ddate": null, "tcdate": 1541264048643, "tmdate": 1542625118403, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "content": {"title": "Small incremental extension of existing work", "review": "This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations. The paper is well written and the equations easy to follow. The results are not strong. And, unfortunately, the model contribution currently is too modest. \n\nInductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach. We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).\n\nMy introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs). Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution. \n\n--- After rebuttal ---\n\nStill not convinced of the value of the work to the community. Will keep my score the same.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "cdate": 1542234336937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842822, "tmdate": 1552335842822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJes1ogi67", "original": null, "number": 5, "cdate": 1542290147442, "ddate": null, "tcdate": 1542290147442, "tmdate": 1542290574436, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "B1lK2M8jnX", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Dear Reviewer 2, thank you for your constructive feedback and for taking the time to review our work. Your specific points are addressed below.\n\n> \u201cThe results are not strong. And, unfortunately, the model contribution currently is too modest.\u201d\n\nIndeed, the model is a minor contribution and, especially in light of a more thorough evaluation of RGCN, the sum attention RGAT results do not improve on those in Schlichtkrull et al. (2017). However, we would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid future research in the area.\n\nTo generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018). We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance model contributions of the paper.\n\nWe also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs.\n\n> \u201cWe should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)\u201d\n\nWe agree that including experiments on the other data sets presented in Wu et al. (2018) would be a valuable addition to the paper. Unfortunately, we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period. This is something we will investigate in the future.\n\n> \u201cMy introduction suggestion: do not talk about Convolutional neural networks (CNNs).\u201d\n\nThank you for this stylistic critique. We see how the approach taken did not provide an intuition about the problem as well as it could have.\n\nWe agree with your point regarding the wealth of graph neural network studies. We feel that the field of geometric deep learning, from which part of the direction of this work originated, is important to keep as part of the development of graph-based machine learning models. Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning, the generalisations of convolutions from grids to graph, and the modifications of convolutions to achieve non-basis dependent methods. We felt that it is important to keep these concepts associated with the field of graph based ML. This introduction could, however, talk less in detail about CNNs themselves, and deal more with graphs - the main focus of the paper.\n\nWe will produce a reworked introduction where graphs play a larger role. This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts."}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "ryxiFneiam", "original": null, "number": 6, "cdate": 1542290562860, "ddate": null, "tcdate": 1542290562860, "tmdate": 1542290562860, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "rkx6mDnd37", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.\n\n> \u201c...the additions proposed are small modifications to existing algorithms\n\nWe concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area.\n\nTo generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.\n\n> \u201c...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017).\u201d\n\nUnfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesn\u2019t support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI.\n\nIn the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks.\n\nAs mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour.\n\n> \u201c...the results achieved in the experiments are very small improvements compared to the baseline of RGCN\u2026\u201d\n\nWe agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.\n\nWe also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.\n\n> \u201c...often these small variations in results can be compensated with better baselines training\u2026\u201d\n\nWe also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. \n\nTo determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above."}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "Bkx-xQ1qpX", "original": null, "number": 4, "cdate": 1542218473462, "ddate": null, "tcdate": 1542218473462, "tmdate": 1542218544308, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "BkemLDvAn7", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.\n\n> i) \u201cThe proposed architecture is mainly adopted from the graph attention networks (Veli\u010dkovi\u0107  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.\u201d\n\nWe concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area.\n\nTo generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the paper\u2019s contribution.\n\n> ii) \u201cIn table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.\u201d\n\nWe agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. \n\nWe also feel that some of the results being \u201csignificantly worse\u201d is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.\n\n> iii) \u201cCould you explain why your MUTAG is now a single graph and is cast as node classification problem?\u201d\n\nThe MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. \n\nThere is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form\nd1 -> hasAtom -> d1_1\nd1 -> hasBond -> bond1\nd1- > hasStructure -> ring_size_6-1\nwhere in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled).\n\nEach molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective.\n\n[1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis\n[2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "BkemLDvAn7", "original": null, "number": 3, "cdate": 1541465930873, "ddate": null, "tcdate": 1541465930873, "tmdate": 1541533541643, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "content": {"title": "A good submission but not good enough", "review": "This paper presented a relational graph attention networks that could consider both node \nfeatures and relational information (edge features) to perform node-level and graph-level \nclassifications. The basic idea is to combine the graph attention networks (Velic\u030ckovic\u0301 et \nal. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid \nnetworks. This paper is generally easy to follow and written clearly. Several experiments \nare conducted to demonstrate the performance of the proposed model. Although some promising \nresults have been achieved, I think there are several limitations regarding the novelty and \nsignificance of the proposed model. \n\ni) The proposed architecture is mainly adopted from the graph attention networks (Velic\u030ckovic\u0301 \net al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple\ncombination is a good attempt to incorporate both node features and edge features but the\nnovelty is quite limited. \n\n\nii) In table 2, I don\u2019t really see any promising results compared to baselines. There are \nlittle improvements over the baselines or even significantly worse. More importantly, \ncompared two schemes of this work, the ones with attentions are \u201calmost\u201d identical with ones\nwithout attentions, which implies that the proposed attentions mechanism is not really useful\nin practice. For most of newly proposed graph embedding algorithms, it is hard to convince \nit is indeed better without some significant improvements (at least 2% absolute accuracy more). \n\niii) For MUTAG dataset, the statistical information of this dataset is quite different from\nwhat I used to use. MUTAG is a standard dataset for testing graph-level classification for \nboth graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and \nheteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled \naccording to whether it has mutagenic effect on the gram- negative bacterium Salmonnella \nTyphimurium. Could you explain why your MUTAG is now a single graph and is cast as node \nclassification problem?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "cdate": 1542234336937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842822, "tmdate": 1552335842822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkx6mDnd37", "original": null, "number": 1, "cdate": 1541093156727, "ddate": null, "tcdate": 1541093156727, "tmdate": 1541533541226, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "content": {"title": "A slight reformulation of RGCN with attention mechanisms with mixed results on graph classification and node classification tasks.", "review": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.\n\nUnfortunately the paper falls short in two main areas:\n\n- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)\n- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)\n\nHowever, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Review", "cdate": 1542234336937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335842822, "tmdate": 1552335842822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1en9VfK57", "original": null, "number": 3, "cdate": 1539019924233, "ddate": null, "tcdate": 1539019924233, "tmdate": 1539032311890, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "rygYzASP5m", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Relation to DPGCNNs", "comment": "Hi Michael, thank you for your interest in our work.\n\nWe also find the approach of Dual-Primal Graph Convolutional Networks interesting.\nThis is due to their property of allowing attention coefficients to be calculated on the basis of being common information propagators to a given node or from another given node - this treatment of an edge and the role it plays (in tandem with edges playing similar roles) isn't a treatment that is immediately accessible to the attention mechanisms of GaT or GaAN.\n\nIn terms of how your approach compares with ours, we view DPGCNN as an improvement operator to GCN, providing an alternative way of calculating the logits to GAT for a GAT-style aggregation (in the same way that GaAN choses a dot-product key/value approach to compute the logits). The key difference with your setup is that the logits themselves are explicitly tied to edge feature representations constructed in the dual graph, giving the benefits we mention above.\n\nOur work on the other hand focuses on extending an attention mechanism that provides logits to incorporate the relation type specified by data source. In our study we form the logits in a relational generalisation of the additive-style of GAT, however, it would be equally valid (and interesting) to construct these logits from a relational extension of the dot-product style of GaAN, or - in the case of a relational DPGCNN - from the the edge features of an RGAT applied to a relational dual graph.\n\nWe will include a note that a relational extension of DPGCNN should be investigated to compute the relational logits in an updated version of the manuscript.\n\nMany thanks,\nThe authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "rygYzASP5m", "original": null, "number": 3, "cdate": 1538903568557, "ddate": null, "tcdate": 1538903568557, "tmdate": 1538903568557, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "content": {"comment": "Interesting paper! I wonder how your approach compares to our recent extension of the graph attention mechanism in \n\nDual-Primal Graph Convolutional Networks, arXiv:1806.00770\n\n", "title": "generalizations of graph attention"}, "signatures": ["~Michael_Bronstein1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["~Michael_Bronstein1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311710928, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bklzkh0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311710928}}}, {"id": "rylu9f7W97", "original": null, "number": 2, "cdate": 1538499216381, "ddate": null, "tcdate": 1538499216381, "tmdate": 1538547698825, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "S1e2bT5k57", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Details of implementation - hope this helps!", "comment": "Thank you for your interest in our paper and comments.\n\nHi Marc, thank you for your interest in our paper and comments.\n\n> I was wondering if you had compared this to the Gated Graph Neural Network model we proposed a few years back (https://arxiv.org/abs/1511.05493) ...\n\nWe have not compared RGAT or RGCN to GGNNs, although it would be interesting to do so.\nIn hindsight, recurrent approaches should have been included in our related work as an alternative way of incorporating relational information into a neural model.\nWe will make this amendment in an updated version of the manuscript.\n\nIn terms of performing the comparison to GGNNs, although we believe it is interesting and worthwhile, to perform a fair comparison, we would want to perform a full hyperparamter search for the model, and feel it lies slightly outside of the current scope of this work.\nThat is, we set out to include relational properties into the GAT architecture of Veli\u010dkovi\u0107 et al. https://arxiv.org/abs/1710.10903 and report our findings, comparing against existing baselines, in particular, its spectral counterpart - RGCN.\n\n> On a related note, that implementation also has a 'use_propagation_attention' hyperparameter (in the _sparse.py model) that implements an attention mechanism over the incoming messages, i.e. the update rule is something like this: ... I was wondering if you could share experiences with the implementation, and how you got the attention mechanism to work efficiently?\n\nThank you for this - we refactored the code multiple times to achieve the performance required to run the experiments we presented.\nThe speed of our layer is now essentially the same as our implementation of RGCN - up to a sparse softmax (which accounts for around half of the compute time of the forward and backward passes).\n\nWe will be making our TensorFlow implementation of the model available, as well as the experimental setup to reproduce the Tox21 results.\nWe may also implement this in PyTorch, although this is currently in-progress.\n\nThe main optimisations we followed for the final implementation were: vectorise everything, compute everything only once, keep everything sparse if possible.\nThis way, we end up in an implementation that is linear in the number of edges (across all relation types), and is fully vectorised. \nFor comparison, an earlier implementation that did not have these optimisations suffered from a large amount of waiting time between the execution of ops, with end-to-end training time being anything from 20x-50x slower than the vectorised one.\n\nSchematically:\n- The nodes start in a dense representation (N,F)\n- The relational adj matrix is a SparseTensor with dense shape (N,RN)\n- The nodes get mapped by a single kernel (F,RHF') to (N,RHF'); this accounts for multiple heads H and multiple relation types R in a single dense (or basis decomposition) layer\n- We map the node features in (N,RHF') to 2x(N,RN,H) using the attention kernel (RH,F',2), and provide the feature base for the logits of the attention mechanism\n- Using the sparse adjacency matrix and tf.gather we can build up the features (E,H) for all incoming connections, and (E,H) for outgoing connections, where E is the number of edges across all relations\n- These are then added together with the LeakyRelu non-linearity applied, forming the logits of our eq (3).\n- Since we have not changed the order of any of the values, these can be placed back into a SparseTensor whose indices are those of the relational adj matrix (above).\n- For WIRGAT, this logits SparseTensor can be transformed into a shape (H,N,R,N), and a SparseSoftmax taken over the final dim, producing an attention SparseTensor - eq (4)\n- For ARGAT, this logits SparseTensor can be transformed into a shape (H,N,RN), and a SparseSoftmax taken over the final dim, producing an attention SparseTensor - eq (5)\n- The remaining step is to transform the attention SparseTensor into (H,N,RN) and sparse matrix multiply it with the transformed node representation (H,RN,F'), for each head - eq(6), eq(7)\n- We then concat or mean across relations, add bias and apply activation - eq (6), eq(7)\n\nIn order to perform batched computation, we concatenate B sets of node features along their 0th dimension and produce a block diagonal sparse matrix containing B adjacency matrices.\nUnfortunately, to ensure that the indices are in the correct order for the forward pass, this concatenatenation process is expensive due to a sparse_reorder. \nIf we were to optimise this further, we would start here.\n\nHope this helps - we think that the implementation is one of the core contributions of our work and are looking forward to releasing it.\n\nThe authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "S1e2bT5k57", "original": null, "number": 2, "cdate": 1538399491550, "ddate": null, "tcdate": 1538399491550, "tmdate": 1538399523631, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "content": {"comment": "Hey,\n\nThis looks like interesting work. I was wondering if you had compared this to the Gated Graph Neural Network model we proposed a few years back (https://arxiv.org/abs/1511.05493). The model always supported different edge/relationship types, so it would be interesting to see how it fares in comparison. We've released a TF reference implementation of this (https://github.com/Microsoft/gated-graph-neural-network-samples), so it should be fairly doable to do a comparison.\n\nOn a related note, that implementation also has a 'use_propagation_attention' hyperparameter (in the _sparse.py model) that implements an attention mechanism over the incoming messages, i.e. the update rule is something like this: \n  h'_v = GRU(h_v, \\sum{u has k-edge to v} a_{u, k, v} m_{u, k, v})\n  m_{u, k, v} = W_k * h_u\n  a_{u,k,v} \\propto exp((h_v * h_u^T) * a_k)\nwith W_k and a_k relationship-specific weights. This is not quite the same as the attention mechanism that you are proposing, so the experiences may differ for good reasons, and I implemented this quickly after seeing the GAT paper to see if the ideas would interact well with each other. However, in my version, I found it to be slow (due to the required softmax over dynamically-sized neighborhoods) and I never found it produce better results than the basic multi-layer GGNN model. I was wondering if you could share experiences with the implementation, and how you got the attention mechanism to work efficiently?", "title": "Relation to GGNN"}, "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["~Marc_Brockschmidt1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311710928, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bklzkh0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311710928}}}, {"id": "rkxFodcJ5X", "original": null, "number": 1, "cdate": 1538398369286, "ddate": null, "tcdate": 1538398369286, "tmdate": 1538398388297, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "HJxMHja3Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "content": {"title": "Thank you for interest in our paper and the suggested articles.", "comment": "Thank you for interest in our paper and the suggested articles.\n\n> Have you attempted to use alternative attentional mechanisms, e.g. the Transformer attention used by Vaswani et al. (NIPS 2017)? It has been found to give better performance on some tasks (see e.g. the Gated Attention Networks (GaAN) work of Zhang et al. (UAI 2018)).\n\nSo far, we have only tried the style of attention used in the original GAT paper, and extending that style (using different softmaxes) to relational structures. We haven't tried to use the dot-product style of Vaswani et al., although this would be an interesting direction to investigate.\n\nThe results of Zhang et al. [GaAN] show improvements on inductive tasks vs GAT, and so we would expect that extending GaAN to incorporate relational information (i.e. RGaAN) would yield similar gains on relational inductive tasks over RGAT on relational inductive tasks (i.e. Tox21 and others).\n\nOn the RDF datasets, it is possible that RGaAN would perform better on AIFB than RGAT (and RGCN) - this would need to be due to the model having a better inductive prior for the task, although it is not immediately clear to us whether this is true. With MUTAG it is unlikely that any gains could be made using RGaAN vs RGCN for the same reasons that RGAT doesn't perform particularly well vs RGCN - as discussed in our results section. This is purely speculative and would need empirical validation.\n\nWe will include a short discussion of alternative attention mechanisms in our related work section in an updated version of the manuscript.\n\n> I would recommend citing both the GaAN paper and the recent EGAT work of Gong & Cheng (2018): https://arxiv.org/abs/1809.02709 as related works on deploying attention mechanisms on graphs. Especially, the EGAT also aims to incorporate edge-specific information (albeit, they consider arbitrary edge features, rather than distinct relational types).\n\nThank you for bringing this paper to our attention. We will include it in the related work section in an updated version of the manuscript as an alternative way of incorporating edge information into GAT models.\n\nMany thanks,\nThe authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper964/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610148, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bklzkh0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper964/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper964/Authors|ICLR.cc/2019/Conference/Paper964/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610148}}}, {"id": "HJxMHja3Y7", "original": null, "number": 1, "cdate": 1538214713632, "ddate": null, "tcdate": 1538214713632, "tmdate": 1538214763955, "tddate": null, "forum": "Bklzkh0qFm", "replyto": "Bklzkh0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "content": {"comment": "Nicely presented work with some interesting results!\n\nHave you attempted to use alternative attentional mechanisms, e.g. the Transformer attention used by Vaswani et al. (NIPS 2017)? It has been found to give better performance on some tasks (see e.g. the Gated Attention Networks (GaAN) work of Zhang et al. (UAI 2018)).\n\nI would recommend citing both the GaAN paper and the recent EGAT work of Gong & Cheng (2018):\n\nhttps://arxiv.org/abs/1809.02709\n\nas related works on deploying attention mechanisms on graphs. Especially, the EGAT also aims to incorporate edge-specific information (albeit, they consider arbitrary edge features, rather than distinct relational types).", "title": "Interesting work!"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper964/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relational Graph Attention Networks", "abstract": "We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.", "keywords": ["RGCN", "attention", "graph convolutional networks", "semi-supervised learning", "graph classification", "molecules"], "authorids": ["dan.busbridge@gmail.com", "danesherbs@gmail.com", "p.cavallo85@gmail.com", "nils.hammerla@babylonhealth.com"], "authors": ["Dan Busbridge", "Dane Sherburn", "Pietro Cavallo", "Nils Y. Hammerla"], "TL;DR": "We propose a new model for relational graphs and evaluate it on relational transductive and inductive tasks.", "pdf": "/pdf/a26a2303cc1e28f1b4936c7954825c9ef7ee5e72.pdf", "paperhash": "busbridge|relational_graph_attention_networks", "_bibtex": "@misc{\nbusbridge2019relational,\ntitle={Relational Graph Attention Networks},\nauthor={Dan Busbridge and Dane Sherburn and Pietro Cavallo and Nils Y. Hammerla},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklzkh0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper964/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311710928, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bklzkh0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper964/Authors", "ICLR.cc/2019/Conference/Paper964/Reviewers", "ICLR.cc/2019/Conference/Paper964/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311710928}}}], "count": 15}