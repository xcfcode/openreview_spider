{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396589398, "tcdate": 1486396589398, "number": 1, "id": "rJB3nzUdg", "invitation": "ICLR.cc/2017/conference/-/paper446/acceptance", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396589918, "id": "ICLR.cc/2017/conference/-/paper446/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396589918}}}, {"tddate": null, "tmdate": 1482186054399, "tcdate": 1482186054399, "number": 3, "id": "H10raArNx", "invitation": "ICLR.cc/2017/conference/-/paper446/official/review", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer2"], "content": {"title": "reasonable paper but the contribution right now is very incremental compared to previous works", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512583035, "id": "ICLR.cc/2017/conference/-/paper446/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer1", "ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512583035}}}, {"tddate": null, "tmdate": 1482038455938, "tcdate": 1482038455938, "number": 2, "id": "rkeah5Q4x", "invitation": "ICLR.cc/2017/conference/-/paper446/official/review", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer3"], "content": {"title": "Reasonable paper, can be improved.", "rating": "6: Marginally above acceptance threshold", "review": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512583035, "id": "ICLR.cc/2017/conference/-/paper446/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer1", "ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512583035}}}, {"tddate": null, "tmdate": 1481899211765, "tcdate": 1481732380391, "number": 1, "id": "SJE7-lkVx", "invitation": "ICLR.cc/2017/conference/-/paper446/official/review", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer1"], "content": {"title": "An interesting direction, but has many flaws", "rating": "3: Clear rejection", "review": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at http://joo.st/ICLR/ReconstructionsFromGroundTruth are generated from a single groundtruth frame (X_0) which is warped several times for each next frame (X_0 --> Y_1 --> Y_2 --> ...), they said that it was the case. However, it is clear that the reconstruction is based on the previous groundtruth frame (X_t-1 --> Y_t) because it would be impossible to render objects that enter the camera field with motion alone (e.g. the dentist sequence, the top of the toothbrush enter the frame at some point). \n  Perhaps I misunderstood something or there was a misunderstanding between the reviewer and the authors. Here is the original question:\n      (1) In the reconstructions shown on this page - http://joo.st/ICLR/ReconstructionsFromGroundTruth, is the reconstruction at time t (say Y_t) created by applying the estimated affine transform on ground truth at t-1 (X_{t-1}), or by applying it on Y_{t-1} ? In other words, do we start from ground truth X_0 and apply a sequence of transforms, or do we apply the transform to each ground truth frame ?\n  And answer:\n      (1) We start from ground truth X_0 and apply a sequence of transforms. \n  \n  In the end, this qualitative result is not convincing. If the output was just trivally Y_t = X_t-1, then the two videos (X and Y) would render almost the same except for a negligible lag.\n\n\nTo conclude, this paper takes an interesting direction but suffers from important flaws. The most important ones are about the experiments: qualitative experiments are not so convincing (Figure 4 is not as good as in the paper by Srivastava et al) and quantitative results are either missing or questionable. In addition, the network is not really trained end-to-end w.r.t. the final task. Training for the affine loss is hard to intepret and seems to lead to unexpected artifacts. Using a spatial transformer layer would make it possible to train in the image space, e.g. with the robust gradient loss of Mathieu et al. Using per-patch affine spatial transformers is also possible, see the \"Universal Correspondence Network\" of Choy et al. (NIPS'16).\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512583035, "id": "ICLR.cc/2017/conference/-/paper446/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer1", "ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512583035}}}, {"tddate": null, "tmdate": 1481583710672, "tcdate": 1481583710666, "number": 6, "id": "rkDPnjn7x", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "rk9VOxPQx", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "writers": ["~Joost_van_Amersfoort1"], "content": {"title": "Reply to Question", "comment": "Thank you for your detailed comment. We verified the image for correctness. Please see http://joo.st/ICLR/GenerationBenchmark which provides the entire predicted sequences for these examples, and for many more examples."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "tmdate": 1481582926351, "tcdate": 1481582926344, "number": 5, "id": "SJILto3me", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "HJ7IDBI7x", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "writers": ["~Joost_van_Amersfoort1"], "content": {"title": "Reply to Questions", "comment": "Thank you for your extensive list of questions. We will address the related work and make sure to include and properly cite these (and others in past comments) in the paper.\n\n1) This paper introduces an action conditioned model for 3D objects that predicts what will happen to 3D objects given an action. This is a different problem, because given the action the future is deterministic. This allows the model to avoid blurriness.\n\nIt is quite similar to this paper by Oh et al, Action-Conditional Video Prediction using Deep Networks in Atari Games, which we discuss in our related work section. Also another paper mentioned in the comment before is similar to this: Unsupervised Learning for Physical Interaction through Video Prediction by Finn et al.\n\n2) This paper describes a conditional VAE model consisting of three towers, an image tower, an encoder tower and a decoder tower. During training the model is given an input image and a set of trajectories, it is trained to reconstruct these input trajectories. The important difference is that during test time, given an input image, the model simply samples from the prior distribution over Z: the goal is to produce trajectories corresponding to that image, that seem likely given the full dataset.\n\n3) Please see our reply to the first comment made on November 13, which also mentions this paper. To iterate, there are a few parts that indeed overlap, but also several parts were our approach is distinctly different. Similarly to our proposal, the paper separates out motion/content and directly models motion. It also employs the Spatial Transformer network (Jaderberg et al., 2015).\n\nThe biggest difference is that our approach is solely convolutional, which makes training fast and the optimization problem simpler. This also allows the model to scale to larger datasets and images, with only modest memory and computational resources. The model directly outputs full affine transforms instead of pixels (rather than only translations as in equation 3 in Patraucean et al.'s). This is done on a patch basis where local smoothness is guaranteed through the use of a convolutional structure. \n\nQ1: The point of our model is to model motion directly, if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results. For a discussion of this, please see Mathieu et al's paper (Deep multi-scale video prediction beyond mean square error), which includes your suggestion as a (poor performing) baseline (it's named single sc. l_2).\n\nQ2: In the unrolled model we describe in figure 3, we indeed reuse frames that the model generated itself. We propagate the gradient through all of this unrolling, thereby finetuning it. Please refer to table 1 for results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "tmdate": 1481280244345, "tcdate": 1481280220130, "number": 4, "id": "rkEkjWOXl", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "SkyGZdm7e", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "writers": ["~Joost_van_Amersfoort1"], "content": {"title": "Analysis of related work", "comment": "Thank you for the pointer to these papers. We'll analyze them here and make sure to update the related work section of the paper accordingly.\n\nDynamic Filter Networks (NIPS 2016):\nThe paper describes a model where filters all learned for all locations in the input frame. The model is trained end-to-end and results on the moving mnist dataset and a private car video dataset are shown. Even though the paper also works on the problem of next frame prediction, it differs quite substantially. The most prominent difference is the fact that it works in the pixelspace. Our model outputs solely the affine transformation, requiring very few parameters to do this.\n\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016):\nThis paper describes a model that allows generating videos which exhibit substantial motion using a model that contains a motion encoder, an image encoder and a cross convolution part with a decoder. This model also focuses on directly generating the pixels; however, as opposed to dynamic filter networks, here, the model is trained to generate the  difference image for the next time step. However, the model makes a strong implicit assumption  that the background is uniform, without any texture, so that the differencing operation captures only the motion for the foreground object.  In contrast, our model does not make such assumptions, and it can apply to natural videos.\n\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016):\nThis paper introduces an action conditioned model that predicts how the future will look given an action by a robot. This is a different problem, because given the action the future is deterministic. This allows the model to avoid blurriness.\n\nIt is quite similar to this paper by Oh et al, Action-Conditional Video Prediction using Deep Networks in Atari Games, which we discuss in our related work section."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "tmdate": 1481209906006, "tcdate": 1481209906001, "number": 3, "id": "rk9VOxPQx", "invitation": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer1"], "content": {"title": "Qualitative results", "question": "Is there a mistake in the qualitative figure 5? It looks like that the frames predicted by the proposed method are the same (or almost the same) that the input frames. For instance, look at the legs of the horse in the first row, or at the canoe in the second row, etc.\nIs it normal that the \"range\" of the predicted images is so limited, and is that why the authors have proposed a non-standard evaluation measure?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481209906599, "id": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2", "ICLR.cc/2017/conference/paper446/AnonReviewer1"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481209906599}}}, {"tddate": null, "tmdate": 1481195402577, "tcdate": 1481195402569, "number": 3, "id": "Sk79JT8ml", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "HkMbS8k7l", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "writers": ["~Joost_van_Amersfoort1"], "content": {"title": "Answer to questions", "comment": "(1) We start from ground truth X_0 and apply a sequence of transforms. \n\n(2) Yes\n\n(3) The reference model takes 4 frames as input (it produces 4 or 8 frames as output). We also tested with 2 or 3 frames as input as can be seen in Table 2, quantitative performance increases with increasing number of input frames. Qualitative performance is also better, but not visible for every sequence.\n\n(4) We use the original framerate of 25FPS, both when training the C3D net as well as the next frame predictor. Note that for training the predictor we also discarded any adjacent frames which did not exhibit enough motion, in that case we continued with the next frame of the sequence, until there was enough motion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "tmdate": 1481164685970, "tcdate": 1481164618951, "number": 2, "id": "HJ7IDBI7x", "invitation": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer2"], "content": {"title": "novelty-finetuning-RGB input", "question": "This is a reasonable paper, the main problem is that the innovation presented as the key insight of the current work is the use of transformations for video prediction which has already been proposed in :  \n1) SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks, of Byravan and Fox\n2) An Uncertain Future, Forecasting from Static Images using Variational Autoencoders\n of Walker et al.\n3) SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY of Taraucean et al, to mention a few.\n(None of which is cited.)\n\nThe evaluation setup of using detection responses for evaluating quality has been proposed in the context of image generation.\n\nWould you like to evaluate also how your model performs if instead of transformation input you get RGB frames as input, as using transformation as input was not present in previous works?\n\nWould you also finetune your model to also predict future frame using its own generated predictions and observe whether quality improves and how much?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481209906599, "id": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2", "ICLR.cc/2017/conference/paper446/AnonReviewer1"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481209906599}}}, {"tddate": null, "tmdate": 1480978695497, "tcdate": 1480978695492, "number": 2, "id": "SkyGZdm7e", "invitation": "ICLR.cc/2017/conference/-/paper446/official/comment", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/areachair1"], "content": {"title": "Question regarding related work", "comment": "Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?\n\nDynamic Filter Networks (NIPS 2016)\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572217, "id": "ICLR.cc/2017/conference/-/paper446/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572217}}}, {"tddate": null, "tmdate": 1480709369879, "tcdate": 1480709369874, "number": 1, "id": "HkMbS8k7l", "invitation": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["ICLR.cc/2017/conference/paper446/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper446/AnonReviewer3"], "content": {"title": "Reconstructions using ground truth affine transforms", "question": "(1) In the reconstructions shown on this page - http://joo.st/ICLR/ReconstructionsFromGroundTruth, is the reconstruction at time t (say Y_t) created by applying the estimated affine transform on ground truth at t-1 (X_{t-1}), or by applying it on Y_{t-1} ? In other words, do we start from ground truth X_0 and apply a sequence of transforms, or do we apply the transform to each ground truth frame ?\n\n(2) Is the same procedure used for creating the frames used in Table 1 (Using ground truth affine transforms, row 2) ?\n\n(3) The experiments use only 4-8 input frames. Does it make things qualitatively better if more frames are used (perhaps with convolution across time, or recurrence to avoid too many input channels) ?\n\n(4) What is the frame rate used in the UCF-101 experiments ? 10Hz ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481209906599, "id": "ICLR.cc/2017/conference/-/paper446/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper446/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper446/AnonReviewer3", "ICLR.cc/2017/conference/paper446/AnonReviewer2", "ICLR.cc/2017/conference/paper446/AnonReviewer1"], "reply": {"forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481209906599}}}, {"tddate": null, "tmdate": 1479856601309, "tcdate": 1479856601304, "number": 2, "id": "BJZkM8MGx", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "B1FQFSUWx", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "writers": ["~Joost_van_Amersfoort1"], "content": {"title": "Reply to question about missing reference and comparison", "comment": "Thanks for pointing out this relevant paper, we will update the references accordingly. The goal of our paper is to provide a strong baseline for next frame prediction, along with a novel evaluation criterion that requires that both pixel and temporal variability is captured in the predicted frames. Comparing our approach and Patraucean et al.'s, there are a few parts that indeed overlap, but also several parts were our approach is distinctly different. Similarly to our proposal, the paper separates out motion/content and directly models motion. It also employs the Spatial Transformer network (Jaderberg et al., 2015).\n\nThe biggest difference is that our approach is solely convolutional, which makes training fast and the optimization problem simpler. This also allows the model to scale to larger datasets and images, with only modest memory and computational resources. The model directly outputs full affine transforms instead of pixels (rather than only translations as in equation 3 in Patraucean et al.'s). This is done on a patch basis where local smoothness is guaranteed through the use of a convolutional structure. \n \nWe have experimental results on the larger dataset UCF101, which we evaluated using a novel evaluation metric. We compare this task against GANs (Matheiu et.al, ICLR2016) and a simpler baseline of optical flow with constant flow."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "tmdate": 1479067937355, "tcdate": 1479067937349, "number": 1, "id": "B1FQFSUWx", "invitation": "ICLR.cc/2017/conference/-/paper446/public/comment", "forum": "HkxAAvcxx", "replyto": "HkxAAvcxx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Missing relevant reference and comparison? ", "comment": "The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 https://arxiv.org/abs/1511.06309, where the optical flow is used to warp the current frame for predicting the next frame. How does your approach (and results) compare with that?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287572342, "id": "ICLR.cc/2017/conference/-/paper446/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkxAAvcxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper446/reviewers", "ICLR.cc/2017/conference/paper446/areachairs"], "cdate": 1485287572342}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478291143767, "tcdate": 1478291143760, "number": 446, "id": "HkxAAvcxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkxAAvcxx", "signatures": ["~Joost_van_Amersfoort1"], "readers": ["everyone"], "content": {"title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.\n\nIn order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "pdf": "/pdf/34dcce511da2ad624524464b80b50a9c93086043.pdf", "TL;DR": "Predict next frames of a video sequence by modelling transformations", "paperhash": "amersfoort|transformationbased_models_of_video_sequences", "conflicts": ["uva.nl", "facebook.com", "fb.com"], "keywords": ["Computer vision", "Unsupervised Learning"], "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc'Aurelio Ranzato", "Arthur Szlam", "Du Tran", "Soumith Chintala"], "authorids": ["joost@joo.st", "akannan@fb.com", "ranzato@fb.com", "aszlam@fb.com", "trandu@fb.com", "soumith@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 15}