{"notes": [{"id": "HygTUxHKwH", "original": "r1gyh5gYvS", "number": 2340, "cdate": 1569439828812, "ddate": null, "tcdate": 1569439828812, "tmdate": 1577168239144, "tddate": null, "forum": "HygTUxHKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jfZlT8lP5L", "original": null, "number": 1, "cdate": 1576798746613, "ddate": null, "tcdate": 1576798746613, "tmdate": 1576800889483, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a method to reduce the instability issues of off-policy deep reinforcement learning.  The proposed solution constructs a simple MDP from the experience in the agent's replay memory.  This graph is used to compute a lower bound for the values from the original problem. Incorporating this bound can make the learning system less prone to soft divergence.\n\nThe reviewers appreciated the motivation of the paper and the direction of this research.  However, the reviewers were not convinced that the formulation was sufficiently complete.  There were concerns that the method makes additional assumptions about the data distribution (the presence of state aggregation and the absence of repeated states in continuous spaces).  Reviewers found related work was missing.  The reviewers also found multiple aspects of the presentation unclear even after the author response.  \n\nThis paper is not ready for publication as the generality of the proposed method was not sufficiently clear to the reviewers after the author response.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715173, "tmdate": 1576800265026, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Decision"}}}, {"id": "SkgJmBIIoH", "original": null, "number": 4, "cdate": 1573442839213, "ddate": null, "tcdate": 1573442839213, "tmdate": 1573442839213, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HJg27xDRtH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment", "content": {"title": "Answer to Review #3", "comment": "Thank you for those comments. \nAs you suggested, we will of course re-iterate the paper to find typos and grammatical errors; add more details on related work, include more explicit pros/cons; explicitly add a list of our contributions (graph-perspective on the replay memory; thereby insights into different classes of nodes and their impact on convergence; the q-graph-based bounds for Q-learning which have a set of positive effects). \nAlso the conclusions will be extended to better link back to the theoretical insights from section 4.\n\nFor the remaining points you raised, we would like to confirm our understanding of your comments:\n1. \"the challenges you face when dealing with this issue\". The challenge we face is soft divergence, but that is a widely known issue. Do you think the paper would benefit from re-iterating more contents from prior work on soft divergence in Q-learning?\n2. Would pseudo-code for our method, including the replay memory, data graph, q-graph and network training, provide a \"clear illustration for the proposed method\"? \n3. What could a similar illustration for the experiments section look like? Do you think an additional paragraph in the beginning of the experiments section that describes the subsection strucuture would be helpful?\n\nIf you feel that any of your requests is not reflected in our suggestions here, please let us know. Thanks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygTUxHKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2340/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2340/Authors|ICLR.cc/2020/Conference/Paper2340/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142823, "tmdate": 1576860551521, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment"}}}, {"id": "rJe54QLUiS", "original": null, "number": 3, "cdate": 1573442353795, "ddate": null, "tcdate": 1573442353795, "tmdate": 1573442353795, "tddate": null, "forum": "HygTUxHKwH", "replyto": "rJl9BeUUiB", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment", "content": {"title": "One more comment", "comment": "Adding to the question whether our graphs are also useful if states are never re-visited.\nThis touches on the question if we can have sufficiently many nodes in the qgraph to derive lower bounds. A 'cheat' related to this issue that we introduce in the paper are zero actions: often some action is known to not change the state (e.g. apply zero force). Then, this zero-actions can be applied to any state and introduce self-loops. Effectively this means that loose ends (which cause highest variance in our educational example) are eliminated."}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygTUxHKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2340/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2340/Authors|ICLR.cc/2020/Conference/Paper2340/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142823, "tmdate": 1576860551521, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment"}}}, {"id": "ryeeYMIUiH", "original": null, "number": 2, "cdate": 1573442167999, "ddate": null, "tcdate": 1573442167999, "tmdate": 1573442167999, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HJgus3ATFH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment", "content": {"title": "Answer to Review #2", "comment": "Thank you for your comments.\nWe are now re-writing parts of the paper and will integrate the following answer eventually to make sure the paper already conveys these messages:\n\nYou are right in your concern that the graph may consist of several disconnected components (or chains, as you call them).\nActually, re-visiting states happens slightly more often than you may think: at corners of the state space, at edges of an obstacle, at narrow goal areas. However, of course, this depends on the environment and may not happen at all. One technique we propose in the paper to deal with this are zero-actions: often some action is known to not change the state (e.g. apply zero force). Then, this zero-actions can be applied to any state and introduce self-loops.\nAdditionally, in off-policy algorithms you can use exploration to re-visit states. Overall we have the impression that passing information along full trajectories is more useful than the cross-trajectory information exchange at re-visited states. Note that we pass this information without introducing the high gradient variance that is typically linked to full trajectory backups in Monte Carlo methods.\n\nThe Q-values we derive from the simplified MDP are only computed on the Qgraph, which is a subgraph of the data graph. Thus, the 'chains' you mentioned would not be included in the Qgraph. We would still use all samples to train the Q-function on the original problem though, just the number of samples for which a lower bound can be provided, varies. If zero actions are used, there is at least one lower bound for every sample.\n\nDoes this make sense to you, do you have any further or follow-up questions?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygTUxHKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2340/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2340/Authors|ICLR.cc/2020/Conference/Paper2340/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142823, "tmdate": 1576860551521, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment"}}}, {"id": "rJl9BeUUiB", "original": null, "number": 1, "cdate": 1573441601936, "ddate": null, "tcdate": 1573441601936, "tmdate": 1573441601936, "tddate": null, "forum": "HygTUxHKwH", "replyto": "rygpYvGTKr", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment", "content": {"title": "Answer to Review #1", "comment": "Thank you for your clear review and suggestions/questions. While we are re-writing parts of the paper in the background, we'd like to answer your questions already. Eventually we will integrate these answers into the paper such that it clearly communicates these points from the beginning.\n\nThe two pieces of related work you pointed out seem related indeed. We will discuss them in the next version of our paper. These approaches are complimentary to our work in the sense that they also point out aspects of Q-learning that lead to instabilities. However, they focus on the off-policy property and suggest ways to constrain the actions that are selected. In our work, the action selection is not constrained or altered at all. Instead, we introduce bounds to the Q-function that we enforce during learning. It may actually be of interest for future work to investigate how these methods can be combined and whether this further stabilizes Q-learning.\n\n1) One state does correspond to a single node. We never merge different states into one node, but of course this is a matter of floating point precision. In our implementation, states are measured in meters and rounded to 4 digits.\n2) Our method focusses on learning a Q-function. How actions are selected is not altered by our method and therefore any existing method may be plugged in. For our experiments, we use the standard DDPG setup in which an actor network is trained and used as a policy. For exploration, we add Gaussian noise to the output of the actor network.\n3) There is no assumption about the initial state distribution. One of the most important insights of our work is that state-of-the-art DDPG and DQN only work on samples in the replay memory. These samples may originate from a problem with any initial state distribution, but the full distribution is never used - only the finite number of samples that is stored in the replay memory. Note that these model-free methods do not execute any virtual rollouts (since no model is learned that could be used). If you could point us to the part of our paper that made you think there were constraints to the distribution of initial states, we'd love to clarify or re-write that paragraph.\n4) If they are actually the same state (up to precision), the nodes are merged; enabling a cross-over of experience from differen trajectories.\n5) The considered environments are deterministic. We are not sure what you refer to as \"stochastic approximation\"? Dynamic programming is used to obtain Q-values for the simplified MDP - however, any other way to obtain the correct Q-values works with our algorithm.  Function approximation is used because the state-action space is continuous.\n\nThe approach is indeed limited to deterministic tasks (although smaller violations to this assumptions may not be practically relevant).\nThere are ways to extend our work to non-deterministic tasks that we will also discuss in the paper:\n\nIf the environment is non-deterministic, less tight bounds can be established under additional assumptions.\nFor instance, let's assume that for all states and any given series of actions \\mathfrak{A}, the empirical returns R_i that an agent can observe when following \\mathfrak{A} differ by at most \\delta.\nThen all Q-values from the simplified MDP apply as lower bounds with margin \\delta:\nQ_true(s,a) >= Q_simplified(s,a) - delta\n\n\nOur method works on the graph structure only. \nIt is possible to build a similar graph from high-dimensional input, see for instance the graph of image inputs in this ICLR submission we discovered recently: https://openreview.net/pdf?id=HkxjqxBYDB\nGiven the graph structure, our method only depends on the number of nodes and edges in the graph but it is independent of the dimensionality of the original input. \nWe will add more details on the complexity of our algorithm in the paper.\nAnother question is whether a graph structure is still useful in high-dimensional spaces, because it may seem that it is even less likely for the same node to reappear: (1) we believe that in most cases, only a manifold of this high-dimensional space is actually visited (e.g. only some specific type of image), (2) there are typical attractor-states in many cases, e.g. in the corner of the state space or along edges of an obstacle, (3) in off-policy algorithms you can actively use exploration to re-visit states, (4) overall we have the impression that passing information along full trajectories is more useful than the cross-trajectory information exchange. Note that we pass this information without introducing the high gradient variance that is typically linked to full trajectory backups in Monte Carlo methods!\n\nDo these comments fully answer your questions, or are there any follow-up questions? Thanks!\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygTUxHKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2340/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2340/Authors|ICLR.cc/2020/Conference/Paper2340/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142823, "tmdate": 1576860551521, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Authors", "ICLR.cc/2020/Conference/Paper2340/Reviewers", "ICLR.cc/2020/Conference/Paper2340/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Comment"}}}, {"id": "HJgus3ATFH", "original": null, "number": 2, "cdate": 1571839135993, "ddate": null, "tcdate": 1571839135993, "tmdate": 1572972351326, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is trying to tackle the soft divergence issue in deep RL when algorithms combine function approximation, off-policy learning and bootstrapping, which is also called deadly triad by Sutton & Barto (2018). The paper proposes a way to represent the transitions in the replay memory as a data graph, then construct a simple MDP from it. Much more accurate Q values could be computed from the simple MDP and it provides a lower bound for the Q-values in the original problem. In this way, the method becomes less prone to soft divergence.\n\nThe idea of constructing a smaller MDP whose Q-values can be computed exactly by dynamic programming on tabular states, then use these Q-values to help dealing with the instability issues in deep RL is very interesting. In the rebuttal, I'd like the authors to address my major concern of the paper, where the proposed method seems to assume that the finite number of transitions could form a graph, which might not be always true. In typical continuous state spaces, the same state might not appear twice in the sampled transitions. In these cases, the graph becomes a number of disconnected chains and the Q-values from this MDP might not be accurate. Maybe I'm missing something, it's not very clear to me how the proposed method could be applied in the common case in deep RL where there's seldom a loop and the states are rarely visited twice. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575520326017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Reviewers"], "noninvitees": [], "tcdate": 1570237724233, "tmdate": 1575520326032, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review"}}}, {"id": "rygpYvGTKr", "original": null, "number": 1, "cdate": 1571788676779, "ddate": null, "tcdate": 1571788676779, "tmdate": 1572972351277, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes Qgraph, an algorithm that  addresses the problem of extrapolation error that appear in RL tasks with continuous action spaces. The authors describe a method to construct a graph from transitions generated by some policy. In this graph nodes correspond to states and (s, a, r, s\u2019, t) define transitions between these nodes. Then this representation is simplified and  used to compute Q-values using methods for tabular MDPs.\n\nThe related work section is missing several methods that attempt to address the same problem. Batch Constrained Q-learning (Fujimoto etal, 2018) introduces a formulation of Q-learning that constrains action selection with a generative model trained on a replay buffer in order to omit unseen actions. BEAR-QL (Kumar etal, 2019) describes a similar approach that uses a hard constraint based on MMD. It would be interesting to discuss connections with the recent work on off-policy batch RL.\n\nThe clarity of the paper can be improved. In particular, I have several questions regarding the method:\n1) How are node of the graph are constructed? Does one state correspond to a single node or several states are merged into a different node?\n2) How the actions are selected?\n3) What are the assumption regarding the initial state distribution? Does the set of initial states have to be finite?\n4) If two similar states appear in different branches of the graphs, are the corresponding nodes merged or not?\n5) If the considered environments are deterministic, what is the motivation for stochastic approximation of dynamic programming?\n\nThe approach has several major limitations. One of the main limitations of the approach is that it can be applied only to deterministic tasks. Although it is not stated clearly in the paper, it seems also requires to have a finite set of initial states. \n\nThe experimental evaluation is performed on a limited set of tasks and it is rather unclear whether the method can be scaled to higher dimensional control problems.\n\nOverall, I feel that the paper needs to be significantly improved. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575520326017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Reviewers"], "noninvitees": [], "tcdate": 1570237724233, "tmdate": 1575520326032, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review"}}}, {"id": "HJg27xDRtH", "original": null, "number": 3, "cdate": 1571872803755, "ddate": null, "tcdate": 1571872803755, "tmdate": 1572972351229, "tddate": null, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "invitation": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper aims to build an understanding of deep RL. Because RL remains under-investigated from a theoretical point of view. Many algorithms use function approximation, off-policy learning and bootstrapping together--This is an unstable combination of techniques. In this paper, the authors propose a graph-perspective on the replay memory which allows to analyze the structure of deep RL.\n\nThe paper aims at an important issue in deep RL. The motivation of the paper is meaningful.\nThe paper gives a good summary of the previous related works in Section2.\n\nThe paper in the current form needs to be polished again. To obtain a better score, I suggest the authors to modify this paper in these ways: First, the introduction section needs to provide more details, including the pros and cons of previous related works on the research problem of this paper, the challenges you face when dealing with this issue and the contributions; Second, there were more than a few spelling and grammatical errors, please proofread the work and improve the writing; Third, the paper lacks logic in writing. The writing from Section.2 to Section.6 needs to be organized better. It is difficult for readers to grasp the key ideas of the paper through a quick assessment.\nThe paper focuses on the understanding of RL when deep Q-learning diverges, however, most of the conclusions in the paper are not based on the necessary theoretical proof, but the observations on the experiments.\n\nIt would be better if this paper can provide a clear illustration for the proposed method as well as the experiments section."}, "signatures": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2340/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sabrina.hoppe@de.bosch.com", "marc.toussaint@informatik.uni-stuttgart.de"], "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "authors": ["Sabrina Hoppe", "Marc Toussaint"], "pdf": "/pdf/f68a27754ceb28198a6bc5348f9b2d7509c3759d.pdf", "TL;DR": "We link the graph-structure of the replay memory to soft divergence and propose Qgraphs to stabilize model-free off-policy deep RL.", "abstract": "In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions.  We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in - resulting in a Qgraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. Qgraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.\n", "keywords": ["deep learning", "reinforcement learning", "model-free reinforcement learning", "Q-learning", "DDPG"], "paperhash": "hoppe|qgraphbounded_qlearning_stabilizing_modelfree_offpolicy_deep_reinforcement_learning", "original_pdf": "/attachment/c5cae7c14b45b2912c5fd0aba8ef47b33b25e21c.pdf", "_bibtex": "@misc{\nhoppe2020qgraphbounded,\ntitle={Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\nauthor={Sabrina Hoppe and Marc Toussaint},\nyear={2020},\nurl={https://openreview.net/forum?id=HygTUxHKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygTUxHKwH", "replyto": "HygTUxHKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2340/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575520326017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2340/Reviewers"], "noninvitees": [], "tcdate": 1570237724233, "tmdate": 1575520326032, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2340/-/Official_Review"}}}], "count": 9}