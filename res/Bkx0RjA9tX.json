{"notes": [{"id": "Bkx0RjA9tX", "original": "Bklwu0TqK7", "number": 942, "cdate": 1538087893880, "ddate": null, "tcdate": 1538087893880, "tmdate": 1557413177457, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bygj0yBxgV", "original": null, "number": 1, "cdate": 1544732626704, "ddate": null, "tcdate": 1544732626704, "tmdate": 1545354512732, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Meta_Review", "content": {"metareview": "All reviewers recommend accept. \nDiscussion can be consulted below.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Clear accept ratings from reviewers."}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper942/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353024771, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353024771}}}, {"id": "ByeMpnB_CQ", "original": null, "number": 7, "cdate": 1543163066208, "ddate": null, "tcdate": 1543163066208, "tmdate": 1543163066208, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "rJxuRDGPCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "Nice experiment, but missing related work", "comment": "When I saw this description, I thought you were comparing against Clark and Gardner 2018 (https://arxiv.org/abs/1710.10723; DocQA).  I hadn't seen Weaver before, and I was surprised there there hasn't been a comparison between Weaver and DocQA (so I'm not actually sure which is better).  DocQA only requires training with two paragraphs at a time, not the full document, so the argument about scalable training rings a bit hollow (it's a constant factor, not dependent on document length).  It'd be best to compare against that work also if you want to make claims about multi-paragraph performance, or anything really on TriviaQA (looks like DocQA has also long since been beaten)."}, "signatures": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "rJg7OxYc3X", "original": null, "number": 3, "cdate": 1541210218782, "ddate": null, "tcdate": 1541210218782, "tmdate": 1543120167437, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "content": {"title": "Good ideas, more clarification about results/relevant work is necessary", "review": "This paper proposes a generative approach to textual QA on SQUAD and visual QA on CLEVR dataset, where, a joint distribution over the question and answer space, given the context (image or Wikipedia paragraphs) is learned (p(q,a|c)). During inference the answer is selected by argmax p(q,a|c) that is equal to p(a|c,q) if the question is given. Authors propose an architecture shown in Fig. 3 of the paper, where generation of each question word is condition on the corresponding answer, context and all the previous words generated in the question so far. The results compared to discriminative models are worse on SQUAD and CLEVR. Nevertheless, authors show that given the nature of the model that captures more complex relationships, the proposed model performs better than other models on a subset of SQUAD that they have created based on answer type (number/date/people), and also on adversarial SQUAD. \n\nComments / questions:\n\nThe paper is well written, except for a few parts mentioned below, all the equations / components are explained clearly. The motivation of the paper is clearly stated as using generative modelling in (V)QA to overcome biases in these systems, e.g., answering questions by just using word matching and ignoring the context (context=image or Wikipedia paragraph). I have the following questions / comments about the paper which addressing them by authors will help to better understand/evaluate the paper:\n1.\tIn page 3 on the top of section 2.3, can authors provide a more clear explanation of the additional 32-dimensional embedding added to each word representation? Also in Table 2, please add an ablation how much gain are you getting from this?\n2.\tIn the same page (page 3), section 2.4, paragraph 2, put the equation in a separate line and number it + clearly explain how you have calculated s^{endpoints} and s{length}.\n3.\tIn page 4 section 2.5.2 paragraph 2, the way the bias term is calculated and the incentive behind it is not clear. Can authors elaborate on this?\n4.\tIn page 6 section 3.2 the first paragraph authors claim that their model is performing multihop reasoning on CLEVR, while there is no explicit component in their model to perform multiple rounds of reasoning. Can authors clarify their statement? \n5.\tIn section 3.3 the third paragraph, where authors explain the question agnostic baselines, can they clarify what they mean by \u201cthe first answer of the correct type\u201d? \n6.\tIn Table 5 and section 3.4 the second paragraph, authors are stating that \u201c\u2026 The improvement may be due to the model\u2019s attempt to explain all question words, some of which may be unlikely under the distractor\u201d. It is very important that the authors do a complete ablation study similar to that of Table 2 to clarify how much gain is achieved using each component of generative model. \n7.\tIn page 8 under related works: \na.\tIn paragraph 2 where authors state \u201cDuan et al. (2017) and Tang et al. (2017) train answering and generation models with separate parameters, but add a regularisation term that encourages the models to be consistent. They focus on answer sentence selection, so performance cannot easily be compared with our work.\u201d. I do not agree that the performance can not be compared, it is easily comparable by labeling a sentence containing the answer interval as the answer sentence. Can authors provide comparison of their work with that of Duan et al. (2017) and Tang et al. (2017)?\nb.\tIn the same paragraph as 7.a, the authors have briefly mentioned \u201cEchihabi & Marcu (2003) describe an earlier method for answering questions in terms of the distribution of questions given answers.\u201d Can they provide a more clear explanation of this work and its relation to / difference with their work? \n\n////////////\nI would like to thank authors for providing detailed answers to my questions. After reading their feedback, I am now willing to change my score to accept. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "cdate": 1542234341821, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838074, "tmdate": 1552335838074, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJe5i_MDRX", "original": null, "number": 6, "cdate": 1543084194093, "ddate": null, "tcdate": 1543084194093, "tmdate": 1543084194093, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "HyxAZ3Q82m", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the helpful comments and feedback, which will let us improve the final version.\n\n- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?\n\nWe don't think there's a problem here: each QA pair can be viewed as a sample from the space of QAs that can be asked for that context, and the model will learn to capture this distribution.\n\n> - Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?\n\nWe believe the issue here is because even though the loss function is mathematically equivalent, our factorization requires us to learn a conditional language model, and the discriminative loss function does not provide enough learning signal to train such a model.\n\n> - Section 3.1 (and section 2.6): Can you back up your claim of \"modeling more complex dependencies\" in the generative case?  Is that really what's going on?  How can we know?  What does \"modeling more complex dependencies\" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.\n\nThanks for the feedback. The intuition behind this statement is that a generative model has to learn more information connecting the question and document, because generating a question is more difficult than answering it. We agree that it is unclear as currently written, and will rephrase or remove it.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "rkehP_GP0m", "original": null, "number": 5, "cdate": 1543084131934, "ddate": null, "tcdate": 1543084131934, "tmdate": 1543084131934, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "ryeIrWQch7", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the review and constructive comments! \n\nThe main concern is that our results on SQuaD are beneath the current state of the art. Our generative approach means that our architecture is very different from existing models - we can't simply change the loss function for BiDAF, for example. These discriminative architectures have been carefully iterated on by a large community for several years. The fact that we are within a few points with the first generative approach is encouraging, and it seems reasonable that significant improvements would be possible with more development. \n\nWe also emphasize that our model outperforms all discriminative models on adversarial SQuAD, demonstrating that it has learnt something more robust. We also show that our architecture can perform multi-hop reasoning, which has not been shown for any other strong SQuAD model.\n\n\n> In Table 1, it should be clear if the authors could category those models into with/without ELMo for easy compassion. Furthermore, it is unclear how the authors select those baselines since there are many results on the SQuAD leaderboard. For example, there are many published systems outperformed e.g., RaSOR. \n\nTo keep the results table to a manageable size, we included a representative sample of existing approaches. \n\n> During inference, generating answer candidates should be important. How the number of candidate affects the results and the inference time? \nThe inference time grows linearly in the number of answer candidates. We found that the beam starts to saturate at about 100 answers, covering about 99% of correct answers. \n\nModel                   \t\t  \t       EM / F1                      Inf Speed for Valid (sec)\nGQA, 250 answer candidates                  76.8 / 83.7\t\t\t         617.69\n200 answer candidates\t\t      76.6 / 83.4\t\t                     535.35\n100 answer candidates\t\t      76.2 / 83.1\t\t\t         359.67\n50 answer candidates\t\t\t      74.6 / 81.4\t\t\t         262.42\n10 answer candidates\t\t\t      55.7 / 61.4\t\t\t         200.91                 \n\n\n> In SQuAD dataset, its answers often contain one or two tokens/words. What is the performance if removed length of answer feature?\n\nModel                   \t\t  Generative (EM)                +Fine Tuning (EM / F1)\nGQA                        \t\t\t  72.3                               76.8 / 83.7\nNo answer length feature                     72.0\t\t\t     73.8 / 80.1\n\nIt is quite interesting that the length feature is particularly helpful for fine-tuning. During generative training, the question generation model is mostly exposed to short answers, because it is only shown the gold answer. However, at test time, it mostly sees very long answers, because most possible answers are long, and its performance may be weak on these. The answer length feature makes it easy for fine-tuning to compensate for this imbalance. We will update the paper with this ablation.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "SklmEufv0Q", "original": null, "number": 4, "cdate": 1543084075304, "ddate": null, "tcdate": 1543084075304, "tmdate": 1543084075304, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "rJg7OxYc3X", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the review and detailed feedback, which we\u2019ll be happy to address in the final submission. Answers to questions are beneath.\n\n> In page 3 on the top of section 2.3, can authors provide a more clear explanation of the additional 32-dimensional embedding added to each word representation? Also in Table 2, please add an ablation how much gain are you getting from this?\n\nThanks, have expanded the explanation, and an ablation is beneath. Including this embedding makes it easier for the model to learn the relationship between the document word and answer. However, it does significantly increase the computational cost of inference with the model, because we have to compute a separate contextualized document representation for each candidate answer.\n\nModel                   \t\t  Generative (EM)                      Fine Tuning (EM / F1)\nGQA                        \t\t\t  72.3                                       76.8 / 83.7\nNo 32 dim embedding                          66.9\t\t\t\t 69.2 / 75.9\n\n\n>\tIn page 6 section 3.2 the first paragraph authors claim that their model is performing multihop reasoning on CLEVR, while there is no explicit component in their model to perform multiple rounds of reasoning. Can authors clarify their statement? \nThe model can perform multiple rounds of reasoning as a by-product of explaining the question word-by-word. On CLEVR, the model must explain each question word in turn, allowing it to track the relevant objects in complex chains of reasoning (see Figure 4).\n\n>\tIn page 4 section 2.5.2 paragraph 2, the way the bias term is calculated and the incentive behind it is not clear. Can authors elaborate on this?\nThe motivation is that it can allow the model to quickly focus its attention on relevant parts of the paragraph (which is typically several hundred words). We found that it improved convergence pruning out irrelevant parts of the input.\n\n> \tIn section 3.3 the third paragraph, where authors explain the question agnostic baselines, can they clarify what they mean by \u201cthe first answer of the correct type\u201d? \nHere, we simply mean that e.g. for a question whose answer is a person, we return the first person in the evidence paragraph. We will clarify this in the paper.\n\n>\tIn Table 5 and section 3.4 the second paragraph, authors are stating that \u201c\u2026 The improvement may be due to the model\u2019s attempt to explain all question words, some of which may be unlikely under the distractor\u201d. It is very important that the authors do a complete ablation study similar to that of Table 2 to clarify how much gain is achieved using each component of generative model. \n\nWe chose not to present ablations for adversarial SQuAD in the submission, because there is no validation data, so we only performed a single run on the data with our best model (selected on the standard SQuAD data). The fact that our approach performs better than models that outperform it on SQuAD is strong evidence that it has learnt something more robust from the same training data. Please let us know if you still think including these ablations would be helpful.\n\n> a.\tIn paragraph 2 where authors state \u201cDuan et al. (2017) and Tang et al. (2017) train answering and generation models with separate parameters, but add a regularisation term that encourages the models to be consistent. They focus on answer sentence selection, so performance cannot easily be compared with our work.\u201d. I do not agree that the performance can not be compared, it is easily comparable by labeling a sentence containing the answer interval as the answer sentence. Can authors provide comparison of their work with that of Duan et al. (2017) and Tang et al. (2017)?\nThere isn't actually enough detail in these papers to replicate their non-standard experimental setup---given that there are only a few sentences in each paragraph, a lot would depend on how exactly they segmented the input into sentences. However, the reported accuracies are only a few percentage points higher than our approach on this much easier task, so it seems unlikely that their results would be competitive. \n\n> b.\tIn the same paragraph as 7.a, the authors have briefly mentioned \u201cEchihabi & Marcu (2003) describe an earlier method for answering questions in terms of the distribution of questions given answers.\u201d Can they provide a more clear explanation of this work and its relation to / difference with their work? \nEchihabi & Marcu train a model for p(q|a,c) using a rather complex combination of heuristics and classical machine translation methods, and return the answer maximizing this distribution. A conceptual difference is that our approach models p(q,a|c), and to our knowledge is the first generative question answering model.  Beyond the fact that they use a model of p(q|a) for question answering, there isn't much overlap in terms of motivation or techniques.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "rJxuRDGPCQ", "original": null, "number": 3, "cdate": 1543083983746, "ddate": null, "tcdate": 1543083983746, "tmdate": 1543083983746, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "New experiment on long-context question answering", "comment": "Thanks to all the reviewers, we are happy that they all found the ideas in the paper to be interesting.\n\nWe\u2019ve added one additional experiment beyond what was requested (Section 3.5). We explore question answering when the answer can be contained in one of many paragraphs. This task is computationally expensive to train properly with discriminative models, because at training time you would ideally want to discriminate against all the negative answers from all paragraphs. In our approach, most of the work is done by the model of p(q|a,c), which only depends the paragraph c containing the gold answer a. That means we can train the model using single paragraphs, but test it on multiple paragraphs.\n\nWe outperform the best previous work that was trained on single paragraphs (as ours was) by almost 10 F1, and the best approaches trained on multiple paragraphs by 2.5 F1. This experiment highlights a further advantage of generative question answering.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "rkllvsQsTX", "original": null, "number": 2, "cdate": 1542302551798, "ddate": null, "tcdate": 1542302551798, "tmdate": 1542302551798, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "This paper should be accepted", "comment": "Just starting a conversation with other reviewers.  I feel pretty strongly that this paper should be accepted.  We should not be fixating on leaderboard performance numbers and blackbox comparisons.  Science is much more broad than \"who has the best experimental result\".  The presented method in the paper works well, it's a very interesting, novel idea, and the paper is well written."}, "signatures": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}, {"id": "HyxAZ3Q82m", "original": null, "number": 1, "cdate": 1540926469637, "ddate": null, "tcdate": 1540926469637, "tmdate": 1541533558538, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "content": {"title": "Great idea, well executed, well written", "review": "This paper introduces a generative model for question answering.  Instead of modeling p(a|q,c), the authors propose to model p(q,a|c), factorized as p(a|c) * p(q|a,c).  This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working.                                                       \n                                                                                                     \nReactions:                                                                                           \n- Section 2.1: Is there a bias problem here, where you're only ever training with the correct answer?  Oh, I see you covered that in section 2.6.  Great.\n- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?\n- Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?\n- Section 3.1 (and section 2.6): Can you back up your claim of \"modeling more complex dependencies\" in the generative case?  Is that really what's going on?  How can we know?  What does \"modeling more complex dependencies\" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.\n- Section 3.3: Your goal here seems similar to the goal of Clark and Gardner (2018), trying to correctly calibrate confidence scores in the face of SQuAD-like data, and similar to the goals of adding unanswerable questions in SQuAD 2.0.  I know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.\n                                                                                                     \nClarity issues:                                                                                      \n- Bottom of page 2, \"sum with a vector of size d\" - it's not clear to me what this means.            \n- Top of page 3, \"Answer Encoder\", something is off with the sentence \"For each word representation\" \n- Section 2.5, \"we first embed words independently of the question\" - did you mean \"of the _context_\"?\n- Section 2.5.2 - it's not clear to me how that particular bias mechanism \"allows the model to easily filter out parts of the context which are irrelevant to the question\".  The bias mechanism is independent of the question.\n- Section 2.7 - when you said \"beam search\", I was expecting a beam over the question words, or something.  I suppose a two-step beam search is still a beam search, it just conjured the wrong image for me, and I wonder if there's another way you can describe it that better evokes what you're actually doing.\n- Section 3.1 - \"and are results...\" - missing \"competitive with\"?                                   \n- Last sentence: \"we believe their is\" -> \"we believe there is\" ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "cdate": 1542234341821, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838074, "tmdate": 1552335838074, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeIrWQch7", "original": null, "number": 2, "cdate": 1541185853832, "ddate": null, "tcdate": 1541185853832, "tmdate": 1541533558048, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "content": {"title": "Interesting ideas but weak experiment results", "review": "In this paper, authors proposed a generative QA model, which optimizes jointly the distribution of questions and answering given a document/context. More specifically, it is decomposed into two components: the distributions of answers given a document, which is modeled by a single layer neural network; and the distribution of questions given an answer and document, which is modeled by a seq2seq model with a copy mechanism. During inference, it firstly extracts the most likely answer candidates, then evaluates the questions conditioned on the answer candidates and document and finally returns the answer with the max joint score from two aforementioned components.\n\n\nPros: \nThe paper is well written and easy to follow. \n\nThe ideas are also very interesting. \n\nIt gives a good ablation study and shows importance of each component in the proposed model.\n\n\nCons:\nThe empirical results are not good. For example, on the SQuAD dataset, since the proposed model also used ELMo (the large pre-trained contextualized embedding), cross attentions and self-attentions, it should be close or better than the baseline BiDAF + Self Attention + ELMo. However, the proposed model is significantly worse than the baseline (83.7 vs 85.6 in terms of F1 score). From my experience of the baseline BiDAF + Self Attention + ELMo, it obtains 1 more point gain if you fine tune the models.  On CLEVER dataset, I agree that incorporating with MAC cells will help the performance.\n\nIn Table 1, it should be clear if the authors could category those models into with/without ELMo for easy compassion. Furthermore, it is unclear how the authors select those baselines since there are many results on the SQuAD leaderboard. For example, there are many published systems outperformed e.g., RaSOR. \n\nQuestions:\nDuring inference, generating answer candidates should be important. How the number of candidate affects the results and the inference time? \n\nIn SQuAD dataset, its answers often contain one or two tokens/words. What is the performance if removed length of answer feature?\n\nDuring the fine turning step, have you tried other number of candidates?   ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper942/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Review", "cdate": 1542234341821, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838074, "tmdate": 1552335838074, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xkn3XCjm", "original": null, "number": 1, "cdate": 1540402342561, "ddate": null, "tcdate": 1540402342561, "tmdate": 1540402342561, "tddate": null, "forum": "Bkx0RjA9tX", "replyto": "Bkx0RjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "content": {"title": "SQuAD Test Results", "comment": "Our SQuAD test results were missing from the submission because of a technical problem with the evaluation server. Our results are now available as 77.090 (Exact Match) 83.931 (F1). This model was submitted before the ICLR deadline,."}, "signatures": ["ICLR.cc/2019/Conference/Paper942/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Question Answering: Learning to Answer the Whole Question", "abstract": "Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our  question  answering  (QA)  model  is  implemented  by  learning  a  prior  over answers,  and  a  conditional  language  model  to  generate  the  question  given  the answer\u2014allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word.  Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code.", "keywords": ["Question answering", "question generation", "reasoning", "squad", "clevr"], "authorids": ["mikelewis@fb.com", "angelafan@fb.com"], "authors": ["Mike Lewis", "Angela Fan"], "TL;DR": "Question answering models that model the joint distribution of questions and answers can learn more than discriminative models", "pdf": "/pdf/f0b651779b65de543999a2032bc9173aba508a2c.pdf", "paperhash": "lewis|generative_question_answering_learning_to_answer_the_whole_question", "_bibtex": "@inproceedings{\nlewis2018generative,\ntitle={Generative Question Answering: Learning to Answer the Whole Question},\nauthor={Mike Lewis and Angela Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkx0RjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper942/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614012, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkx0RjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper942/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper942/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper942/Authors|ICLR.cc/2019/Conference/Paper942/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper942/Reviewers", "ICLR.cc/2019/Conference/Paper942/Authors", "ICLR.cc/2019/Conference/Paper942/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614012}}}], "count": 12}