{"notes": [{"id": "HkfwpiA9KX", "original": "HJx5SVpqKQ", "number": 812, "cdate": 1538087871216, "ddate": null, "tcdate": 1538087871216, "tmdate": 1545355433786, "tddate": null, "forum": "HkfwpiA9KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkeSGjpelE", "original": null, "number": 1, "cdate": 1544768269328, "ddate": null, "tcdate": 1544768269328, "tmdate": 1545354482870, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Meta_Review", "content": {"metareview": "The authors present an interesting approach for combining finite state automata to compose new policies using temporal logic. The reviewers found this contribution interesting but had several questions that suggests that the current paper presentation could be significantly clarified and situated with respect to other literature. Given the strong pool of papers, this paper was borderline and the authors are encouraged to revise their paper to address the reviewers\u2019 feedback.\n\n\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Interesting combination of temporal logic for constructing new RL policies, presentation should be clearer"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper812/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353078716, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353078716}}}, {"id": "BJgNUg4wl4", "original": null, "number": 6, "cdate": 1545187403603, "ddate": null, "tcdate": 1545187403603, "tmdate": 1545187403603, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "BJx8nZ85JN", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "Thanks for the followup", "comment": "Thank you for the additional comments and adjustment to the score. We acknowledge that comparison with a good number of state-of-the-art methods would better situate our work in the field. Our work presented here is a combination of both reward engineering (using TL) and skill composition, along with the hierarchical policy structure that arises naturally with the framework. It is difficult to find other work with a similar combination. Therefore, an elaborate and fair comparison with other methods would be a contribution in it self which we will consider in the future. As for the high variance on the right side of figure 5, it mostly depends on how the task is initialized at each episode (initialization is random). Some initialization makes it easier for the task to be accomplished than others. As long as the episode length is consistently below the max value, the agent is always able to complete the task. As the reviewer mentioned, a more thorough analysis will be helpful which we will try to incorporate in future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper812/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "BJx8nZ85JN", "original": null, "number": 5, "cdate": 1544343982026, "ddate": null, "tcdate": 1544343982026, "tmdate": 1544344260904, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "BJxpVMfqCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "Appreciate Improvements", "comment": "I'd like to say that I appreciate the improvements to the paper and have updated my previous rating accordingly. I'm still not totally convinced that the other methods I mentioned aren't relevant and also now I have some \nmild concerns about the high variance in the std reported, making it difficult to assess real performance gains are real or not (right side of figure 5). I would also liked to have seen the additional analysis mentioned in this post."}, "signatures": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "BylKx8NIpX", "original": null, "number": 3, "cdate": 1541977584644, "ddate": null, "tcdate": 1541977584644, "tmdate": 1544343997131, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "content": {"title": "Review", "review": "This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. This work was very difficult to follow, so it is somewhat unclear what were the main contributions (since much of this seems to be covered by other works as referenced within the paper and as related to similar unreferenced works below). Moreover, regarding the experiments, many things were unclear (some of the issues are outlined below). While the overall idea of using logic in this way to help with skill composition is interesting and exciting, I believe several things must be addressed with this work. This includes: situating this work more clearly against existing similar works which use logic in this way, clearly defining the novel contributions of this work as compared to those and others, overall making the methodology more clear and specific (including experimental methodology), and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., HRL multi-task learning, meta-learning) to emphasize the need/importance of this work \u2014 I am aware that at least 1 HRL work is mentioned, but this work is not really contrasted against it to help situate it.\n\nQuestions/Concerns about Experiments:\n\n+ Does Figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step, or 5 episodes, each from a separate run averaged together? It is a bit unclear especially because the main text and the figure caption slightly differ. Also, average discounted return is somewhat different than average return,  suggest updating the label to be clear also with the discount factor used.\n+ What were the standard deviations for this across experiments? Even with averaging it seems that these runs are very high variance, would be good to understand what variance bounds to expect if using this method.\n+ Why were average discounted returns reported in Figure 5 and not in Table 1?\n+  What were the standard deviations on success rate and training time? Also what about sample complexity? \n+ To my understanding the benefit here is reusability of learned skills via the automata methods described here. It would have made sense to compare against other HRL or multi-task learning methods in addition to just SQL or learning from scratch. For example how would MAML compare to this?\n+ It is also unclear whether the presented results in Table 1 and Figure 5 are on the real robot or in simulation. The main text says, \u201cAll of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning.\u201d So does this mean that Figure 5 is simulated results and Table 1 is on the real robot?\n\n\n\nCitations that should likely be made:\n\n+ Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. \"Reinforcement Learning for LTLf/LDLf Goals.\"\u00a0arXiv preprint arXiv:1807.06333\u00a0(2018).\u00a0\n+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Decision-making with non-markovian rewards: From LTL to automata-based reward shaping.\"\u00a0 In\u00a0Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017. \n+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping.\" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017.\u00a0\n\n\nTypos/Suggested grammar edits:\n\n\u201cSkills learned through (deep) reinforcement learning often generalizes poorly across tasks and re-training is necessary when presented with a new task.\u201d \u2014> Often generalize poorly\n\n\u201cWe present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for convenient specification of complex temporal dependent tasks with logical expressions and construction of new skills from existing ones with no additional exploration.\u201d \u2014> Sentence kind of difficult to parse and is a run-on\n\n\u201cPolicies learned using reinforcement learning aim to maximize the given reward function and is often difficult to transfer to other problem domains.\u201d \u2014> ..and are often..\n\n\u201cby authors of (Todorov, 2009) and (Da Silva et al., 2009)\u201d \u2014> by Todorov (2009) and Da Silva et al. (2009) Also several other places where you can use \\citet instead of \\cite", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "cdate": 1542234371383, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808010, "tmdate": 1552335808010, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Sylz7XQ5CQ", "original": null, "number": 4, "cdate": 1543283482270, "ddate": null, "tcdate": 1543283482270, "tmdate": 1543283482270, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "H1gEQFfc0m", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "Response for reviewer 2", "comment": "Thank you for your comments.  The dimensional explosion of automaton states when composing many policies and its effect on composition is an interesting and practical problem worth looking into. Thank you also for catching the typos, we have incorporated the modifications in the updated paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "H1gEQFfc0m", "original": null, "number": 4, "cdate": 1543280924477, "ddate": null, "tcdate": 1543280924477, "tmdate": 1543280924477, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "content": {"title": "Nice paper that combines RL and constraints expressed by logical formulas", "review": "The contribution of the paper is to set up an automaton from scTLTL formulas, then corresponding MDP that satisfies the formulas is obtained by augmenting the state space with the automaton state and zeroing out transitions that do not satisfy the formula. This approach seems really useful for establishing safety properties or ensuring that constraints are satisfied, and it is a really nice algorithmic framework. The RL algorithm for solving the problem is entropy-regularized MDPs. The approach \u201cstitches\u201d policies using AND and OR operators, obtaining the overall optimal policy over the aggregate. Proofs just follow definitions, so they are straightforward, but I think this is a quality. The approach is quite appealing because it provides composition automatically. The paper is very well written.  The main problem I see with the work is that composition can explode the number of states in the new automaton and hence the new MDP. It would be interesting in future work to do \u201csoft\u201d ruling out of transitions rather than the \"hard\" approach used in the paper. The manipulation task provided is quite appealing, as the robot arm is of high dimensionality but the FSAs obtainedare discrete. Overall, the paper provides a very good contribution.\n\nSmall comments:\nEquation equation in Def 3 also proof of Theorem 2\nIn section,  -> In this section\nare it has -> and it has", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "cdate": 1542234371383, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808010, "tmdate": 1552335808010, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxpVMfqCQ", "original": null, "number": 3, "cdate": 1543279157169, "ddate": null, "tcdate": 1543279157169, "tmdate": 1543279157169, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "BylKx8NIpX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "response for reviewer 4", "comment": "Thank you for your comments and providing additional references. We try to address your questions as follows:\n\n1. \u201csituating this work more clearly against existing similar works which use logic in this way, ..., and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., HRL multi-task learning, meta-learning) to emphasize the need/importance of this work\u201d\n\nTo the best of our knowledge, the presented work is the first to use techniques in formal methods to simultaneously address optimal -AND- and -OR- task compositions and demonstrate the process in tasks with continuous state and action spaces. We make the distinction between skill composition and multi-task learning/meta-learning (such as MAML) where the latter often requires a predefined set of tasks/task distributions to learn and generalize from, whereas the focus of the former is to construct new policies from a library of already learned policies that achieve new tasks (often some combination of the constituent tasks) with little to no additional constraints on task distribution at learning time. Our focus here is on task composition and therefore did not compare with multi-task / meta learning methods. HRL is also not the focus here, it so happens that incorporating FSA into the MDP gives the resulting policy a hierarchical representation. Therefore, we chose mainly to contrast against other skill composition methods in Section 2. We have made this more clear in the updated paper.\n\t\n 2. \"Citations that should likely be made ...\"\n\nThe set of provided references aim to solve the non-Markovian reward decision process (NMRD) using temporal logic and automaton. The idea is similar to that of the FSA augmented MDP that we adopted with some differences (such as the requirement to manually define a set of rewards in additional to the logic specification, separation of state features and temporal goals, etc). However, the comparison is mainly between the above references and the FSA augmented MDP (Li et al., 2018) which is not the contribution of our work. \n\n3. \u201cDoes Figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step ...\u201d\n\nThe original Figure 5 shows the undiscounted episodic return (sum of undiscounted rewards over one episode) averaged over 5 evaluation episodes (without updating the policy in between). We have updated this result to be discounted return with standard deviations.\t\n\n4. \u201cWhat were the standard deviations for this across experiments? Even with averaging it seems that these runs are very high variance, would be good to understand what variance bounds to expect if using this method.\u201d\n\nWe have included the standard deviation in the learning curve. To our current understanding, the variance comes from two sources. The first is randomization of the environment configurations - some configurations make the task considerably easier to accomplish than others. The second is randomization over the automaton states at initialization. Some q states as easier to learn than others (for example $q_2$ compared to $q_1$ in Figure 4b). At each initialization, if a difficult q state is on the agent\u2019s path of reaching $q_f$, the agent may get stuck in that state receiving a low episodic return whereas in other episodes the agent may not have to deal with this state at all. \n\n5. \u201cWhy were average discounted returns reported in Figure 5 and not in Table 1?\u201d\n\nOriginally, Table 1 aims to report the performance of the learned policies in terms of task success rate whereas Figure 5 reports learning progress in terms of returns. We have updated Table 1 to include the average discounted returns.\n\n6. \u201cWhat were the standard deviations on success rate and training time? Also what about sample complexity?\u201d\n\nWe have added the standard deviations to Table 1. We don't currently have a quantitative analysis on sample complexity other than the learning curves. Hopefully we will perform such analysis in the future.\n\n7. \u201cIt is also unclear whether the presented results in Table 1 and Figure 5 are on the real robot or in simulation. The main text says, \u201cAll of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning.\u201d So does this mean that Figure 5 is simulated results and Table 1 is on the real robot?\u201d\n\nThis is correct, training is in simulation and evaluation is on the real robot. We have modified the text to make this clear in the paper.\n\nThank you also for catching the typos and suggesting grammar edits, those have been incorporated in the updated paper. We have also updated the experiment and results section.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "SylO40-9RQ", "original": null, "number": 2, "cdate": 1543278127880, "ddate": null, "tcdate": 1543278127880, "tmdate": 1543278127880, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "SygGIu4n2X", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "response for reviewer 3", "comment": "Thank you for your comments. The following are our attempts to address your concerns:\n\n1. \u201cWill this method work on composing scTLTL formula with temporal operators other than disjunction and conjunction?\u201d\n\nNot directly, however, if we have learned a policy for \u201ceventually A\u201d and a policy for \u201ceventually B\u201d where \u201cA\u201d and \u201cB\u201d are predicates, then it is possible to compose policies that satisfy any given scTLTL formula consisting of and only of \u201cA\u201d, \u201cB\u201d, \u201cnot A\u201d and \u201cnot B\u201d. This is an extension that we are working on.\n\n\n2. \u201cCan this approach deal with continuous state space and actions? This paper describes a discretization way, which, however, can introduce inaccuracies.\u201d \n\nOur method is able to learn with continuous state and action spaces as is shown in the robotic experiment. The only discrete state here is the automata state which corresponds to decompositions of the high level task.\n\n3. \u201cThe design of the skills is by hand, which restricts badly its usability.\u201d\n\nThe only hand-designed component is the scTLTL formula that specifies the task. This corresponds to the reward function that needs to be provided for most reinforcement learning algorithms. \n\n4. \u201cThe experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? \u201c\n\nThe FSA augmented MDP provides a natural hierarchy regardless of the RL algorithm used. Even using plain SQL results in a hierarchical policy. The reason we did not compare our method with other RL algorithms on a regular MDP is that it is difficult to specify a complex task using a non-temporal logic reward function. In our experience, if enough effort is put into reward design, we will end up with something very similar to the robustness of the original scTLTL formula, anything less will result in a faulty reward that makes the comparison less meaningful. Again, the focus of this work is more on the effective composition of learned skills and less on actually learning a skill.\n\nWe have incorporated a summary of our algorithm in Section 5 and also updated the experiment and results section with more information.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "BJxCCT-90X", "original": null, "number": 1, "cdate": 1543278038232, "ddate": null, "tcdate": 1543278038232, "tmdate": 1543278038232, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "ryeWe3Dc3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "content": {"title": "response for reviewer 1", "comment": "Thank you for your comments, We try to address your questions as follows:\n\n1. \u201cThe experiments demonstrate that this method can outperform SQL at skill composition. However, it is unclear how much prior knowledge is used to define the automaton. If prior knowledge is used to construct the FSA, then a missing comparison would be to first find the optimal path through the FSA and then optimize a controller to accomplish it. As the paper is not very clear, that might be the method in the paper. \u201d\n\t\nIn this work, all prior knowledge is encoded in the scTLTL formula which effectively acts\nas a \u201creward function\u201d. As mentioned in the end of Section 3.2, the automata is automatically generated from the scTLTL formula without taking additional information. Implicitly, learning with the FSA augmented MDP simultaneously finds a path in the FSA and the corresponding controller that leads the system towards the satisfying q-state. SQL and skill composition have access to the same amount of prior information.\n\n2. \u201cHow do you obtain the number of automaton states? \u201d\n\nThe automaton states are also automatically generated with off-the-shelf libraries. The translation from temporal logic formula to automaton is a topic in its own.\n\n3. \u201cIn Figure 1, are the state transitions learned or handcoded? Are they part of the policy's action space?\u201d\n\nState transitions are automatically generated with the FSA, they are not part of the action space. The states of the FSA (q states) are part of the state space and the transitions are augmented with the MDP\u2019s transitions (definition 3).\n\n4. \u201cIn section 3.2, you state  s_{t:t+k} |= f(s)<c \u21d4 f(s_t)<c    What does s without a timestep subscript refer to? Why does this statement hold?\u201d\n\nThis statement is a definition. It says that trajectory s_{t:t+k} satisfies predicate f(s) < c if  and only if the first state of the trajectory (s_t) satisfies the predicate. For example, if the predicate is  f(s) = 2*s+1, c =5, then a trajectory {s_0=0, s_1=7, s_2=8} satisfies the predicate f(s) < c because f(s_0) < c while trajectory {s_0=7, s_1=0, s_2=-1} does not satisfy. \n\n5. \u201cCan you specify more clearly what you assume known in the experiments? What is learned in the automata? In Figure 5, does SQL have access to the same information as Automata Guided Composition?\u201d \n\t\nLearning follows the same procedure as regular reinforcement learning. We design the scTLTL formula as task specification and we know the state and action spaces. The automata is embedded into the MDP using Definition 3. In Figure 5, SQL is used to learn a FSA augmented MDP and therefore has access to the same information.\n\nWe have added a summary of our algorithm in Section 5 and updated the experiment and results section with more information and clarity.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607224, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkfwpiA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper812/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper812/Authors|ICLR.cc/2019/Conference/Paper812/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers", "ICLR.cc/2019/Conference/Paper812/Authors", "ICLR.cc/2019/Conference/Paper812/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607224}}}, {"id": "SygGIu4n2X", "original": null, "number": 2, "cdate": 1541322826464, "ddate": null, "tcdate": 1541322826464, "tmdate": 1541533670486, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "content": {"title": "Interesting topic but little technique contribution", "review": "This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks. This method provides a structured solution for reusing learned skills (with scTLTL formulas), and can also help when new skills need to be involved in original tasks. The topic of the composition of skills is interesting. However, the joining of LTL and RL has been developed previously. The main contribution of this work is limited to the application of the previous techniques.\n\nThe proposed approach also has some limitations. \nWill this method work on composing scTLTL formula with temporal operators other than disjunction and conjunction?\nCan this approach deal with continuous state space and actions? This paper describes a discretization way, which, however, can introduce inaccuracies. \nThe design of the skills is by hand, which restricts badly its usability.\nThe experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? \n  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "cdate": 1542234371383, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808010, "tmdate": 1552335808010, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeWe3Dc3Q", "original": null, "number": 1, "cdate": 1541204968931, "ddate": null, "tcdate": 1541204968931, "tmdate": 1541533670278, "tddate": null, "forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "content": {"title": "More explanations are needed", "review": "This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies. This idea is motivated by the desirability of compositional policies. I find the idea compelling, but I am not sure the proposed method is a useful solution. Overall, the description of the method is difficult to follow. With more explanations (perhaps an algorithm box?), I would consider increasing my score.\n\nThe experiments demonstrate that this method can outperform SQL at skill composition. However, it is unclear how much prior knowledge is used to define the automaton. If prior knowledge is used to construct the FSA, then a missing comparison would be to first find the optimal path through the FSA and then optimize a controller to accomplish it. As the paper is not very clear, that might be the method in the paper. \n\nQuestions:\n- How do you obtain the number of automaton states? \n- In Figure 1, are the state transitions learned or handcoded? Are they part of the policy's action space?\n- In section 3.2, you state  s_{t:t+k} |= f(s)<c \u21d4 f(s_t)<c    What does s without a timestep subscript refer to? Why does this statement hold?\n\nCan you specify more clearly what you assume known in the experiments? What is learned in the automata? In Figure 5, does SQL have access to the same information as Automata Guided Composition?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper812/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automata Guided Skill Composition", "abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly\nacross tasks and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in formal methods with reinforcement\nlearning (RL) that allows for the convenient specification of complex temporal\ndependent tasks with logical expressions and construction of new skills from existing\nones with no additional exploration. We provide theoretical results for our\ncomposition technique and evaluate on a simple grid world simulation as well as\na robotic manipulation task.", "keywords": ["Skill composition", "temporal logic", "finite state automata"], "authorids": ["xli87@bu.edu", "yaoma@bu.edu", "cbelta@bu.edu"], "authors": ["Xiao Li", "Yao Ma", "Calin Belta"], "TL;DR": "A formal method's approach to skill composition in reinforcement learning tasks", "pdf": "/pdf/26a3f8873bc0bfc0cdb73c3398b919c75853568b.pdf", "paperhash": "li|automata_guided_skill_composition", "_bibtex": "@misc{\nli2019automata,\ntitle={Automata Guided Skill Composition},\nauthor={Xiao Li and Yao Ma and Calin Belta},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfwpiA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper812/Official_Review", "cdate": 1542234371383, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkfwpiA9KX", "replyto": "HkfwpiA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper812/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808010, "tmdate": 1552335808010, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper812/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}