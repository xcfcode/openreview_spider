{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573616352, "tcdate": 1521573616352, "number": 307, "cdate": 1521573616016, "id": "Hyugk11cM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJD_sj0If", "replyto": "BJD_sj0If", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No Spurious Local Minima in a Two Hidden Unit ReLU Network", "abstract": "Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al. (2017); Lee et al. (2016) show that  gradient descent converges to the global minimizer.", "pdf": "/pdf/9fd682c4a719a9720dc9f16e0ef306624b7c3a22.pdf", "TL;DR": "Recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs.", "paperhash": "wu|no_spurious_local_minima_in_a_two_hidden_unit_relu_network", "_bibtex": "@misc{\nwu2018no,\ntitle={No Spurious Local Minima in a Two Hidden Unit Re{LU} Network},\nauthor={Chenwei Wu, Jiajun Luo, Jason D. Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=B14uJzW0b},\n}", "keywords": ["Non-convex optimization", "Deep Learning"], "authors": ["Chenwei Wu", "Jiajun Luo", "Jason D. Lee"], "authorids": ["wucw14@mails.tsinghua.edu.cn", "jiajunlu@usc.edu", "jasonlee@marshall.usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730166256, "tcdate": 1518414703695, "number": 91, "cdate": 1518414703695, "id": "BJD_sj0If", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJD_sj0If", "original": "B14uJzW0b", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "No Spurious Local Minima in a Two Hidden Unit ReLU Network", "abstract": "Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al. (2017); Lee et al. (2016) show that  gradient descent converges to the global minimizer.", "pdf": "/pdf/9fd682c4a719a9720dc9f16e0ef306624b7c3a22.pdf", "TL;DR": "Recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs.", "paperhash": "wu|no_spurious_local_minima_in_a_two_hidden_unit_relu_network", "_bibtex": "@misc{\nwu2018no,\ntitle={No Spurious Local Minima in a Two Hidden Unit Re{LU} Network},\nauthor={Chenwei Wu, Jiajun Luo, Jason D. Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=B14uJzW0b},\n}", "keywords": ["Non-convex optimization", "Deep Learning"], "authors": ["Chenwei Wu", "Jiajun Luo", "Jason D. Lee"], "authorids": ["wucw14@mails.tsinghua.edu.cn", "jiajunlu@usc.edu", "jasonlee@marshall.usc.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730166256, "tcdate": 1509134187903, "number": 755, "cdate": 1518730166246, "id": "B14uJzW0b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "B14uJzW0b", "original": "rkQO1MbAb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "No Spurious Local Minima in a Two Hidden Unit ReLU Network", "abstract": "Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al. (2017); Lee et al. (2016) show that  gradient descent converges to the global minimizer.", "pdf": "/pdf/1d79cc1c095e26c810f84f769c88abf960d0e2af.pdf", "TL;DR": "Recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs.", "paperhash": "wu|no_spurious_local_minima_in_a_two_hidden_unit_relu_network", "_bibtex": "@misc{\nwu2018no,\ntitle={No Spurious Local Minima in a Two Hidden Unit Re{LU} Network},\nauthor={Chenwei Wu and Jiajun Luo and Jason D. Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=B14uJzW0b},\n}", "keywords": ["Non-convex optimization", "Deep Learning"], "authors": ["Chenwei Wu", "Jiajun Luo", "Jason D. Lee"], "authorids": ["wucw14@mails.tsinghua.edu.cn", "jiajunlu@usc.edu", "jasonlee@marshall.usc.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}