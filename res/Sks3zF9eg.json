{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396604036, "tcdate": 1486396604036, "number": 1, "id": "H14a3fUux", "invitation": "ICLR.cc/2017/conference/-/paper473/acceptance", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejecting the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396604543, "id": "ICLR.cc/2017/conference/-/paper473/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396604543}}}, {"tddate": null, "tmdate": 1484576329247, "tcdate": 1484309714624, "number": 5, "id": "rJs0NS8Le", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "rJsjvK6Xg", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "answers update", "comment": "Update:\n\n(1) The deviation grows for the first few iterations and then it remains stuck to the same value (an example from 8 Digit sum vanilla RNN, https://drive.google.com/file/d/0B0gSUcSu6RNEZDlWdGJBQWlKbUk/view?usp=sharing)\n\n(3) Experiments on new tasks (such as sorting, repeating, reversing and max) gave mixed results (in some cases sin is better, in some tanh is better, in others they behave similarly). Finding more tasks where sin is beneficial is currently outside of the scope of the paper (see the general comment), even though we do agree that it would be good to have such results. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "tmdate": 1484310531984, "tcdate": 1484310531984, "number": 6, "id": "rJ2Z_SLUg", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "S1pwhsbEl", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "review reply AnonRev4", "comment": "The other main points of the paper, that go beyond showing that there are tasks where sin is potentially beneficial, is evidence that sine as activation function makes for a difficult optimization problem and that when learning is successful the periodic part of the function is often largely ignored for typical datasets (as sine has already been successfully used in several works mentioned in Section 2). Please, see the Comments to all reviewers.\n\nWe do agree that having more evidence of sine working on larger tasks would make for a stronger case supporting its use. However we didn't have this as a goal of the paper, and \u2014 instead of finding what tasks sin excels at \u2014 at this stage our work has been mostly focused on analyzing what the consequences are on the representation learned and in characterizing what makes these nets difficult to train. From the results presented in the paper (and the appendix, with the case of larger networks closing or reversing the gap between sin and tanh) it does seem that it might be difficult to find tasks and conditions where sin is more beneficial, so we would propose this for investigation in future work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "tmdate": 1484309331842, "tcdate": 1484309331842, "number": 4, "id": "r13UmBI8g", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "B1OE_6HVe", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "review reply AnonRev3", "comment": "There has probably been a misunderstanding. The scope of the paper is not to show that sine is overall better than tanh as an activation function. The main point of the paper is to impartially analyze the effect of having sine as an activation function, how it affects the representation learned and what consequences it has on learning. Our findings actually indicate that sine is probably not a good choice for typical datasets, as the analysis conducted in Section 3 and the experiments on MNIST in Section 4 demonstrate. We also offer a simple and plausible explanation to its previous successful uses (presented Section 2), i.e. that when it works it\u2019s acting very similarly to a tanh in its non-saturated region. The algorithmic experiments are only supposed to show that there might still be tasks where sine turns out to be helpful, see O3 for more details.\u00a0\n\nIn order to make the main contributions of the paper more clear, we have more explicitly mentioned these in the introduction to the paper. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "tmdate": 1484309297927, "tcdate": 1484309297927, "number": 3, "id": "BJ9EXB88x", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "H1NdulUVe", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "review reply AnonRev2", "comment": "\u00a0 Review: Summary:\n\u00a0 In this paper, the authors explore the advantages/disadvantages of using a sin activation function.\n\u00a0 They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\n\u00a0 They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\n\u00a0 They then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nThis is an accurate summary. More specifically concerning point 2: from our experiments it emerges that when the task is learned well the periodicity is not used, and when the task is not learned (i.e. the network just overfits to the training set) the periodicity is used heavily.\u00a0\n\u00a0\u00a0\n\u00a0 Pros:\n\u00a0 The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\n\u00a0 Cons:\u00a0\n\u00a0 Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant\n\u00a0\u00a0\nWe agree that more evidence would make for a stronger case. We are now considering a few options in this direction, in order to support the main contribution of the paper. O2 would benefit from more experiments similar to the MNIST experiment to validate/falsify it. We are planning to choose a couple more datasets to test the claim on, for now we added the Reuters dataset. What else would you recommend to investigate in order to further improve the impact of the paper?\n\n\n-- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable.\u00a0\n\nThis is correct. However, the main contribution of the results on MNIST is evidence showing that when training is successful for typical datasets (as it's the case for most previous works using sine presented in Section 2) the network is not relying on the periodicity of the function. The network relies almost exclusively on the central part of the function, that is monotonic and similar to a tanh.\u00a0\n\n-- The toy algorithmic tasks are hard to conclude something concrete from.\n\nOur conclusion from the toy tasks is the following:\nThere might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial, since there are at least two simple tasks (i.e. sum and diff) where sin can outperform a standard RNN baseline using tanh.\u00a0\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "tmdate": 1484309234896, "tcdate": 1484309234896, "number": 2, "id": "BkjgQS88e", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "Comment to all reviewers", "comment": "Thank you to all the reviewers for the helpful comments. Probably due to a lack of clarity of the paper, some details in the proposed summaries differ from the main points we tried to convey. Let us then attempt to clarify what the point of the paper is supposed to be, both here and in the manuscript.\n\nThe main point of the paper is to analyze the effect of sine as an activation function, how it affects the representation learned and what consequences it has on learning.\u00a0\n\nThe main contributions of the paper are:\nO1 - a proof that confirms the claim that networks with sine might easily get stuck into local minima, obtained by analytically characterizing the loss surface.\nO2 - evidence showing that when training is successful for typical datasets (as it's the case for most previous works using sine presented in Section 2) the network is actually not relying on the periodicity of the function. The network relies almost exclusively on the central part of the function, that is monotonic and similar to a tanh. Evidence for this is presented on MNIST and in the new version on Reuters too.\nO3 - there might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial, since there are at least two simple tasks (i.e. sum and diff) where sin can outperform a standard RNN baseline using tanh.\n\nWe are trying to make these points more clear and explicit in the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484309180001, "tcdate": 1478296253891, "number": 473, "id": "Sks3zF9eg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sks3zF9eg", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "content": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482193004269, "tcdate": 1482193004269, "number": 3, "id": "H1NdulUVe", "invitation": "ICLR.cc/2017/conference/-/paper473/official/review", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["ICLR.cc/2017/conference/paper473/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper473/AnonReviewer2"], "content": {"title": "Nice preliminary theoretical results of using sin activations, but more evidence needed", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512574877, "id": "ICLR.cc/2017/conference/-/paper473/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper473/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper473/AnonReviewer4", "ICLR.cc/2017/conference/paper473/AnonReviewer3", "ICLR.cc/2017/conference/paper473/AnonReviewer2"], "reply": {"forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512574877}}}, {"tddate": null, "tmdate": 1482180656016, "tcdate": 1482180656016, "number": 2, "id": "B1OE_6HVe", "invitation": "ICLR.cc/2017/conference/-/paper473/official/review", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["ICLR.cc/2017/conference/paper473/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper473/AnonReviewer3"], "content": {"title": "Mainly theoretical idea with insufficient evidence of being practical", "rating": "4: Ok but not good enough - rejection", "review": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.\nThis change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. \nThe experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512574877, "id": "ICLR.cc/2017/conference/-/paper473/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper473/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper473/AnonReviewer4", "ICLR.cc/2017/conference/paper473/AnonReviewer3", "ICLR.cc/2017/conference/paper473/AnonReviewer2"], "reply": {"forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512574877}}}, {"tddate": null, "tmdate": 1481911397329, "tcdate": 1481911397329, "number": 1, "id": "S1pwhsbEl", "invitation": "ICLR.cc/2017/conference/-/paper473/official/review", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["ICLR.cc/2017/conference/paper473/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper473/AnonReviewer4"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512574877, "id": "ICLR.cc/2017/conference/-/paper473/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper473/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper473/AnonReviewer4", "ICLR.cc/2017/conference/paper473/AnonReviewer3", "ICLR.cc/2017/conference/paper473/AnonReviewer2"], "reply": {"forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512574877}}}, {"tddate": null, "tmdate": 1481639843096, "tcdate": 1481639843090, "number": 1, "id": "rJsjvK6Xg", "invitation": "ICLR.cc/2017/conference/-/paper473/public/comment", "forum": "Sks3zF9eg", "replyto": "HyM_Pz9ml", "signatures": ["~Giambattista_Parascandolo1"], "readers": ["everyone"], "writers": ["~Giambattista_Parascandolo1"], "content": {"title": "Answers", "comment": "Thank you for your interesting questions!\n\n1. After writing the code needed to compute what you suggest in question 1, we will re-run the experiments in figure 5 and report the results.\n2. Our current hypothesis is that the model with sine might implement some sort of soft modulo operation. We might get a better intuition about this from the same code used to answer the point above. Note that since the accuracy for a given sequence is 1 only if the entire sequence is predicted correctly (and 0 otherwise), even always relying on a single neuron's non-central part might be necessary for good performance. Also, it \u1e3fight turn out that only a very small amount of neurons actually rely on the non-central part.\n3. We are now running experiments on a few more algorithmic tasks with similar setting. We'll report the results once they are ready.\n4. There are tasks that are challenging for quasiconvex functions and very simple for sine: computing parity (with a DNN) is probably the most well known example. More in general I suspect that it might help for tasks that would benefit from modulo operations with several divisors, but we definitely need more evidence to support this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287562364, "id": "ICLR.cc/2017/conference/-/paper473/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sks3zF9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper473/reviewers", "ICLR.cc/2017/conference/paper473/areachairs"], "cdate": 1485287562364}}}, {"tddate": null, "tmdate": 1481414505824, "tcdate": 1481414505816, "number": 1, "id": "HyM_Pz9ml", "invitation": "ICLR.cc/2017/conference/-/paper473/pre-review/question", "forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "signatures": ["ICLR.cc/2017/conference/paper473/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper473/AnonReviewer4"], "content": {"title": "Details", "question": "- Table 1's right most column seems to indicate that low deviation from the central monotonic segment correlates strongly with good performance, can you plot a similar measure in Fig 5, to see what the average deviation is and how it evolves during training?\n- Can you hypothesize an explanation why the sin models in Fig 5 seem to be exclusively using the non-central part (since accuracy drops to 0 when replacing with clipped sin), and why it learns faster?\n- Could the positive result on the 2 simple tasks be an artifact very specific to these tasks or is there some general advantage there?\n- What other tasks do you suspect Sine might be good at?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming the waves: sine as activation function in deep neural networks", "abstract": "Most deep neural networks use non-periodic and monotonic\u2014or at least\nquasiconvex\u2014 activation functions. While sinusoidal activation functions have\nbeen successfully used for specific applications, they remain largely ignored and\nregarded as difficult to train. In this paper we formally characterize why these\nnetworks can indeed often be difficult to train even in very simple scenarios, and\ndescribe how the presence of infinitely many and shallow local minima emerges\nfrom the architecture. We also provide an explanation to the good performance\nachieved on a typical classification task, by showing that for several network architectures\nthe presence of the periodic cycles is largely ignored when the learning\nis successful. Finally, we show that there are non-trivial tasks\u2014such as learning\nalgorithms\u2014where networks using sinusoidal activations can learn faster than\nmore established monotonic functions.", "pdf": "/pdf/5b46760a646ca9fecbe8ce41647b6a3cca4f6948.pdf", "TL;DR": "Why nets with sine as activation function are difficult to train in theory. Also, they often don't use the periodic part if not needed, but when it's beneficial they might learn faster", "paperhash": "parascandolo|taming_the_waves_sine_as_activation_function_in_deep_neural_networks", "conflicts": ["tut.fi"], "keywords": ["Theory", "Deep learning", "Optimization", "Supervised Learning"], "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "authorids": ["giambattista.parascandolo@tut.fi", "heikki.huttunen@tut.fi", "tuomas.virtanen@tut.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481414506425, "id": "ICLR.cc/2017/conference/-/paper473/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper473/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper473/AnonReviewer4"], "reply": {"forum": "Sks3zF9eg", "replyto": "Sks3zF9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper473/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481414506425}}}], "count": 12}