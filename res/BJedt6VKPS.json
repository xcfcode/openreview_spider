{"notes": [{"id": "BJedt6VKPS", "original": "HJx07ZpPPS", "number": 674, "cdate": 1569439103541, "ddate": null, "tcdate": 1569439103541, "tmdate": 1577168286053, "tddate": null, "forum": "BJedt6VKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2QdfhKRESc", "original": null, "number": 1, "cdate": 1576798702985, "ddate": null, "tcdate": 1576798702985, "tmdate": 1576800933020, "tddate": null, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a new design space for initialization of neural networks motivated by balancing the singular values of the Hessian. Reviewers found the problem well motivated and agreed that the proposed method has merit, however more rigorous experiments are required to demonstrate that the ideas in this work are significant progress over current known techniques. As noted by Reviewer 2, there has been substantial prior work on initialization and conditioning that needs to be discussed as they relate to the proposed method. The AC notes two additional, closely related initialization schemes that should be discussed [1,2]. Comparing with stronger baselines on more recent modern architectures would improve this work significantly.\n\n[1]: https://nips.cc/Conferences/2019/Schedule?showEvent=14216\n[2]: https://arxiv.org/abs/1901.09321.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728887, "tmdate": 1576800281390, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper674/-/Decision"}}}, {"id": "r1lkMDOaKB", "original": null, "number": 2, "cdate": 1571813127188, "ddate": null, "tcdate": 1571813127188, "tmdate": 1573794378625, "tddate": null, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The authors propose a new initialization scheme for training neural networks. The initialization considers fan-in and fan-out, to regularize the range of singular values of the Hessian matrix, under several assumptions.\n\nThe proposed approach gives important insights for the problem of weight initialization in neural networks. Overall, the method makes sense. However, I have several concerns:\n\n- The authors do not consider more recent neural network designs such as normalization layers, skip connections, etc. It would be great if the authors could discuss how these layers would change the derivation of the initialization method. Also, preliminary experimental results using these layers are needed. Additionally, to me, normalization layers [Huang et al. Decorrelated Batch Normalization. CVPR 2018] implicitly precondition the Hessian matrix as well. It would be great if the authors also compare their approach to [Huang et al. 2018].\n\n- The authors compared to other initialization schemes such as [He et al., 2015] and [ Glorot and Bengio 2010]. But as the authors mentioned, there are approaches that scales backpropagation gradients also [Martens and Grosse, 2015; Grosse and Martens, 2016; Ba et al., 2017; George et al., 2018]. Since these methods are highly related to the proposed method, it would be great if the authors could show time complexities and performance differences of these methods as well.\n\n- Experiments on the CIFAR-10 dataset with AlexNet seem not exciting: the proposed Preconditioned approach only outperforms the Fan-out approach marginally. I would say that training a [He et al. 2015]-initialized neural network for 500 more iterations than a preconditioned neural network, yields a similar or better loss.\n\nOverall I think the work is very important and interesting. However, it lacks comprehensive comparison and consideration of more recent neural network layers.\n\nPost Rebuttal Comments\nI have read all reviewer comments and the author feedback. I appreciate that the authors addressed the skip connections in Appendix.\n\n1. The authors agree that batch norm requires different initialization schemes that are not included in this paper.\n2. I agree with the authors that their approach is complementary to the baseline optimization methods; and both approaches can be applied together. However, I still believe that it is informative to compare the two approaches because: (a). Both approaches address the same problem. Since the optimization based approach adds complexity and computational overhead to implementation, it would be great to show if using the proposed approach eliminates the need for the optimization based approach. (b). Is it necessary to use both approaches, or one of them is good enough?\n3. I understand that strong experimental evidence is not always required. However, I believe that the new technical insights of the paper alone is not significant enough (part of the reasons in point 1). Thus I was expecting stronger experimental evidences.\n\nOverall I agree with reviewer 1 that the topic is interesting, but in the paper\u2019s current form, it is not ready. I keep my initial rating of weak reject.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575076833381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper674/Reviewers"], "noninvitees": [], "tcdate": 1570237748734, "tmdate": 1575076833397, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Review"}}}, {"id": "B1xH_I6MoS", "original": null, "number": 4, "cdate": 1573209708750, "ddate": null, "tcdate": 1573209708750, "tmdate": 1573209708750, "tddate": null, "forum": "BJedt6VKPS", "replyto": "rJepzjNJoB", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment", "content": {"title": "Acknowledgment on size of dataset", "comment": "Hi, \n\nIndeed libsvm seems to contains quite a variation of tasks. I feel like you've done a disservice to the amount of experiments you have done by only having a summary table (table 2) in the main article. \n\nThat being said, why did you pick this set of tasks, with which I'm not familiar with and have it not seen been used in recent publications. I think picking some more mainstream datasets would again been advantageous for you as it would have easier to evaluate and understand the results.   \n\nAlso since you have these runs, and multiple seeds for cifar, can you grab and show stats that motivate the various choices made in this work? (e.g. not using min/max singular value and so forth). "}, "signatures": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJedt6VKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper674/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper674/Authors|ICLR.cc/2020/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167955, "tmdate": 1576860532991, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment"}}}, {"id": "r1xbGZBkir", "original": null, "number": 3, "cdate": 1572978952842, "ddate": null, "tcdate": 1572978952842, "tmdate": 1572978952842, "tddate": null, "forum": "BJedt6VKPS", "replyto": "B1e3e6yTKH", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for taking the time to review our work. We provide the following replies:\n\n-Our results are quite robust, note that the input and output dimensionality varies across the 27 datasets we compare on, resulting in different layer widths at the beginning and end of the network for every dataset. In addition, the AlexNet we test on has a very different architecture from the other experiments.\n\n- Thank you for the feedback about assumption A2. Some assumption is needed to control the behavior at the top of the network. Note that we are talking about the network at initialization, a situation where the assumption is mild.\n\nGiven the extensive theory that results from this simple assumption we believe it is interesting in any case. In general, developing assumptions that result in fruitful and simple analysis is one of the primary goals of theory, and should not be seen as a weakness. \nWe are looking to provide a theory that handles more general cases in future work.\n\n\nWe hope that Reviewer #3 reconsiders their assessment given the context from other reviewers and our replies above.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper674/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJedt6VKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper674/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper674/Authors|ICLR.cc/2020/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167955, "tmdate": 1576860532991, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment"}}}, {"id": "rJgWYpNJor", "original": null, "number": 2, "cdate": 1572978041072, "ddate": null, "tcdate": 1572978041072, "tmdate": 1572978041072, "tddate": null, "forum": "BJedt6VKPS", "replyto": "r1lkMDOaKB", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment", "content": {"title": "Reply", "comment": "\nThanks for the detailed comments.\n\n1. We address skip connections in detail in the Appendix. In terms of BatchNorm, you are correct that it affects the conditioning, are requires different initialization schemes. The extra analysis needed is significant and we could not fit it into one conference paper. We are looking to address normalization in followup work.\n\n2. Our initialization method is complementary to the mentioned optimization methods as it can be used together with them. \n\n3. Only small improvements can be expected by changing initialization methods, it's too much to ask to expect large improvements to final error from a change from arithmetic mean to geometric mean. We are proposing a design principle with theory behind it, the initialization improvements are a secondary result."}, "signatures": ["ICLR.cc/2020/Conference/Paper674/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJedt6VKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper674/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper674/Authors|ICLR.cc/2020/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167955, "tmdate": 1576860532991, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment"}}}, {"id": "rJepzjNJoB", "original": null, "number": 1, "cdate": 1572977428985, "ddate": null, "tcdate": 1572977428985, "tmdate": 1572977428985, "tddate": null, "forum": "BJedt6VKPS", "replyto": "Hkl5cQtQ9r", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for the reference on Gauss-Newton methods, we will include this citation.\n\nWe would like to disagree respectfully with your assessment that our experimental results are not sufficient. We use 26 different datasets from the LIBSVM repository! These datasets are highly varied in size, dimensionality and difficulty of prediction.  This is far more comprehensive than 90% of published ICLR papers. "}, "signatures": ["ICLR.cc/2020/Conference/Paper674/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJedt6VKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper674/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper674/Authors|ICLR.cc/2020/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167955, "tmdate": 1576860532991, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper674/Authors", "ICLR.cc/2020/Conference/Paper674/Reviewers", "ICLR.cc/2020/Conference/Paper674/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Comment"}}}, {"id": "B1e3e6yTKH", "original": null, "number": 1, "cdate": 1571777779847, "ddate": null, "tcdate": 1571777779847, "tmdate": 1572972566298, "tddate": null, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a set of rules for the design and initialization of well-conditioned neural networks by naturally balancing the diagonal blocks of the Hessian at the start of training. Overall, the paper is well written and clear in comparison and explanation. However, the reviewer is concerned with the following questions:\nAssumption A2 does not make sense in the context. In particular, it is not praised to assume it only for the convenience of computation without giving any example when the assumption would hold. Also the assumptions are vague and hard to understand, it is better to have concrete mathematical formulation after text description.\nAre the experiment results sensitive to the choice of different models with different width and layers or different batch sizes? Does it have a strong improvement than random initialization? It\u2019s less clear the necessity of guaranteeing well-conditioned at initialization since during the training procedure, the condition number is harder to control.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575076833381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper674/Reviewers"], "noninvitees": [], "tcdate": 1570237748734, "tmdate": 1575076833397, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Review"}}}, {"id": "Hkl5cQtQ9r", "original": null, "number": 3, "cdate": 1572209553689, "ddate": null, "tcdate": 1572209553689, "tmdate": 1572972566211, "tddate": null, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "invitation": "ICLR.cc/2020/Conference/Paper674/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I think the topic of the paper is interesting, though I think in its current form the paper is not ready. \n\nFirst of all I find the empirical section quite weak. While the authors attempt to formalize their intuition, as they mention in the work itself, such works are somewhat outside mathematical proof. This is due to the many approximations needed, and assumptions that can not hold in practice. As such the main experimental results in running AlexNet on Cifar-10 and LIBSVM datasets. I think more experimental evidence is needed, e.g. more architectures, more dataset (maybe different data modalities). Is hard to tell from this one main experiment (Cifar 10) to what extend one can trust this initialization. \n\n\nI think is worth noting that KFAC and (all?) cited methods actually use the Fisher Information matrix (hence being forms of natural gradient) and not the Hessian. The extended Gauss-Newton approximation is indeed the Fisher matrix, I think as discussed in Martens' work (which is heavily cited) though in other works as well. Just as an additional note, the extended Gauss-Newton was introduced in  Schraudolph, N. N. (2002). \"Fast curvature matrix-vector products for second-order gradient descent\" where it was presented as an approx to the Hessian, this was used by Martens and later the community (and he himself) observed that actually rather than an approx to Hessian this can be thought of as the Fisher Information Matrix.\n\nRelying on the expected squared singular value should be motivated better. The reasoning sounds fine (i.e. minimum and maximum would be too pessimistic) but no data is given. Some statistics over multiple runs. Overall this is a repeating theme in the work. While everything makes sense intuitively, I would have hoped more rigor. More empirical evidence for any such choice. The weight-to-gradient ratio is not a commonly used measure. Maybe show how this ratio progresses over time when training a model. Having multiple runs showing the correlation between things. Table 2 is not referenced in the text. While an average over 10 seeds is provided for Cifar, no error bars. \n\nOverall I think the direction of the work is interesting. And definitely we have not heard the last word on initialization. It plays a crucial role (and indeed bad initialization can easily induce bad local minima (https://arxiv.org/abs/1611.06310)). But I think the paper needs to be written more carefully, with a more thorough empirical exploration, showing different architectures, different datasets. Maybe trying to break the usual initialization, and showing you can do considerably better with newer initialization. "}, "signatures": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper674/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "authors": ["Aaron Defazio", "Leon Bottou"], "authorids": ["aaron.defazio@gmail.com", "leon@bottou.org"], "keywords": ["initialization", "mlp", "relu"], "TL;DR": "A theory for initialization and scaling of ReLU neural network layers", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "pdf": "/pdf/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "paperhash": "defazio|scaling_laws_for_the_principled_design_initialization_and_preconditioning_of_relu_networks", "original_pdf": "/attachment/70475c6e80c040efd5a4369b625969ac3d4f6fa6.pdf", "_bibtex": "@misc{\ndefazio2020scaling,\ntitle={Scaling Laws for the Principled Design, Initialization, and Preconditioning of Re{\\{}LU{\\}} Networks},\nauthor={Aaron Defazio and Leon Bottou},\nyear={2020},\nurl={https://openreview.net/forum?id=BJedt6VKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJedt6VKPS", "replyto": "BJedt6VKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper674/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575076833381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper674/Reviewers"], "noninvitees": [], "tcdate": 1570237748734, "tmdate": 1575076833397, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper674/-/Official_Review"}}}], "count": 9}