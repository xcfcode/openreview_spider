{"notes": [{"id": "mCLVeEpplNE", "original": "7lxLyebxRTJ", "number": 1088, "cdate": 1601308122386, "ddate": null, "tcdate": 1601308122386, "tmdate": 1611803437655, "tddate": null, "forum": "mCLVeEpplNE", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "u1lT_BrBfu6", "original": null, "number": 1, "cdate": 1610040367601, "ddate": null, "tcdate": 1610040367601, "tmdate": 1610473958552, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper initially received mixed ratings but after the rebuttal, all reviewers recommended acceptance. Reviewers appreciate the novel technical ideas and extensive experimental results. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040367587, "tmdate": 1610473958532, "id": "ICLR.cc/2021/Conference/Paper1088/-/Decision"}}}, {"id": "mlTIpDngMw0", "original": null, "number": 3, "cdate": 1603949909518, "ddate": null, "tcdate": 1603949909518, "tmdate": 1606805097532, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review", "content": {"title": "Compelling interpretability methodological work but minor flaws in motivation, lack of discussion about practical limitations", "review": "The authors did a fantastic job of answering questions, revising their manuscript in accord with reviewer feedback (Sec 3.4 title), and even adding new experimental results based on reviewer suggestions (mid-training hierarchy) and reflecting best practices in interpretability research. I was really impressed by their nimbleness and responsiveness. I will raise my score to a 7: I think this is a very solid paper and excellent research effort around a nascent idea. In particular, I think its impact is limited by\n\n- its close coupling to naturally hierarchical problems, e.g., multi-class classification with a taxonomy\n- its close coupling to image data and tasks\n- its heuristic nature: fully train neural net, infer hierarchy via clustering, retrain neural net, then map a priori labels onto inferred hierarchy\n\nThe \"10\" version of this paper (maybe future work?) would propose a way to infer the hierarchy on the fly and show how to apply it onto other kinds of data and problems with different structures.\n\n-----\n\nThis submission proposes a modification of neural networks that replaces the \"final linear layer with a decision tree.\" The term \"decision tree\" is applied somewhat loosely to a hierarchical neural architecture akin to a hierarchical softmax. In the current work (as I understand it), this hierarchy is induced from a pre-trained multi-output, e.g., multiclass, neural network via a hierarchical clustering and subsequent averaging of the output weights. At inference time, path probabilties can be computed based on the chain rule. Predictions can be made based on either a greedy traversal of the tree (choosing the most likely child at each step, a la hierarchical softmax) or by choosing the most probable leaf, which requires computing all path probabilities. Empirical results across three standard image datasets are suggestive, if not conclusive, and the paper concludes with some interesting, albeit cursory, examples of potential \"interpretability\" applications.\n\nThe submission summarizes its contributions at the end of Section 1 as follows:\n1. It proposes a tree-structured loss to augment supervised neural network training (predominantly for multiclass classification problems).\n2. It describes a heuristic to induce a hierarchy in the output weights of a pre-trained multi-output neural network, enabling decision tree-like inference and provide evidence it is more effective than other approaches for inducing hierarchies.\n3. It presents simple case studies of how the induced hierarchy can be used for traditional \"interpretability\" tasks, like debugging and generating explanations.\n\nI appreciate the idea at the center of this paper -- adding simple hierarchical structure to a multi-output neural network, with the aim of increased interpretability -- but I feel the work as it is presented is nascent and the manuscript itself is flawed. I lean toward rejection at the moment, but I could be persuaded to change my mind by some combination of solid revisions, convincing author response, or vociferous advocacy from other reviewers.\n\nI will briefly extol the paper's strengths before providing a longer discussion of what I consider to be its key weakness. First, I really like the last sentence in the paper:\n\n\"This challenges the conventional supposition of a dichotomy between accuracy and interpretability, paving the way for jointly accurate and interpretable models in real-world deployments.\"\n\nWeaknesses in the evaluation of its interpretability claims aside, I agree with this statement. I think the case studies presented do provide evidence of improved interpretability alongside small accuracy improvements. I think this paper does succeed in demonstrating that accuracy and interpretability are not necessarily competing objectives, at least for certain tasks (multiclass classification of images).\n\nA laundry list of other strengths:\n\n- The motivation is strong (modulo weakness discussed below): there is a growing need to provide human-understandable insights into decisions made by complex machine learning models.\n- The proposed approach is simple and elegant, easy to implement, and empirically effective. I'm quite impressed that the proposed tree loss appears to improve accuracy (!) on multiple tasks.\n- I also think this paper lays groundwork for a direction of research that the community could continue to build on.\n\nI think that the manuscript's largest flaw, ironically, regards interpretability, its primary motivation. The work's central claim is that the tree-structured decision layer delivers improved interpretability with comparable or slightly improved accuracy. In its discussion of this claim, the manuscript provides no precise definition of \"interpretable,\" making it difficult to verify the claim qualitatively or quantitatively. Section 5 presents a vignette of case studies, but the discussion of each is quite limited. In particular, none of the use cases is fully motivated or placed in the context of previous research on interpretability definitions [1][2]. The cursory presentation of results for each do the results a disservice by making it difficult for the reader to recognize and assess their significance.\n\nTo quote the introduction from Lipton's _The Mythos of Model Interpretability_ [1],\n\n\"Despite the absence of a definition, papers frequently make claims about the interpretability of various models. From this, we might conclude that either: (i) the definition of interpretability is universally agreed upon, but no one has managed to set it in writing, or (ii) the term interpretability is ill-defined, and thus claims regarding interpretability of various models may exhibit a quasi-scientific character.\"\n\nI believe the paper would be strengthened by focusing on one use case, e.g., debugging or human trust, using the ~1 page dedicated to Section 5 to motivate it more fully and to present the results in detail. If the primary use case is generalization or debugging, then I suggest designing a quantitative analysis so defend against claims of cherry picking the best results (a common problem in presenting \"example\" interpretability results).\n\nSection 5.4 includes a quantitative evaluation, but I question whether mere human preference is evidence of \"human trust.\" More recent research on trust appear to use more elaborate studies in which trust is measured by subjects' rate of success in performing a particular task aided by the machine learning model [3].\n\nI want to caveat the above: I really appreciate this line of work and think it has value. There is an ongoing discussion in our community about rewarding good ideas, rather than punishing imperfect or incomplete execution. I also acknowledge that I am far from an expert in the latest interpretability research. Nonetheless, my understanding is that interpretability researchers have grown more skeptical of interpretability claims about new methods absent a rigorous framework (definitional and/or experimental) for evaluating those claims.\n\nWhen I read this paper, I find it hard to escape the conclusion that its interpretability claims rest on the presupposition that trees are naturally more interpretable (and further that readers will accept this dogma). I disagree with this assertion (see below), but even if it were generally true, I still think the paper would be strengthened by adding a more rigorous discussion and analysis of its claims. Propose a definition or criterion (see [1][2] for ideas), ideally one that could be assessed qualitatively and evaluated empirically, then apply it.\n\nRegarding the claim about trees in Section 5: \"The interpretability of a decision tree is well-established when input features are easily understood (e.g. tabular data in medicine or finance).\"\n\nI would dispute that this is \"well-established\" for anything but the simplest decision tree models, with a single tree consisting of a small number of splits using a handful of features, which are rare in realistic settings. The most commonly used tree-structured models (gradient boosted decision trees and random forests) are not readily interpretable, even for tabular data and especially for high dimensional inputs. This has made research like SHAP [4] of great interest to practitioners.\n\nWhat is more, even for tabular data, the neural decision trees described in this paper are (to my understanding) basically a cascade of linear classifiers, with split each having access to all features at once. This does do not lend itself to the same kind of \"interpretation\" one gets for classic decision trees that use one feature per split. With even modestly deep hierarchies, the resulting \"explanations\" would rapidly become quite complex.\n\nI see one other weakness in the proposed method itself: as I understand things, it requires access to a pretrained neural network. At the very least, one needs pre-existing output weights to cluster in order to induce a hierarchy -- and the induced hierarchy is a necessary component in the presented results. This isn't a fatal flaw -- learning a hierarchy on the fly could be left for future work. Nonetheless, it limits the work's usefulness and potential impact.\n\nWhat is more, I don't think the manuscript is sufficiently clear about this requirement: on my first pass through the paper, I came away with the impression that there was a way to learn the hierarchy while training the neural net -- the inclusion of a section entitled \"Training with Tree Supervision Loss\" seems to imply this. I suggest revising the text to make it crystal clear that it is not possible to use the tree loss to train a neural net from scratch -- at least, not without a predefined hierachy (perhaps from a previous training run or prior knowledge).\n\nI will now summarize the improvements I suggest for strengthening the manuscript:\n1. Focus on one definition of interpretability and then analyze central claims through that lens. Introduce it early in the paper (introduction) and then dedicate Section 5 to it, rather than trying to cover lots of use cases superficially.\n2. Make the limitations of the proposed approach VERY clear. In particular, you need a predefined hierarchy to train with the tree loss, and you need pretrained neural net (or pre-existing weights, at least) to induce a hierarchy based on clustering.\n3. If the intention is for this approach to be used exclusively for finetuning or adapting an existing neural network, then this should be made clear in the text. Consider renaming Section 3.3.\n4. Justify (or reword) statements like \"the interpretability of a decision tree is well-established\" or \"neural features are visually interpretable\" (a single reference does not suffice...the Olah distill survey draws no such definitive conclusions).\n\nI have a few questions:\n- One thing that is not clear: when training with tree loss, are weights shared across nodes? In particular, the weight vector for an inner node is the average of its descendent leaf node weight vectors. When training with tree loss, do we then treat that inner node weight vector as a set of independent parameters with separate updates? Or do we continue to treat it as a sum of leaf parameters, so that leaf and inner node updates affect the same parameters, as in an RNN or recursive network.\n- Is there possibly a heuristic that could approximate \"learning the hierarchy?\" For example, train with a basic loss for enough iterations until the output weights start to converge, then pause training to induce the hierarchy and then resume training with the tree loss. Part of the heuristic could be guidance about how to detect when the output weights have sufficiently converged.\n- What are the key differences between this approach and a hierarchical softmax? My understanding is that they're basically equivalent at inference time (except maybe traditional hierarchical softmax uses hard decisions?). What about during training? Is it maybe the use of negative sampling for hierarchical softmax?\n- How would this approach perform for extremely high dimensional output spaces -- one of the primary motivations for hierarchical softmax? I imagine that for some output cardinality, \"soft\" inference becomes computationally infeasible.\n\n[1] Lipton. The Mythos of Model Interpretability. https://arxiv.org/abs/1606.03490.\n[2] Doshi-Velez and Kim. Towards A Rigorous Science of Interpretable Machine Learning. https://arxiv.org/abs/1702.08608.\n[3] Poursabzi-Sangdeh, et al. Manipulating and Measuring Model Interpretability. https://arxiv.org/abs/1802.07810.\n[4] Lundberg, et al. From local explanations to global understanding with explainable AI for trees. https://www.nature.com/articles/s42256-019-0138-9.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127272, "tmdate": 1606915763374, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1088/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review"}}}, {"id": "jLwYNTMXJT", "original": null, "number": 1, "cdate": 1603845161040, "ddate": null, "tcdate": 1603845161040, "tmdate": 1606789022104, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review", "content": {"title": "Interesting results but unconvincing claims.", "review": "The paper proposes a method to make neural networks more accurate and interpretable by replacing their final layers with a probabilistic decision tree. As a result, the network can produce a sequence of decisions that leads to the final classification result, given an input image. The method is trained with soft decisions by assigning probabilities to each leaf, which are associated with a single class. The tree decision hyperplanes are constructed automatically from the backbone networks final dense layer and finetuned. The fact that decisions are soft solves the differentiablility problem of decisions as in various other similar papers, cited or uncited (more below).\n\nThe paper is not written very clearly, so it would be hard to reproduce. It's not clear in places if indices correspond to nodes or classes, as it is used interchangeably. The text misses a proper mathematical formulation of the operations done in inner nodes, and this all makes it difficult to understand what the loss is and how it can update the decisions in the tree. Perhaps it's possible to understand all the details by re-reading the text several times, but the paper definitely lacks clarity. A nice comparison would be with the Deep Neural Decision Forest (DNDF) paper. I'd expect that level of clarity from an ICLR paper.\n\nThe justification behind using the hyperplanes from the last layer of the CNN for constructing inner nodes is not explained. The visualization in Figure 2 indicates that the averages used for the parent nodes act like clusters - and somehow averaging them forms bigger clusters, but what really happens is that the rows of the final layer, which are unnormalized hyperplanes, are averaged to form new hyperplanes that are assumed to cover both classes, which would not be the case most of the time. I'm not sure if this has a reasonable geometric meaning, but the visualization gives the wrong idea.\n\nThe paper makes the claim: \"Unlike previous decision trees or hierarchical classifiers, NBDTs use path probabilities for inference\".\nThere are certainly many papers that use path probabilities for inference. In fact it's been the norm for discrete decisions, since hard decisions are much harder to formulate in a differentiable manner. DFDN uses path probabilities, as do older papers like \"Decision Forests, Convolutional Networks and the Models in-Between\" by Ioannou et al. (uncited). I don't understand this claim.\nThe biggest difference here is that each class gets exactly one dedicated leaf, instead of each leaf storing a distribution. It is not clear to me why this is a good idea though - it's clearly not preferred in the decision tree literature.\n\nAnother interesting point - most papers I know on the subject actually try to enable sparse activation at test time for efficiency, which is the harder problem to solve. See CondConv (Neurips 2019 - uncited), SplineNets (Neurips 2018 - uncited), Outrageously Large Neural Networks by Shazeer et al. (uncited), Conditional Information Gain by Bicici et al., etc. Activating all the branches of the tree or graph becomes prohibitively costly for deeper trees.\n\nInterpretability is another major claim. The way it works is by using WordNet to assign higher level compound classes to images, such as Animal -> Turtle. This way by grouping similar leaves together, the inner nodes are assigned a meaning. This needs to be done when constructing the hierarchy, so the tree structure is manually given from what I understand. But then how are the pairs of nodes selected? It's not clear from the text, at least it is not explained clearly. In the Figure 5, a decision tree is given, which could only have been constructed by hand - i.e. with someone knowing that cats and dogs are the closest pairs. Where does this information come from for ImageNet when there are 1000 classes? WordNet?\n\nAnother concern about interpretability is that it is claimed that other good performing methods like DFDN are not interpretable. I don't see why that's the case if we use WordNet to assign meaning for the inner nodes of such solutions. I'd agree that it is not straightforward to do so, but with WordNet I can imagine how it could be done. So I'm not convinced about the claim that this solution is the most interpretable one when the paper does not explain why other solutions aren't in a persuasive manner.\n\nOne final concern is with the comparisons with similar models. The paper says these were reimplemented and tested with different backbones. Where are these implementations taken from, for instance for DFDN, which is quite complex? How can the reader trust these numbers?\n\nI find the claims unconvincing and the results unpersuasive. I think the paper needs i) a better mathematical formulation to clarify the method, ii) better explanation of how the trees are constructed (e.g. if WordNet is used), iii) better understanding of differences from similar work. Currently I don't think it meets the bar for ICLR.\n \n\nTreeSup result is likely wrong in Table 3.\n\nAfter Rebuttal:\n\nThe authors did a great job in addressing most of the issues I have, and made many changes that helped with the clarity of the paper. There are still some remaining issues like Figure 2 assigning a wrong geometric meaning to the clusters formed by taking means of hyperplanes, and the uncited references, which are simply added to the references section (which should be fixed). But I think the added survey results are a great addition and a persuasive proof about the increased interpretabililty of these models. Therefore I'll increase my score to 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127272, "tmdate": 1606915763374, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1088/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review"}}}, {"id": "BBZBZrza9qP", "original": null, "number": 5, "cdate": 1604809478647, "ddate": null, "tcdate": 1604809478647, "tmdate": 1606784558348, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review", "content": {"title": "Take a step further on integrating the NN and Decision Tree", "review": "Aim to improve the interpretability and the accuracy of the neural network, this paper takes a step further on the integration of NN with a decision tree. It will replace the final linear layer of the NN with a decision tree induced by pre-trained model weights. It takes advantage of both hard and soft decision trees and designs suitable tree supervision loss thereon. Extensive experiments verify the design choice of the proposed components. On both small-scale and large-scale datasets, it beats the decision tree counterparts. Also, on the aspects of generalization and interpretability, it shows the strength compared to NN.\n\nThis work is a good try to combine the two techniques NN and decision tree. It finally makes the combination to achieve comparable accuracy with the NN and also enjoy the benefit in the aspects of generalization and interpretability. Recent SOTA of capsule networks which are based on the NN backbone and this work are both achieved comparable performance with NN. They show a promising direction for studying representation learning. Researchers can delve deeper based on this work to further exploit how to integrate decision tree into NN and the characteristics of the combination (e.g. adversarial examples). \n\nWith the decision tree, we can visualize the decision process the bring the benefits of interpretability. The paper proposes to label the decision nodes with WordNet and show the applications of zero-shot generalization, high-level concepts, dataset debugging, and improved human trust. There are lots to do on the aspects. Also, the zero-shot and high-level concept experiments are really intriguing. Using the pre-trained model weights to construct the tree and the proposed tree losses to train can help the generalization in such a significant way, though the performance would depend on the accuracy of the superclasses labeling and the agglomerative clustering. Where the benefits come from? The method is only used the same information as the NN and the tree is also constructed based on the pre-trained weights. Does the way of making hierarchy decisions help here? If you do not enforce the second term of the equation (3), will the phenomenon be the same?\n\nOverall, the paper is very easy to follow and the figures really help understanding. Extensive experiments help to know the performance, effectiveness of the proposed components, and also its unusual applications.\n\n----------------------------\nSome concerns and comments are listed below:\n\nWill you update the weights of the intermediate nodes? \n\nOn large-scale datasets, the paper currently only tests using the EfficientNet. The reviewer wonders if the author can use more advanced backbones to see the performance changes.\n\nThe reviewer is unsure of the specific way to label the decision nodes. Will you use the wordvec provided in the wordnet and compare it with the decision nodes' feature? Since your structure is different from the WordNet, how do you match the classes with the nodes?\n\n-----------------after rebuttal------------\n\nI would like to keep my origin score due to the pros listed above.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127272, "tmdate": 1606915763374, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1088/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review"}}}, {"id": "NUR-BkW6PrC", "original": null, "number": 20, "cdate": 1606291880584, "ddate": null, "tcdate": 1606291880584, "tmdate": 1606291880584, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "jLwYNTMXJT", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Manuscript Update", "comment": "Thank you for your feedback and suggestions. We rewrote major portions of Sec 3 Methods, incorporating your suggestions regarding technical correctness and clarity. We have also added the recommended citations."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "_1tFLMbWMDj", "original": null, "number": 19, "cdate": 1606291786786, "ddate": null, "tcdate": 1606291786786, "tmdate": 1606291786786, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "i7xh1WgJ05w", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Manuscript Update", "comment": "Thank you again for your feedback and suggestions. We have incorporated the surveys in a revised Sec 5 (Interpretability), mid-training hierarchy into Sec 4 (Analysis), and the other suggestions for technical and presentation clarity. We have also rewritten major portions of Sec 3 (Methods) to hopefully address the concerns regarding clarity and technical correctness."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "ds4cWhLrA4_", "original": null, "number": 17, "cdate": 1606291472396, "ddate": null, "tcdate": 1606291472396, "tmdate": 1606291662557, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "zwMJgzKxwzR", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Manuscript Update", "comment": "Thank you again for your feedback and suggestions. We have added the suggested clarifications; we have also improved Sec 3's clarity by incorporating responses to your clarification questions above."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "Lc9JaoLEi4N", "original": null, "number": 18, "cdate": 1606291642054, "ddate": null, "tcdate": 1606291642054, "tmdate": 1606291642054, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "PKM7fn-K-ui", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Backbones, Manuscript Update", "comment": "Thank you again for your feedback and suggestions. Although these new experiments won't make it in time for the rebuttal deadline, we will definitely consider the suggestion of validating NBDT improvements on common backbones like ResNet and SENet, on ImageNet. In the meantime, we have improved Sec 3's clarity by incorporated responses to your clarification questions above."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "yS2iJPsaU9E", "original": null, "number": 16, "cdate": 1606291382837, "ddate": null, "tcdate": 1606291382837, "tmdate": 1606291382837, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "t80M6goIAs", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Manuscript Update", "comment": "Thank you again for your feedback and suggestions. We have incorporated the surveys in a revised Sec 5 (Interpretability), mid-training hierarchy into Sec 4 (Analysis), and the other suggested revisions -- including a more specific interpretability definition in the introduction."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "KQSuPoXNhSb", "original": null, "number": 15, "cdate": 1606262107683, "ddate": null, "tcdate": 1606262107683, "tmdate": 1606262107683, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "i7xh1WgJ05w", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Results: Additional Surveys, Results without Pretrained Weights ", "comment": "Thank you for your patience and update; per your and Reviewer 2\u2019s request, we have run 2 more human experiments to solidify our claim of interpretability. Since we are focusing specifically on how much humans trust our NBDTs decisions, we refer to Reviewer 2\u2019s comment that human trust can be better measured by having humans complete tasks with the model. We are currently working on manuscript updates; in the interim, here are our results:\n\n**More Human Studies**\n\nWe conducted 2 more human studies aimed at answering the following questions raised in [3]: (1) \u201cHow well can people detect when a model has made a sizable mistake?\u201d and (2) \u201dTo what extent do people follow a model\u2019s predictions when it is beneficial to do so?\u201d. \n\n**Study 1: \u201cHow well can people detect when a model has made a sizable mistake?\u201d**\n\nTo address the first question, each user is given 3 images, 2 of which are correctly classified and 1 is mis-classified. We use ResNet18 trained and evaluated on CIFAR10. Users must predict which image was incorrectly classified given a) the model explanations and b) *without* the final prediction. For saliency maps, this is a near impossible task as saliency usually highlights the main object in the image, regardless of wrong or right. However, hierarchical methods provide a sensible sequence of intermediate decisions that can be easily checked to see if they match the expected result. This is reflected in the results: For each explainability technique, we collected 600 survey responses. When given saliency maps and class probabilities, only 87 predictions were correctly identified as wrong. In comparison, when given the NBDT series of predicted classes and child probabilities (e.g., \u201cAnimal (90%) -> Mammal (95%)\u201d, without the final leaf prediction) 237 images were correctly identified as wrong.\n\nOne note: Although NBDT provides more information than saliency maps about misclassification, a majority -- 363 predictions -- were not correctly identified. To explain this, we note that ~37% of all NBDT errors occur at the final binary decision, between two leaves; since we provide all decisions except the final one, these leaf errors would be impossible to distinguish.\n\n**Study 2: \u201dTo what extent do people follow a model\u2019s predictions when it is beneficial to do so?\u201d**\n\nTo address the second question, each user is given a blurred image (kernel size 5x5) and two sets of predictions: (1) the original neural network\u2019s predicted class and its saliency map, and (2) the NBDT predicted class and the sequence of decisions that led up to it (\u201cAnimal, Mammal, Cat\u201d). For all examples, the two models predict different classes. In 30% of the examples, NBDT is right and the original model is wrong. In another 30%, the opposite is true. In the last 40%, both models are wrong. Since the image is extremely blurry (http://people.eecs.berkeley.edu/~alvinwan/nbdt/cifar10-downsampled/0-img-downsampled.jpg), the user must rely on the models to inform their prediction. From this survey we show that humans choose to rely on the NBDT-explained prediction more often than the saliency-explained predictions. Out of 600 responses, 312 responses chose the NBDT\u2019s prediction, 167 responses chose the base model\u2019s prediction, and 119 responses chose neither model\u2019s predictions. Note that a majority of user decisions (~80%) agreed with either model prediction, even though neither model prediction was correct in 40% of examples, showing our images were sufficiently blurred to force reliance on the models. When both models are incorrect, ~45% of responses agree with NBDT, ~34% with the original model, and 22% with neither.\n\n**Experiments using a partially trained network**\n\nIf the hierarchy is induced part way through training and then trained for the rest of the time with the TreeSupLoss, accuracy for the NBDT using soft inference is ~0.15% better than the original network, while the original network\u2019s accuracy is boosted by ~0.6%. These results were obtained using ResNet18 on CIFAR10. This shows that removing the requirement of pretrained weights has promise but that a hierarchy built at the end of training is preferred, for supervision during training. \n\n*We note that in the original review, you had requested a comparison with Adaptive Neural Trees. We were unable to reproduce Adaptive Neural Trees in time due to technical limitations (out-of-date PyTorch with incompatible CUDA) but will continue working on this accordingly, for the appropriate hierarchy comparison you recommended*\n\n[3] Poursabzi-Sangdeh, et al. Manipulating and Measuring Model Interpretability. https://arxiv.org/abs/1802.07810. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "1nKwhrh-k8w", "original": null, "number": 12, "cdate": 1606261316970, "ddate": null, "tcdate": 1606261316970, "tmdate": 1606262032700, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "t80M6goIAs", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Results (Part 1): Mid-Training Hierarchy + Surveys", "comment": "Thanks for the reminder! Per your suggestions, we have (1) picked a definition of interpretability using previous work, (2) run user studies to support our claim to interpretability, under this definition, and (3) run the mid-training hierarchy experiments. We are currently working on manuscript updates; in the interim, here are our results:\n\n**1. Mid-training hierarchy improves accuracy but sees less improvement than pre-trained hierarchy**\n\nIf the hierarchy is induced part way through training and then trained for the rest of the time with the TreeSupLoss, accuracy for the NBDT using soft inference is ~0.15% better than the original network, while the original network\u2019s accuracy is boosted by ~0.6%. These results were obtained using ResNet18 on CIFAR10.\n- If we construct the hierarchy only once midway through training, accuracy decreases by ~1%, so we instead reconstruct the hierarchy every 10 epochs for our final result.\n- As you suggested, tree supervision is best performed after hierarchy has stabilized. Accuracy can change by up to 1%, by changing which epoch the hierarchy is first constructed at.\n\nFrom this we can conclude that pre-trained weights for the hierarchy are preferred over weights obtained mid-training.\n\n**2. A model is interpretable if a user can tell if a model has made an incorrect prediction.**\n\nAfter reading the suggested related works, we define a model to be interpretable if it is able to indicate to the user that either it has made a mistake or the input is significantly different from the training data. This increases humans\u2019 trust in the model, which can be measured by how well they perform on certain tasks with the model's assistance, as you mentioned. Thus, we show that NBDT\u2019s are more interpretable than other methods via human studies as well as show an application of this method for dataset debugging. \n\nIn our additional human studies, we aim to answer the following questions raised in [3] that are directly applicable to computer vision: (1) \u201cHow well can people detect when a model has made a sizable mistake?\u201d and (2) \u201dTo what extent do people follow a model\u2019s predictions when it is beneficial to do so?\u201d. In all surveys, the images are viewed at a larger size, 150x150, even though the image itself is low-resolution (from CIFAR10).\n\n**3. Study: \u201cHow well can people detect when a model has made a sizable mistake?\u201d**\n\nTo address the first question, each user is given 3 images, 2 of which are correctly classified and 1 is mis-classified. We use ResNet18 trained and evaluated on CIFAR10. Users must predict which image was incorrectly classified given a) the model explanations and b) *without* the final prediction. For saliency maps, this is a near impossible task as saliency usually highlights the main object in the image, regardless of wrong or right. However, hierarchical methods provide a sensible sequence of intermediate decisions that can be easily checked to see if they match the expected result. This is reflected in the results: For each explainability technique, we collected 600 survey responses. When given saliency maps and class probabilities, only 87 predictions were correctly identified as wrong. In comparison, when given the NBDT series of predicted classes and child probabilities (e.g., \u201cAnimal (90%) -> Mammal (95%)\u201d, without the final leaf prediction) 237 images were correctly identified as wrong.\n\nOne note: Although NBDT provides more information than saliency maps about misclassification, a majority -- 363 predictions -- were not correctly identified. To explain this, we note that ~37% of all NBDT errors occur at the final binary decision, between two leaves; since we provide all decisions except the final one, these leaf errors would be impossible to distinguish.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "9DMAf1qyZw", "original": null, "number": 13, "cdate": 1606261348622, "ddate": null, "tcdate": 1606261348622, "tmdate": 1606261348622, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "t80M6goIAs", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Results (Part 2)", "comment": "(continued from previous comment)\n\n**4. Study: \u201cTo what extent do people follow a model\u2019s predictions when it is beneficial to do so?\u201d**\n\nTo address the second question, each user is given a blurred image (kernel size 5x5) and two sets of predictions: (1) the original neural network\u2019s predicted class and its saliency map, and (2) the NBDT predicted class and the sequence of decisions that led up to it (\u201cAnimal, Mammal, Cat\u201d). For all examples, the two models predict different classes. In 30% of the examples, NBDT is right and the original model is wrong. In another 30%, the opposite is true. In the last 40%, both models are wrong. Since the image is extremely blurry (http://people.eecs.berkeley.edu/~alvinwan/nbdt/cifar10-downsampled/0-img-downsampled.jpg), the user must rely on the models to inform their prediction. From this survey we show that humans choose to rely on the NBDT-explained prediction more often than the saliency-explained predictions. Out of 600 responses, 312 responses chose the NBDT\u2019s prediction, 167 responses chose the base model\u2019s prediction, and 119 responses chose neither model\u2019s predictions. Note that a majority of user decisions (~80%) agreed with either model prediction, even though neither model prediction was correct in 40% of examples, showing our images were sufficiently blurred to force reliance on the models. When both models are incorrect, ~45% of responses agree with NBDT, ~34% with the original model, and 22% with neither."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "PKM7fn-K-ui", "original": null, "number": 11, "cdate": 1606259437804, "ddate": null, "tcdate": 1606259437804, "tmdate": 1606259437804, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "yXgHLDdEed4", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thanks for your thorough answers. Your comments address my questions and fit my original thoughts and intuition. For the backbone, you can refer to [ImageNet Benchmark](https://paperswithcode.com/sota/image-classification-on-imagenet). Not need to test on the top one. Test on several different great backbones (e.g. ResNet and SENet) would help verify whether the method can achieve improvements in a general way."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "t80M6goIAs", "original": null, "number": 10, "cdate": 1606169415913, "ddate": null, "tcdate": 1606169415913, "tmdate": 1606169415913, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "SKIW7O26yVG", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Updated manuscript or results?", "comment": "Hey authors, just wanted to remind you that the author discussion period ends tomorrow (November 24...not sure what time). Don't forget to upload your updated manuscript and to share the results from the mid-training hierarchy construction! If you find yourself in a time crunch, I think the priority for me would be any new results you have."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "NPMVhTm0jCh", "original": null, "number": 6, "cdate": 1605339129714, "ddate": null, "tcdate": 1605339129714, "tmdate": 1605393791465, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "jLwYNTMXJT", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Interpretability, Related Work (Part 1)", "comment": "**Interpretability and WordNet Assignment \u201cIt is claimed that other good performing methods like DNDF are not interpretable. I don't see why that's the case if we use WordNet to assign meaning for the inner nodes of such solutions\u201d**\n\nIt is not clear how WordNet meaning can be assigned to inner nodes for methods like DNDF, where leaves represent mixtures of classes. By contrast, each NBDT leaf represents a single class (e.g., Dog), making it possible to find the corresponding nodes in WordNet and compute the earliest ancestor of those two WordNet nodes (e.g., the ancestor of \u201cCat\u201d and \u201cDog\u201d is Animal, Sec 5.1).\n\n- **\u201cIn Figure 5, a decision tree is given, which could only have been constructed by hand\u2026 [How does it scale to] ImageNet? [Using] WordNet?\u201d** Per Sec 3.2, we construct the hierarchy automatically, by running hierarchical agglomerative clustering in weight-space. The process is the same for ImageNet. To obtain meanings of each inner node, we assign WordNet meanings (Sec 5.1)\n- **\u201cThe visualization [of the induced hierarchy]... gives the wrong idea\u2026 not sure if this has a reasonable geometric meaning.\u201d** Thank you for raising this point. We will need to either revisit this figure or the motivation in writing for the method. Preliminary experiments suggest similar results (downstream accuracy benefits), if we normalize the hyperplane normals before performing clustering.\n\n**Comparing with related work: \u201cThe paper needs\u2026 better understanding of differences from similar work\u2026 There are certainly many papers that use path probabilities for inference.\u201d**\n\nIn Sec 3.1, we state that our use of path probabilities comes from Deng et al (2012). We will also cite the papers mentioned from Kontscheider et al (2016). We should clarify that many (but not all) previous methods use hierarchical softmax (softmax per node), instead of a path probability from the root to the leaves (Sec 2 \u201cHierarchical Classification\u201d last 2 sentences). In contrast to other works using path probabilities, our key differences were to not parameterize the inner nodes explicitly (meaning all classification architectures can be trained as NBDT without modifications), to keep leaves pure (to enable WordNet assignment, Sec 2), and to employ a hierarchy built from neural network weights rather than input data (for improved accuracy, Sec 3.2).\n\n- **\u201cWhere are these implementations [of related work] taken from, for instance for DNDF, which is quite complex? How can the reader trust these numbers?\u201d** Numbers (Table 1) were taken from the original papers, except for 3 methods that did not report top-1 accuracy on comparable datasets: classic DTs, oblique DTs, and DNDF. DNDF authors did not provide top-1 accuracies on CIFAR or ImageNet (top-5 reported). However, given the popularity of the work, we had to compare with DNDF results somehow: 1) The DNDF authors did not open-source code. 2) Thus, we used a third-party implementation of DNDF that we are not affiliated with https://github.com/jingxil/Neural-Decision-Forests. The original results with LeNet were low, so we replaced the backbone with ResNet18 to improve results by 5%+. The two DTs have commonly-accepted implementations in scikit-learn, OC1 and were run on outputs of pre-trained backbones from torchvision.\n- **Most papers I know on the subject actually try to enable sparse activation at test time... Activating all the branches of the tree or graph becomes prohibitively costly for deeper trees.** We show results for a conditionally-executed variant of NBDT (NBDT-H) in Appendix Table 6. Although this variant also outperforms alternative decision-tree-and-neural-network hybrids, we relegated it to the appendix for a few reasons. We can cite the mentioned papers as suggested, but we believe this is out of the scope of our current work:\n    - We agree that test-time computational efficiency is also an important and challenging task. Not only that but enabling k-way classification for extremely large k. However, we chose to narrow the scope and focus on accuracy and interpretability on more commonplace classification datasets.\n    - Unlike these conditionally-executed models, as we show in Fig 1 C, the NBDT can recover from highly-uncertain, incorrect predictions (Sec 3.1 last paragraph). This accounts for the NBDT\u2019s 2-3% win over its conditionally-executed variant.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "R813z3nl5Jo", "original": null, "number": 8, "cdate": 1605379801448, "ddate": null, "tcdate": 1605379801448, "tmdate": 1605379801448, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "SKIW7O26yVG", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Thanks for the thoughtful response!", "comment": "Hey authors, just wanted to let you know that I read and appreciated your thoughtful response. I especially want to commend you on receiving the constructive feedback with an open mind and humility -- while nonetheless advocating for your work, as you should. I look forward to hearing about the results from the \"learning the hierarchy\" heuristic and to reading the updated manuscript. In the meantime, I plan to re-read the submission with your responses in mind and will post any follow up questions that come to mind."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "5s6-1t6ACcl", "original": null, "number": 7, "cdate": 1605339152951, "ddate": null, "tcdate": 1605339152951, "tmdate": 1605339437370, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "jLwYNTMXJT", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Clarity (Part 2)", "comment": "**Clarity: \u201cThe paper is not written very clearly, so it would be hard to reproduce. A nice comparison would be with the Deep Neural Decision Forest (DNDF) paper. I'd expect that level of clarity from an ICLR paper.\u201d**\n\nWhile the first and second reviewers (from the topmost to bottom-most reviews) found the paper clear, we will open-source all code needed to reproduce our results. We are also happy to clarify any confusions in the paper, starting with the suggestions made in these reviews. We currently already compare against DNDF in Table 1 and show that NBDT outperforms DNDF by a large margin across datasets.\n\n- **\u201cThe text misses a proper mathematical formulation of the ... inner nodes\u201d** We have a formulation in Sec 3.1 but recognize the definition of r_i is unclear. We will clarify Sec 3.1.\n- **\u201cIt's not clear in places if indices correspond to nodes or classes\u201d** We will make this explicit and amend Sec 3. Node indices are denoted using i (Eq 2) and class indices are denoted using lowercase c (Eq 3).\n- **\u201cTreeSup results is probably wrong in Table 3\u201d** Thank you for catching this. TreeSup in Table 3 for CIFAR10 should be 94.76%, making TreeSup (t) the best result."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "i7xh1WgJ05w", "original": null, "number": 5, "cdate": 1605338387815, "ddate": null, "tcdate": 1605338387815, "tmdate": 1605338387815, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "ITf63wjbo4F", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Expanded Survey, Experiments without Pre-trained Weights", "comment": "**\u201cAdding more [survey] questions could strengthen the paper.\u201d Authors should compare with similar methods like ANT and provide a summary of participant demographics].**\n\nWe will run further human studies in light of both suggestions, to a) compare with ANT and b) ask more questions to test different aspects of interpretability. For our surveys, we used Amazon\u2019s Mechanical Turks, which doesn\u2019t provide demographic information by default, but we will explore alternatives.\n\n**The paper will be strengthened if the authors run an experiment without using a pre-trained neural network on a small dataset like MNIST to demonstrate their algorithm's effectiveness.**\n\nThis is an interesting idea; thank you for the recommendation. We are now running experiments without a pre-trained neural network to test the idea (hierarchy is built from the currently-trained model\u2019s weights, partway through training). We will follow-up with method details and results on MNIST and CIFAR as they become available.\n\n**Technical Clarity and Presentation**\n\n- **A tree of depth-2 is more interpretable than a tree of depth-4.** Thank you for pointing this out -- we agree. We should clarify to instead say that for many classes, depth-2 trees are suboptimal, because each decision rule will then have too many options to pick from. E.g., A 512-way decision is difficult to interpret. In the limit, we could consider a fully-connected layer a depth-1 tree, and our hope is to distance ourselves from decision rules with too many options.\n- **All leaf nodes should have the same depth, but in the example shown in Figure 6 and supplement, the leaf nodes are in different depths.** Since we use hierarchical agglomerative clustering (Sec 3.2), our trees can have leaves at different levels. For example, say classes 1, 2, and 3 are extremely close but class 4 is far. Classes 1 and 2 could be clustered first. Then, 1-2 is clustered with 3. Finally, 1-2-3 is clustered with 4. This creates an imbalanced binary tree. We will clarify this in the manuscript, as suggested.\n- **Various Presentation Notes: Forgot a period, mean \u201cwithout NBDT\u201d instead of \u201cwithout ResNet\u201d, word \u201cAblation\u201d seems inappropriate** Thank you for noting these. We will amend the manuscript to incorporate all your suggestions.\n- **Labeling the tree's nodes ... should be included in the main paper.** We briefly mention the node labeling strategy in Sec 5.1 but will move details from the supplement to the main manuscript as suggested.\n- **While the authors claim that linearly increasing the weight in NBDT is the superior method, why the NBDT with the constant rate overperforms on CIFAR-10 as shown in Table 3?** Thank you for catching this. TreeSup in Table 3 for CIFAR10 should be 94.76%, making TreeSup (t) the best result."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "SKIW7O26yVG", "original": null, "number": 4, "cdate": 1605338223718, "ddate": null, "tcdate": 1605338223718, "tmdate": 1605338223718, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mlTIpDngMw0", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Interpretability Definition, Making Limitations Explicit, Rewording", "comment": "**Suggestion 1. Focus on one definition of interpretability and then analyze central claims through that lens.**\n\nThank you for the suggestion; we are actively following this: in the context of previous interpretability definitions, we will pick an aspect of interpretability to focus on, then evaluate it qualitatively and quantitatively (likely by redoing the surveys to be task-oriented, as suggested). We will follow up with a longer response shortly.\n\n**Suggestion 2 + 3. Make the limitations of the proposed approach VERY clear. (You need pre-existing weights). If exclusively made for fine-tuning, consider renaming Section 3.3.**\n\nThank you for pointing this out. We understand now that this is not explicit enough and will amend descriptions in the introduction, Sec 3 (intro paragraph), and Sec 3.2 to make this clearer -- a sentence to the tune of \u201cThis induced hierarchy requires a pre-trained neural network\u2019s weights\u201d.\n\nPer your suggestion below, we will also run experiments without a pre-trained network, generating the hierarchy with weights from the model currently being trained, partway through training. If these experiments are successful, we will make clear there are two training strategies: (1) fine-tuning, which requires a pre-trained network for the hierarchy (and possibly weight initialization) and (2) training from scratch, without using a pre-trained network for the hierarchy or weight initialization.\n\n**Suggestion 4. Justify (or reword) statements like \"the interpretability of a decision tree is well-established\".**\n\nThese are fair critiques, and we will reword this section. We agree that the simplest of decision trees with tabular data is the only scenario where a decision tree is directly interpretable. Our hope was to illustrate why it is not straightforward to show NBDT interpretability, due to higher-dimensional inputs, motivating our analysis in Sec 5.\n\n**Questions**\n\n- **When training with tree loss, are weights shared across nodes?** Indeed, any two inner nodes, where one is the ancestor of the other, will share weights. We will clarify this in Sec 3.1.\n- **Is there a heuristic that could approximate \"learning the hierarchy?\u201d (+idea)** This is an interesting idea; thank you for the suggestion. We are now running experiments to test hierarchies built partway through training. We will follow-up with results as they become available.\n- **What are the key differences between this approach and a hierarchical softmax?** At test time, hierarchical softmax will indeed make a hard decision at each node. (This means any incorrect inner decision leads to an incorrect prediction. By contrast, the \u201csoft\u201d inference could tolerate incorrect but highly-uncertain inner decisions, Fig 1 C) During training, hierarchical softmax (HS) uses one softmax term for each node (Appendix C). We compare HS training with our \u201csoft\u201d tree loss in Appendix Table 7, where HS achieves 0.5-2.5% lower accuracy than tree loss. (Apologies, the columns are shifted forward by 1. \u201cNone\u201d should be 94.32%, \u201cTSL\u201d should be 94.50% and \u201cHS\u201d should be 93.94%) Conceptually, we believe that the tree loss attains better accuracy than hierarchical softmax, because our loss disproportionally up-weights the decisions higher up in the tree, thus putting a greater emphasis on learning high level concepts (Sec 3.3).\n- **How would this approach perform for extremely high dimensional output spaces -- one of the primary motivations for hierarchical softmax? I imagine that for some output cardinality, \"soft\" inference becomes computationally infeasible.** We concur that \u201csoft\u201d inference would not scale (e.g., to a million-way classification problem). Soft inference scales only as far as a regular classification neural network could. There are two possibilities we have in mind:\n    1. One possibility is to use just the NBDT\u2019s induced hierarchy (but with hierarchical softmax and \u201chard\u201d inference). We describe the process in Appendix C and show results in Table 7. Relative to the original neural network, accuracy degrades, but Table 2 suggests the induced hierarchy at least obtains higher-accuracy than other hierarchies\n    2. Another possibility we did not explore is executing a \u201clocally-soft\u201d inference and training. One possible manifestation is to cluster several inner nodes in the tree into a \u201csuper node\u201d. Apply HS and \u201chard\u201d inference between \u201csuper nodes\u201d but within each supernode, use soft inference and tree loss. The limits of a neural network (probably around several thousand classes) could determine the \u201csuper node\u201d size.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "jilwklhWXJt", "original": null, "number": 3, "cdate": 1605338003114, "ddate": null, "tcdate": 1605338003114, "tmdate": 1605338003114, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "zwMJgzKxwzR", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Intuition for Improvement, Leaf Ordering", "comment": "**\u201cWhat is the main intuition why NBDTs outperform the original network?\u201d**\n\nWe believe NBDTs outperform the original neural networks by learning higher-order concepts, explicitly (like Animal, Vehicle). This is supported by the generalization results in Table 5, where NBDTs see disproportionate accuracy boost higher up in the hierarchy (9-16%); this is in stark contrast to the overall accuracy boost of 1-2%, suggesting higher-order decisions dominate the improvements.\n\n**\u201cDoes the ordering of the classes in the leaf layer matter?\u201d How do you determine which pairs of classes are grouped together?**\n\nAs you suggest, the ordering of the leaf layer is unimportant. The two classes with the closest weight embeddings are grouped together. The next two closest are also grouped together. And so on and so forth. In short, we run hierarchical agglomerative clustering (Sec 3.2), so that iteratively, the closest two clusters are combined.\n\n**Clarifications**\n- **Figure 2 Step A: $y_d$ -> $y_k$ or $y_4$.** Thank you for the suggestion; we will change the subscript to a k or 4, as suggested.\n- **Why are there so many N/A results in Table 1?** N/A means that the original paper did not report results for that dataset; For example, ANT-A* did not test on CIFAR100. However, we produced the DNDF (no reported CIFAR results), classic DT, and oblique DT (using public OC1 from original authors) results ourselves. We will clarify this in the caption. \n- **In the \u201cCompute node probabilities\u201d paragraph, the definition of $r_i$ seems confusing.** We will clarify the definition of r_i below and also in the manuscript. First, we consider the k classes and their corresponding row vectors in the fully-connected layer\u2019s weights. The jth class\u2019s representative r_i is the jth row vector in the fully-connected layer\u2019s weights. Second, for every inner node in the tree, we consider the subtree that node is the root of. Take all leaves in that subtree, and average all leaf r_i\u2019s. That average is the inner node\u2019s r_i.\n- **What does NN mean? How does it differ from CNN-RNN?** NN means the original Neural Network with standard training, without involving hierarchies. By contrast, CNN-RNN utilizes an RNN to generate sequential labels (the RNN rather than a fixed hierarchy generates the sequence of decisions). On a subset of ILSVRC 2010, Wang et al (2016) show significant improvement over a baseline neural network. We compare NBDT accuracy with both the NN and CNN-RNN in Table 1."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "yXgHLDdEed4", "original": null, "number": 2, "cdate": 1605337883159, "ddate": null, "tcdate": 1605337883159, "tmdate": 1605337883159, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "BBZBZrza9qP", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment", "content": {"title": "Intuition for Improvement, WordNet Assignment", "comment": "**Where do the benefits come from? Does the hierarchy help?**\n\nWe believe benefits come from explicitly learning higher-order concepts (like Animal, Vehicle). This is supported by the generalization results in Table 5, where NBDTs see disproportionate accuracy boost higher up in the hierarchy (9-16%); this is in stark contrast to the overall accuracy boost of 1-2%, suggesting higher-order decisions dominate the improvements.\n\nWe agree that the hierarchy is both helpful and important.  We also show that the wrong choice of hierarchy can actually hurt accuracy (Table 2).\n\n**How do you match WordNet concepts with your hierarchy, since the tree structures differ? Do you use Word2Vec?**\n\nAlthough this is an interesting suggestion we will consider, we currently do not need Word2Vec. \n\nTo assign WordNet meaning to nodes, we compute the earliest common ancestor for all leaves in a subtree: For example, say \u201cDog\u201d and \u201cCat\u201d are two leaves with a parent node. To find WordNet meaning for the parent, find all ancestor concepts that \u201cDog\u201d and \u201cCat\u201d share, like \u201cMammal\u201d, \u201cAnimal\u201d, and \u201cLiving Thing\u201d (according to WordNet). The earliest shared ancestor is \u201cMammal,\u201d so we assign \u201cMammal\u201d to the parent node for \u201cDog\u201d and \u201cCat\u201d. We can repeat this for any intermediate node. In summary, for all leaves that are descendants of this intermediate node, find the earliest shared ancestor. We briefly mention this in Sec 5.1 and will expand that description.\n\nHowever, WordNet is missing many concepts that an intermediate node may represent: for example, the leaves may share context (e.g., \u201cfish\u201d and \u201cboat\u201d are both \u201caquatic\u201d) or an attribute (e.g., \u201cpencil\u201d and \u201cwire\u201d are both cylindrical). We show examples of non-WordNet concepts in Appendix Figure 13. Note that these non-WordNet meanings require human intervention to annotate.\n\n**Clarifications**\n- **What happens if you enforce only the tree supervision loss (no cross entropy term)?** We found that, without the cross entropy term, the tree supervision loss alone does not boost accuracy. It seems that regular cross entropy is needed to first learn appropriate leaf representations before tree supervision loss can force the network to learn higher-order concepts like Animal. We will gladly include this ablation in Table 3.\n- **Do you update the weights of intermediate nodes?** Yes, but not directly. The intermediate nodes do not have weights themselves. Instead, the intermediate node weights are an average of its leaf weights (Sec 3.2). This means the intermediate node weights are updated when the leaf weights (i.e., fully-connected layer weights) are updated.\n- **Could you run the large scale datasets on more advanced backbones than EfficientNet?** We would be happy to test on more advanced backbones; did you have any in particular in mind? We chose EfficientNet (2019) simply because it had a) small enough models to fit in our compute budget and b) at the time, one of the latest open-sourced training pipelines."}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mCLVeEpplNE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1088/Authors|ICLR.cc/2021/Conference/Paper1088/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863821, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Comment"}}}, {"id": "ITf63wjbo4F", "original": null, "number": 2, "cdate": 1603863666059, "ddate": null, "tcdate": 1603863666059, "tmdate": 1605024533833, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review", "content": {"title": "The paper presents an algorithm (NBDT) for interpretable image classification, demonstrating a higher preference rate (65.9%) when compared to a saliency map for the model explanation among 374 participants.  ", "review": "Significance:\nThis article seems to be a useful contribution to the literature on interpretable deep networks. However, the paper could be strengthened by demonstrating and analyzing the interpretability of approaches to other types of data, such as sequential data.\n\nNovelty:\nThis paper's main contribution is to offer a new hybrid model that combines a deep neural network with a tree. The authors used the weights of the last layer of a DNN to build a tree from bottom to top, where each inner node in the tree is the average weight of each child. Then, the authors used softmax to compute the probability of  routing for each child. \n\nPotential Impact:\nThe approach presented in this paper is well-evaluated in computer vision but potentially useful in many other settings.\n\nTechnical Quality:\n+ The technical content of the paper appears to be correct, albeit there is some room for improvement. \n+ Page 2, the authors said, \"These models likewise limit interpretability by supporting no more than depth-2 trees.\" Having the depth-2 tree actually improves the interpretability since it is easier to follow the model prediction. For example, a tree of depth-2 considers more interpretable than a tree of depth-4. The authors should rephrase this sentence.\n+ While the authors claim that linearly increasing the weight in NBDT is the superior method, why the NBDT with the constant rate overperforms on CIFAR-10 as shown in Table 3?\n+ Based on the NBDT's explanation, all leaf nodes should have the same depth, but in the example shown in Figure 6 and supplement, the leaf nodes are in different depths. The authors need to explain why the final tree has leaf nodes with different depths.\n+ The authors did not compare their method's interpretability with a similar methods, such as Adaptive Neural Trees (Tanno et al. 2019).  I suggest running this experiment since Adaptive Neural Trees has a different interface than NBDT. . While the number of participants is noticeable in the interpretability study, it seems that participants only answer one question. Adding more questions could strengthen the paper. Further, the authors should provide a summary of participants (e.g., age, education, and gender).\n+ The paper will be strengthened if the authors run an experiment without using a pre-trained neural network on a small dataset like MNIST to demonstrate their algorithm's effectiveness. \n\nPresentation/Clarity\n+ While the paper is fairly readable, there is room for improvement in the clarity.\n+ Page 3, the last paragraph forgot a period after the parentheses. \"path (Figure 1 C, Appendix Table 6) This ...\"\n+ Page 7, figure 4, I believe that the authors mean \"without NBDT\" instead of \"without ResNet.\"\n+ While the authors explained how they label the tree nodes with WordNet in the supplement, there is no explanation in the main paper. Labeling the tree's nodes is an important part of the algorithm and should be included in the main paper.\n+ Page 5, the authors used \"Hierarchy Ablation\" and \"Loss Ablation\" subtitle in the paper. The word \"Ablation\" seems inappropriate in this context.\n\nReproducibility\nThe paper describes all the algorithms in full detail and provides enough information for an expert reader to reproduce its results. However, the authors did not discuss when they start to increase the W in equation 3, how to determine to stop the W from increasing, and with what rate the W should be increased.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127272, "tmdate": 1606915763374, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1088/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review"}}}, {"id": "zwMJgzKxwzR", "original": null, "number": 4, "cdate": 1603951000273, "ddate": null, "tcdate": 1603951000273, "tmdate": 1605024533770, "tddate": null, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "invitation": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review", "content": {"title": "A Nice Paper", "review": "This paper proposes a neural-backed decision tree that aims to improve both the accuracy and the interpretability of deep learning models. Training under a newly introduced tree supervision loss, the authors show that NBDTs can outperform and out-generalize some modern architectures on several image datasets.\n\nOverall this paper is well written and established. The idea of using a differentiable oblique decision tree to replace the final linear layer is interesting. The authors provide clear illustration of the procedure and promising experimental results.\n\nQuestions:\n\n1. What is the main intuition that NBDTs can outperform the original network?\n2. Given the classes are in the leaves, does the ordering of classes in the leaf layer matter? How should one determine which two classes are in the same bottom subtrees?\n\nMinor comments:\n\n1. Figure 2 Step A: y_d -> y_k or y_4.\n2. Why are there many n/a results in Table 1?\n3. Section 3.1 in the Compute node probabilities paragraph, the definition of r_i seems confusing.\n4. What does NN mean in Table 1? How different is it from CNN-RNN?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1088/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1088/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NBDT: Neural-Backed Decision Tree", "authorids": ["~Alvin_Wan1", "~Lisa_Dunlap1", "~Daniel_Ho2", "~Jihan_Yin1", "~Scott_Lee2", "~Suzanne_Petryk1", "~Sarah_Adel_Bargal1", "~Joseph_E._Gonzalez1"], "authors": ["Alvin Wan", "Lisa Dunlap", "Daniel Ho", "Jihan Yin", "Scott Lee", "Suzanne Petryk", "Sarah Adel Bargal", "Joseph E. Gonzalez"], "keywords": ["explainability", "computer vision", "interpretability"], "abstract": "Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wan|nbdt_neuralbacked_decision_tree", "one-sentence_summary": "Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.", "supplementary_material": "/attachment/94bd42b47c7e7edb6bf163f4779bb89476043685.zip", "pdf": "/pdf/9083652f9da499bb55946d63a399c08093258e1c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwan2021nbdt,\ntitle={{\\{}NBDT{\\}}: Neural-Backed Decision Tree},\nauthor={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mCLVeEpplNE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mCLVeEpplNE", "replyto": "mCLVeEpplNE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1088/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127272, "tmdate": 1606915763374, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1088/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1088/-/Official_Review"}}}], "count": 24}