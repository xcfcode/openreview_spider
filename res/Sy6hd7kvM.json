{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573617475, "tcdate": 1521573617475, "number": 312, "cdate": 1521573617136, "id": "SJFgykJ9f", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Sy6hd7kvM", "replyto": "Sy6hd7kvM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance-based Gradient Compression for Efficient Distributed Deep Learning", "abstract": "Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.", "pdf": "/pdf/3aa9756ca3e59de38fa50133474bf99c15f13738.pdf", "TL;DR": "A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing \u2018unambiguous\u2019 gradients.", "paperhash": "tsuzuku|variancebased_gradient_compression_for_efficient_distributed_deep_learning", "_bibtex": "@misc{\ntsuzuku2018variancebased,\ntitle={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\nauthor={Yusuke Tsuzuku, Hiroto Imachi, Takuya Akiba},\nyear={2018},\nurl={https://openreview.net/forum?id=rkEfPeZRb},\n}", "keywords": ["distributed deep learning", "gradient compression", "collective communication", "data parallel distributed sgd", "image classification"], "authors": ["Yusuke Tsuzuku", "Hiroto Imachi", "Takuya Akiba"], "authorids": ["tsuzuku@ms.k.u-tokyo.ac.jp", "imachi@preferred.jp", "akiba@preferred.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730171866, "tcdate": 1518446773556, "number": 129, "cdate": 1518446773556, "id": "Sy6hd7kvM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Sy6hd7kvM", "original": "rkEfPeZRb", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Variance-based Gradient Compression for Efficient Distributed Deep Learning", "abstract": "Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.", "pdf": "/pdf/3aa9756ca3e59de38fa50133474bf99c15f13738.pdf", "TL;DR": "A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing \u2018unambiguous\u2019 gradients.", "paperhash": "tsuzuku|variancebased_gradient_compression_for_efficient_distributed_deep_learning", "_bibtex": "@misc{\ntsuzuku2018variancebased,\ntitle={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\nauthor={Yusuke Tsuzuku, Hiroto Imachi, Takuya Akiba},\nyear={2018},\nurl={https://openreview.net/forum?id=rkEfPeZRb},\n}", "keywords": ["distributed deep learning", "gradient compression", "collective communication", "data parallel distributed sgd", "image classification"], "authors": ["Yusuke Tsuzuku", "Hiroto Imachi", "Takuya Akiba"], "authorids": ["tsuzuku@ms.k.u-tokyo.ac.jp", "imachi@preferred.jp", "akiba@preferred.jp"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730171866, "tcdate": 1509127948492, "number": 597, "cdate": 1518730171851, "id": "rkEfPeZRb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkEfPeZRb", "original": "SJNMPgWA-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Variance-based Gradient Compression for Efficient Distributed Deep Learning", "abstract": "Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.", "pdf": "/pdf/5a65da15a9a221c610aa17e90ceb3b8095ecd15d.pdf", "TL;DR": "A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing \u2018unambiguous\u2019 gradients.", "paperhash": "tsuzuku|variancebased_gradient_compression_for_efficient_distributed_deep_learning", "_bibtex": "@misc{\ntsuzuku2018variancebased,\ntitle={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\nauthor={Yusuke Tsuzuku and Hiroto Imachi and Takuya Akiba},\nyear={2018},\nurl={https://openreview.net/forum?id=rkEfPeZRb},\n}", "keywords": ["distributed deep learning", "gradient compression", "collective communication", "data parallel distributed sgd", "image classification"], "authors": ["Yusuke Tsuzuku", "Hiroto Imachi", "Takuya Akiba"], "authorids": ["tsuzuku@ms.k.u-tokyo.ac.jp", "imachi@preferred.jp", "akiba@preferred.jp"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}