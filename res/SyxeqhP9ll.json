{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487877433877, "tcdate": 1478290568435, "number": 434, "id": "SyxeqhP9ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SyxeqhP9ll", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396583030, "tcdate": 1486396583030, "number": 1, "id": "ryy3hzL_l", "invitation": "ICLR.cc/2017/conference/-/paper434/acceptance", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviews clearly support acceptance of the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396583506, "id": "ICLR.cc/2017/conference/-/paper434/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396583506}}}, {"tddate": null, "tmdate": 1485183004283, "tcdate": 1485183004283, "number": 7, "id": "HkE7uqXwx", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "ByBt6CyPg", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": " ", "comment": "Thank you for the additional comments. The HMC idea will be an interesting experiment to do, though it may not be easy to make it work in high-dimensional cases. Also, it is related to an alternative view of energy-based GAN, which is to train an energy-based model (discriminator) where the negative phase samples come from a trainable directed sampler (generator). We believe future work should take this direction into consideration. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1484938620806, "tcdate": 1484938620806, "number": 1, "id": "ByBt6CyPg", "invitation": "ICLR.cc/2017/conference/-/paper434/official/comment", "forum": "SyxeqhP9ll", "replyto": "SJYMo9zUx", "signatures": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "content": {"title": "Response to rebuttal", "comment": "Thanks for your response. I appreciate the fact that you went ahead and performed additional experiments to address my questions. I think they are nice additions and paint an even clearer paper.\nAs I already wrote in my main review I believe this paper should definitely be published at the conference as is also reflected in my score.\n\nI wanted to quickly respond to your responses to give some further comments and will leave my initial review intact to preserve the conversation:\n\n[Response to comment 2]\nYes, I meant to write that the procedure will by construction only result in an improved \"discriminator\". I agree with your response here, I also of course understand that there is no silver bullet for stabilizing GANs (yet). As you wrote it is just important that we don't convey such a message in our paper (which I also don't believe your paper is doing).\n\n[Response to comment 3]\n-Regarding the additional experiment: cool to see that this works as expected, thanks for carrying out the experiment.\n-Regarding the use of the final discriminator to separate real from fake exampls: fair points I think you are right that this would likely not be meaningful.\n-\"To evaluate the relative goodness, it is most natural to think about ranking. Unfortunately, we do not have the ground truth ranking for direct evaluation.\"\nYes, I  agree that would make most sense. Another idea for evaluating the learned energy function that goes in a similar direction and might work at least for the low-dimensional cases - and perhaps with some additional regularization could also be made to work for images - would be to use the learned energy in combination with a general purpose gradient based sampler to sample from the data distribution. That is: you could fix the discriminators after training and do Hamiltonian Monte Carlo sampling using the energy as a proxy for the likelihood for the true distribution using these samples additional evaluations could be performed. To be clear: this is not meant as an additional request for additional experiments I would like to see in this paper; just an additional thought."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287577885, "id": "ICLR.cc/2017/conference/-/paper434/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper434/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper434/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287577885}}}, {"tddate": null, "tmdate": 1484069784581, "tcdate": 1481943764244, "number": 3, "id": "BJ2A5mMVl", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "HJXpDiAQg", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": "Response to reviewer1", "comment": "Thank you very much for the questions and suggestions. We address your comments one by one as follows.\n\n====================================================================================\n[Comment 0]: The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\n[Response]:\n- The primary purpose of Section 5.3 is to show that adding the calibrating term K(p_gen) does not degrade sample quality compared to existing convolutional GAN variants (see DCGAN https://arxiv.org/pdf/1511.06434v2.pdf).\n- The reason we do not provide samples from baseline models is mostly a space issue. We can add baseline samples in the appendix. For now, a good visual reference is the improved GAN (https://arxiv.org/pdf/1606.03498v1.pdf) as well as https://openai.com/blog/generative-models/ for CIFAR10 samples. Note that the improved GAN uses many additional techniques to improve sample quality, including label information (which gives a big boost). For celebA, the EBGAN (https://openreview.net/pdf?id=ryh9pmcee) paper provides some samples for comparison (see Figure 6). EBGAN also uses additional techniques to boost sample quality, including margin-based cost and a pulling-away term.\n- As a proxy measure, the inception scores in Table 1 provide some supporting evidence to the statement of comparable quality.\n\n====================================================================================\n[Comment 1]: First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible.\n\n[Response]:\nIn theory, the optimal generator distribution p_gen(x) should match the true data distribution p_data(x) as stated by Proposition 3.1. To verify that empirically, we can definitely provide more samples from the trained generator as well as the mean generator sample later in the appendix. For now, from Figure 9, we can already see the generator samples (samples without the white frame) have great quality, and mix well with the real samples.\n\n====================================================================================\n[Comment 2]: Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the-art?\n\n[Response]\nAs mentioned above, we believe the sample quality of the proposed method is comparable with the state-of-the-art results, which are arguably either from GAN variants or from Pixel RNN and Pixel CNN (https://arxiv.org/pdf/1601.06759.pdf). \nSamples from GAN variants usually have better global structure, while samples from Pixel RNN have better local details. For state-of-the-art samples from GAN variants, please refer to the papers mentioned above.\n\n====================================================================================\n[Comment 3]: Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\n[Response]:\nWe think this is a good question worth some systematic future work. For now, based on our empirical observations and some properties of the training objective, we make the following comments:\n- Computational overhead is not a big issue. For the KNN based approach, the heaviest computation is to compute the cross-term in the Euclidean distance, which requires a matrix-matrix multiplication of shape [BxD] x [DxB], where B is the mini-batch size and D is the feature size. With a reasonable mini-batch size (we used 200), the computation overhead is equal to that of a fully-connected layer with a weight matrix of shape [DxB], which is quite common. As for the VI based approach, the inference network is essentially a replica of the discriminator. Thus, it requires strictly less than double the original computation.\n- A potential issue of the proposed formulation is the quality of the approximate entropy gradient. On the one hand, when the approximate entropy gradient (1) is not accurate, and (2) has a large magnitude, it can significantly slow down the training. Basically, the inaccurate and large-magnitude entropy gradient will act like some chaotic noise, which dominates the discriminator gradient with its large magnitude, pushes the generator to wander around purposelessly, and thus makes the training very slow. On the other hand, when the estimated entropy gradient is accurate, it can actually speed up the training by encouraging the generator to \u201cexplore\u201d regions where p_gen(x) is currently low. Thus, how to obtain a good entropy gradient estimate is an important topic under our formulation.\n\n====================================================================================\n[Open question]: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n[Response]:\nGenerally speaking, in order to obtain a normalized density estimation, either the training procedure and the parameterization automatically guarantee a normalized result, or some proper post-normalization will be needed. Actually, our current proposed formulation may potentially fall into the second case (see below). \n\nFor the first case, the question effectively becomes whether one can derive a GAN-type formulation where the discriminator is self-normalized at the optimum. Intuitively, this self-normalization requirement sounds like posing an additional regularization on the discriminator. However, it\u2019s difficult to specify the form of the regularization, and it can be a set constraint on the allowed parameterization of the discriminator or some additional term in the training objective. Either way, the high-level idea is to absorb the explicit normalization step into either the parameterization or the training process.\n\nAs for the second case (post normalization), our current proposed formulation offers some directions of exploration, though there are still many challenges as we will discuss in the following.\n\n1. Because we have provided a case in Equation (6) where the discriminator recovers the negative density function plus some underdetermined terms, we can consider how to eliminate the ambiguity caused by the extra underdetermined terms and thus obtain an estimate of the (negative) data density. For this, note that the true difficulty is in the weak support discriminator term $mu(x)$, without which we can calculate the global bias $lambda$ using the normalization condition sum_x p(x) = 1. Actually, there are two situations where $mu(x)$ is zero and can be dropped. The first one is when the data distribution has infinite support. However, since high-dimensional data usually resides on a low-dimensional manifold, this condition may not hold. The second situation is that we have a clear idea about the boundary of the data support and thus we can only consider the region inside the data support. But in general, without prior knowledge, identifying the boundary of data support from empirical data is an ill-posed problem.\n\n2. One may also consider whether we can properly renormalize the obtained energy function into the data density. To do the normalization properly, we will actually encounter the same data support problem as mentioned above. In addition to that, another challenge is that the approximate entropy gradient may have an imprecise scale. For example, in Equation (10), we normalize the gradient into unit norm and use the hyper-parameter $alpha$ to control the overall scale. Effectively, the imprecise scale is equivalent to adding a multiplicative weight to the calibrating term in Equation (1), i.e. the objective becomes max_{c} min_{p_gen} E_{p_gen} [c(x)] - E_{p_data} [c(x)] + weight * K(p_gen). Hence, the discriminator calibrated by negative entropy should converge to c(x) = - weight * log p_data(x) + lambda + mu(x). Basically, the weight plays the same role as the \u201ctemperature\u201d hyper-parameter in a Gibbs distribution. Although the temperature does not change the relative ranking of samples in terms of energy/density, it changes the absolute value of the density. When the temperature is smaller than 1, it will flatten the distribution. When the temperature is larger than 1, it will make the distribution spikier. So, the quality of the entropy gradient is again a core issue here.\n\nFinally, since the question is quite open and has different interpretations, if we've misinterpreted the question, we'd be glad to answer further clarifying questions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1484069648600, "tcdate": 1484069648600, "number": 6, "id": "SJYMo9zUx", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "ByqjAeGEg", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": "Response to Reviewer2", "comment": "Thank you very much for the questions and suggestions. We address your comments one by one as follows.\n\n====================================================================================\n[Comment 1]: The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below).\n\n[Response]:\nThanks for pointing out this potentially confusing point. In order to make it clearer, we have included some additional text following Equation (1) to describe the energy based view of adversarial training along with the intuitive interpretation of Equation (1). Also, we have included architecture details in the appendix B.1. We believe, however, stating that so early in the text that adding the calibrating term to GAN will not work will diverge the attention of readers, and add some unnecessary burden.\n\n====================================================================================\n[Comment 2]: The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper.\n\n[Response]:\n- Firstly, we guess the first sentence in the comment actually reads \u201cThe proposed procedure will by construction only result in an improved \u2018discriminator\u2019\u201d instead of \u2018generator\u2019. Theoretically, under the non-parametric setting, both the original GAN and our proposed formulation guarantee that the generator distribution matches the data distribution, i.e., p_gen(x) = p_data(x). Therefore, when training reaches the optimal, the generator obtained from the proposed formulation will be indistinguishable to that of the original GAN.\n- However, when it comes to the question whether the proposed formulation will improve the training stability or not, we do not have theoretical analysis nor empirical results for verifying it currently. But the fact that our model achieves better Inception score may suggest that there exist some additional advantages of the energy based training for GAN. To faithfully verify or falsify this statement, much more work (both theoretical and empirical) will be needed. Since it is not the focus of this work, we hope to leave it for future work. Meanwhile, we will carefully add this discussion into the text, and make sure readers have a precise understanding of the content of our work. Thanks for pointing this out.\n\n====================================================================================\n[Comment 3]: The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories).\n\n[Response]:\n- For quantitative evaluation of the discriminator, we ran experiments on MNIST with the procedure you suggested, i.e., using the last layer features learned by a discriminator as fixed input to train a linear classifier, in order to evaluate the generalization quality of the learned features. We report the result in appendix B.5. The experiment results support the fact that the discriminator from our proposed formulation maintains more information than both GAN and EGAN-Const.\n- Using the final discriminator to separate real from fake examples should not be a good evaluation metric, because \n    (1) At the optimal \u201cfake\u201d samples are essentially real as p_gen(x) = p_data(x). So, the discriminator will fail to distinguish the generator output (\u201cfake\u201d) from the real.\n    (2) On the other hand, being able to distinguish real and fake samples does not necessarily mean the discriminator is in a better shape. Instead, it suggests the generator is not well trained.\n- Generally speaking, the core difficulty of evaluating our discriminator lies in that an energy function is measuring the relative \u201cgoodness\u201d among samples on the data manifold, rather than distinguishing whether samples are on the data manifold or not (fake vs. real). To evaluate the relative goodness, it is most natural to think about ranking. Unfortunately, we do not have the ground truth ranking for direct evaluation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1483280429015, "tcdate": 1483280429015, "number": 5, "id": "HJSEeqUHg", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "ryS91-G4e", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": "Clarification on mdel architecture", "comment": "We have included the architecture information in the appendix. The reason why we didn\u2019t specify it initially was that the architecture used is almost identical to that in the DCGAN paper (https://arxiv.org/pdf/1511.06434v2.pdf), except for that the discriminator c() does not have the last sigmoid layer which squashes a scalar value into a probability in [0, 1]. Similarly, following the DCGAN setting, Adam was chosen as the optimization algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1482376933496, "tcdate": 1482376933496, "number": 4, "id": "Skp1vTu4e", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "rklqb1P4x", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": "Response to Reviewer2", "comment": "Thank you very much for the comments.\n\nFirstly, we are really sorry about the writing problem at the top of page 5. Due to some careless editing after submission, a paragraph that was originally in the paper was erratically deleted. We have recovered that part and it should read clearly now.\n\nWe agree that a scalable entropy estimation method is the core to the proposed formulation. Also, it is true the entropy estimation problem is closely related to density estimation, especially for the exponential families. However, for the proposed formulation, what we really need is the estimation of entropy \u201cgradient\u201d, which can be practically easier. As shown by Equation (9) of the updated paper, it amounts to estimating the score function: d log p_gen(x) / d x. In this work, the nearest neighbors approximation is an example of thinking in this direction.\n\nAnother direction of thinking is that since we only need a proper gradient, is it possible to build another network to provide the gradient estimation. Actually, this extra network can provide the gradient estimation by backward propagation, or even by forward propagation [1]. For now, the variational inference is an example of using the backward propagation to provide a gradient estimation. More interestingly, as GANs were initially designed to bypass an explicit density estimation, it is conceptually tempting to think about using an adversarial process to get an implicit entropy estimation.\n\nWe believe all these ideas are worth exploring as future work.\n\nIn addition to these technical possibilities, another \u201cadvantage\u201d here is that in theory, we can have infinite samples from the generator to estimate the entropy (gradient), which is usually impossible for normal density estimation based on empirical data.\n\n[1] Jaderberg, Max, et al. \"Decoupled neural interfaces using synthetic gradients.\" arXiv preprint arXiv:1608.05343 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1482252679700, "tcdate": 1482252679700, "number": 3, "id": "rklqb1P4x", "invitation": "ICLR.cc/2017/conference/-/paper434/official/review", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["ICLR.cc/2017/conference/paper434/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper434/AnonReviewer2"], "content": {"title": "A mathematically elegant extension of GANs to approximate density estimation", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data.\n\nI would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature.\n\nAlso, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 \"Finally, let's whose discriminative power\". I'm not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512586874, "id": "ICLR.cc/2017/conference/-/paper434/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper434/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper434/AnonReviewer1", "ICLR.cc/2017/conference/paper434/AnonReviewer3", "ICLR.cc/2017/conference/paper434/AnonReviewer2"], "reply": {"forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512586874}}}, {"tddate": null, "tmdate": 1481932685465, "tcdate": 1481932685465, "number": 1, "id": "ryS91-G4e", "invitation": "ICLR.cc/2017/conference/-/paper434/pre-review/question", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "content": {"title": "missing details on architecture and chosen energy function definition", "question": "The paper is missing some details in the experimental setup, you refer to the potentially released code but I feel like a few more details regarding the experimental setup should also be included in the main paper or the appendix."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481932686083, "id": "ICLR.cc/2017/conference/-/paper434/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper434/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "reply": {"forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481932686083}}}, {"tddate": null, "tmdate": 1481932587292, "tcdate": 1481932450480, "number": 2, "id": "ByqjAeGEg", "invitation": "ICLR.cc/2017/conference/-/paper434/official/review", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper434/AnonReviewer3"], "content": {"title": "Interesting well written paper on improving the stability of discriminators in GANs.", "rating": "7: Good paper, accept", "review": "The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution. In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term.\n\nThe paper is clearly written, including the theoretical derivation. The derivation of the additional regularization term seems valid and is well explained. The experiments also empirically seem to support the claim that the proposed changed objective results in a \"better\" discriminator. There are only a few issues with the paper in its current form:\n- The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below).\n- The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper.\n- The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories).\n- Main complaint: It is completely unclear what the generator and discriminators look like for the experiments. You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix).\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512586874, "id": "ICLR.cc/2017/conference/-/paper434/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper434/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper434/AnonReviewer1", "ICLR.cc/2017/conference/paper434/AnonReviewer3", "ICLR.cc/2017/conference/paper434/AnonReviewer2"], "reply": {"forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512586874}}}, {"tddate": null, "tmdate": 1481811520391, "tcdate": 1481811520385, "number": 2, "id": "HyurUXxVl", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "BJL1OAAQg", "signatures": ["~Zihang_Dai1"], "readers": ["everyone"], "writers": ["~Zihang_Dai1"], "content": {"title": "Response to the questions above", "comment": "Thank you for the questions.\n\n1. Firstly, just for clarification, the reason we mention f-GAN in the appendix is to support the point that \u201cadding the same calibrating term K(p_gen) to the original GAN formulation (or to the f-GAN family) will NOT enable the discriminator to recover the energy function while ensuring the generator matches the data distribution\u201d. Hopefully, this point is well conveyed.\n\nFor the comparison among GMMN, f-GAN, and our proposed calibrating energy-based GAN (CEGAN for short), the difference lies in how they achieve the central target: matching p_gen and p_data. Specifically, GMMN tries to do the matching directly in the primal space (generator space) by employing the MMD as the divergence criterion between distributions. On the contrary, f-GAN and CEGAN choose to train a criterion (the discriminator) at the same time via a minimax game, where the discriminator corresponds to the dual variable in the dual space.\n\nIt may be true that the surface form of MMD (squared difference) shares some similarity to the divergence form of CEGAN (direct difference). Note that it is the \u201csquared\u201d difference in MMD that gives rise to the inner products between statistics and thus allows the kernel trick to be applied. In the case of CEGAN, \u201cdirect\u201d difference is sufficient enough because the dual variable (discriminator) can dynamically adjust itself based on the specific type of divergence. For example, for some point x, if p_gen(x) < p_data(x), the discriminator will assign a relatively small value to x (i.e. c(x) is a small value), encouraging the generator to generate samples there. But when p_gen(x) > p_data(x), c(x) will be a large scalar, discouraging generator to generate samples there. So, the trainable nature of the discriminator (dual variable) is the key difference here.\n\n2. To be precise, we say f-GAN objective is \u201crestrictive\u201d only in the narrow sense that given p_gen(x), the f-divergence only allows one fixed point for the discriminator (see Equation 24). As a result, it rules out the possibility that both p_gen(x) = p_data(x), and c(x) is non-degenerate, and adding the same term K(p_gen) as in CEGAN won\u2019t change this nature of the f-divergence. Maybe, a more proper word should be \u201crigid\u201d instead of \u201crestrictive\u201d. We will modify the paper to make this point clear, and thanks for pointing it out."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1481735039708, "tcdate": 1481725917647, "number": 1, "id": "BJL1OAAQg", "invitation": "ICLR.cc/2017/conference/-/paper434/public/comment", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Questions", "comment": "Interesting papers. \nGood explanation using a fair amount of math.\n\nI have some questions. \n\n1. In the appendix, you compare your algorithm to f-GAN. However, It would be more natural that you compare it with GMMN because the formulation of maximum mean discrepancy is more similar to  (1).\n\n2. You claim that f-GAN including the usual GAN is too restrictive.  Would that statement be applied to GMMN? \nFrom my understanding, GAN is not restrictive because the discriminator is updated once or twice continuously and the obtained ratio is not optimal. This early stopping is a kind of regularization that prevents the discriminator from overfitting. In reality, GANs work using that stopping rule. However, GMMN would be restrictive because such a stopping rule cannot be used.\n\n\n[1] https://arxiv.org/pdf/1505.03906v1.pdf\n[2] http://www.inference.vc/another-favourite-machine-learning-paper-adversarial-networks-vs-kernel-scoring-rules/"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578013, "id": "ICLR.cc/2017/conference/-/paper434/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyxeqhP9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper434/reviewers", "ICLR.cc/2017/conference/paper434/areachairs"], "cdate": 1485287578013}}}, {"tddate": null, "tmdate": 1481713647179, "tcdate": 1481713595143, "number": 1, "id": "HJXpDiAQg", "invitation": "ICLR.cc/2017/conference/-/paper434/official/review", "forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "signatures": ["ICLR.cc/2017/conference/paper434/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper434/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.\nSpecifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal.\nWe derive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two effective approximation techniques.\nEmpirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "pdf": "/pdf/b8945bf8e29ebf04fdda251269f36cbd8986c26b.pdf", "paperhash": "dai|calibrating_energybased_generative_adversarial_networks", "keywords": ["Deep learning"], "conflicts": ["cmu.edu", "umontreal.com", "maluuba.com"], "authors": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "authorids": ["zander.dai@gmail.com", "amjadmahayri@gmail.com", "phil.bachman@gmail.com", "hovy@cmu.edu", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512586874, "id": "ICLR.cc/2017/conference/-/paper434/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper434/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper434/AnonReviewer1", "ICLR.cc/2017/conference/paper434/AnonReviewer3", "ICLR.cc/2017/conference/paper434/AnonReviewer2"], "reply": {"forum": "SyxeqhP9ll", "replyto": "SyxeqhP9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper434/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512586874}}}], "count": 14}