{"notes": [{"id": "Hygvln09K7", "original": "SygkcCs9FQ", "number": 1085, "cdate": 1538087919159, "ddate": null, "tcdate": 1538087919159, "tmdate": 1545355395620, "tddate": null, "forum": "Hygvln09K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Meta Learning with Fast/Slow Learners", "abstract": "Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative \u201cslow learners\u201d on low-level basic feature representation layers and \u201cfast learners\u201d on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.", "keywords": ["computer vision", "meta learning"], "authorids": ["chengzhuoyuan07@gmail.com"], "authors": ["zhuoyuan@fb.com"], "TL;DR": "We applied multiple meta-strategy to improve meta-learning performance on base CNNs. ", "pdf": "/pdf/50464de80c7928c82c356173530565720a58249a.pdf", "paperhash": "zhuoyuanfbcom|meta_learning_with_fastslow_learners", "_bibtex": "@misc{\nzhuoyuan@fb.com2019meta,\ntitle={Meta Learning with Fast/Slow Learners},\nauthor={zhuoyuan@fb.com},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygvln09K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkgtE3dgxN", "original": null, "number": 1, "cdate": 1544748081513, "ddate": null, "tcdate": 1544748081513, "tmdate": 1545354515940, "tddate": null, "forum": "Hygvln09K7", "replyto": "Hygvln09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1085/Meta_Review", "content": {"metareview": "The paper introduces an interesting idea of using different rates of learning for low level vs high level computation for meta learning. However, the experiments lack the thoroughness needed to justify the basic intuition of the approach and design choices like which layers to learn fast or slow need to be further ablated.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Useful idea, requires more thorough experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper1085/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1085/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Learning with Fast/Slow Learners", "abstract": "Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative \u201cslow learners\u201d on low-level basic feature representation layers and \u201cfast learners\u201d on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.", "keywords": ["computer vision", "meta learning"], "authorids": ["chengzhuoyuan07@gmail.com"], "authors": ["zhuoyuan@fb.com"], "TL;DR": "We applied multiple meta-strategy to improve meta-learning performance on base CNNs. ", "pdf": "/pdf/50464de80c7928c82c356173530565720a58249a.pdf", "paperhash": "zhuoyuanfbcom|meta_learning_with_fastslow_learners", "_bibtex": "@misc{\nzhuoyuan@fb.com2019meta,\ntitle={Meta Learning with Fast/Slow Learners},\nauthor={zhuoyuan@fb.com},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygvln09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1085/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352971893, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygvln09K7", "replyto": "Hygvln09K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1085/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1085/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1085/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352971893}}}, {"id": "Bke71ZD5nX", "original": null, "number": 3, "cdate": 1541202138780, "ddate": null, "tcdate": 1541202138780, "tmdate": 1541533436083, "tddate": null, "forum": "Hygvln09K7", "replyto": "Hygvln09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "content": {"title": "The paper addresses fast/slow learning modules in deep networks. Good paper. Needs more clarity/work. ", "review": "The overall contribution makes sense. Consider solving a linear system i.e., learning an unknown matrix. Splitting it into two components (like in NMF or MMF) and learning each separately gives more control on the conditioning of the matrices. This is the basis of residual networks (at least the theory for linear resnets). Within this, the technical/theoretical results presented in the paper are sensible. Couple of issues: \n1) Where are we breaking the slow/fast learners in terms of the depth of the network? I.e., How many of the layers are slow? Does this break point influence the overall convergence? \n2) It is unclear what the aim of simulations is? The reported figures are not conveying useful information. It makes sense to do a repeatability experiment here with multiple sets of simulated datasets. \n3) Put confidence intervals on the results (table/figure). \n4) What is the nature and choice of g()? The evaluations uses LSTM but will the structure of g() influence the rate of learning? \n5) The authors should choose a better reference than miracle for the ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1085/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Learning with Fast/Slow Learners", "abstract": "Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative \u201cslow learners\u201d on low-level basic feature representation layers and \u201cfast learners\u201d on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.", "keywords": ["computer vision", "meta learning"], "authorids": ["chengzhuoyuan07@gmail.com"], "authors": ["zhuoyuan@fb.com"], "TL;DR": "We applied multiple meta-strategy to improve meta-learning performance on base CNNs. ", "pdf": "/pdf/50464de80c7928c82c356173530565720a58249a.pdf", "paperhash": "zhuoyuanfbcom|meta_learning_with_fastslow_learners", "_bibtex": "@misc{\nzhuoyuan@fb.com2019meta,\ntitle={Meta Learning with Fast/Slow Learners},\nauthor={zhuoyuan@fb.com},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygvln09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "cdate": 1542234309732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygvln09K7", "replyto": "Hygvln09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1085/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335869349, "tmdate": 1552335869349, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1085/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlSVKRY2Q", "original": null, "number": 1, "cdate": 1541167405073, "ddate": null, "tcdate": 1541167405073, "tmdate": 1541533435874, "tddate": null, "forum": "Hygvln09K7", "replyto": "Hygvln09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "content": {"title": "Training slow and fast learners using different strategies is an interesting idea. ", "review": "[Summary:]\nThis paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce \u201cmiracle representations\u201d and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.\n\n[Strengths:]\nThe paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. \n\n[Weaknesses:]\n- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].\n- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. \n- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].\n\n[Specific comments:]\n- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. \n-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.\n-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.\n-The use of the term \u201cmiracle\u201d keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one \u201cmiracle X\u201d was defined and used as these are all essentially saying the same thing.\n\nReferences\n[1]https://arxiv.org/abs/1803.02999\n[2]https://arxiv.org/abs/1801.05558\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1085/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Learning with Fast/Slow Learners", "abstract": "Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative \u201cslow learners\u201d on low-level basic feature representation layers and \u201cfast learners\u201d on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.", "keywords": ["computer vision", "meta learning"], "authorids": ["chengzhuoyuan07@gmail.com"], "authors": ["zhuoyuan@fb.com"], "TL;DR": "We applied multiple meta-strategy to improve meta-learning performance on base CNNs. ", "pdf": "/pdf/50464de80c7928c82c356173530565720a58249a.pdf", "paperhash": "zhuoyuanfbcom|meta_learning_with_fastslow_learners", "_bibtex": "@misc{\nzhuoyuan@fb.com2019meta,\ntitle={Meta Learning with Fast/Slow Learners},\nauthor={zhuoyuan@fb.com},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygvln09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "cdate": 1542234309732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygvln09K7", "replyto": "Hygvln09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1085/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335869349, "tmdate": 1552335869349, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1085/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gXHkH5hm", "original": null, "number": 2, "cdate": 1541193530675, "ddate": null, "tcdate": 1541193530675, "tmdate": 1541533435665, "tddate": null, "forum": "Hygvln09K7", "replyto": "Hygvln09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "content": {"title": "An interesting treatment to meta-learning with fast/slow learning components. ", "review": "[Summary]\nThe paper presents a novel learning framework for meta-learning that is motivated by neural learning process of human over long periods. Specifically, the process of meta-learning is divided into a slow and a fast learning modules, where the slowly-learnt component accounts for low-level representation that is progressively optimized over all data seen so far to achieve generalization power, and the fastly-learnt component is supposed to pick up the target in a new task for quick adaptation. It is proposed that meta-learning should focus on capturing the meta-information for the fast learning module, and leave the slow module being updated steadily without task-specific adaptation. Theoretical analysis is presented on a linear MLP examples to shed some light on the properties of the proposed algorithm. Results on both synthetic dataset and benchmarks justify the theoretical observation and advantages.               \n\nPros\nNovel treatment and formulation of meta-learning from the perspective of fast and slow  learning process\nCons\nSome interesting cases not tested\nPresentation could be improved \n\n[Originality]\nThe paper approaches the recently popular meta-learning from a novel perspective by decomposing the learning process into slow and fast ones. \n\n[Quality]\nOverall,  the paper is well motivated and implemented with both theoretical study and empirical justification. There are a few questions / areas for further improvements, though:\n- It seems that to initialize the slow module, another set of data is needed to pretrain it before the actual meta-learning takes place to learn to optimize the fast learner (as opposed to other meta-learning methods where all parameters in a base model were meta-learnt over the meta training set). How does this affect the performance? E.g., what if the slow module is only updated over the meta-training set (still without reinitialization across different batches) without pre-training?\n- In the current formulation, the base model is decomposed into two distinct (slow and fast) modules. What is the rule to decide which layers should belong to slow or fast modules? How does different choice affect the performance? Can we decompose the base model into finer granularities for different learning behaviors? E.g., a third module module in-between the fast and slow ones that follows medium learning pace.          \n- The theoretical study can be better organized. The proofs can be left in appendix to make room for more discussion on conclusions, non-linear and / or non-Gaussian cases.     \n- The write-up can be improved too at some places: proper reference at line 4 of section 1 is missing; \\phi in (1) is not well defined, as well as \u201cSOA\u201d in section 2;\n\n[Clarity]\nThe paper is generally clearly written, with a few places to improve (see comments above).\n\n[Significance]\nThe paper brings in an interesting perspective to meta-learning. It can also inspire more follow-up work to better understand the problem.   \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1085/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Learning with Fast/Slow Learners", "abstract": "Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative \u201cslow learners\u201d on low-level basic feature representation layers and \u201cfast learners\u201d on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.", "keywords": ["computer vision", "meta learning"], "authorids": ["chengzhuoyuan07@gmail.com"], "authors": ["zhuoyuan@fb.com"], "TL;DR": "We applied multiple meta-strategy to improve meta-learning performance on base CNNs. ", "pdf": "/pdf/50464de80c7928c82c356173530565720a58249a.pdf", "paperhash": "zhuoyuanfbcom|meta_learning_with_fastslow_learners", "_bibtex": "@misc{\nzhuoyuan@fb.com2019meta,\ntitle={Meta Learning with Fast/Slow Learners},\nauthor={zhuoyuan@fb.com},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygvln09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1085/Official_Review", "cdate": 1542234309732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygvln09K7", "replyto": "Hygvln09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1085/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335869349, "tmdate": 1552335869349, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1085/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}