{"notes": [{"id": "H1ggKyrYwB", "original": "SJg_jmA_wS", "number": 1828, "cdate": 1569439607986, "ddate": null, "tcdate": 1569439607986, "tmdate": 1577168294690, "tddate": null, "forum": "H1ggKyrYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QGMBIDoC9y", "original": null, "number": 1, "cdate": 1576798733460, "ddate": null, "tcdate": 1576798733460, "tmdate": 1576800902985, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a technique for incorporating prior knowledge as relations between training instances.\n\nThe reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work. Some reviewers also found the clarity lacking, but were satisfied with the revision. One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic.\n\nFollowing the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722147, "tmdate": 1576800273383, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Decision"}}}, {"id": "SyxXGwWjKB", "original": null, "number": 2, "cdate": 1571653386917, "ddate": null, "tcdate": 1571653386917, "tmdate": 1574164143321, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper argues for encoding external knowledge in the (linguistic) embedding layer of a multimodal neural network, as a set of hard constraints. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. A technique which involves distillation is used to satisfy those constraints during learning.\n\nThe question of how to encode external knowledge in neural networks is a crucial one, and the limitations of end-to-end learning with supervised data is well-made. Overall I feel that this is a potentially interesting paper, addressing an important question in a novel way, but I found the current version a highly-frustrating read (and I read the paper carefully a number of times); in fact, so frustrating that it is hard for me to recommend acceptance in its current form. More detailed comments below.\n\nMajor comments\n--\nThe main problem I have with the paper lies with the first part of section 3, which is a key section describing the main method by which the constraints are satisfied during learning. This is very confusing. The need for the two-step procedure, in particular, and the importance of distillation needs much more explanation, and not relegated to the Appendix (which reviewers are not required to read - see call for papers). I'm not suggesting that the whole of the appendix needs moving to the body of the paper, but I would suggest perhaps 1/2 a page.\n\nA related comment is the use of the distillation technique. This looks crucial, but I don't believe distillation is mentioned at all until the end of the related work section, and even there it comes as a bit of a surprise since there's no mention anywhere of this technique in the introduction.\n\nI would say a little more about the distinction between the embedding space and parameter space, since you say that the external knowledge is encoded in the former and not the latter, and this is important to the overall method. Since embeddings are typically learned (or at least fine-tuned) it's not clear where the boundary is here. Another comment is that embedding space in this paper means the linguistic embedding space. Since this is ICLR and not, eg, ACL, I would make clear what you mean by embedding space.\n\nI don't understand the diagram in Fig. 3 of the architecture, nor the explanation. What's an operation here? Is it *, or *6? I don't get why 3 is embedded by itself in the diagram, and then combined with the remainder using the MLP. Why not just run the RNN over the sequence?\n\nWhy are the training instances {3,+1...} and {4,*2,...} equivalent. I stared at this a while, and still have no idea. Also, how are these \"known to be equivalent\" - what's the procedure?\n\nMinor comments including typos etc.\n--\nThe paper has the potential to be really nicely written and well-presented. Currently it reads like it was thrown together just before the deadline (which only adds to the overall frustration as a reader).\n\nIn fig. 1 the second equivalent question example is interesting, since strictly speaking \"box\" and \"rectangular container\" are not synonyms (e.g. boxes can be round). Since strict synonymy is hard to find, does that matter? (I realise the dataset already exists and was presented elsewhere, but this might be worth a footnote).\n\nmissing (additional) right bracket after Herbert (2016)\n\nNot sure footnote 1 needs to be a footnote. It's already been said, I think, but if it does need repeating it probably deserves to be in the body of the text.\n\nbetween pairs questions\n\nsee Fig.3 -> figure 2?\n\nsee Fig.1 -> Tab. 1? (on p.5)\n\nfootnote 1 missing a right bracket\n\nusually involve -> involves\n\n+9]) - extraneous bracket\n\nFig. 4.1 -> Fig. 3? (p.6)\n\np.7 wastes a lot of space. In order to bring some of the appendix into the main body, I would do away with the very large bulleted list. (I don't mean lose the content - just present it more efficiently)\n\nRemember than\n\nFinally in Fig. 4.2 - some other figure\n\ndue of the long chains", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634189884, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Reviewers"], "noninvitees": [], "tcdate": 1570237731715, "tmdate": 1575634189898, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review"}}}, {"id": "BJg3HDxnoS", "original": null, "number": 7, "cdate": 1573812036180, "ddate": null, "tcdate": 1573812036180, "tmdate": 1573812036180, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "SkxKo1QIjB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment", "content": {"title": "Response to new version", "comment": "Thanks for the changes made to the paper, which makes it much more readable, and hence I will be happy to increase my score accordingly.\n\nJust a few typos that have been introduced that may as well get fixed:\n\np.2 typo: hlWe s\n\n. Our contribution, on the opposite -> . Our contribution, in contrast\n\nthat perfectly fit of\nthe desired constraints. - remove \"of\"\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ggKyrYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1828/Authors|ICLR.cc/2020/Conference/Paper1828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150307, "tmdate": 1576860529245, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment"}}}, {"id": "rJlYRymUsB", "original": null, "number": 4, "cdate": 1573429200582, "ddate": null, "tcdate": 1573429200582, "tmdate": 1573429200582, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "B1l7jf5rFS", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Hi, thanks a lot for your time and valuable comments. They really helped pinpoint sections that required clarifications. We have significatly revised the paper, as summarized below. We believe it to be much improved as a result (updates are highlighted in the PDF).\n\n1,3> Related works\n- [1] \"Harnessing deep neural networks with logic rules\"\nThis paper used posterior regularization to improve how a learned model complies with hand-designed rules. In comparison, we use *instance-level* auxiliary annotations. They be seen as rules that apply to *some* of the training examples. The scope of the two methods is complementary. The major innovation in our method is to enforce *hard* constraints on the learned embeddings, whereas general rules in [1] are softly balanced with learned predictions. [1] uses teacher/student distillation during training, but their best results are obtained with the teacher network, not with the student. In our case, the distillation step is critical to enforce the hard constraints.\n\n- [2] \"Constrained Convolutional Neural Networks for Weakly Supervised Segmentation\"\nThis one shows how to use image-level tags to learn semantic image segmentation. The tags are turned into linear constraints, which is similar to how we handle program annotations (one of our three use cases). Their contribution is to turn their constrained optimization problem into an objective amenable to SGD that is robust to hard-to-enforce/competing constraints. Our contribution, on the opposite, shows how to enforce constraints strictly. In our particular applications, it proved superior to soft-regularized objectives.\n\n- [3] \"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences from Natural Supervision\"\nThis paper combines symbolic AI with neural networks for VQA. The vocabulary of visual concepts is learned, but the operation of the model otherwise depends on a pre-specified domain-specific language, and on manually-design modules. They do not require or seek to exploit program annotations for training. Their contributions are essentially orthogonal to ours. Our method could, for example, apply to the representations of questions learned in their semantic parser.\n\n\n2> Distillation process\nExcellent question: we realized of a gap in the presentation of our motivation for the distillation phase.\nStandard training usually leads to overfitting, which is avoided with early stopping. If one uses a soft regularizer, there is no principled way to design the regularizer to converge right before overfitting occurs.\nWith the proposed method, one stops the first training phase once overfitting occurs. Then, the learned regularized embeddings are frozen. We retrain the earlier layers, while the classifier is fixed. The network outputs do not change and there is no further risk of overfitting on the task labels. Two reasons why the second phase succeeds in practice are that (1) we retrain only a handful of layers and (2) we use dense/continuous targets (quite the opposite of training a large-capacity network on sparse labels).\n\n\n4> Time complexity\nThe time complexity (as a function of dataset size) is the same as with standard training.\n- There is a very small fixed overhead, for each mini-batch, to retrieve the data needed to compute the regularizer (e.g. equivalent questions), which is stored alongside the training examples.\n- There is an additional cost in running the second training phase (distillation). This only involves retraining a few layers for a handful of epochs. In our experiments with VQA, we retrain only the question embedding (a word embedding and GRU) for about 5 epochs, whereas the first training phase takes in the order of 20 epochs. The added cost is a very small fraction of the total training time."}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ggKyrYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1828/Authors|ICLR.cc/2020/Conference/Paper1828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150307, "tmdate": 1576860529245, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment"}}}, {"id": "SkxKo1QIjB", "original": null, "number": 3, "cdate": 1573429152908, "ddate": null, "tcdate": 1573429152908, "tmdate": 1573429152908, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "SyxXGwWjKB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Hi, thanks for your time and for the excellent quality of the review. We took your comments on-board and did a major revision of the paper, which is now much clearer as a result. Updates are highlighted in the PDF. Thanks a lot for your contribution.\n\nSummary of updates:\n- Introduction: better introduction of technical method, including distillation step.\n\n- Section 3: justification for the 2-phase procedure and for the distillation step; most of technical description moved from the appendix.\n\n- Discussion of constraints in parameter/embedding space. In embedding space, one can constrain how the network represents data. In parameter space, one can guide what the network does with these representations. Both can be useful. The former can be closer semantically to task- or domain-specific knowledge of the data used. Our applications indeed focuses on the *linguistic* embedding space in VQA.\n\n- Fig. 3: thanks for the feedback, this was indeed confusing in many ways. One operation is \"*6\" or \"+3\", for example. The input digit and operation sequence are embedded separately to apply constraints on the embeddings of the latter independently. The examples \"...+1,*2...\" and \"...*2,-2,+4...\" are marked as equivalent because (x+1)*2 = ((x*2)-2)+4.\nThe annotations of equivalent sequences are the assumed \"prior knowledge\". We made it clearer that the objective was to verify that these annotations bring a useful training signal, complementary to training examples where these sequences are applied on specific x's.\n\n- All typos fixed. LaTeX had shifted all refs to figures and tables.\n\n- Also added a mention of other possible applications in the conclusions (embedding of graph- and tree-structured data)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ggKyrYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1828/Authors|ICLR.cc/2020/Conference/Paper1828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150307, "tmdate": 1576860529245, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment"}}}, {"id": "HylXQyX8jB", "original": null, "number": 2, "cdate": 1573429018789, "ddate": null, "tcdate": 1573429018789, "tmdate": 1573429018789, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "rkgQW97pKS", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment", "content": {"title": "Revision uploaded", "comment": "We have revised the paper taking into account all points raised in the reviews. We believe that the new version is now much clearer as a result (updates highlighted in the PDF). Thanks again for your contribution !"}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ggKyrYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1828/Authors|ICLR.cc/2020/Conference/Paper1828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150307, "tmdate": 1576860529245, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment"}}}, {"id": "SyeasshEor", "original": null, "number": 1, "cdate": 1573338020988, "ddate": null, "tcdate": 1573338020988, "tmdate": 1573338020988, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "rkgQW97pKS", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Hi, thanks for the thorough review.\n\n1> Cycle-consistent learning\nShah et al. essentially learned a generative model of the question conditioned on the answer, for data augmentation while ensuring that the generated rephrasings lead to the same answer. The only connection with our method is to have multiple examples of a same question, which is really just one of three use cases that we demonstrate. Is there another common aspect that we missed ? I feel that their work and ours are pushing in different directions (they focus on *learning* rephrasings from the data).\n\n2> Constraints in parameter/embedding space\nIn embedding space, one can constrain how the network represents data. In parameter space, one can guide what the network does with these representations. Both can be useful. The former can be closer semantically to a domain expert's knowledge of the data used.\n\n3> Natural language not compositional\nExcellent point. That is why we did not seek to use the annotations as full program trees, but simply the fact that some questions have some operations in common (e.g. a counting operation, using a 'color' attribute, referring to a 'dog' in the image, etc.). These seem realistic to identify in real questions. Fully agree with the correction \"defined\"->\"translated\".\n\n4> Other methods on GQA\nThere are even a few others with higher absolute performance on GQA, but their contributions seem orthogonal to ours. No published method has shown how to benefit from additional annotations as we did. There's nothing in principle that precludes those methods to be combined with our technique. If accepted, we'll certainly include up-to-date references to the state-of-the-art at the time of publication."}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ggKyrYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1828/Authors|ICLR.cc/2020/Conference/Paper1828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150307, "tmdate": 1576860529245, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Authors", "ICLR.cc/2020/Conference/Paper1828/Reviewers", "ICLR.cc/2020/Conference/Paper1828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Comment"}}}, {"id": "B1l7jf5rFS", "original": null, "number": 1, "cdate": 1571295899508, "ddate": null, "tcdate": 1571295899508, "tmdate": 1572972418471, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. To illustrate the idea, the authors use 3 different annotated knowledge that are already available in a public dataset that contains equivalent statements, entailed statements as well as functional programs and show that the final performance indeed increases. \n\nIn general, the paper is well-written and easy to follow. The motivation is clear, i.e., to boost the performance of supervised learning tasks with additional knowledge constraints in a hard way. Compared with the existing models that treat the constraints as soft regularizers, the authors propose to additionally distill the knowledge using teacher-student framework. And this paper contributes in a novel way to incorporate the constraints with both soft and hard training strategies. However, there are several considerations which limits the contribution of this paper:\n\n1. As a teach-student distillation framework, there are several papers using a posterior regularizer with hard constraints, e.g., \"Harnessing deep neural networks with logic rules\", \"Constrained Convolutional Neural Networks for Weakly Supervised Segmentation\". More discussions and comparisons with these models should be addressed, and even experimental comparisons if possible, since they also use knowledge distillation to convey the knowledge expressed in the constraints.\n\n2. The proposed model differs with other soft-regularization-based methods in terms of an additional distillation process. The authors state that the combination of task loss with soft regularization lead to over-fitting. To my point of view, the distillation step actually makes similar effect with the case when only optimize the regularizer without the task loss. Hence, I am wondering what's the performance of first using the combined loss and then fix the subsequent layers to only optimize the embedding layers using only the regularization loss. This could demonstrate the difference between the distillation process and the regularization process.\n\n3. Many recent models for VQA have been proposed, e.g, \"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences from Natural Supervision\" which also combines extra knowledge as symbolic reasoning. The authors should also compare with such models.\n\n4. It seems the model need to sample a pair of data each time at training to compute the regularizer and also conducting the distillation process. In this case, the time cost should be non-trivial because the distillation process requires optimizing the distance between the current embedding with the hard constraint. Then the question comes as how's the time complexity of the model? What's the convergence speed?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634189884, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Reviewers"], "noninvitees": [], "tcdate": 1570237731715, "tmdate": 1575634189898, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review"}}}, {"id": "rkgQW97pKS", "original": null, "number": 3, "cdate": 1571793402708, "ddate": null, "tcdate": 1571793402708, "tmdate": 1572972418398, "tddate": null, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "invitation": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes the incorporation of \u201cprior knowledge\u201d which enters in the form of the relations between training instances in neural network training. The proposed method is tested on VQA problem, bringing improvements upon the popular soft regularizer. \nThe authors claim that their method is a general technique but in fact, the constraints are drawn from specific tasks (VQA for example). So, I believe the contribution is rather domain-dependent and not general. Can you explain more how this method can be applied to general problems?\n\nOther than that, I have some concerns:\n1. Although the authors claim that they are the first to bring these annotations to VQA, I see their training procedure is closely related to cycle-consistent learning. Recent work in VQA also applied cycle consistency as an online data-augmentation technique (See Shah et al. 2019).\n\u201cShah, M., Chen, X., Rohrbach, M., & Parikh, D. (2019). Cycle-consistency for robust visual question answering.\u201d\n2. In Section 2, the authors say \u201cconstraints on the parameter space of a model are often non-intuitive\u201d. How are they \"non-intuitive\" and why the proposed method is more intuitive in terms of theory? Please clarify this.\n3. Each question in Hud et al. is associated with a functional program, therefore, questions are compositional. However, arbitrary questions don\u2019t need to strictly follow this constraint. Natural language is not exactly suited to functional programming I think. I have doubts about the claim in Section 4 \u201cOur method can use partial annotations and should more easily extend to other datasets and human-produced annotations\u201d. Also, the definition \u201cA question is defined as a set of operations\u201d does not seem correct. A question can be translated into a program that is composed of a set of operations.\n4. Experimental results are not strong enough for such strong claims I believe. Regarding GQA dataset, the authors should compare the proposed method with more works, for example, Hu et al. 2019 and Hudson et al. 2019 achieve much favorable performance upon MAC.\n\"Hu, R., Rohrbach, A., Darrell, T., & Saenko, K. (2019). Language-Conditioned Graph Networks for Relational Reasoning.\" \n\"Hudson, D. A., & Manning, C. D. (2019). Learning by abstraction: The neural state machine.\"\n\nMinor comments: The paper is not really well written. I even found a wrong reference (Section 3).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1828/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints", "authors": ["Damien Teney", "Ehsan Abbasnejad", "Anton van den Hengel"], "authorids": ["damien.teney@adelaide.edu.au", "ehsan.abbasnejad@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "keywords": ["regularizers", "vision", "language", "vqa", "visual question answering"], "TL;DR": "Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.", "abstract": "The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.", "pdf": "/pdf/0869aa5fa37d5844ab3de30a838970607b226cd3.pdf", "paperhash": "teney|on_incorporating_semantic_prior_knowlegde_in_deep_learning_through_embeddingspace_constraints", "original_pdf": "/attachment/7dcd9c9c4868bf2e5de8b4616dbed05c58fbeee9.pdf", "_bibtex": "@misc{\nteney2020on,\ntitle={On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints},\nauthor={Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ggKyrYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ggKyrYwB", "replyto": "H1ggKyrYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634189884, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1828/Reviewers"], "noninvitees": [], "tcdate": 1570237731715, "tmdate": 1575634189898, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1828/-/Official_Review"}}}], "count": 10}