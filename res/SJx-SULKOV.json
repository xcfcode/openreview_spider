{"notes": [{"id": "SJx-SULKOV", "original": "rkeiZWiOOV", "number": 45, "cdate": 1553716793336, "ddate": null, "tcdate": 1553716793336, "tmdate": 1562083045826, "tddate": null, "forum": "SJx-SULKOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Interactive Image Generation Using Scene Graphs", "authors": ["Gaurav Mittal", "Shubham Agrawal", "Anuva Agarwal", "Sushant Mehta", "Tanya Marwah"], "authorids": ["gaurav.mittal.191013@gmail.com", "shubham.new@gmail.com", "anuvaagarwal1216@gmail.com", "sushant0523@gmail.com", "tmarwah@andrew.cmu.edu"], "keywords": ["Generative Models", "Image Generation", "Adversarial Learning", "Scene Graphs", "Interactive", "Graph Convolutional Network", "Image Translation", "Cascade Refinement Network"], "TL;DR": "Interactively generating image from incrementally growing scene graphs in multiple steps using GANs while preserving the contents of image generated in previous steps", "abstract": "Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image).\n We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs. ", "pdf": "/pdf/221a5860ebef78c0e958875ed118a3fcc82e3a43.pdf", "paperhash": "mittal|interactive_image_generation_using_scene_graphs"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "SklGnjYzqV", "original": null, "number": 2, "cdate": 1555368873651, "ddate": null, "tcdate": 1555368873651, "tmdate": 1556906135204, "tddate": null, "forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Official_Review", "content": {"title": "interactive image generation using scene graphs", "review": "This paper proposes a conditional adversarial model that iteratively generates images given a scene graph. The scene graph describes the relations between the different objects and components of the image. It is shown that images can be generated iteratively by augmenting the scene graph with new objects and relations, and the existing image content will be maintained.\n\nThis work uses a combination of many different building blocks that have recently gained traction in literature, including graph convolutional networks to process the scene graphs, networks for bounding box prediction and conditional generative adversarial networks. These are combined with a variety of loss functions (5 in total).\n\nThe resulting system is shown to work reasonably well, but it is quite complex and I feel that the importance of each individual component could be demonstrated better by including some ablations -- what would happen if a GAN were conditioned directly on the output of the GCN that processes the scene graph, for example? Is the intermediate step that produces segmentations and bounding boxes strictly necessary?\n\nIn two different places in the manuscript, it is stated that the model is the first of its kind to the authors' knowledge. I find such statements a bit inappropriate when they refer to very specific problem settings. There has definitely been a lot of closely related work e.g. on image generation conditioned on captions (including some that uses scene graphs as an intermediate representation). Stating that the work is presumably the first to use this particular specific combination of input representations and model structure is not very meaningful.\n\nThe manuscript contains quite a few grammatical and spelling errors and would benefit from proofreading.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interactive Image Generation Using Scene Graphs", "authors": ["Gaurav Mittal", "Shubham Agrawal", "Anuva Agarwal", "Sushant Mehta", "Tanya Marwah"], "authorids": ["gaurav.mittal.191013@gmail.com", "shubham.new@gmail.com", "anuvaagarwal1216@gmail.com", "sushant0523@gmail.com", "tmarwah@andrew.cmu.edu"], "keywords": ["Generative Models", "Image Generation", "Adversarial Learning", "Scene Graphs", "Interactive", "Graph Convolutional Network", "Image Translation", "Cascade Refinement Network"], "TL;DR": "Interactively generating image from incrementally growing scene graphs in multiple steps using GANs while preserving the contents of image generated in previous steps", "abstract": "Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image).\n We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs. ", "pdf": "/pdf/221a5860ebef78c0e958875ed118a3fcc82e3a43.pdf", "paperhash": "mittal|interactive_image_generation_using_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Official_Review", "cdate": 1554234171132, "reply": {"forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234171132, "tmdate": 1556906093113, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "B1gr_eV2tV", "original": null, "number": 1, "cdate": 1554952301117, "ddate": null, "tcdate": 1554952301117, "tmdate": 1556906134988, "tddate": null, "forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Official_Review", "content": {"title": "Good performance improvements over baselines, but not entirely clear what techniques are new", "review": "For scene-graph-to-image generation, this paper proposes some changes to the baseline (Johnson et al 2018) in order to produce the image iteratively. The scene-graph is accumulated over three steps, and for each step an image is produced. The intermediate images are not required; i.e. no intermediate supervision necessary. The proposed model conditions each step on the previous image, and introduces losses to encourage continuity in the sequence of images. The authors show performance improvements on COCO-Stuff in inception score (quality / realisticness) and mean perceptual similarity loss (consistency across steps) compared to Johnson et al. \n\nPros:\n- Performance improvements seem solid, and the examples in Figures 3 and 4 seem convincing.\n- It seems that this work has some novelty in that it is the first to generate real-world images iteratively without intermediate supervision\n\nCons:\n- The description of the baseline and proposed model (sections 3.1 and 3.2) would benefit from some more mathematical detail, i.e. introduce some notation in 3.1 so that in 3.2 you can precisely explain how you are changing the model. \n- As a person who is not familiar with the related work, I was unsure what techniques are new here. The bullet-point list in section 3.2 seems to be a list of the differences between the baseline and the proposed model. As far as I can tell, these differences seem quite minor, except for the first bullet point which sounds like it might be quite complicated (but it's not described in any detail, so perhaps it is simple). This is where some more precise mathematical notation would be useful. Similarly, in the list of losses in section 3.2 I'm not sure what's part of the baseline and what's new.\n- In particular, while reading the paper I was under some uncertainty about whether the baseline methods (Johnson et al 2018 and Xu et al 2017) are \"iterative\". Some lines imply they are iterative in some sense (\"AttnGANs also begin with a low-resolution image, and then improve it over multiple steps to come up with a final image\" / \"The layout is passed to a cascaded refinement network which generates an output image at increasing spatial scales.\"), but this paper claims to be the first to iteratively generate real-world images without intermediate supervision. So I'm unsure what exactly is different about this paper compared to previous.\n- This paper doesn't include any human evaluation - instead relying on automatic metrics only. For comparison, Johnson et al 2018 include some human evaluation.\n\nOther comments:\n- \"We also note that Stage 1 performs the best. From our observations this is because the vividness of the image colors and object definitions is the best at the stage 1, and begin to fade out stage 2 onwards.\" Though this explains some more fine-grained ways in which stage 1 is the best (vividness, object definitions), it doesn't explain *why* stage 1 is the best (i.e. why is the model most able to make realistic images on the middle step? If it can make realistic images on step 1 why can't it improve on them on step 2?)\n- \"coarse-to-fine\" not \"course-to-fine\"\n\nNote: Though I am familiar with Deep Learning, I am not very familiar with computer vision, so it is possible that I am missing something in my reading of this paper.", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interactive Image Generation Using Scene Graphs", "authors": ["Gaurav Mittal", "Shubham Agrawal", "Anuva Agarwal", "Sushant Mehta", "Tanya Marwah"], "authorids": ["gaurav.mittal.191013@gmail.com", "shubham.new@gmail.com", "anuvaagarwal1216@gmail.com", "sushant0523@gmail.com", "tmarwah@andrew.cmu.edu"], "keywords": ["Generative Models", "Image Generation", "Adversarial Learning", "Scene Graphs", "Interactive", "Graph Convolutional Network", "Image Translation", "Cascade Refinement Network"], "TL;DR": "Interactively generating image from incrementally growing scene graphs in multiple steps using GANs while preserving the contents of image generated in previous steps", "abstract": "Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image).\n We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs. ", "pdf": "/pdf/221a5860ebef78c0e958875ed118a3fcc82e3a43.pdf", "paperhash": "mittal|interactive_image_generation_using_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Official_Review", "cdate": 1554234171132, "reply": {"forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234171132, "tmdate": 1556906093113, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper45/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "Bkgw74uw9E", "original": null, "number": 1, "cdate": 1555690526901, "ddate": null, "tcdate": 1555690526901, "tmdate": 1556906134734, "tddate": null, "forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "Accepted"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interactive Image Generation Using Scene Graphs", "authors": ["Gaurav Mittal", "Shubham Agrawal", "Anuva Agarwal", "Sushant Mehta", "Tanya Marwah"], "authorids": ["gaurav.mittal.191013@gmail.com", "shubham.new@gmail.com", "anuvaagarwal1216@gmail.com", "sushant0523@gmail.com", "tmarwah@andrew.cmu.edu"], "keywords": ["Generative Models", "Image Generation", "Adversarial Learning", "Scene Graphs", "Interactive", "Graph Convolutional Network", "Image Translation", "Cascade Refinement Network"], "TL;DR": "Interactively generating image from incrementally growing scene graphs in multiple steps using GANs while preserving the contents of image generated in previous steps", "abstract": "Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image).\n We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs. ", "pdf": "/pdf/221a5860ebef78c0e958875ed118a3fcc82e3a43.pdf", "paperhash": "mittal|interactive_image_generation_using_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper45/Decision", "cdate": 1554814602291, "reply": {"forum": "SJx-SULKOV", "replyto": "SJx-SULKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814602291, "tmdate": 1556906102665, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}