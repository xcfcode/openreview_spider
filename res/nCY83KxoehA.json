{"notes": [{"id": "nCY83KxoehA", "original": "3JGPQ4rJxDun", "number": 1096, "cdate": 1601308123293, "ddate": null, "tcdate": 1601308123293, "tmdate": 1614985696259, "tddate": null, "forum": "nCY83KxoehA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Ei-OeJhS67b", "original": null, "number": 1, "cdate": 1610040450822, "ddate": null, "tcdate": 1610040450822, "tmdate": 1610474052918, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a method for using multiple word embeddings in structured prediction tasks. The reviewers shared the concerns that the method seems rather specific to this use case and the empirical improvements do not justify the complexity of the approach. They also questioned the definition of the method as \"architecture search\" vs a particular ensembling method.\nFinally, I think the authors should provide more discussion of why using all the embeddings (in the sense of bias-variance tradeoffs).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040450809, "tmdate": 1610474052904, "id": "ICLR.cc/2021/Conference/Paper1096/-/Decision"}}}, {"id": "QzpGk_O_TP4", "original": null, "number": 2, "cdate": 1603862410132, "ddate": null, "tcdate": 1603862410132, "tmdate": 1607146329089, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review", "content": {"title": "Some nice results, but I'm not fully convinced by the technique", "review": "This paper explores a way of learning how to automatically construct a concatenated set of embeddings for structured prediction tasks in NLP. The paper's model takes up to L embeddings concatenated together and feeds them into standard models (BiLSTM-CRFs or the BiLSTM-Biaffine technique of Dozat and Manning) to tackle problems like POS tagging, NER, dependency parsing, and more.  Search over embedding concaneations is expressed as a search over binary masks of length L.  The controller for this search is parameterized by an independent Bernoulli for each mask position.  The paper's approach learns the controller parameters with policy gradient, where the reward function is (a modified version of) the accuracy on the development set for the given task. This modified reward uses all samples throughout training to effectively get a more fine-grained baseline for the current timestep based on prior samples.  Notably, the paper uses embeddings that are already fine-tuned for each task, as fine-tuning the concatenated embeddings is hard due to divergent step sizes and steep computational requirements.\n\nResults show gains over randomly searching the space of binary masks. The overall model outperforms XLM-R in a range of multilingual settings.\n\nThis paper has some nice empirical results and the simplicity of its approach is attractive. But there are two shortcomings of the paper I will discuss.\n\nMOTIVATION/COMPARISONS\n\nThe authors motivate their technique by drawing parallels to neural architecture search. But I actually think what the authors are doing more closely resembles ensembling, system combination, or model stacking, e.g.:\nhttps://www.aclweb.org/anthology/N09-2064.pdf\nhttps://www.aclweb.org/anthology/N18-1201.pdf\n\nWhen you take large Transformer models (I have to imagine that the Transformers are contributing more to the performance than GloVe and other static word embeddings -- and Table 11 supports this somewhat) and staple a BiLSTM-CRF on top of them, most of the computation is happening in the (fixed) large Transformer. Most NAS methods I'm familiar with re-learn fundamental aspects of the architecture (e.g., the Evolved Transformer), while fixing most of the architecture and re-learning a last layer or two is more suggestive of system combination or model stacking.\n\nMy main question is: did the authors try comparing to an ensemble or post-hoc combination of the predictions according to different models?  Computationally this would be cheaper than what the authors did. It's also much faster to search over 2^L-1 possibilities when checking each possibility just requires decoding the dev set rather than training the BiLSTM-CRF -- actually, this can be done very efficiently if each model's logits are cached.\n\nThere are more sophisticated variants of this like in the papers I linked above where each model has its own weights or additional inputs are used. Intellectually, I think these approaches are related, and they should be discussed and compared to.\n\nRESULTS\n\nAs for the results, Table 1's gains are small -- they are consistent over random search, but I don't find them all that convincing.  There are too many embeddings here for ALL to work well -- my guess would be that a smaller set would yield better performance.\n\nTables 2-4 show improvements over existing baselines, XLM-R, and XLNet. This performance is commendable. However, again, I don't know how this compares to ensembling across a few leading approaches (like mBERT and XLM-R for the cross-lingual tasks).\n\nCONCLUSION\n\nIn the end, I'm not sure how readily this approach will be picked up by others. Because the embeddings aren't themselves fine-tuned as part of the ensemble, it really feels more like a fine-tuned ensemble of existing models rather than true NAS. And the overhead of this approach is significant: it requires running many training runs over large collections of existing pre-trained models to get a small improvement over the current state-of-the-art. This is a possibly useful datapoint to have in the literature, but it feels like the technique isn't quite right to lead to more work in this area.\n\nMINOR:\n\n\"use BiLSTM-Biaffine model (Dozat & Manning, 2017) for graph-structured outputs\"\n\nThis is a very particular structure, namely a directed minimum spanning tree (MST), though projective trees are also possible using the Eisner algorithm. The paper should specify that it's these, and not arbitrary graphs that are being produced here.\n\n-------------------\n\nUPDATE AFTER RESPONSE\n\nThanks for the response and the additional experiments. The comparison between ACE and these other techniques is nice to see, although I'll note that both SWAF and voting shouldn't make totally independent predictions in tasks like NER, but should at least respect constraints in the label space (not sure if there were applied or not).\n\nIn the end, my opinion of this paper largely comes down to the practicality of this technique and its likelihood to be adopted more generally. This results in a large, complex model, and while I am now convinced that the authors have a better ensembling/combination technique than some others, I think it still falls short of a real \"neural architecture search\" contribution or a really exciting result.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127102, "tmdate": 1606915786942, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review"}}}, {"id": "aycUbSu26Y", "original": null, "number": 4, "cdate": 1603998662355, "ddate": null, "tcdate": 1603998662355, "tmdate": 1606777529942, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review", "content": {"title": "reasonably interesting, though some concerns remain", "review": "Updates after discussion/revision period:\n\nI think the revisions have improved the paper, but I'm not willing to increase my score or to fight for the paper. Overall, I think the paper represents a minor contribution, with its rigorous experimentation and some of its ideas, and that others may benefit from reading it, but I don't know that it is at the level of a typical ICLR publication. \n\n\n--------\n\nThis paper describes an approach to choosing a subset of several options for (optionally contextualized) word embeddings to use in NLP tasks. Ideas are drawn from neural architecture search (NAS) and RL. The basic\u00a0idea is to maximize dev set accuracy by searching over the space of embedding sets to use. There are some tweaks,\u00a0including avoiding retraining-from-scratch for each set by keeping a single generalized model with all embeddings in it (where subsets can be chosen by setting some matrices to zero). Experiments are done on many NLP tasks (tagging, chunking, NER, parsing, etc.), leading to state-of-the-art results on nearly all test sets.\u00a0\n\nThis paper represents an\u00a0impressive number of experiments, considering several types of embeddings and many NLP tasks/datasets. It is well-written on the whole, though there are a few things I had confusion or concern about (details below). I lean positive on this paper, as it has an interesting algorithm that is more practical than prior work in NAS and has some promising results. However, I also have a few high-level concerns, described below:\n\n1. I'm not sure if I'm thoroughly convinced of the empirical superiority of ACE. The primary baselines are All (using all embeddings always) and Random (random search over subsets of embeddings). Random is a little better than All on average,\u00a0and ACE is a little better than Random on average. The average difference of 0.5 in Table 1 between ACE and Random is largely due to the 8 aspect extraction (AE) datasets, for which the differences are sometimes sizable. However, across the 17 other\u00a0results in the table (tagging, NER, chunking, and parsing), none differ by more than 0.4, and the average difference between Random and ACE on those other 17 numbers is 0.17 (computed by me). Possibly statistically significant, especially because there are so many different datasets, but less impressive. If one were to deploy a method like this in practice, one would likely start by trying random search because it's so simple and doesn't require the slightly specialized learning framework and reward function in ACE.\u00a0\u00a0\n\nRelatedly, I am concerned that the All baseline is not strong enough. Another natural baseline would be to start with All but then add a_l parameters as gates on the input embeddings, just as they are present in ACE. (The a_l parameters could be normalized to be between 0 and 1 by passing them each through a sigmoid before being multiplied with embedding vectors.) By having a single parameter to weight each embedding type in this way, the new version of All could switch on or off entire\u00a0embedding types without adding many more parameters,\u00a0which would make it more similar to the other methods. This would let us see the results of this stronger version of All (which, I would argue, is more likely to be used in practice than the current version of All).\u00a0\n\n2. My second concern is about the following sentence in Sec. 4.2: \"If the pretrained contextualized embeddings are not available for a particular language, we use the pretrained contextualized embeddings for English instead.\" I find this to be a rather surprising decision, as it could add a great deal of noise for non-English languages. This could be especially problematic for the All baseline which doesn't have an easy\u00a0way to switch off a noise type of embedding. It would be nice to know for which tasks/datasets this English embedding replacement was done in practice. I looked at Appendix A.4, but I wasn't able to determine from that section which embedding types were missing for which datasets.\u00a0\n\n3. CoNLL 2003 does not contain gold chunk labels. It contains automatic chunk labels (as can be confirmed by checking the original paper). CoNLL 2003 should not be used for chunking experiments. Unfortunately this mistake has been repeated in many papers. Please remove all CoNLL 2003 chunking experiments.\u00a0\n\nSome additional (less major) questions are below:\n\nIn Appendix B.2, why does ACE work better than retraining? I\u00a0wouldn't have expected this to happen.\u00a0\n\nIn Sec. 3.1, it's odd that the BiLSTM-CRF and BiLSTM-Biaffine functions only take in V as the only argument, where V is a function solely of the input x, not of the output y. Why is it not a function of y as well?\n\nIn Sec. 3.2, I find the phrasing \"concatenation with the mask\" to be a bit confusing. I don't think the mask is being concatenated; I think it's being multiplied elementwise with the embeddings.\u00a0\n\nRight above Eq. (7), there is the text \"Taking m = 1\" -- what is m?\n\nIn Sec. 4.2: What exactly is meant by \"character embeddings\"? There are many ways to embed words using characters. How are the character embeddings composed to form a word embedding?\n\nHow was the random search done? I see the sentence \"For Random, we use the same training settings as our approach\", which makes me assume there were 30 steps, but I think this should be made more explicit. Since the random approach and ACE are different algorithms with\u00a0different hyperparameters, it's not clear to me what is meant by using \"the same training settings\".\u00a0\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127102, "tmdate": 1606915786942, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review"}}}, {"id": "n3PLzndLxs", "original": null, "number": 12, "cdate": 1606227390189, "ddate": null, "tcdate": 1606227390189, "tmdate": 1606227645111, "tddate": null, "forum": "nCY83KxoehA", "replyto": "MzXdCaU-6T8", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Response to the questions", "comment": "Thanks for your great questions!\n\n---\n### 1.\t\nHere is the test accuracy of single embedding models with the highest development set accuracy for each task.\n\n|Approach|NER|POS| AE| Chunk|DP-UAS|DP-LAS|SDP-ID|SDP-OOD|\n|-|-|-|-|-|-|-|-|-|\n|Best Single Model|92.0|89.7|73.7|96.5|96.8|95.2|94.1|90.3|\n\nBy comparing these results with the results in Table 6 (or the first table above), we can see that the accuracy of single embedding models is not stronger than that of Ensemble$_\\text{dev}$ in most of the cases except AE. But the accuracy of AE is still inferior to Ensemble$_\\text{test}$ and ACE.\n\n---\n### 2.\n\nYes, we use the same dataset settings in all the experiments of Table 6. Therefore, the validation set of ACE and the other approaches is identical.\n\n---\n### 3.\nThere are two conditions that can be considered as \u201cthe same number of parameters\u201d:\n\n(1)\tThe number of parameters of the model including all the embeddings is the same.\n\n(2)\tThe number of trainable parameters of the model is the same. More specifically, the number of parameters excluding non-trainable embeddings.\n\nFor (1), it is impractical to train such a model as the All baseline has more than 1 billion parameters in total. It possibly requires dozens of GPU to train such a model efficiently. \n\nFor (2), following your suggestions, we build a deeper trainable network by adding a linear layer to project the embeddings to the same hidden size as the concatenation of all embeddings for the best single embedding model (which is ELMo) in NER. The number of trainable parameters of the resulting task model is larger than that of the model with all embeddings concatenated since there is an additional $k \\times d$ weight in the model, where $k$ is the hidden size of the single embedding and $d$ is the hidden size of concatenated embeddings. With the linear projection layer, the accuracy of NER reduces from $92.0$ to $91.9$. The accuracy is still inferior to the accuracy of the All baseline and ACE, which is $92.4$ and $93.0$ on average in CoNLL NER respectively. For the reduction of accuracy, one possible reason is that additional parameters make the task model difficult to train. We also tried some smaller hidden sizes $d$ for the projection, but we did not find further improvement.\n\nIn conclusion, the cause of the accuracy gap between a single embedding model and the concatenated embeddings model is possibly not due to the number of model parameters.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "MzXdCaU-6T8", "original": null, "number": 9, "cdate": 1606162180749, "ddate": null, "tcdate": 1606162180749, "tmdate": 1606162180749, "tddate": null, "forum": "nCY83KxoehA", "replyto": "blxJzPwEoWp", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Quick questions", "comment": "Dear authors,\nThanks for the response. Would be great if you could quickly address the following points:\n1. A simple baseline is to choose a single embedding based on a validation set and use just this one. This would correspond to an ensemble of one in Table 6. Since there is a gap between Ensemble_dev and Ensemble_test, it seems like there is some overfitting\nin the choice of ensemble and therefore reducing the search space could improve things (assuming there is a single good embedding). Have you tried this?\n2. For table 6, did you use the same validation set for ACE and the other methods?\n3. The concatenated models seem to be using more parameters than the baseline models (i.e., using a single embedding). Thus, it seems to make sense to compare to a single embedding model that has the same number of parameters as the one with concatenated embeddings. Have you explored this?\nThanks   \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "blxJzPwEoWp", "original": null, "number": 4, "cdate": 1606023518998, "ddate": null, "tcdate": 1606023518998, "tmdate": 1606112676466, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Detailed Results:", "comment": "Due to the character limitation of comment, here we post the results of our two additional analyses for your information:\n\n---\n## Analysis 1 (Section 5.3)\n\n|Approach|NER|POS| AE| Chunk|DP-UAS|DP-LAS|SDP-ID|SDP-OOD|\n|-|-|-|-|-|-|-|-|-|\n|All|92.4 |90.6 |73.2 |96.7 |96.7 |95.1 |94.3 |90.8|\n|Random|92.6|91.3|74.7|96.7| 96.8|95.2 |94.4 |90.8 |\n|ACE|**93.0**| **91.7**| **75.6**|**96.8**| **96.9**| **95.3** |**94.5** |**90.9**|\n|All+Weight|92.7 |90.4 |73.7 |96.7 |96.7 |95.1 |94.3 |90.7|\n|Ensemble |92.2 |90.6 |68.1 |96.5 |96.1 |94.3 |94.1 |90.3|\n|Ensemble$_\\text{dev}$|92.2 |90.8 |70.2 |96.7 |96.8 |95.2 |94.3 |90.7|\n|Ensemble$_\\text{test}$|92.7 |91.4 |73.9 |96.7 |96.8 |95.2 |94.4 |90.8|\n\n---\n## Analysis 2 (Section 5.4):\n+sent/+doc: models with sentence-/document-level embeddings\n\n|Approach | de | de$_\\text{06}$ | en | es | nl |\n|-|-|-|-|-|-|\n|All+sent | 86.8 | 90.1 | 93.3 | 90.0 | 94.4 |\n|ACE+sent | 87.1 | 90.5 | 93.6 | 92.4 | 94.6| \n|BERT [1] | - | - | 92.8 | - | - |\n|Akbik et al. (2019) [2] | - | 88.3 | 93.2 | - | 90.4 |\n|Yu et al. (2020) [3] | 86.4 | 90.3 | 93.5 | 90.3 | 93.7 |\n|All+doc | 87.6 | 91.0 | 93.5 | 93.3 | 93.7 |\n|ACE+doc | **88.0** | **91.4** | **94.1** | **95.6** | **95.5** |\n\n---\n## References:\n\n[1] Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 2019.\n\n[2] Akbik et al. Pooled Contextualized Embeddings for Named Entity Recognition. In Proceedings of NAACL-HLT. 2019.\n\n[3] Yu et al. Named Entity Recognition as Dependency Parsing. In Proceedings of ACL 2020.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "ToV5VebKh12", "original": null, "number": 3, "cdate": 1606023361971, "ddate": null, "tcdate": 1606023361971, "tmdate": 1606112659123, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "General Response to all reviewers:", "comment": "We thank all the reviewers for their time and valuable feedback. We revised the paper with the feedback and added two new analyses in the paper. \n\n---\n## 1.\nThe first analysis answers the reviewers\u2019 the main concern that the All baseline might not be strong enough. In Section 5.3, we conduct a comparison among ACE, All and two additional approaches suggested by the reviewers. \n\nThe first additional approach is a variant of the All baseline, which is weighting each embedding with a learnable parameter (suggested by Reviewer #3). \n\t\nThe second additional approach is ensemble of the task models, which is mentioned by Reviewer #1, #2 and #4. Specifically, we train one model with each embedding candidate and then combine the predictions from all the trained models through an ensemble algorithm. We tried to implement the approach of \u201cStacking With Auxiliary Features\u201d (SWAF) [1] suggested by Reviewer #2, which is a model stacking approach. We trained a meta-classifier to predict the outputs based on the prediction and confidence of task models and word features as input. We first tried the ensemble model for sequence labeling tasks (NER, POS, Chunking, AE). The model predicts the label at each position separately. The meta-classifier is trained on the development set and tested on the test set. However, we found that the accuracy improvement of the ensemble model is moderate comparing to the best task model. Besides stacking, we also tried voting with confidence, which is much simpler. We found that its accuracy is higher than stacking. As a result, we use voting as the ensemble approach in the new experiments in the revision. Here is a comparison between voting and SWAF in sequence labeling with an ensemble of all task models:\n\n|Approach| NER | POS | AE | Chunk |\n|-|-|-|-|-|\n|SWAF | 91.8 | 89.0 | **68.1**| **96.5** |\n|Voting | **92.2** | **90.6** | **68.1** | **96.5** |\n\nOne of the benefits of voting is that it combines the predictions of the task models efficiently without any training process. We search all possible $2^L-1$ model ensembles in a short period of time through caching the outputs of the models (Searching the whole search space with SWAF is impractical. Training a single SWAF model requires about 5 minutes in our implementation for NER. Therefore, searching for all possible ensemble requires about 5$\\times$2047 minutes $\\approx$ 7 days). Therefore, we search for the best ensemble of models on the development set and then evaluate the best ensemble on the test set. Moreover, we additionally search for the best ensemble on the test set for reference, which is the upper bound of the approach. \n\nWe conduct the experiment on one of the datasets for each task for efficiency, which is the same setting as Appendix B.2. Empirical results show that ACE outperforms all the settings of these two additional approaches and even the ensemble approach searching over the test set, which shows the effectiveness of ACE. The All baseline is competitive with the two additional approaches in most of the cases. \nThere is no clear winner of these approaches on all the datasets, which shows that All is as strong as the two additional approaches in our experiments. Additionally, the All baseline is stronger than the ensemble model with all individual task models. These results show the strength of embedding concatenation. Concatenating the embeddings incorporates information from all the embeddings and forms stronger word representations for the task model, while in model ensemble, it is difficult for the individual task models to affect each other through ensemble.\n\n\n\n---\n\n## 2.\nThe second analysis is about our new results on NER. Recently, models with document-level word representations extracted from transformer-based embeddings significantly outperform models with sentence-level word representations on NER tasks [2], [3]. To show the effectiveness of ACE with document-level representations, we replace the sentence-level word representations from transformer-based embeddings (i.e., XLM-R and BERT embeddings) with the document-level word representations. \n\nEmpirical results show that the document-level representations can significantly improve the accuracy of ACE. Comparing with models with sentence-level representations, the averaged accuracy gap between ACE and the All baseline is enhanced from 0.7 to 1.1 with document-level representations, which shows that the advantage of ACE becomes stronger with document-level representations. Please refer to Section 5.4 for more details.\n\n---\nWe hope the two analyses will help the reviewers to better understand the strength of ACE. \n\n---\n# References:\n\n[1] Rajani and Mooney, Stacking With Auxiliary Features. In Proceedings of IJCAI 2017.\n\n[2] Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 2019.\n\n[3] Yu et al. Named Entity Recognition as Dependency Parsing. In Proceedings of ACL 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "chMnEt5Zu_t", "original": null, "number": 5, "cdate": 1606023861534, "ddate": null, "tcdate": 1606023861534, "tmdate": 1606112646677, "tddate": null, "forum": "nCY83KxoehA", "replyto": "aycUbSu26Y", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for the great review! We really appreciate you taking the time out to share your thoughtful comments.\n\n---\n## For the concerns:\n\n### 1.\nThough the design of ACE is slightly specialized, we believe that the algorithm framework is general over different tasks and the implementation is not such difficult. If anyone wants to achieve higher accuracy on a specific dataset, ACE is fast and effective comparing with Random. \n\nFor the All baseline, we follow your suggestion to weight each embedding in the All baseline. We conduct the experiment on one of the datasets for each task for efficiency, which is the same setting as Appendix B.2. Results (in Section 5.3 of the revised paper) show that All+Weight is stronger than All baseline by 0.06 on average. However, All+Weight is not consistently stronger than All baseline in all of the tasks. One possible reason is that the linear layer in Eq. 2 is able to implicitly represent the weight of each embedding. Therefore, All+Weight is possibly a stronger version of All baseline but the improvement is limited. Please check Section 5.3 in our revision and the first part of our general response to all reviewers for more details.\n\n### 2. \nWe use this setting for the following reasons. \n\n(1) we want to make the search spaces of most of the datasets having almost the same size so that the hyperparameter settings (especially the running iterations) are unchanged. \n\n(2) we want to see whether the embeddings pretrained with a large corpus of other languages (i.e. the English embeddings) are able to transfer to another language to improve the accuracy. \n\nMore specifically, we applied this setting on the Turkish and Russian datasets in aspect extraction since we didn\u2019t have Flair and ELMo embeddings for Turkish (\u201ctr\u201c) and we didn\u2019t have language-specific BERT, Flair and ELMo embeddings for Russian (\u201cru\u201c). We run the All baseline for \u201ctr\u201c and \u201cru\u201c without English embeddings and get 69.7 and 70.2 for \u201ctr\u201c and \u201cru\u201c respectively. The results are higher than the All baseline in Table 1 but are still inferior to ACE. In the searched embedding concatenation of ACE, English BERT is selected in all runs for \u201cru\u201c while English ELMo is not selected in either \u201ctr\u201c or \u201cru\u201c. The English Flair embeddings is selected in a majority of the runs. The observation shows that the pretrained sub-token and character information in English BERT and Flair might be helpful to the tasks of other languages with limited embedding resources. On the other hand, the pretrained word information in English ELMo might add noises to the word representations on these tasks. We have made the setting clearer in the revision.\n\n### 3.\t\nAccording to your suggestion, we removed all the experiments of CoNLL 2003 chunking experiments in the revision. We split 10% of the training set of CoNLL 2000 as the development set instead and rerun the experiments. Please check our revision for more details.\n\n---\n## For the additional questions:\n\nFor 1, one possible reason is that the retrained models are randomly initialized while ACE models are initialized by the parameters of trained model of previous step.\n\nFor 2 \u2013 4, we have revised the paper according to your comments.\n\nFor 5, \"character embeddings\" is the non-contextual character embeddings proposed by [1].\n\nFor 6, you are right. The random search is done by searching 30 steps. We have made it more explicit in the revision.\n\n\n\n---\n## References:\n[1] Lample et al., Neural Architectures for Named Entity Recognition. In Proceedings of NAACL 2016. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "5kup9Wg2jCB", "original": null, "number": 6, "cdate": 1606024000446, "ddate": null, "tcdate": 1606024000446, "tmdate": 1606112633928, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nQ-wfd0Xx7a", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We want to thank you for your feedback and criticisms.\n\n---\n## For the concerns:\n### 1.\t\nACE does not pretrain the task model for each of the embeddings independently in the beginning. ACE trains a single model with different embedding concatenations at each step. At the first step, the task model of ACE is trained with all embeddings concatenated. We think that the description in Section 3.4 might be unclear. We have made it clearer in the revision. \n\nFor the issue of ensemble models, we add an analysis in section 5.3 comparing the All baseline and ACE with ensemble models. For the ensemble learning, we use voting with confidences of models instead of other techniques due to the following reasons:\n \n(1) One of the benefits of voting is that it combines the predictions of the task models efficiently without any training process. We can search all possible $2^L-1$ model ensembles in a short period of time through caching the outputs of the models. \n\n(2) we tried SWAF [1] suggested by reviewer #2, and found it underperforms voting. Therefore, we believe that voting is a strong baseline of ensemble. \n\nIn the first part of our general response, results show that ACE outperforms all the settings of ensemble model and even the ensemble approach searching over the test set. Moreover, the ensemble model with all task models is weaker than the All baseline in most of the cases, which shows that the All baseline is strong. These results show the strength of embedding concatenation. Concatenating the embeddings incorporates information from all the embeddings and forms stronger word representations for the task model, while in model ensemble, it is difficult for the individual task models to affect each other through ensemble. For more details, please check Section 5.3 in the revision and the first part of our general response to all the reviewers.\n\n### 2.\t\nSince the target of ACE is finding a better concatenation of embeddings to achieve higher accuracy, we believe that using accuracy in the reward function design is a natural choice. Though it might not be the best choice, there is a lot of previous work (for example: [2], [3], [4]) that used accuracy in the reward function design to improve the model performance and got good empirical results. Taking the confidence estimation into account in the reward function design is a very interesting idea. We will try it in the future.\n\n\n\n---\n## References:\n\n[1] Rajani and Mooney, Stacking With Auxiliary Features. In Proceedings of IJCAI 2017.\n\n[2] Zoph and Le. Neural Architecture Search with Reinforcement Learning. In Proceedings of ICLR 2017.\n\n[3] Fan, et al. Learning to Teach. In Proceedings of ICLR 2018.\n\n[4] Qin et al. Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning. In Proceedings of ACL 2018.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "p-VWUFC-njL", "original": null, "number": 7, "cdate": 1606024178874, "ddate": null, "tcdate": 1606024178874, "tmdate": 1606112622277, "tddate": null, "forum": "nCY83KxoehA", "replyto": "QzpGk_O_TP4", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you so much for your feedbacks. We really appreciate you taking the time out to share your thoughtful comments.\n\n---\n## For MOTIVATION/COMPARISONS:\nIn model ensemble or model stacking, the individual models can predict the task outputs by themselves. However, the embeddings themselves are not independent models that can predict the task outputs and ACE targets at finding a single model to predict the outputs of the task. As a result, we believe ACE is quite different from the model ensemble or model stacking.\n\nWe take your suggestion of comparing ACE with model stacking. We first tried SWAF [1] and voting in sequence labeling tasks and found that voting is stronger than training a meta-classifier for model stacking. Moreover, one of the benefits of voting is that it combines the predictions of the task models efficiently without any training process. We can search all possible $2^L-1$ model ensembles in a short period of time through caching the outputs of the models. However, searching the whole search space with SWAF is impractical. Training a single SWAF model requires about 5 minutes in our implementation for NER and searching for all possible ensemble requires about 7 days. We search for the best ensemble of models on the development set and then evaluate the best ensemble on the test set. We additionally search for the best ensemble on the test set for reference, which is the upper bound of the approach. Empirical results show that ACE is stronger than voting in all cases including searching over all possibilities of ensemble model on the test set. The All baseline is competitive with Ensemble model searching over all possible combinations on the development set. It is also stronger than the Ensemble model with all task models, which shows that the All baseline is strong enough. These results show the strength of embedding concatenation. Concatenating the embeddings incorporates information from all the embeddings and forms stronger word representations for the task model, while in model ensemble, it is difficult for the individual task models to affect each other through ensemble. Please refer to the first part of our general response to all reviewers and Section 5.3 in our revision for more details.\n\n\n---\n## For RESULTS:\n### 1.\nAs we discussed above, we have compared several approaches suggested by the reviewers. The empirical results show that the All baseline is competitive with all the other approaches. Although a smaller set for the All baseline might results in better accuracy, deciding the best smaller set requires high level domain knowledge of the task. What ACE does is automatically finding a smaller set that has better accuracy without such knowledge.\n### 2.\t\nIn our analysis, all possible $2^L-1$ combinations of ensemble models are searched including ensembling across the models with contextual embeddings. The results show that ACE outperforms ensemble approaches consistently.\n\n---\n## For CONCLUSION:\nComparing with current state-of-the-art approaches, we believe the improvements cannot be considered as \u201csmall\u201d. ACE outperforms current state-of-the-art approaches on a considerable scale in most of the tasks with a unified framework. Taking NER as an example, ACE is stronger than the current state-of-the-art proposed by [2] by 2.0 F1 scores and outperforms the All baseline by 1.1 F1 scores on average in our new analysis in Section 5.4 of the revision. For the speed concern, in practice the training speed is often less of a concern than the prediction speed, while the prediction of ACE is faster than that of the ALL baseline since some useless embeddings are filtered out after training. \n\n---\n## For MINOR:\nWe use MST to ensure the tree output structure. We specified this in the Appendix A.3 of the revision.\n\n---\n## References:\n\n[1] Rajani and Mooney, Stacking With Auxiliary Features. In Proceedings of IJCAI 2017.\n\n[2] Yu et al. Named Entity Recognition as Dependency Parsing. In Proceedings of ACL 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "6FJjIP-PykY", "original": null, "number": 8, "cdate": 1606024279965, "ddate": null, "tcdate": 1606024279965, "tmdate": 1606112607603, "tddate": null, "forum": "nCY83KxoehA", "replyto": "NyAPJqvXtyH", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We want to thank you for your feedback and criticisms. We really appreciate you taking the time out to share your thoughtful comments.\n\n---\n### For the review:\nWe take your suggestion of comparing ACE with ensemble model. We compared our approach with the ensemble model through voting. Furthermore, we can search all possible $2^L-1$ model ensembles in a short period of time through caching the outputs of the models because voting combines the predictions of the task models efficiently without any training process. We searched for all $2^L-1$ possible combinations on development sets and then evaluate the best ensemble on the test set. We additionally search for the best ensemble on the test set for reference, which is the upper bound of the approach. Empirical results show that ACE is stronger than voting in all the cases including searching over all possibilities of ensemble model on the test set. In conclusion, using the concatenation of those contextual embeddings is necessary. Concatenating the embeddings incorporates information from all the embeddings and forms stronger word representations for the task model, while in model ensemble, it is difficult for the individual task models to affect each other through ensemble. Please refer to the first part of our general response to all reviewers and Section 5.3 in our revision for more details of the settings.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Paper1096/Reviewers", "ICLR.cc/2021/Conference/Paper1096/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nCY83KxoehA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1096/Authors|ICLR.cc/2021/Conference/Paper1096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863744, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Comment"}}}, {"id": "NyAPJqvXtyH", "original": null, "number": 1, "cdate": 1603076554967, "ddate": null, "tcdate": 1603076554967, "tmdate": 1605024532244, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review", "content": {"title": "Interesting, good results, but too heavy and complex", "review": "This paper introduced an interesting application of reinforcement learning in the selection of concatenation of contextual/non-contextual word embeddings.  It is clever to limit the search space on the selection of embedding sources rather than search the whole network structure, as the current strategy is much easier in the training step. The author(s) conducted many experiments that compared many other models (including SOTA models, and ablation study). Those results are pretty good and impressive.\n\nThe main concern is the necessity of using the concatenation of those contextual embeddings. Calculating different contextual embeddings will cost many computing resources and affect the model's speed. Instead of using reinforcement learning to learn the concatenation of different embeddings, why not use the ensemble model to aggregate the results from different contextual embedding-based models? For example, we can use three embeddings BERT, ELMO, Glove to build three separate models and then aggerate their predicted results, the computing speed/resource may be similar to the learned embedding concatenation model, but is it possible that the ensemble model outperforms the ACE model? More experiments of the comparison with ensemble models should be conducted to prove the necessity of ACE. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127102, "tmdate": 1606915786942, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review"}}}, {"id": "nQ-wfd0Xx7a", "original": null, "number": 3, "cdate": 1603897880078, "ddate": null, "tcdate": 1603897880078, "tmdate": 1605024532128, "tddate": null, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "invitation": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review", "content": {"title": "AUTOMATED CONCATENATION OF EMBEDDINGS FOR STRUCTURED PREDICTION", "review": "Summary:\n \nThis paper proposes to automate the concatenation of word embeddings (obtained using different strategies) to produce powerful word representations for a given downstream task. To this end, the paper develops an approach based on Neural Architecture Search, wherein the search space is comprised of embedding candidates obtained using different concatenations. Using an accuracy-based reward function, it is showed that ACE can determine more effective concatenations. ACE is evaluated using  extensive experiments with different tasks and datasets, and it outperforms the two baselines (to different degrees) -- random search and concatenating all embeddings with no subselection.\n\n##########################################################################\n\nPositives:\n- The idea of using NAS to construct concatenated embeddings is interesting and the formulation is clearly developed in the paper.\n- The proposed approach is generic and can support different types of structured outputs (sequences, graphs etc.)\n- The search process is computationally efficient and can be even run on a single GPU.\n- A simple modification (based on a discount factor) is proposed to the reward function design that leads to non-trivial performance improvements.\n- Strong experiment design: The proposed approach is evaluated on a large suite of datasets and tasks, and in many cases \n\nConcerns:\n- While the overall idea is interesting, the design choices made in the paper are not fully justified. Since ACE already pretrains the task model for each of the embeddings independently to begin with, why not adopt a \"boosting\" style approach instead of the naive \"ALL\" baseline. It is not surprising that even random search (known to be a strong baseline) consistently outperforms \"ALL\". The key challenge in concatenating disparate emebddings is that they can predict with varying degrees of confidence in different parts of the data and sequential inclusion of embeddings could be effective. In my opinion, the baselines chosen for concatenation are weak.\n- Why is \"accuracy\" the best choice for reward design? There could be two different embeddings that could produce the same accuracy with varying levels of confidence (or empirical calibration). Unlike conventional ensemble learners, each contextualized representation is not a weak learner and hence it will be critical to take into account confidence estimates.\n\nOverall, though the paper is experimentally strong, the design choices and the baselines need to be better justified.\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease respond to the questions under concerns.\n\n##########################################################################\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1096/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1096/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Concatenation of Embeddings for Structured Prediction", "authorids": ["~Xinyu_Wang3", "~Yong_Jiang1", "~Nguyen_Bach1", "~Tao_Wang4", "~Zhongqiang_Huang1", "~Fei_Huang2", "~Kewei_Tu1"], "authors": ["Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu"], "keywords": [], "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations.", "one-sentence_summary": "We propose ACE, which automatically searches the best word embedding concatenation as word representation. ACE achieved state-of-the-art results on 6 structured prediction tasks over 19 out of 21 datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|automated_concatenation_of_embeddings_for_structured_prediction", "pdf": "/pdf/da962eb025209acdff429e018647f5bccdbf6d73.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PZnmQnazBN", "_bibtex": "@misc{\nwang2021automated,\ntitle={Automated Concatenation of Embeddings for Structured Prediction},\nauthor={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},\nyear={2021},\nurl={https://openreview.net/forum?id=nCY83KxoehA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nCY83KxoehA", "replyto": "nCY83KxoehA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127102, "tmdate": 1606915786942, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1096/-/Official_Review"}}}], "count": 14}