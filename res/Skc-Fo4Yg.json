{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028596653, "tcdate": 1490028596653, "number": 1, "id": "H1pE_tpjx", "invitation": "ICLR.cc/2017/workshop/-/paper97/acceptance", "forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Variational Intrinsic Control", "abstract": "We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.", "pdf": "/pdf/c71b044701227ea42c3f32eb31b1c668d3736ff9.pdf", "paperhash": "gregor|variational_intrinsic_control", "conflicts": ["google.com"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Karol Gregor", "Danilo Jimenez Rezende", "Daan Wierstra"], "authorids": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028597206, "id": "ICLR.cc/2017/workshop/-/paper97/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028597206}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489311811552, "tcdate": 1489310793056, "number": 2, "id": "Hk-UEczjl", "invitation": "ICLR.cc/2017/workshop/-/paper97/official/review", "forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "signatures": ["ICLR.cc/2017/workshop/paper97/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper97/AnonReviewer2"], "content": {"title": "Great approach, promising results, space too limited for proper presentation", "rating": "7: Good paper, accept", "review": "The paper presents an interesting variational bound on the computationally intractable empowerment criterion.\nIt then gives a simple algorithm for learning options to maximize meta-controller empowerment, in the absence of an external reward signal.\nThis approach seems novel, insightful and likely impactful, and the results look promising.\n\nThe first part of the paper is very clear, principled and promising.\nStarting from the right part of Figure 1, through Algorithm 2, to some of the results in Figure 2, the descriptions are vague or missing.\n\nWhile the lower bound is computationally tractable to update iteratively, the convergence properties are unclear. Algorithm 1 is attempting to estimate an informational quantity, and these require notoriously high sample complexity (Paninski, 2003).\n\nPros:\n- Principled approach\n- Computationally tractable lower bound\n- Promising results\n\nCons:\n- Unclear convergence properties\n- Figure 1 (right part) and Algorithm 2 are inscrutable (with undefined notation)\n- Important details are missing of the experiments in Figure 2 (architectures, number of iterations, etc.)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Variational Intrinsic Control", "abstract": "We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.", "pdf": "/pdf/c71b044701227ea42c3f32eb31b1c668d3736ff9.pdf", "paperhash": "gregor|variational_intrinsic_control", "conflicts": ["google.com"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Karol Gregor", "Danilo Jimenez Rezende", "Daan Wierstra"], "authorids": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489310793852, "id": "ICLR.cc/2017/workshop/-/paper97/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper97/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper97/AnonReviewer1", "ICLR.cc/2017/workshop/paper97/AnonReviewer2"], "reply": {"forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper97/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper97/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489310793852}}}, {"tddate": null, "tmdate": 1489057533458, "tcdate": 1489057533458, "number": 1, "id": "HyrZDnR5g", "invitation": "ICLR.cc/2017/workshop/-/paper97/official/review", "forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "signatures": ["ICLR.cc/2017/workshop/paper97/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper97/AnonReviewer1"], "content": {"title": "Tractable objective, options embeddings", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper tackles the options discovery problem through the notion of \n\"empowerment\", formalized in information-theoretic terms. The paper builds \non similar recent ideas, such as Mohamed & Rezende (2015), but proposes \na tractable objective : a variational lower bound on the \nmutual information between options and the set of reachable states upon termination \nof that option. A policy gradient approach is used to optimize this objective. \n\nWhile the idea of using policy gradient methods for options discovery has \nrecently been explored by other authors, one of the contribution of this particular\npaper is the definition of a new objective, beyond the usual expected sum of \ndiscounted rewards. In fact, as shown in Bacon & al. (2017), the expected discounted\nreturn is not sufficient to guarantee temporally extension and regularization\nmight have to be employed. The proposed variational lower bound could provide \nsuch regularization. \n\nAnother contribution of this paper is the idea of generalizing options \nfrom discrete entities to \"embeddings\" aka. families of options.  This is a \ngeneralization which some other authors have also recently started adopting\n(cf. Schaul & al (2015), Borsa & al. (2016), Oh & al. (2017), Vezhnevets & al. (2017))\nbut I appreciate that this paper develops it more clearly in the text.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Variational Intrinsic Control", "abstract": "We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.", "pdf": "/pdf/c71b044701227ea42c3f32eb31b1c668d3736ff9.pdf", "paperhash": "gregor|variational_intrinsic_control", "conflicts": ["google.com"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Karol Gregor", "Danilo Jimenez Rezende", "Daan Wierstra"], "authorids": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489310793852, "id": "ICLR.cc/2017/workshop/-/paper97/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper97/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper97/AnonReviewer1", "ICLR.cc/2017/workshop/paper97/AnonReviewer2"], "reply": {"forum": "Skc-Fo4Yg", "replyto": "Skc-Fo4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper97/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper97/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489310793852}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487350017590, "tcdate": 1487350017590, "number": 97, "id": "Skc-Fo4Yg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "Skc-Fo4Yg", "signatures": ["~Karol_Gregor1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Variational Intrinsic Control", "abstract": "We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.", "pdf": "/pdf/c71b044701227ea42c3f32eb31b1c668d3736ff9.pdf", "paperhash": "gregor|variational_intrinsic_control", "conflicts": ["google.com"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Karol Gregor", "Danilo Jimenez Rezende", "Daan Wierstra"], "authorids": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}