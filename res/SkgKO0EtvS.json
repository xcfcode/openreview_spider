{"notes": [{"id": "SkgKO0EtvS", "original": "HJx5RpD_vS", "number": 1218, "cdate": 1569439344552, "ddate": null, "tcdate": 1569439344552, "tmdate": 1583912040676, "tddate": null, "forum": "SkgKO0EtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "6n6eyLBht6", "original": null, "number": 1, "cdate": 1582138370740, "ddate": null, "tcdate": 1582138370740, "tmdate": 1582138370740, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Public_Comment", "content": {"title": "Interesting paper", "comment": "This an interesting paper on representing/learning graph algorithms with graph neural networks. \nWe have two very related papers on this direction: \n1. Learning Combinatorial Optimization Algorithms over Graphs. Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song. NeurIPS 2017.\n2. Learning Steady-States of Iterative Algorithms over Graphs. Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alexander J. Smola, Le Song. ICML 2018. \nDiscussing them in the paper can enrich the context of the paper. \n"}, "signatures": ["~Le_Song1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Le_Song1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgKO0EtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504197908, "tmdate": 1576860577230, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Public_Comment"}}}, {"id": "aJtXl1ZqJZ", "original": null, "number": 1, "cdate": 1576798717758, "ddate": null, "tcdate": 1576798717758, "tmdate": 1576800918807, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "It seems to be an interesting contribution to the area. I suggest acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706664, "tmdate": 1576800254783, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Decision"}}}, {"id": "HklfytHTKB", "original": null, "number": 2, "cdate": 1571801306180, "ddate": null, "tcdate": 1571801306180, "tmdate": 1574372362916, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper investigates using GNNs to learn graph algorithms. It proposes a model which consists of algorithm-dependent encoder and decoder, and algorithm-independent processor. \nAuthors try to learn BFS, Bellman-Ford and Prim algorithms on various types of random graphs. \nExperimental results suggest that MPNN with max aggregator outperforms other variants significantly in terms of generalization. \n\nOverall, this paper presents a solid contribution on learning graph algorithms using GNNs, despite the caveat for some clarifications on the model and experiments. \nGiven these clarifications in an author response, I would be willing to increase the score.\n\nPros:\n\n1, Although it is less satisfying to learn to solve graph problems where polynomial-time algorithms exist, I still appreciate the contribution of the paper, especially the algorithm-independent processor. It is one step forward to models which could learn meta-level representation of algorithms. The findings of this paper may suggest that different types of operators used in the GNN may have different inductive bias in learning different types of algorithms.\n\n2, Experimental comparisons are adequate and convincing. The detailed analysis of empirical results also provide good explanation.\n\nCons & Suggestions:\n\n1, Given the dynamic programming nature of the most of tasks, it is not that surprising that MPNN with max aggregation could solve them pretty well. What surprises me is that MPNN-mean / MPNN-sum could not generalize well (i.e., performance drops significantly on 50 and 100 nodes settings) on the reachability task. In my opinion, the reachability task could be easily handled by any diffusion/propagation based models including, e.g., MPNN, GCN, GAT, as long as the information is spread over the input graph. Could you explain why does this happen? \n\n2, Training details are sparse. If I understood correctly, the training of various GNNs is done by teacher forcing such that each step some supervised information collected from the underlying graph algorithms is provided to GNNs. However, it is not clear that what supervised information is exactly provided under each graph algorithm. It would be great to have a table to summarize what the input and supervised output information is for each graph algorithm. \n\n3, The processor is trained to learn multiple graph algorithms simultaneously. A natural question to ask is how does the performance change when you train the processor with different combinations of algorithms? What is the impact of correlations between different graph algorithms? Did you explore learning with graph algorithms following some sequential schedule, e.g., curriculum learning?\n\n4, It would be great to provide more results on larger size graphs, e.g., trained on 20 nodes but test on 1000 or more nodes. I saw one set of experiments trained on 100 and test 1000 nodes in the appendix. More results along this line would make the paper more convincing on the generalization.\n\n5, It would be great to discuss [1] as it also studies how to use GNNs to learn a special class of graph algorithms, i.e., probabilistic inference algorithms on graphs. It is shown in [1] that belief propagation algorithm could be seen as a specially constructed GNN. Interestingly, some well-known graph algorithms could be re-formulated as probabilistic inference algorithms, e.g., graph-cut could be reformulated as max-product [2].\n\n[1] Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., Zemel, R. and Pitkow, X., 2018. Inference in probabilistic graphical models by graph neural networks. arXiv preprint arXiv:1803.07710.\n[2] Tarlow, D., Givoni, I.E., Zemel, R.S. and Frey, B.J., 2011, July. Graph Cuts is a Max-Product Algorithm. In UAI (pp. 671-680).\n\n======================================================================================================\n\nThanks for the thorough response! It resolves my concerns. I improved my score.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576022062316, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Reviewers"], "noninvitees": [], "tcdate": 1570237740588, "tmdate": 1576022062329, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review"}}}, {"id": "HkekL5I2sS", "original": null, "number": 4, "cdate": 1573837383134, "ddate": null, "tcdate": 1573837383134, "tmdate": 1573837383134, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment", "content": {"title": "Summary of revisions made to the paper in the discussion period", "comment": "We hope that the revisions we have made to the paper have properly addressed the comments of the reviewers on our work - and that its overall contribution, quality and clarity is now improved! We would like to thank everyone once again for their thoughtful comments on our paper.\n\nWe provide a summary of the changes made to the paper:\n\n* We have now provided additional experiments, testing the MPNN-max model on graphs of 500, 1000 and 1500 nodes (after training on graphs of 20 nodes). They may be found in Table 4. We find that the generalisation properties of MPNN-max carry over to these graph sizes and further solidify our contribution in terms of scale.\n\n*  We provide results for a \u201ccurriculum learning\u201d strategy, in which a network is first pre-trained on breadth-first search (until reaching perfect validation accuracy), followed by learning to imitate Bellman-Ford. These results are added in Tables 2 and 3 as an additional row, and demonstrate that such a sequential learning strategy performs worse than the simultaneous one in this case -- although it still provides benefits over the no-algo variant.\n\n* To aid clarity, we summarise all inputs and supervised signals of the algorithms considered here, within Table 7 in Appendix A;\n\n* We confirm that, when learnt in isolation, all of the GNN architectures considered are capable of strongly generalising on the reachability task, and have now made that point clear in the paper (see Footnote 2 on page 6).\n\n* In Section 1, we now cite the related work of Yoon et al.\n\n* We have increased the size of the plots and modified the colour of the MPNN-sum curve in Figure 3, hoping that this will further aid clarity."}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgKO0EtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1218/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1218/Authors|ICLR.cc/2020/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159407, "tmdate": 1576860543892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment"}}}, {"id": "rJetet8hjB", "original": null, "number": 3, "cdate": 1573837041048, "ddate": null, "tcdate": 1573837041048, "tmdate": 1573837041048, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SJg7FXl6KS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "Thank you very much for the kind review, and we are very glad you enjoyed the paper!\n\nAs recommended, we have now provided additional experiments, testing the MPNN-max model on graphs of 500, 1000 and 1500 nodes (after training on graphs of 20 nodes). They may be found in Table 4. We find that the generalisation properties of MPNN-max carry over to these graph sizes and further solidify our contribution in terms of scale. We agree that graphs of these sizes don\u2019t really pose any problems for classical algorithms, and instead focus on the multi-task and transfer learning aspect, which could enable us to tackle more challenging problems in the future.\n\nWe have also increased the size of the plots and modified the colour of the MPNN-sum curve in Figure 3, hoping that this will further aid clarity."}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgKO0EtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1218/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1218/Authors|ICLR.cc/2020/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159407, "tmdate": 1576860543892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment"}}}, {"id": "Sye99OU2sr", "original": null, "number": 2, "cdate": 1573836946351, "ddate": null, "tcdate": 1573836946351, "tmdate": 1573836946351, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "HklfytHTKB", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "Thank you for the very careful review and kind words about our contributions.\n\nWe provide responses to your comments in turn, hoping that they have adequately addressed your concerns. We are happy to discuss further on any of these points, of course!\n\n1. Regarding the performance of MPNN-mean/sum on reachability, it should be highlighted that the results are presented in the *multi-task* setup, where reachability has to be learnt simultaneously with shortest-paths. In this case, the summation architecture suffers from exploding messages (exemplified by the NaN values on MSE), while the averaging architecture ends up allocating most of its capacity to learning Bellman-Ford. We confirm that, when learnt in isolation, all of these architectures are capable of strongly generalising on reachability, and have now made that point clear in the paper (see Footnote 2 on page 6).\n\n2. Thank you for the comment regarding the clarity of the experimental setup! We fully agree, and now provide a table summarising all the inputs and supervision signals used -- see Table 7 in Appendix A. We also refer to this Appendix properly within Section 3.\n\n3. Initially, we haven\u2019t experimented with a sequential schedule. As per your recommendation, we provide results for a \u201ccurriculum\u201d strategy, in which a network is first pre-trained on breadth-first search (until reaching perfect validation accuracy), followed by learning to imitate Bellman-Ford. These results are added in Tables 2 and 3 as an additional row, and demonstrate that such a sequential learning strategy performs worse than the simultaneous one in this case -- although it still provides benefits over the no-algo variant.\n\n4. As recommended, we have now provided additional experiments, testing the MPNN-max model on graphs of 500, 1000 and 1500 nodes (after training on graphs of 20 nodes). They may be found in Table 4. We find that the generalisation properties of MPNN-max carry over to these graph sizes and further solidify our contribution in terms of scale.\n\n5. We now directly cite reference [1] in the paper -- namely, in Section 1, where we mention other related work. Thank you for pointing this reference out to us, it is certainly relevant to our work!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgKO0EtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1218/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1218/Authors|ICLR.cc/2020/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159407, "tmdate": 1576860543892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment"}}}, {"id": "rygAWdU2ir", "original": null, "number": 1, "cdate": 1573836805610, "ddate": null, "tcdate": 1573836805610, "tmdate": 1573836843214, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "BkgYLHqx5H", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for their comments. We would like to address a number of points raised here:\n\n1. As pointed out by the review, we trained a graph neural network to execute a graph algorithm on an arbitrary graph. Our objective is to enforce an inductive bias towards \u201calgorithmic reasoning\u201d within the GNN\u2019s update rule. This bias can be a very useful prior for e.g. discovering novel algorithms, or improving existing ones. We also experimentally demonstrate transfer between learning two different algorithms (BFS and Bellman-Ford), which directly points out the potential for one learnt algorithm executor to reinforce the predictions of another.\n\n2. Besides the transfer learning aspect, the algorithm execution task is a challenging problem in its own right, as shown in our baseline experiments (namely, many state-of-the-art GNN architectures such as GATs or MPNN variants are unable to generalise). Note that previous work has studied specific cases of these problems as well (e.g., Pointer Networks, Neural Turing Machines, etc).\n\n3. In the current version of the paper, our model is explained in one place (in Section 2), and our related work is outlined in one place (in the Introduction). Is there a specific part of the submission the reviewer would like us to move to the model description to help improve clarity of the paper?\n\n4. We have upgraded the clarity of the paper now by making substantial changes and performing additional experiments, such as:\n\n* Summarising all inputs and supervised signals of the algorithms considered here (Table 7 in Appendix A);\n* Additional related work cited throughout the paper;\n* Ablation studies, such as testing the generalisation of MPNN-max on larger graphs and assessing the benefits of a \u201ccurriculum learning\u201d strategy.\n\n5. Regarding executing two algorithms simultaneously, this is done by, at each step, concatenating the relevant x(t) and y(t) together as inputs/targets for the network. We mention this at the bottom of page 4, right after introducing the term \u201csimultaneously\u201d.\n\nWe are happy to discuss further on any of these points!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgKO0EtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1218/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1218/Authors|ICLR.cc/2020/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159407, "tmdate": 1576860543892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Authors", "ICLR.cc/2020/Conference/Paper1218/Reviewers", "ICLR.cc/2020/Conference/Paper1218/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Comment"}}}, {"id": "SJg7FXl6KS", "original": null, "number": 1, "cdate": 1571779450882, "ddate": null, "tcdate": 1571779450882, "tmdate": 1572972497435, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper suggests training neural network to imitate graph algorithms in a more fine-grained way than done before: by learning primitives and subroutines rather than the final output. The paper makes quite a strong case for the advantage of this approach, citing the fact that many graph algorithms share subroutines, which could simplify learning and support joint training and transfer learning. The experiments are detailed an elaborate.\n\nThe main weakness I see is the size of the graphs in the experiments. They are mainly limited to graphs with up to 100 nodes, with additional brief results for 1000 nodes in the appendix. These sizes are so small so as to raise doubts if the reported accuracy results would indeed scale, or whether they might require a significantly heavier network architecture. Moreover graphs of this size do not actually pose any difficulty for classic graph algorithms, that would justify invoking such heavy cannons like neural networks.\n\nNonetheless, I find this to be a conceptually strong paper with interesting ideas and thorough experiments (which in the least establish proof of concept; I am willing to accept leaving the issue of handling larger graphs to future work). I think the paper should be accepted.\n\nOther comments:\n1. The legend font in Figure 3 is to small (I am positively unable to read it off page) and the MPNN-sum plot is invisible in print. I hope the authors can reproduce the plots more clearly.\n2. I don't quite see the point in Appendix A, brief as it is. It apparently just states the fact that if two random variables share mutual information then knowing one reduces the entropy of the other. This is rather obvious both intuitively and formally."}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576022062316, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Reviewers"], "noninvitees": [], "tcdate": 1570237740588, "tmdate": 1576022062329, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review"}}}, {"id": "BkgYLHqx5H", "original": null, "number": 3, "cdate": 1572017489265, "ddate": null, "tcdate": 1572017489265, "tmdate": 1572972497284, "tddate": null, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "invitation": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper uses message-passing neural networks to learn to predict which\nnodes to update with what metadata. Experiments are shown that the algorithm\ncan learn to visit nodes in the same order as a breadth-first-search as well\nas the Bellman-Ford shortest-path algorithm.\n\nFeedback:\n\nI'm not sure what problem this paper is trying to solve. We are provided\nthe graph and graph algorithm as input to the network so that we can\nlearn what the algorithm will do next? I'm not sure why this is a problem?\nAre these algorithms badly written and need to be improved? Do we want to\nlearn how to execute graph algorithms in general?\n\nI found the paper fairly hard to read and follow. I wish the\nmodel were described all in one place and the related work also\nin just one place. I think this paper would be greatly improved\nby more work on explaining the motivation as well as more\nclearly. I also feel the paper could be much stronger if it was grounded\nin an existing problem others in the community might have.\n\nClarifications:\n\nIn page 4, I don't know what it means to execute these two algorithms\nsimulatenously?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1218/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["petarv@google.com", "rexying@stanford.edu", "mp861@cam.ac.uk", "raia@google.com", "cblundell@google.com"], "title": "Neural Execution of Graph Algorithms", "authors": ["Petar Veli\u010dkovi\u0107", "Rex Ying", "Matilde Padovano", "Raia Hadsell", "Charles Blundell"], "pdf": "/pdf/c0a6e40b48c2e2ed3ead72556fef85e539b1e58d.pdf", "TL;DR": "We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.", "abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.", "keywords": ["Graph Neural Networks", "Graph Algorithms", "Learning to Execute", "Program Synthesis", "Message Passing Neural Networks", "Deep Learning"], "paperhash": "velikovi|neural_execution_of_graph_algorithms", "_bibtex": "@inproceedings{\nVeli\u010dkovi\u01072020Neural,\ntitle={Neural Execution of Graph Algorithms},\nauthor={Petar Veli\u010dkovi\u0107 and Rex Ying and Matilde Padovano and Raia Hadsell and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgKO0EtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ce63527f3a102fa08682b8e5acbc05c1725d0118.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgKO0EtvS", "replyto": "SkgKO0EtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576022062316, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1218/Reviewers"], "noninvitees": [], "tcdate": 1570237740588, "tmdate": 1576022062329, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1218/-/Official_Review"}}}], "count": 10}