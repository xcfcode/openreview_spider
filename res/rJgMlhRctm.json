{"notes": [{"id": "rJgMlhRctm", "original": "ryxWHqa9KX", "number": 1053, "cdate": 1538087913713, "ddate": null, "tcdate": 1538087913713, "tmdate": 1556259662320, "tddate": null, "forum": "rJgMlhRctm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkl972bf9E", "original": null, "number": 1, "cdate": 1555336226405, "ddate": null, "tcdate": 1555336226405, "tmdate": 1555336226405, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Public_Comment", "content": {"comment": "Hi, nice paper! \n\nTwo quick questions:\n1) Can you elaborate how (if) the models learn how many attributes/concepts are there? E.g. in CLEVR there are 4 attributes that take 3, 4, 4, 8 values. Are these numbers learn by the model, or are they given? I read the appendix but I am still not sure I understand.\n2) Do you by any chance plan to release the code?\n\n", "title": "a few questions"}, "signatures": ["~Dzmitry_Bahdanau1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dzmitry_Bahdanau1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311689986, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJgMlhRctm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311689986}}}, {"id": "r1lTyZCSxN", "original": null, "number": 1, "cdate": 1545097444604, "ddate": null, "tcdate": 1545097444604, "tmdate": 1545354479420, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Meta_Review", "content": {"metareview": "Strong paper in an interesting new direction.\nMore work should be done in this area.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Oral)", "title": "Strong paper in an interesting new direction"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1053/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352983348, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352983348}}}, {"id": "r1g6tF8F3X", "original": null, "number": 2, "cdate": 1541134724770, "ddate": null, "tcdate": 1541134724770, "tmdate": 1543558715649, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "content": {"title": "Concern of invalid evaluation and vague demonstration of the contribution", "review": "To achieve the state-of-the-art on the CLEVR and the variations of this, the authors propose a method to use object-based visual representations and a differentiable quasi-symbolic executor. Since the semantic parser for a question input is not differentiable, they use REINFORCE algorithm and a technique to reduce its variance. \n\nQuality: \nThe issue of invalid evaluation should be addressed. CLEVR dataset has train, validation, and test sets. Since the various hyper-parameters are determined with the validation set, the comparison of state-of-the-art should be done using test set. As the authors mentioned, REINFORCE algorithm may introduce high variance, this notion is critical to report valid results. However, the authors only report on the validation set in Table 2 including the main results, Table 4. For Table 5, they only specify train and test splits. Therefore, I firmly recommend the authors to report on the test set for the fair comparison with the other competitive models, and please describe how to determine the hyperparameters in all experimental settings. \n   \nClarity:\nAs mentioned above, please specify the experimental details regarding setting hyperparameters.\nIn Experiments section, the authors used less than 10% of CLEVR training images. How about to use 100% of the training examples? How about to use the same amount of training examples in the competitive models? The report is incomplete to see the differential evident from the efficient usage of training examples.\n\nOriginality and significance:\nThe authors argue that object-based visual representation and symbolic reasoning are the contributions of this work (excluding the recent work, NS-VQA < 1 month). However, bottom-up and top-down attention work [1] shows that attention networks using object-based visual representation significantly improve VQA and image captioning performances. If the object-based visual representation alone is the primary source of improvement, it severely weakens the argument of the neuro-symbolic concept learner. Since, considering the trend of gains, the contribution of the proposing method seems to be incremental, this concern is inevitable. To defend this critic, the additional experiment to see the improvement of the other attentional model (e.g, TbD, MAC) using object-based visual representations, without any other annotations, is needed.\n\nPros:\n- To confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the CLEVR dataset for visual reasoning diagnosis.\n\nCons:\n- Invalid evaluation to report only on the validation set, not test set.\n- The unclear significance of the proposed method combining object-based visual representations and symbolic reasoning\n- In the original CLEVR dataset paper, the authors said \"we stress that accuracy on CLEVR is not an end goal in itself\" and \"..CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems.\" Based on this suggestion, can this work generalize to real-world settings? This paper lacks to discuss its limitation and future direction toward the general problem settings.\n\nMinor comments:\nIn 4.3, please fix the typos, \"born\" -> \"brown\" and \"convlutional\" -> \"convolutional\".\n\n\n[1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., & Zhang, L. (2018). Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. IEEE Computer Vision and Pattern Recognition (CVPR'18).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "cdate": 1542234317043, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335862343, "tmdate": 1552335862343, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylnXIUC0Q", "original": null, "number": 17, "cdate": 1543558692139, "ddate": null, "tcdate": 1543558692139, "tmdate": 1543558692139, "tddate": null, "forum": "rJgMlhRctm", "replyto": "r1xJIbv5A7", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "No reason not to raise the score", "comment": "The authors sufficiently clarified the experimental procedures for fair comparisons what I had concerned. Although the work seems to be limited in natural images and language (VQS), I appreciate the authors to include in the paper for the future works.\n\nI decide to increase my rating by 1."}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "HJeHGTF5nX", "original": null, "number": 3, "cdate": 1541213453202, "ddate": null, "tcdate": 1541213453202, "tmdate": 1543357491689, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "content": {"title": "Interesting end-to-end joint learning of visual concepts and semantic parsing but  experiments are limiting", "review": "\nSummary:\n=========\nThe paper proposes a joint learning of visual representation and word and semantic parsing of the sentences given paired images and paired Q/A with a model called neuro-symbolic concept learner using curriculum learning. The paper reads well and is easy to follow. The idea of jointly learning visual concepts and language is an important task. Human reasoning involves learning and recall from multiple moralities. The authors use the CLEVR dataset for evaluation.\n\nStrength:\n========\n- Jointly learning the language parsing and visual representations indirectly from paired Q/A and paired images is interesting. Combining the visual learning with the visual questions answers by decomposing them into primitive symbolic operations and reasoning in symbolic space seems interesting.\n\n- End-to-end learning of the visual concepts, Q/A decomposition into primitives and program execution was shown to be competitive to baseline methods.\n\nWeakness:\n=========\n- Although, the joint learning and composition is interesting, the visual task is simplistic and it is not obvious how this would generalize into other complex VQA tasks.\n\n- Experiments are not as rigorous as the discussion of the methods suggests. Evaluation on more datasets would have made the comparisons and drawn conclusions more stronger. Although CLEVR is suited for learning relational concepts from referential expressions, it is a toy dataset. Applicability of the proposed method on other realistic datasets would have made the paper more stronger.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "cdate": 1542234317043, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335862343, "tmdate": 1552335862343, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkx7KxOjpX", "original": null, "number": 2, "cdate": 1542320250822, "ddate": null, "tcdate": 1542320250822, "tmdate": 1543305108532, "tddate": null, "forum": "rJgMlhRctm", "replyto": "r1g6tF8F3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Our Response to Reviewer 3 (Part 2)", "comment": "4. Specific Questions\n- Choice of hyperparameters.\nWe use the open-sourced implementation of Mask-RCNN [5] to generate object proposals. For all the training processes described in the rest of the paper, we used learning rate 1e-3 with a weight decay of 5e-4. We decay the learning rate by a factor 0.1 after 60% of the designated training epochs. The REINFORCE optimizer uses a discount factor of 0.95. In the main text, the variance of REINFORCE means the variance of the gradient estimation but not the variance of the performance (accuracy). We will also add the standard deviation of the model performance in the revision. \n\n- Data-efficiency\nThanks for the very nice suggestion. We have conducted a more systematic study on the data efficiency and will include it the revision. The results are\n\nTrained on 10% of the images:\nTbD: 54.2%.\nMAC: 67.3%.\nNS-CL: 98.9%.\n\nTrained on 100% of the images:\nTbD: 99.1%.\nMAC: 98.9%.\nNS-CL: 99.2%.\n\nThese results demonstrate that our model is more data-efficient.\n\nWe have also listed all other planned changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n\n[1] Anderson et al. \"Bottom-up and top-down attention for image captioning and visual question answering.\" In CVPR, 2018.\n[2] Baradel et al. \"Object Level Visual Reasoning in Videos.\" In ECCV, 2018.\n[3] Artzi, Yoav, and Zettlemoyer. \"Weakly supervised learning of semantic parsers for mapping instructions to actions.\" TACL 1 (2013): 49-62.\n[4] Oh et al. \"Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning.\" In ICML, 2017.\n[5] https://github.com/roytseng-tw/Detectron.pytorch"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "r1xJIbv5A7", "original": null, "number": 15, "cdate": 1543299399305, "ddate": null, "tcdate": 1543299399305, "tmdate": 1543299399305, "tddate": null, "forum": "rJgMlhRctm", "replyto": "SJgbnnCgRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Thanks for your patience. Revision uploaded.", "comment": "Dear reviewer, we have updated our paper with the promised results.\n\n1. Train/Val/Test split:\nWe have included the new results of NS-CL using 100% of the CLEVR training images. We use 95% of the training images for learning, and the remaining 5% for validation, hyper-parameter tuning, and model selection. Validation images are used only in testing. This further pushes the overall accuracy of NS-CL to 99.2% on the validation split. Please refer to Section 4.2 for the new results. We adopt the same strategy in all newly added experiments, including those on the Minecraft dataset and the VQS dataset. For the results using only 10% of the CLEVR training images, we simply used the training set accuracy for model selection. \n\nWe have tried to contact the authors of the CLEVR dataset, and will be pleased to share further information regarding the test split upon receiving any responses.\n\n2. Object-based Representations and Data Efficiency.\nThank for suggesting the related work and the additional baselines. We have added additional experiments that incorporate object-based representations into TbD/MAC (Section 4.2). NS-CL achieves higher data efficiency. We believe that this comes from the full disentanglement of visual concept learning and symbolic reasoning: how to execute program instructions based on the learned concepts is programmed in NS-CL.\n\nCompared with the attention-based baselines, our use of symbolic programs enables better integration with object-based representations, e.g., in modelling relations and quantities. For the detailed implementation of the baselines, please refer to Appendix E.3.\n\nThanks again for your comments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "ryxKPxw9Rm", "original": null, "number": 13, "cdate": 1543299168949, "ddate": null, "tcdate": 1543299168949, "tmdate": 1543299168949, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "General Response: Revision Uploaded", "comment": "We thank all reviewers for their constructive comments and have updated our paper accordingly. Please check out the new version!\n\nSpecific changes include\n\n1) We have compared with additional baselines that incorporate object-based representation with attention-based methods (MAC/TbD). The results are in Section 4.2 and the implementation details are in Appendix E.3. The symbolic program execution module in NS-CL shows better utilization of object-based representations. \n\n2) We provided a systematic analysis of data efficiency in Section 4.2. NS-CL achieves higher data efficiency by disentangling visual concept learning and program-based symbolic reasoning.\n\n3) We added the results on a new visual reasoning testbed --- the Minecraft dataset. Results can be found in Appendix F.1.\n\n4) We added both quantitative and qualitative results on the VQS dataset, composed of natural images from the COCO dataset and human-annotated question-answering pairs. Please kindly find these results in Section 4.6 and the implementation details in Appendix F.2. NS-CL achieves a comparable results with the baselines and learns visual concepts from the noisy inputs.\n\n5) We have cited and discussed the suggested related work.\n\n6) We have also included more discussions on future work.\n\nPlease don\u2019t hesitate to let us know for any additional comments on the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "rJxoZ-_ipQ", "original": null, "number": 5, "cdate": 1542320386922, "ddate": null, "tcdate": 1542320386922, "tmdate": 1543208359427, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Our General Response", "comment": "We thank all reviewers for their comments. In addition to the specific response below, here we summarize our goal and the changes planned to be included in the revision. \n\nWe study concept learning---discovering both visual concepts and language concepts from natural supervision (unannotated images and question-answer pairs). With these learned concepts, our model can solve many problems, such as image captioning, retrieval, as well as VQA. But here the ability to solve VQA is really a by-product, not our end goal---learning accurate (Sec. 4.1), interpretable (Sec. 4.2), and transferrable (Sec. 4.5) concepts. \n\nWe agree with the reviewers that it\u2019s important to demonstrate how our model works on real images with more complex visual appearance. As suggested, we plan to include the following changes in the revision by Nov. 26 (the new official revision deadline, extended from Nov. 23):\n- We will include quantitative and qualitative results on new datasets: the VQA dataset of real-world images [1] and the Minecraft dataset used by Yi et al. [2].\n- We will add a systematic study regarding the data efficiency of our model, compared with other VQA baselines in Sec. 4.2.\n- We will compare our model with other baselines (TbD and MAC) built upon the object-based representations.\n- We will include additional discussions on limitation and future work.\n\nPlease don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n\n[1] Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. \"Vqa: Visual question answering.\" In ICCV, 2015.\n[2] Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. \"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.\" In NIPS, 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "SJgbnnCgRX", "original": null, "number": 9, "cdate": 1542675625403, "ddate": null, "tcdate": 1542675625403, "tmdate": 1542675625403, "tddate": null, "forum": "rJgMlhRctm", "replyto": "Bkx7KxOjpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Still waiting for the results with fair comparisons w.r.t Author Feedback 2.", "comment": "Sincerely thank you for the detailed explanations and comments for a constructive rebuttal.\n\nRe: 1. Train/test split\nR1-1) I understand the current evaluation issue on CLEVR. Then, could you confirm that your hyperparameters are found using **ONLY** training split since you have used the validation split like the test split to compare state-of-the-art?\nR1-2) Have you asked the authors of CLEVR regarding this issue? And, what's their response? I appreciate if you can cite the authors' reply in Appendix as a pointer to refer in future works.\n\nRe: 2. Object-based representations and baselines\nR2) With the positive results, I would like to consider increasing my rating considering the authors' argument of fair comparison."}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "rJl4slOsa7", "original": null, "number": 3, "cdate": 1542320284111, "ddate": null, "tcdate": 1542320284111, "tmdate": 1542320440933, "tddate": null, "forum": "rJgMlhRctm", "replyto": "r1g6tF8F3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Our Response to Reviewer 3 (Part 1)", "comment": "Thank you very much for the constructive comments. \n\n1. Train/test split\nOur evaluation is valid and fair, because all previous papers have also reported results only on the validation set, and we follow the tradition in this paper. They did this because there are no ground-truth labels or evaluation servers provided for the CLEVR test split. Evaluation on the test split is therefore impossible. We agree that it\u2019s important to ensure all evaluation valid, and we\u2019ll include this clarification into the revision.\n\n2. Object-based representations and baselines\nThanks for the suggestion. We\u2019ll cite and discuss the paper that used object-based visual representation. We will also add additional experiments that incorporate object-based representations into TbD/MAC: Instead of the image feature extracted from a ResNet, we change the input visual feature to the reasoning neural architecture to be an object-based representation as in [1]. Please let us know if you have any suggestion regarding the comparison.\n\nWe also want to clarify that the object-based representation alone is not the main contribution of the paper. Instead, our key contribution is the integration of object-based representations and symbolic reasoning. Such combination helps us disentangle visual concept learning and language understanding, and has three advantages over alternatives, as explored in the paper:\n\n1) Executing symbolic programs on object-based representations naturally facilitates complex reasoning that includes quantities (counting), comparisons, and relations. It also brings combinatorial generalization by design (Sec. 4.4): for example, trained on scenes with <= 6 objects, our model (but not the baselines) can also perform counting on scenes with 10 objects.\n\n2) It fully disentangles the visual concept learning and reasoning: once the visual concepts are learned, they can be systematically evaluated (Sec. 4.1) and deployed in any visual-semantic applications (such as image caption retrieval, as shown in Sec. 4.5). In contrast, earlier methods like IEP, TbD, and MAC learn visual concepts and reasoning in an entangled manner and cannot be easily adapted to new problem domains (e.g., show in Table 6, VQA baselines are only able to infer the result on a partial set of the image-caption data).\n\n3) Symbolic execution over the object space brings full transparency. One can easily trace back the error answer and even detect adversarial (ambiguous or wrong) questions (please refer to Appendix. E for some examples).\n\n3. Limitation and future work\nWe\u2019d like to clarify that we are not targeting at a specific application such as VQA; instead, we want to build a system that learns accurate (Sec. 4.1), interpretable (Sec. 4.2), and transferrable (Sec. 4.5) concepts from natural supervision: images and question-answer pairs. To achieve this, we propose a novel framework that 1) disentangles the learning of both, but 2) bridges them with a reasoning module and 3) lets them bootstrap the learning of each other.\n\nToward concept learning from realistic images and complex language, the current model design suggest multiple research directions. First, our model relies on object-based representations; constructing 3D object-based representations for realistic scenes (or videos) needs further exploration [1,2]. Second, our model assumes a domain-specific language for a formal description of semantics. The integration of formal semantics into the processing of complex natural language would be meaningful future work [3,4]. We hope our paper could motivate future research in visual concept learning, language learning, and compositionality.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "SyxhAx_jpm", "original": null, "number": 4, "cdate": 1542320340336, "ddate": null, "tcdate": 1542320340336, "tmdate": 1542320365997, "tddate": null, "forum": "rJgMlhRctm", "replyto": "Sklo1V_znQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Our Response to Reviewer 1", "comment": "Thank you very much for the constructive comments.\n\n1. Semantic parsing.\nIn short, the semantic parsing module is a neural sequence-to-tree model. Given a natural language question, the module translates into an executable program with a hierarchy of primitive operation. We present an overview in Sec. 3.1 (last paragraph of Page 4), with more implementation details in Appendix B. We\u2019ll revise the text for better clarity.\n\nThe module begins with encoding the question into a fixed-length embedding vector using a bidirectional GRU. The decoder, taking the sentence embedding as input, recovers the hierarchy of the operations in a top-down manner: It first predicts the root token (the question type: query/count/\u2026 in the VQA case); then, conditioned on the root token, it predicts the tokens of the root\u2019s children. The decoding algorithm runs recursively.\n\n2. Counting.\nWe perform counting in a quasi-symbolic manner, based on the object-based scene representation. As an example, consider a simple program: Count(Filter(Red)), which counts the number of red objects in the scene. The operation Filter(Red) assigns each object with a value p_i, as the confidence of classifying this object as a red one. Counting is performed as: $\\sum_i p_i$. During inference, we round this value to the nearest integer. More details can be found in Sec. 3,1. (Page 5) and Appendix C. We will also revise the text for better clarity.\n\nCompared with alternatives, our method enjoys combinatorial generalization with the notion of `objects\u2019: for example, trained on scenes with <= 6 objects, our model can also perform counting on scenes with 10 objects.\n\n3. Future direction\nWe thank the reviewer for the suggestions on future directions and will include the following discussions in the revision:\n\nCompositionality. We currently view the scene as a collection of objects with latent representations. Building scene (or video) representations that also reflects the compositional nature of objects (e.g., an object is a combination of multiple primitives) will be an interesting research direction. \n\nInfer relations from words and behavior. Modelling actions (e.g., push and pull) as concepts is another interesting direction. People have studied the symbolic representation of skills [1] and learning word (instruction) meanings from interaction [2].\n\nVideos and words. Our framework can also be extended to the video domain. Video techniques such as detection and tracking are needed to build the object-based representation [3]. Also, the semantic representation of sentences should be extended to include actions / interactions besides static spatial relations. \n\nWe have also listed all other planned changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n\n\n[1] Konidaris, George, Leslie Pack Kaelbling, and Tomas Lozano-Perez. \"From skills to symbols: Learning symbolic representations for abstract high-level planning.\" Journal of Artificial Intelligence Research 61 (2018): 215-289.\n[2] Oh, Junhyuk, Satinder Singh, Honglak Lee, and Pushmeet Kohli. \"Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning.\" In ICML, 2017.\n[3] Baradel, Fabien, Natalia Neverova, Christian Wolf, Julien Mille, and Greg Mori. \"Object Level Visual Reasoning in Videos.\" In ECCV, 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "rJx2mlOjTQ", "original": null, "number": 1, "cdate": 1542320164421, "ddate": null, "tcdate": 1542320164421, "tmdate": 1542320164421, "tddate": null, "forum": "rJgMlhRctm", "replyto": "HJeHGTF5nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "content": {"title": "Our Response to Reviewer 2", "comment": "Thank you very much for the encouraging and constructive comments. We agree that generalizing to more complex visual domains would be essential for our task. In the revision, we will include the results of NS-CL on new datasets, including the VQA dataset of real-world images [1] and the Minecraft dataset used by Yi et al. [2].\n\nWe have also listed all other planned changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n\n[1] Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. \"Vqa: Visual question answering.\" In ICCV, 2015.\n[2] Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. \"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.\" In NIPS, 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605670, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgMlhRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1053/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1053/Authors|ICLR.cc/2019/Conference/Paper1053/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers", "ICLR.cc/2019/Conference/Paper1053/Authors", "ICLR.cc/2019/Conference/Paper1053/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605670}}}, {"id": "Sklo1V_znQ", "original": null, "number": 1, "cdate": 1540682723099, "ddate": null, "tcdate": 1540682723099, "tmdate": 1541533464106, "tddate": null, "forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "content": {"title": "Excellent paper including many cutting edge techniques ", "review": "The paper is well written and flow well. The only thing I would like to see added is an elaboration of \n\"run a semantic parsing module to translate a question into an executable program\". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. \n\nThis is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. \n\nIn concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words?   ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1053/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.", "keywords": ["Neuro-Symbolic Representations", "Concept Learning", "Visual Reasoning"], "authorids": ["maojiayuan@gmail.com", "ganchuang1990@gmail.com", "pushmeet@google.com", "jbt@mit.edu", "jiajunwu@mit.edu"], "authors": ["Jiayuan Mao", "Chuang Gan", "Pushmeet Kohli", "Joshua B. Tenenbaum", "Jiajun Wu"], "TL;DR": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.", "pdf": "/pdf/a0f215dd254bc10d946ccc26632ca8e42defee17.pdf", "paperhash": "mao|the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision", "_bibtex": "@inproceedings{\nmao2018the,\ntitle={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\nauthor={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgMlhRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1053/Official_Review", "cdate": 1542234317043, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgMlhRctm", "replyto": "rJgMlhRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1053/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335862343, "tmdate": 1552335862343, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1053/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}