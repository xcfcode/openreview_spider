{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396483763, "tcdate": 1486396483763, "number": 1, "id": "rJ2H3zI_x", "invitation": "ICLR.cc/2017/conference/-/paper290/acceptance", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors present a framework for online structure learning of sum-product network. They overcome challenges such as being able to learn a valid sum product network and to have an online learning mechanism. Based on the extensive discussions presented by the reviewers, our recommendation is to accept this paper for a workshop.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396484273, "id": "ICLR.cc/2017/conference/-/paper290/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396484273}}}, {"tddate": null, "tmdate": 1484828455947, "tcdate": 1484828455947, "number": 13, "id": "HylVk4AUl", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "S1m2FIDIe", "signatures": ["~Sang-Woo_Lee1"], "readers": ["everyone"], "writers": ["~Sang-Woo_Lee1"], "content": {"title": "Reply", "comment": "Thank you for your consideration!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484698448833, "tcdate": 1478283755486, "number": 290, "id": "S1QefL5ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1QefL5ge", "signatures": ["~Wilson_Hsu1"], "readers": ["everyone"], "content": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": ["By7LxZNFe"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484383269339, "tcdate": 1484383269339, "number": 12, "id": "HyT74wv8g", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "r1gVRtvXg", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "variation in results as a function of the parameters", "comment": "We just uploaded a new version of the paper that shows how the results vary as a function of the correlation threshold and the maximum number of variables in each leaf."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1484383087228, "tcdate": 1484383087228, "number": 11, "id": "HyPd7DvUg", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "rkn-RBHEx", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "response to review", "comment": "Thank you for the review.  We just uploaded a new version that addresses many of the concerns raised.\n\n\"The paper is heavily updated between submission deadline and submission of reviews.\"\n\nWe realize that the paper has evolved substantially since the first submission.  All the revisions were done in response to requests by the reviewers.  ICLR differs from other conferences since authors are encouraged to submit revisions that address the reviewers's concerns.  This is better since the reviewers can evaluate the final version as it is instead of wondering whether the authors will fulfill the requests of the reviewers in the final version.\n\n\"Comparison to literature is severely lacking\"\n\nWe added a new subsection about parameter learning in the background  section and expanded the subsection about structure learning.  The main characteristics of each algorithm are now described.  We also added some text in the section where we describe the new parameter and structure learning technique to clarify the differences with the closest algorithms.\n\n\"Table 3: Random structure as baseline ok, but how were the parameters here learned? \"\n\nThe parameters are updated using the same parameter learning technique as in oSLRAU.\n\n\"Table 1: you are presenting *positive* average log-likelihood values? This should be an average of log(p<=1) < 0 values? What am I missing here?\"\n\nLog likelihood is positive when the density is higher than 1.  Recall that we are working with continuous variables and therefore we have a density function (instead of probabilities) and that a density function can have values higher than 1.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1484382227432, "tcdate": 1484382227432, "number": 10, "id": "H1ifxPPIl", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "rJGrY38Vl", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "response to review", "comment": "Thank you for your review.  We just uploaded a new version f the paper that addresses many of the concerns raised.  We included an empirical comparison with RealNVP and we explained that the algorithm scales quadratically with the number of features.\n\nWe note the request for more comparisons to more types of generative deep neural networks.  However, it is important to not loose track of the main contribution of the paper: an online structure learning algorithm for SPNs with Gaussian leaves. By comparing SPNs to other generative models, we are not comparing different structure learning techniques, but simply different models.  In fact, no structure learning technique has been proposed for generative models other than SPNs (as far as we know).  \n\nWe also not the insistence on scalability with respect to the number of features.  As mentioned above, oSLRAU scales quadratically with respect to the number of features.  While this does not permit to scale to large images with many pixels, please remember that this is the first online structure learning technique for SPNs with Gaussian leaves while other types of networks do not have any online structure learning technique. A fair comparison would be to compare the scalablity of parameter learning techniques for different types of neural nets.  Since SPNs can be trained by SGD just like any other type of deep neural net, SPNs are as scalable as other types of network when the structure is fixed.  In fact, Poon and Domingos; UAI-2011 paper showed results where SPNs beat other types of deep neural nets on an image completion task.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1484380587378, "tcdate": 1484380587378, "number": 9, "id": "S1m2FIDIe", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "ByKmOAYSl", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "related work", "comment": "Thank you for pointing out your paper.  We just uploaded a new version of our paper that cites your work and explains the differences.  \n\nIn summary, oSLRAU is related to, but different from the online structure learning technique proposed by Lee et al.  Lee et al.'s technique was applied to discrete datasets while oSLRAU learns SPNs with Gaussian leaves based on real-valued data.  Furthermore, Lee et al.'s technique incrementally constructs a network in a top down fashion by adding children to sum nodes by online clustering.  Once a product node is constructed, it is never modified.  In contrast, oSLRAU incrementally constructs a network in a bottom up fashion by detecting correlations and modifying product nodes to represent these correlations.  Finally, Lee et al.'s technique updates the parameters by hard EM (which implicitly works with a max-product network) while oSLRAU updates the parameters by Alg 1 (which retains the original sum-product network) as explained in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1483496392704, "tcdate": 1483495456760, "number": 8, "id": "ByKmOAYSl", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["~Sang-Woo_Lee1"], "readers": ["everyone"], "writers": ["~Sang-Woo_Lee1"], "content": {"title": "Comparing to the paper \"Online Incremental Structure Learning for Sum-Product Networks\" (ICONIP, 2013)", "comment": "Hi. I hope the paper mentions my previous work \"Online Incremental Structure Learning for Sum-Product Networks\" (ICONIP, 2013).\nI think the idea is quite similar, both for online parameter learning and online structure learning, although there is no consideration over Gaussian leaves in my paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1482242362424, "tcdate": 1482242362424, "number": 3, "id": "rJGrY38Vl", "invitation": "ICLR.cc/2017/conference/-/paper290/official/review", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer4"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The authors present an online learning method for learning the structure of sum-product networks. The algorithm assumes Gaussian coordinate-wise marginal distributions, and learns both parameters and structure online. The parameters are updated by a recursive procedure that reweights nodes in the network that most contribute to the likelihood of the current data point. The structure learning is done by either merging independent product Gaussian nodes into multivariate leaf nodes, or creating a mixture over the two nodes when the multivariate would be too large.\n\nThe fact that the dataset is scaled to some larger datasets (in terms of the number of datapoints) is promising, although the number of variables is still quite small. Current benchmarks for tractable continuous density modeling with neural networks include the NICE and Real-NVP families of models, which can be scaled to both large numbers of datapoints and variables. Intractable methods like GAN, GenMMN, VAE have the same property. \n\nThe main issue with this work for the ICLR audience is the use of mainly a set of SPN-specific datasets that are not used in the deep learning generative modeling literature. The use of GenMMN as a baseline is also not a good choice to bridge the gap to the neural community, as its Parzen-window based likelihood evaluation is not really meaningful. Better ways to evaluate the likelihood through annealed importance sampling are discussed in \"On the Quantitative Analysis of Decoder-Based Generative Models\" by Wu et al. I would recommend the use of a simple VAE type model to get a lower bound on the likelihood, or something like Real-NVP.\n\nMost neural network density models are scalable to large numbers of observations as well as instances, and it is not clear that this method scales well \"horizontally\" like this. Evaluating the feasibility of modeling something like MNIST would be interesting.\n\nSPNs have the strength that not only marginal but also various type of conditional queries are tractable, but performance on this is not evaluated or compared. One interesting application could be in imputation of unknown pixels or color channels in images, for which there is not currently a high-performing tractable model.\n\nDespite the disconnect from other ICLR generative modeling literature, the algorithm here seems simple and intuitive and convincingly works better than the previous state of the art for online SPN structure learning. I think VAE is a much better baseline for continuous data than GenMMN when attempting to compare to neural network approaches. Further, the sum-product network could actually be combined with such deep latent variable models as an observation model or posterior, which could be a very powerful combination. \n\nI would like it if these SPN models were better known by the ICLR probabilistic modeling community, but I do not know if this paper does enough to make them relevant. As with the other reviewers, I am not an expert on SPNs. However, this seems to be a simple and effective algorithm for online structure induction, and the scalability aspect is something that is important in much recent work in the learning of representations. I think it is good enough for publication, although I would prefer to see many of the above additions to more clearly bridge the gap with other literature in deep generative modeling.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635028, "id": "ICLR.cc/2017/conference/-/paper290/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6", "ICLR.cc/2017/conference/paper290/AnonReviewer4"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635028}}}, {"tddate": null, "tmdate": 1482149379881, "tcdate": 1482149379881, "number": 2, "id": "rkn-RBHEx", "invitation": "ICLR.cc/2017/conference/-/paper290/official/review", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer6"], "content": {"title": "An algorithm to learn the structure of continuous SPNs in a single pass - too unfinished", "rating": "4: Ok but not good enough - rejection", "review": "# Summary\nThis paper proposes an algorithm to learn the structure of continuous SPNs in a single pass through the data,\nbasically by \"growing\" the SPN when two variables are correlated.\n\n## NOTE\nI am not an expert on SPNs, and can not really judge how impressive the presented results are due to lack of familiarity with the datsets.\n\n# Pro\n- This looks like possibly impactful work, proposing a simple and elegant algorithm for learning SPN structure single-pass, rather than just using random structure which has been done in other work in the online settings.\n\n# Con\n- The paper is heavily updated between submission deadline and submission of reviews.\n- The paper reads like a rush job, sloppily written - at least the first version.\n- Comparison to literature is severely lacking; eg \"several automated structure learning techniques have been proposed\" followed by 6 citations but no discussion of any of them, which one is most related, which ideas carry over from the offline setting to this online setting, etc. Also since this work presents both joint structure & *parameter* learning, comparison to the online parameter learning papers (3 cited) would be appreciated, specifically since these prior approaches seem to be more principled with Bayesian Moment Matching in Jaini 2016 for example.\n- I do not know enough about SPNs and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt Random\n\n# Remaining questions after the paper updates\n- Table 3: Random structure as baseline ok, but how were the parameters here learned? Your simple running average or with more advanced methods?\n- Table 1: you are presenting *positive* average log-likelihood values? This should be an average of log(p<=1) < 0 values? What am I missing here?\n\nI recommend reject mostly because this paper should have been finished and polished at submission time, not at review deadline time.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635028, "id": "ICLR.cc/2017/conference/-/paper290/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6", "ICLR.cc/2017/conference/paper290/AnonReviewer4"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635028}}}, {"tddate": null, "tmdate": 1481856318100, "tcdate": 1481856318100, "number": 7, "id": "r1LrrAx4e", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "r1ToU62mx", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "response", "comment": "Thank you for the review.  We just uploaded a revised version of the paper.  Regarding the lack of theoretical justification, we included an explanation about how we derived the parameter update algorithm, including a theorem that shows that it necessarily increases the likelihood of the last data point processed.\n\nRegarding the similarities between this paper and Jaini et al.'s paper, the only content that was copied from Jaini are the results for alternative approaches in Table 1.  This saved us the trouble of re-implementing these approaches and re-running them.  This is common practice when a set of benchmarks can be reused.  Note also that the background section in both papers introduces some basics about SPNs, but this is necessary for completeness.  The contributions of both papers are different.  Jaini et al. introduced an online parameter update algorithm (Bayesian moment matching).  There was no structure learning algorithm and therefore random structures were used.  In our paper, we introduce an online structure learning algorithm.  The algorithm is simple and therefore it might be tempting to write that there is not enough work for a publication, but simple is good in practice.  Its simplicity is indeed what allows it to scale.  We also hope that its simplicity will incite others to use it and perhaps enhance it. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481855024746, "tcdate": 1481855024746, "number": 6, "id": "rJYEg0eNg", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "HJZg0LkXl", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "update", "comment": "We just uploaded a revised version of the paper with the requested clarification regarding the evaluation with generative moment matching networks.\n\nAlso we contacted Laurent Dinh (author of NICE and Real-NVP) who gave us links to publicly available code.  After inspection of the code, we determined that we will need to modify it for online training and dataset compatibility.  We are currently working on this.  Stay tuned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481854366792, "tcdate": 1481854366792, "number": 5, "id": "SJPjaalEe", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "B1k-C88mg", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "update", "comment": "We just uploaded a revised version of the paper with additional experiments that show how the log-likelihood, training time and SPN size change as we vary the correlation threshold.  In short, when the threshold is too small, superfluous correlations tend to be detected which might lead to overfitting and when the threshold is too large, some correlations might be missed which might lead to underfitting.\n\nWe are currently working on additional experiments to illustrate the effect of the threshold for the maximum number of variables in a multivariate Gaussian leaf node.  Stay tuned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481853935803, "tcdate": 1481853935803, "number": 4, "id": "S1OenpeVg", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "SyE_pq6Qg", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "some answers", "comment": "We just uploaded a revised version of the paper.  It includes some additional explanation about how we derived the parameter update algorithm, including a theorem that shows that it necessarily increases the likelihood of the last data point processed.  In the next version that we plan to upload we will add a discussion about the relationship to other parameter learning algorithms.\n\nIn Section 4.2, we added experiments with 5 datasets (in addition to VoxForge) and created several tables.  At the moment, the only baseline we have for those experiments consist of random structure SPNs (which is the state of the art). That being said we are working on including a comparison to NICE and RealNVP as suggested by another reviewer.\n\nIn Section 3.2, the two approaches to introduce a correlation (i.e., multivariate Gaussian and mixture of Gaussians) are not equivalent.  The mixture approach is more expressive since mixture of Gaussians with arbitrarily many components can approximate any distribution while a multivariate Gaussian can only represent a unimodal distribution with a specific shape.  So, in principle, we could use mixtures of Gaussians only, however the use of multivariate Gaussians in the leaves can often reduce the number of components in a mixture.  This is why we use both approaches.  If the children are subtrees rather than leaves, we use the mixture approach to combine them as described in the paper.  We will add an explanation to that effect in the next version of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481660861039, "tcdate": 1481660861031, "number": 3, "id": "Byr6KATXe", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "SyE_pq6Qg", "signatures": ["~Agastya_Sanjiv_Kalra1"], "readers": ["everyone"], "writers": ["~Agastya_Sanjiv_Kalra1"], "content": {"title": "Stuff to Add", "comment": "Thank you for your feedback. We will be incorporating it into our update with answers to this and a lot more hopefully by tonight or tomorrow morning."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481645420470, "tcdate": 1481645420464, "number": 3, "id": "SyE_pq6Qg", "invitation": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer6"], "content": {"title": "pre-review questions", "question": "Some questions that I believe should be answered in the paper:\n+ sec3.1: Where does this parameter update algorithm come from? Was something similar done before? How does this compare with the previous algorithms for learning SPN parameters?\n+ sec3.2: Two approaches are introduced.\n   (1) Are they equivalent? (no)\n   (2) Which one is more expressive?\n   (3) Which approach under which conditions?\n+ sec3.2: If the children are subtrees rather than leaves, how are these subtrees combined?\n  The text states \"it works the same way\" but this is quite nontrivial to me.\n+ sec4.2 why is VoxForge not included in the table?\n   Are there results with the other baseline methods?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481645420986, "id": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer4", "ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481645420986}}}, {"tddate": null, "tmdate": 1481590436593, "tcdate": 1481590436585, "number": 1, "id": "r1ToU62mx", "invitation": "ICLR.cc/2017/conference/-/paper290/official/review", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer5"], "content": {"title": "A novel constructive algorithm for SPNs with Gaussian observations.", "rating": "4: Ok but not good enough - rejection", "review": "The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.\n\nThe above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.\n\nOverall: \nFirst let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  \n\nMy overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.\n\nSmallish Problems\nI wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.\n\nThe SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635028, "id": "ICLR.cc/2017/conference/-/paper290/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6", "ICLR.cc/2017/conference/paper290/AnonReviewer4"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635028}}}, {"tddate": null, "tmdate": 1481248296456, "tcdate": 1481248296451, "number": 2, "id": "r1gVRtvXg", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "B1k-C88mg", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "variation in the results as a function of the parameters", "comment": "Thank you for asking how the results vary as a function of the parameters of the algorithm.  We will include some additional results to answer this question in a future version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481248007668, "tcdate": 1481248007662, "number": 1, "id": "SJez6Kvme", "invitation": "ICLR.cc/2017/conference/-/paper290/public/comment", "forum": "S1QefL5ge", "replyto": "HJZg0LkXl", "signatures": ["~Pascal_Poupart1"], "readers": ["everyone"], "writers": ["~Pascal_Poupart1"], "content": {"title": "evaluation techniques", "comment": "Thank you for the question.  As mentioned in the paper, the results in Table 1 are from Jaini et al. 2016 (except for oSLRAU).  Jaini et al. wrote that Parzen windows were used to approximate the likelihood of the generative moment matching networks and also cautioned the reader that this approximation may not be close to the true likelihood as pointed out by Theis et al.  We will repeat this explanation in the next version of the paper.   \n\nThank you for recommending to compare SPNs to NICE and Real-NVP. We are looking into the feasibility of doing such a comparison. Stay tuned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637826, "id": "ICLR.cc/2017/conference/-/paper290/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1QefL5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper290/reviewers", "ICLR.cc/2017/conference/paper290/areachairs"], "cdate": 1485287637826}}}, {"tddate": null, "tmdate": 1481170423096, "tcdate": 1481170423086, "number": 2, "id": "B1k-C88mg", "invitation": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer5"], "content": {"title": "Relation of correlation coefficient threshold to performance and structure of learned graph structure.", "question": "How do the structure learning results vary as a function of the chosen correlation coefficient threshold (0.1 in experiments).  Same question for threshold (4 in manuscript) for choosing which structure technique to chose (combining distributions in multivariate leaf vs. mixture)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481645420986, "id": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer4", "ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481645420986}}}, {"tddate": null, "tmdate": 1480711656845, "tcdate": 1480711656841, "number": 1, "id": "HJZg0LkXl", "invitation": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "forum": "S1QefL5ge", "replyto": "S1QefL5ge", "signatures": ["ICLR.cc/2017/conference/paper290/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper290/AnonReviewer4"], "content": {"title": "Question", "question": "How are the likelihoods computed for the generative moment matching networks? Using Parzen windows? \"A note on the evaluation of generative models\" by Theis et al provides good evidence that these can be unreliable. Have the authors considered comparing to the NICE and Real-NVP models which have fully tractable likelihoods and model real-valued data?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes the first {\\em online} structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "pdf": "/pdf/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf", "TL;DR": "This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves.", "paperhash": "hsu|online_structure_learning_for_sumproduct_networks_with_gaussian_leaves", "conflicts": ["uwaterloo.ca"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Wilson Hsu", "Agastya Kalra", "Pascal Poupart"], "authorids": ["wwhsu@uwaterloo.ca", "a6kalra@uwaterloo.ca", "ppoupart@uwaterloo.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481645420986, "id": "ICLR.cc/2017/conference/-/paper290/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper290/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper290/AnonReviewer4", "ICLR.cc/2017/conference/paper290/AnonReviewer5", "ICLR.cc/2017/conference/paper290/AnonReviewer6"], "reply": {"forum": "S1QefL5ge", "replyto": "S1QefL5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper290/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481645420986}}}], "count": 21}