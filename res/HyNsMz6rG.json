{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582955352, "tcdate": 1520177181797, "number": 1, "cdate": 1520177181797, "id": "BkIXgcKuM", "invitation": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "signatures": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer3"], "content": {"title": "Interesting new method, although the article could be clarified, and an important piece of literature is missing.", "rating": "7: Good paper, accept", "review": "Summary\nThe authors propose a method to define the \"smoothness\" of the representation computed by a neural network (or a layer of a neural network). If I understood correctly, the idea is to train a random forest on top of the learned representation. A set of \"wavelets\" can be associated to this random forest. These wavelets define Besov spaces with different degrees of smoothness, and it is possible to determine the \"most regular\" Besov space that the classification function belongs to.\nThe authors test their method with networks trained on Urban8K and CIFAR10. They find that the smoothness is increasing accross layers. They also investigate the link between smoothness and mislabeling of the dataset.\n\nNovelty: the idea of looking at the smoothness of representations is not novel (see \"Cons\"), but the method to evaluate smoothness is new (as far as I know).\nClarity: can be improved; there are more comments about this in the last part of this review.\nSignificance: good.\nQuality: the article seems rigorous.\n\nPros\n1. This article proposes a notion of smoothness that has a solid mathematical basis. This notion is a priori sufficiently sophisticated to capture subtle properties of neural networks, and maybe allow comparison between different architectures. So, in my opinion, it looks promising.\n2. The experiments of the authors suggest a strong link between smoothness of the learned representation and generalization. This part of the work seems a bit preliminary, but it might be very interesting if it was further developed. \n\nCons\n1. The authors do not discuss the implications of their work for the understanding of deep learned representations. Knowing that the representations become smoother and smoother accross the layers is an interesting and \"self-sufficient\" properties, but the method proposed by the authors tells more than that. It allows to precisely quantify the smoothness; as highlighted by the authors, it is also based on a solid mathematical theory. How do the authors expect these properties to be useful in future work?\n2. Studying the smoothness of learned representations is not something new. It has in particular been studied by Oyallon in \"Building a regular decision boundary with deep networks\", although with less sophisticated tools (nearest neighbors instead of Besov spaces). I think a comparison with this work is necessary.\n(There might be other works on the subject, but, since I am not a specialist, I do not know them.)\n\nTypos / minor remarks\n- \"Function Space\" should not have capital letters (as well as \"Approximation Theory\" in 1.3).\n- \"mis-labeling\" -> \"mislabeling\"?\n- First paragraph: there should be parentheses around \"Tao (2008)\". What is a \"quantity nature\"? A verb is also lacking in this sentence. I do not understand \"One of the practical ... space.\" The expression \"error decay rate\" for a function is unclear. In the line before the last one, the second \"besov\" should be removed.\n- I did not understand the last paragraph of Subsection 1.1.\n- The first paragraph of Subsection 1.2 is, in my opinion, a bit unclear. What is a node of a convex domain? What are the polynomials associated to? How can the approximating polynomials be uniquely defined, since they have not been required to satisfy a single property? I recommend rewriting this paragraph in a more pedagogical style.\n- Last line of the first paragraph of 1.2: \"belongs\" should be \"belong\".\n- Subsection 1.2: The notation j(Omega) should be defined before it is used, not afterwards. I also do not think that \"J\" is formally defined somewhere. By the way, how is J chosen in the numerical tests?\n- In Equation (3), k_3 should be k_2.\n- In the pictures, the legends are too small, and difficult to read.\n- End of 1.1, and title of 1.3: \"revile\" -> \"reveal\".\n- Subsection 1.3: I do not understand \"a correspondence ... smoothness index alpha\". A function can be (Besov-)smooth without being sparse. I think the \"sparsity\" is the sparsity of its wavelet decomposition.\n- End of 1.3: what does the index k represent in c_k and alpha_k? Why is log(sigma_M) modeled as log(c_k) - alpha log(M) (with a dependency on k for c_k, but not for alpha)?\n- Last sentence of 1.3: capital letter missing.\n- My understanding of this article is that the smoothness is computed by defining a random forest on top of a deep representation. Is this correct? If yes, I think it could be explained more clearly in the article.\n- Subsection 2.2: why is smoothness computed only for the last layer? Do the other layers behave the same? If no, why? Which network architectures are used? Do they perfectly classify almost all training samples, even with mislabeling?\n- Last sentence: capital letter missing.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "USING FUNCTION SPACE THEORY FOR UNDERSTANDING INTERMEDIATE LAYERS", "abstract": "The representational change of input along the intermediate layers is an important\naspect of understanding deep learning architectures. To this end, we propose an\napproach that relies on the foundation of Function Space theory. In particular, we\nargue that a weak-type Besov smoothness index can quantify the geometry of the\nclustering in the feature space of each layer. Therefore, our approach may provide\nan additional perspective for understanding data-models fit in the setting of deep\nlearning. While using a different framework and perspective, the experiments we\nperformed are in line with the results described by Tishby & Zaslavsky (2015) and\nMontavon et al. (2010) in the sense that for well-performing trained networks, the\nquality of the representation increases from layer to layer. Our approach could\nalso be used for addressing generalization (Zhang et al., 2016), (Kawaguchi et al.,\n2017) as we also show that the Besov smoothness of the layer representations of\nthe training set decreases as we add more mis-labeling.", "pdf": "/pdf/86fb4dfbe6a74a5ca6a03c96d9a4b30c2646713d.pdf", "TL;DR": "We propose a Function Space theory approach, that describes the change of the input along the intermediate layers in deep learning architectures", "paperhash": "elisha|using_function_space_theory_for_understanding_intermediate_layers", "keywords": ["deep learning", "representation layers", "Function Space", "wavelets", "approximation", "Besov smoothness"], "authors": ["Oren Elisha", "Shai Dekel"], "authorids": ["orenelis@gmail.com", "shaidekel6@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582955134, "id": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper5/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer1"], "reply": {"forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582955134}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582833769, "tcdate": 1520601922493, "number": 2, "cdate": 1520601922493, "id": "Hy9Bobetf", "invitation": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "signatures": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer2"], "content": {"title": "It is hard to say that this result is new or non-trivial.", "rating": "4: Ok but not good enough - rejection", "review": "- Outline -\nThis paper investigates an effect of an low-rank approximation to tensors from an activation of DNNs.\nAuthors experimentally show that when tensors is reconstructed as low-rank, accuracy by DNNs is decreased and values of parameters are changed.\n\n- Comment -\nIn my opinion, it is hard to say that this result is new or non-trivial.\nBasically, it is well known that volume of approximation error decreases as a number of ranks increases.\nIn addition, the approximation effect of tensors for DNNs is already studied.\nThus, the decrease of accuracy is obviously explained by the low-rank approximation.\nIt is hard to say their finding is new.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "USING FUNCTION SPACE THEORY FOR UNDERSTANDING INTERMEDIATE LAYERS", "abstract": "The representational change of input along the intermediate layers is an important\naspect of understanding deep learning architectures. To this end, we propose an\napproach that relies on the foundation of Function Space theory. In particular, we\nargue that a weak-type Besov smoothness index can quantify the geometry of the\nclustering in the feature space of each layer. Therefore, our approach may provide\nan additional perspective for understanding data-models fit in the setting of deep\nlearning. While using a different framework and perspective, the experiments we\nperformed are in line with the results described by Tishby & Zaslavsky (2015) and\nMontavon et al. (2010) in the sense that for well-performing trained networks, the\nquality of the representation increases from layer to layer. Our approach could\nalso be used for addressing generalization (Zhang et al., 2016), (Kawaguchi et al.,\n2017) as we also show that the Besov smoothness of the layer representations of\nthe training set decreases as we add more mis-labeling.", "pdf": "/pdf/86fb4dfbe6a74a5ca6a03c96d9a4b30c2646713d.pdf", "TL;DR": "We propose a Function Space theory approach, that describes the change of the input along the intermediate layers in deep learning architectures", "paperhash": "elisha|using_function_space_theory_for_understanding_intermediate_layers", "keywords": ["deep learning", "representation layers", "Function Space", "wavelets", "approximation", "Besov smoothness"], "authors": ["Oren Elisha", "Shai Dekel"], "authorids": ["orenelis@gmail.com", "shaidekel6@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582955134, "id": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper5/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer1"], "reply": {"forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582955134}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582620643, "tcdate": 1520863746401, "number": 3, "cdate": 1520863746401, "id": "Byq-qWEKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "signatures": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer1"], "content": {"title": "Smoothness (in a learned wavelet basis) is claimed to increase with depth and training time of a neural network.", "rating": "3: Clear rejection", "review": "The paper relies heavily on recent (cited) results of the authors that define and study the function smoothness results and their associated wavelet basis. It is unclear from this manuscript what usefulness the smoothness measure scalar provides in empirical use of networks. Further, the experimental evaluation is not sufficient to make the above claims on smoothness. For example, \"we also see that the smoothness improves after 50 epochs, correlating with the improvement of the accuracy\" is true but not enough to deliver on the stated promise that smoothness could be used to predict accuracy. \n\nThe writing needs further clarifying - it can be difficult to understand sentences like \"The function Space approach is trying to revile the sparsity and the geometric properties of this representation rather than its accuracy.\" ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "USING FUNCTION SPACE THEORY FOR UNDERSTANDING INTERMEDIATE LAYERS", "abstract": "The representational change of input along the intermediate layers is an important\naspect of understanding deep learning architectures. To this end, we propose an\napproach that relies on the foundation of Function Space theory. In particular, we\nargue that a weak-type Besov smoothness index can quantify the geometry of the\nclustering in the feature space of each layer. Therefore, our approach may provide\nan additional perspective for understanding data-models fit in the setting of deep\nlearning. While using a different framework and perspective, the experiments we\nperformed are in line with the results described by Tishby & Zaslavsky (2015) and\nMontavon et al. (2010) in the sense that for well-performing trained networks, the\nquality of the representation increases from layer to layer. Our approach could\nalso be used for addressing generalization (Zhang et al., 2016), (Kawaguchi et al.,\n2017) as we also show that the Besov smoothness of the layer representations of\nthe training set decreases as we add more mis-labeling.", "pdf": "/pdf/86fb4dfbe6a74a5ca6a03c96d9a4b30c2646713d.pdf", "TL;DR": "We propose a Function Space theory approach, that describes the change of the input along the intermediate layers in deep learning architectures", "paperhash": "elisha|using_function_space_theory_for_understanding_intermediate_layers", "keywords": ["deep learning", "representation layers", "Function Space", "wavelets", "approximation", "Besov smoothness"], "authors": ["Oren Elisha", "Shai Dekel"], "authorids": ["orenelis@gmail.com", "shaidekel6@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582955134, "id": "ICLR.cc/2018/Workshop/-/Paper5/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper5/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper5/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper5/AnonReviewer1"], "reply": {"forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582955134}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573595630, "tcdate": 1521573595630, "number": 225, "cdate": 1521573595288, "id": "rk4yJJyqz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyNsMz6rG", "replyto": "HyNsMz6rG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "USING FUNCTION SPACE THEORY FOR UNDERSTANDING INTERMEDIATE LAYERS", "abstract": "The representational change of input along the intermediate layers is an important\naspect of understanding deep learning architectures. To this end, we propose an\napproach that relies on the foundation of Function Space theory. In particular, we\nargue that a weak-type Besov smoothness index can quantify the geometry of the\nclustering in the feature space of each layer. Therefore, our approach may provide\nan additional perspective for understanding data-models fit in the setting of deep\nlearning. While using a different framework and perspective, the experiments we\nperformed are in line with the results described by Tishby & Zaslavsky (2015) and\nMontavon et al. (2010) in the sense that for well-performing trained networks, the\nquality of the representation increases from layer to layer. Our approach could\nalso be used for addressing generalization (Zhang et al., 2016), (Kawaguchi et al.,\n2017) as we also show that the Besov smoothness of the layer representations of\nthe training set decreases as we add more mis-labeling.", "pdf": "/pdf/86fb4dfbe6a74a5ca6a03c96d9a4b30c2646713d.pdf", "TL;DR": "We propose a Function Space theory approach, that describes the change of the input along the intermediate layers in deep learning architectures", "paperhash": "elisha|using_function_space_theory_for_understanding_intermediate_layers", "keywords": ["deep learning", "representation layers", "Function Space", "wavelets", "approximation", "Besov smoothness"], "authors": ["Oren Elisha", "Shai Dekel"], "authorids": ["orenelis@gmail.com", "shaidekel6@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1517261467863, "tcdate": 1517261467863, "number": 5, "cdate": 1517261467863, "id": "HyNsMz6rG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyNsMz6rG", "signatures": ["~Oren_Elisha1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "USING FUNCTION SPACE THEORY FOR UNDERSTANDING INTERMEDIATE LAYERS", "abstract": "The representational change of input along the intermediate layers is an important\naspect of understanding deep learning architectures. To this end, we propose an\napproach that relies on the foundation of Function Space theory. In particular, we\nargue that a weak-type Besov smoothness index can quantify the geometry of the\nclustering in the feature space of each layer. Therefore, our approach may provide\nan additional perspective for understanding data-models fit in the setting of deep\nlearning. While using a different framework and perspective, the experiments we\nperformed are in line with the results described by Tishby & Zaslavsky (2015) and\nMontavon et al. (2010) in the sense that for well-performing trained networks, the\nquality of the representation increases from layer to layer. Our approach could\nalso be used for addressing generalization (Zhang et al., 2016), (Kawaguchi et al.,\n2017) as we also show that the Besov smoothness of the layer representations of\nthe training set decreases as we add more mis-labeling.", "pdf": "/pdf/86fb4dfbe6a74a5ca6a03c96d9a4b30c2646713d.pdf", "TL;DR": "We propose a Function Space theory approach, that describes the change of the input along the intermediate layers in deep learning architectures", "paperhash": "elisha|using_function_space_theory_for_understanding_intermediate_layers", "keywords": ["deep learning", "representation layers", "Function Space", "wavelets", "approximation", "Besov smoothness"], "authors": ["Oren Elisha", "Shai Dekel"], "authorids": ["orenelis@gmail.com", "shaidekel6@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}