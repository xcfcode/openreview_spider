{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730161236, "tcdate": 1509137101842, "number": 912, "cdate": 1518730161225, "id": "H1LAqMbRW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "H1LAqMbRW", "original": "r1BR5GbRZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260079777, "tcdate": 1517250058199, "number": 733, "cdate": 1517250058185, "id": "SJzMLypSf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "There was certainly some interest in this paper which investigates learning latent models of the environment for model-based planning, particularly articulated by Reviewer3.  However, the bulk of reviewer remarks focused on negatives, such as:\n\n--The model-based approach is disappointing compared to the model-free approach.\n--The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling.\n--I feel the paper overstates the results in saying that the learned forward model is usable in MCTS.\n-- the paper in it\u2019s current form is not written well and does not contain strong enough empirical results\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642530438, "tcdate": 1511701672798, "number": 1, "cdate": 1511701672798, "id": "BJ-32VOxf", "invitation": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "signatures": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting way of re-using pre-trained agents with a lot of room for improvement", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning.\nThe idea of re-using a pretrained agent has both pros and cons. On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment. On the other hand, the usefulness of the learned representation for planning is unclear. A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions.\nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation.\u2028In addition to that, one could fine-tune the representation during forward model training. \nIt would be interesting to see if this can improve the results.\n\nI personally miss a more technical and detailed exposition of the ideas. For example, it is not described anywhere what loss is used for learning the model. MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained. \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way.\nI also could find the details on how figure 1 was produced. As I understand, MCTS was not used in this experiment. If so, how would one play with just a forward model?\n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability. Is mini-RTS a deterministic environment? \nWould it be possible to include a non-deterministic baseline in the experimental comparison?\n\nExperimentally, the results are rather weak compared to pure model-free agents. Somewhat unsatisfying, longer-term prediction results into weaker game play. Doesn\u2019t this support the argument about need in stochastic prediction? \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance. \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names\n* Figure 1 that introduces them contains typos. \n* Formatting of figure 8 needs to be fixed. This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642530339, "id": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper912/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer1", "ICLR.cc/2018/Conference/Paper912/AnonReviewer3", "ICLR.cc/2018/Conference/Paper912/AnonReviewer2"], "reply": {"forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper912/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642530339}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642530402, "tcdate": 1511754737742, "number": 2, "cdate": 1511754737742, "id": "B1qenWKxM", "invitation": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "signatures": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Interesting direction of research, but analysis is not complete and exposition is unclear.", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games. The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.\n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.\n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.\n2. Evaluates how informative the latent states are via state reconstruction.\n3. trains variatns of a forward model f on the hidden states of the various learned agents.\n4. evaluates different f within MCTS for MiniRTS.\n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods.\n- The experimental setting is very non-trivial and novel.\n\nCon:\n- The manuscript is unclear in many parts -- this should be greatly improved.\n1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model-free agent?\n2. How is the forward model / value function used in MCTS? I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.\n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X. Yet it is unclear how this informs us about the quality of the learned forward models f. It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix.\n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models. Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8.\n\n- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent. Is this because of the MCTS approach? Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis.\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model?\n- What is the error of the raw h-predictions? Only the state-reconstruction error is shown now.\n- Figure 6 / sect 4.2: which model-free agent is used? Also fig 6 misses captions.\n- Figure 8: scrambled caption.\n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642530339, "id": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper912/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer1", "ICLR.cc/2018/Conference/Paper912/AnonReviewer3", "ICLR.cc/2018/Conference/Paper912/AnonReviewer2"], "reply": {"forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper912/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642530339}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642530362, "tcdate": 1511821235728, "number": 3, "cdate": 1511821235728, "id": "HJh2yfcgz", "invitation": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "signatures": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting approach to learning a model, but underperforms model-free methods", "rating": "4: Ok but not good enough - rejection", "review": "Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS). The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information. They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%).\n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory.\n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available.\n\nCons:\n- The model-based approach is disappointing compared to the model-free approach.\n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct, though I found some of the figures and definitions confusing, specifically:\n\n- The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.). I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly.\n- In Figure 3b, it is not clear to me what the difference between the red and blue curves is.\n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI.\n- The caption in Figure 8 is malformatted.\n- In Figure 7, the baseline of \\hat{h_t}=h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if \\hat{h_t}=h_t) to see how much performance suffers as a result of model error.\n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain. However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features. The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below).\n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS. The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms. The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642530339, "id": "ICLR.cc/2018/Conference/-/Paper912/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper912/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper912/AnonReviewer1", "ICLR.cc/2018/Conference/Paper912/AnonReviewer3", "ICLR.cc/2018/Conference/Paper912/AnonReviewer2"], "reply": {"forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper912/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642530339}}}, {"tddate": null, "ddate": null, "tmdate": 1515177554934, "tcdate": 1515177554934, "number": 1, "cdate": 1515177554934, "id": "rko8LBpXG", "invitation": "ICLR.cc/2018/Conference/-/Paper912/Official_Comment", "forum": "H1LAqMbRW", "replyto": "H1LAqMbRW", "signatures": ["ICLR.cc/2018/Conference/Paper912/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper912/Authors"], "content": {"title": "We thanks reviewers for their comments. ", "comment": "We thank the reviewers for their insightful comments. \n\nOur paper points to an interesting direction that uses learned latent space from model-free approaches as the latent space for dynamics models. In the MiniRTS game, we verified that the latent space that leads to strong performance of model-free methods is both compact and contains crucial information of the game situation, which could be interesting. We agree that the analysis can be done more thoroughly and the final performance (e.g., MCTS with learned dynamics model) is not that satisfactory, compared to model-free approaches. We will continue working on it in the future. \n\nDetails:\nWhat is MiniRTS?\n\nMiniRTS is recently proposed as part of ELF platform [Tian et al (NIPS 2017)]. It is a miniature 2-player real-time strategy game with basic functionality (e.g., resource gathering, troop/facilities building, incomplete information (fog of war), multiple unit types, continuous motion of units, etc). \n\nThe symbols \"MatchPi\", \u201cMatchA\", etc, are now defined properly in the text (paper is updated). \n\nWe have fixed the broken captions of Fig. 8.\n\nR2:\n3. In Fig. 3b, red curves are the value average on won games, while blue curves are on lost games.\n4. \"\\hat{h_t} = h_t\" would be cheating since the baseline would have access to the most recent observation, which the forward modeling does not. Note that the forward model can only access information in the previous frames, say, 2 frames ago. Performance wise, \"\\hat{h_t} = h_t\" would yield higher performance than the learned forward model.\n\nR3:\n1. We have updated the paper to explain different training paradigms (MatchPi etc). \n\n2. How the forward (or dynamics) model is used in MCTS: \nThe forward model is used to predict the future states given the current state. The predicted future state is thus used as the latent representation of child nodes, and so on. This is useful when the game is imperfect information and the game dynamics is unknown (like what MiniRTS is). In comparison, systems like AlphaGo knows the complete information and perfect game dynamics. Other than this difference, the MCTS algorithm is like what AlphaGo does: in each rollout it expands a leaf node to get its value and policy distribution, and use the value to backpropagate the winrate estimation at each intermediate nodes. \n\n3. In Figure 6, the PrevSeen agent is used.\n\n4. We haven't tried scheduled sampling / Dagger (Ross et al.) yet. We acknowledge that this is an interesting direction to explore. \n\nR1:\n1. Fig. 1 is an illustrative fig about different ways of training forward models. Fig. 2 is the training curves for model-free agents and no MCTS is involved.  \n\n2. MiniRTS is indeed an deterministic environment. This means that if all the initial states are fixed (including random seeds), then the game simulator will give exactly the same consequence. However, in the presence of Fog of War (each player cannot see the opponent's behavior if his troops are not nearby), the environment from one player's point of view may not be deterministic. We acknowledge that modeling uncertainty could be a good direction to work on. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent forward model for Real-time Strategy game planning with incomplete information", "abstract": "Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.", "pdf": "/pdf/179b5295fbb1c3bf0b81234694eba23e977e6805.pdf", "TL;DR": "The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.", "paperhash": "tian|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information", "_bibtex": "@misc{\ntian2018latent,\ntitle={Latent forward model for Real-time Strategy game planning with incomplete information},\nauthor={Yuandong Tian and Qucheng Gong},\nyear={2018},\nurl={https://openreview.net/forum?id=H1LAqMbRW},\n}", "keywords": ["Real time strategy", "latent space", "forward model", "monte carlo tree search", "reinforcement learning", "planning"], "authors": ["Yuandong Tian", "Qucheng Gong"], "authorids": ["yuandong@fb.com", "qucheng@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725805, "id": "ICLR.cc/2018/Conference/-/Paper912/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1LAqMbRW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper912/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper912/Authors|ICLR.cc/2018/Conference/Paper912/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper912/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper912/Authors|ICLR.cc/2018/Conference/Paper912/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper912/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper912/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper912/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper912/Reviewers", "ICLR.cc/2018/Conference/Paper912/Authors", "ICLR.cc/2018/Conference/Paper912/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725805}}}], "count": 6}