{"notes": [{"id": "8cpHIfgY4Dj", "original": "rRumf43bxv9", "number": 405, "cdate": 1601308052545, "ddate": null, "tcdate": 1601308052545, "tmdate": 1611607674226, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Guus-yZTSh", "original": null, "number": 1, "cdate": 1610040359401, "ddate": null, "tcdate": 1610040359401, "tmdate": 1610473949450, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Meta-learning for offline RL is an understudied topic with lots of potential impact in the research community. This paper takes the first stab against that challenging problem by proposing a solution similar based on PEARL and distance metric learning. The results look good and it seems like the authors have addressed some of the concerns raised by the reviewers. As a result, I suggest to accept this paper.\n\nHowever, this paper still has some shortcomings as reviewers suggested more baselines with more experiments on standardized benchmarks such as D4RL or RL Unplugged could make the paper stronger."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040359385, "tmdate": 1610473949432, "id": "ICLR.cc/2021/Conference/Paper405/-/Decision"}}}, {"id": "mJeWJ1Mc9vH", "original": null, "number": 7, "cdate": 1606166136274, "ddate": null, "tcdate": 1606166136274, "tmdate": 1606305095708, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "DaTzPKtkzC", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We appreciate that the reviewer recoginizes our well-motivated contribution in addressing offline meta-RL problem, as well as clarify in writing.\n\nTo address your questions:\n\n- \"Format: violation of 8-page limit\":\n\nThank you for bringing this up. We indeed stuggled to fit in all content within the 8-page limit for the first draft and had to put quite a lot in the Appendix. We have strictly sticked to the 9-page rule for the rebuttal version this time.\n\n- \"Missing experiments\":\n\nThis is a good point. **We have now included the experiments on walker in 5.2.3 of the rebuttal version.** The reason we didn't do walker and humanoid (two most complex meta-envs tested by PEARL (Rakelly et al., 2019)) was due to constrained computation budget, and unfortunately within the rebuttal period, we are still short on resources to finish humanoid experiment.  For humanoid, we have already collected over 1TB (100 tasks, over 20 checkpoints, 50 sampled trajectories for each checkpoint) data, unfortunately the complete training/testing procedure is still ongoing. This is something surely we can carefully prepare and add to the camera-ready version should this paper be accepted. \n\nMost importantly, we prioritize walker mainly because it's a unique environment where tasks differ in transition function, whereas all other envs differ in rewards. **With walker included, we think we have now covered all representative cases (sparse reward, different reward/transition functions, binary/near continuous goals) for testing the performance/robustness of FOCAL. So we think the experiments presented in the latest version should be sufficient enough for validation of our algorithm.** Please let us know if you think differently or have any further questions on this. We can experiment with more complex environments for the camera-ready version if the paper is accepted.\n\n- \"Ablations\":\n\nIn fact, we included the said ablation in Appendix E.2 and figure 6 of the first draft. On sparse-point-robot in \nparticular, the behavior regularization has little effect and FOCAL still outperforms BatchPEARL by a large margin. It is our fault that this is not in the main text since we really struggled to make space for everything within the page constraint. We now have added such ablation on walker env in 5.2.3, our method FOCAL still clearly outperforms the baseline.\n\n- \"Novelty\":\n\nWe agree that our main framework builds on PEARL by using DML and BRAC, but it is by no means a naive combination of the existing methods. As we stressed in the main text (like last paragraph of section 1, first paragraph of section 4), we consider the novelty/contribution of our method as twofold:\n  1. Proposes the first model-free and end-to-end algorithm that successfully address the important and practical (reviewer quote: \"well-motivated\") offline meta-RL problem.\n  2. Incorporate several key design choices customized for offline meta-RL setting, all well-motivated and different from SOTA baselines such as PEARL:\n\n    i. Deterministic context encoder instead of a probabilistic variant adopted in SOTA baselines such as PEARL\n\n    ii. Novel Negative power variant of the metric loss instead of common L1, L2 objective\n  \n    iii. Decoupling the task inference training from control policy using the metric loss\n  \n- minor comments\n\nAlso, to address your concerns in the minor comments, we finally obtained permission to publish the code and now it's in the supplementary material. We also recovered some missing data points in Fig 3 from log, and indeed there are moderate performance deterioration on environments like sparse-point-robot and half-cheetah-vel. Accordingly, we have included our insight and discussion regarding this observation in 5.2.1, hope this helps.\n\nOverall, we are really grateful for your comments and advice, which are all valuable in helping us improve the work. Hope our explanation and updated material address your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "LLn3SFxNYs", "original": null, "number": 10, "cdate": 1606302966543, "ddate": null, "tcdate": 1606302966543, "tmdate": 1606304844376, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Updates", "comment": "Dear Readers,\n\nWe have updated our final rebuttal version as well as the source code as supplementary material, including the following major revisions:\n\n1. Added full-range return curves for all five meta-envs in Fig 6 (Appendix D.1 ). **On all envs, FOCAL outperforms baselines by a large margin with no significant performance deterioration.**\n\n2. Updated Half-Cheetah-Vel return curve with smaller meta batch size (80 -> 16) in Fig 3, to be consistent with our other experiments. The relevant hyperparameters for reproducing the result is also updated in Table 3. **Like our previous updates on Sparse-Point-Robot, no significant performance deterioration is observed this time**.\n\n3. At request of the reviewers, we further strengthen our discussion in the experiments section. Particullay in 5.1 and 5.2.1, we provide in-depth analyses on characteristics of our offline meta-RL algorithm. Since reviewers are concerned with limitation of our Assumption 1, which we totally concur and provide argument on why this is important. **However, we also stress based on the experimental evidence that overall, FOCAL is able to achieve excellent performance and robustness even when Assumption 1 is not satisfied. We would like to think Assumption 1 more as a sufficient codition and strong abstration which gives intuitive theoretical insight on why FOCAL works well in OMRL setting. But the message we got from the experimental results is that it's not necessary for FOCAL to work well on many (real-world) tasks where Assumption 1 is never strictly satisfied.**\n\n4. Updated description of meta-envs in Appendix D.2 to explain how tasks differ, particularlly in reward or transition functions.\n\nAgain thank you all for your valuable input. Since we have not received any feedback regarding our rebuttal so far, we have done our best based on the previous reviews to work towards a stronger submission. Hope the rebuttal version can help make things more clear. We look forward to your comments and final decision."}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "T9Lmwyk23I", "original": null, "number": 6, "cdate": 1606164285169, "ddate": null, "tcdate": 1606164285169, "tmdate": 1606304531972, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "Jt42PVBWdKp", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for recognition of several key elements of our paper:\n  1. Well-motivated contribution in addressing the important and practical problem of offline meta-RL\n  2. Insight of several key design choices customized for offline meta-RL setting (elaborated in the three ablations):\n\n    i. Deterministic context encoder instead of a probabilistic variant adopted in SOTA baselines such as PEARL\n\n    ii. Novel Negative power variant of the metric loss instead of common L1, L2 objective\n  \n    iii. Decoupling the task inference training from control policy using the metric loss\n  \n  3. Comprehensive experimental evidence for design choices listed above (section 5.2)\n\nTo address your questions:\n\n- \"key assumptions is limiting\": \n\nFirst of all, we fully concur that this is a very good point. We realize that in real-world envs, Assumption 1 can almost never be strictly satisfied. **In response, we have added in-depth discussion in 5.1 and 5.2.1, to show that although Assumption seems like a severe constrant to the scope of FOCAL, the experimental evidence demonstrated that the algorithm is robust enough to achieve good performance without strictly satisfying Assumption 1, like on Sparse-Point-Robot with sparse reward for example.** \n\nFormally, we would like to consider Assumption 1 a sufficient condition for the feasibility of our permutation-invariant and deterministic context encoder design. It provides intuitive theoretical insight but is too strong (not necessary) for the algorithm to work well. On the other hand, Assumption is still reasonable for tasks associated with unique reward/transition functions, which covers 4 out of 5 of the meta environments we tested: Half-Cheetah-Vel, Ant-Fwd-Back, Half-Cheetah-Fwd-Back and Walker-2D-Params. Therefore our analyses based on such assumption still has value in its own right.\n\nIf we think FOCAL as one step closer to general/robust model-free OMRL algorithm, then this \"limiting factor\" should be looked more from the bright side. As discussed in the conclusion section, we totally concur that this is exactly the point (or at least one of the most important) where FOCAL can be improved upon in future studies. We consider techniques for more robust task inference, potentially including but not restricted to such as active learning of task labels, attention mechanism of transition sequence or certain stochasticity for quantifying the uncertainty introduced by the confounding samples as possible future directions to improve FOCAL.   \n\n- \"Not convinced by Appendix C\":\n\nWe totally agree that the decoupled training, as one of the key design choices of FOCAL (5.2.3, Appendix E.2), makes it easier to learn effective task representation on embedding space. But we think it is compatible with our argument in Appendix C. The point of Appendix C is to show that without effective separation of different tasks realized by technique such as DML, it's difficult or even impossible for neural networks to accurately approximate z-conditioned value functions across multiple tasks simultaneously. The DML and decoupled training are synergetic by design.\n\nOverall we are really grateful for you appreciation of our work and recognizing its key contributions. We hope this response addresses your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "93HbWcD6zYo", "original": null, "number": 9, "cdate": 1606168265253, "ddate": null, "tcdate": 1606168265253, "tmdate": 1606304404165, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "3YLkYsLHCUB", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Response to Reviewer 3 - Part 2", "comment": "- \"Add more details on reproducing the results, such as the setup of 'ContextualBCQ':\n\nIn the rebuttal version, we describe \"ContextualBCQ\" as \"By incorporating a latent space z in the state space for task inference, it can be interpreted as the task-augmented variant of offline BCQ algorithm (Fujimoto et al., 2019). Like PEARL, the\ntask inference module is trained using Bellman gradients.\" We directly adopted ContextualBCQ and DistilledBCQ from the cited paper \"Multi-task Batch Reinforcement Learning withMetric Learning\" (Li et al., 2019b) and implemented the algorithms on our own datasets. Since we directly used the linked source code for reproducing the result in Fig 3, for further information, we would encourage the reviewers to refer to\nhttps://papers.nips.cc/paper/2020/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf for experimental details. Please let us know if you have further questions on this issue.\n\n- miscellaneous (add more discussion in experiments section):\n\nThis is a good point, we didn't do this in the first draft because we really struggled to make space for everything within the 8-page limit. Now with 1 more page, please check section 5 of the latest version for more discussion (**also our latest updates for all readers**), as well as one more ablation study of FOCAL vs. Batch PEARL + BRAC at the request of reviewr 3 & 4. Also now we have attached the appendix as part of the main pdf, it should be easier for you to link the related content. Please check Appendix D for the experimental details. We found it's the most appropriate place to present them. If you still think anything should be added to the main tex or the appendix, please feel free to  let us know.\n\nOverall, we are really grateful for your comments and advice, which are all valuable in helping us improve the work. Hope our explanation and updated material address your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "3YLkYsLHCUB", "original": null, "number": 8, "cdate": 1606167083345, "ddate": null, "tcdate": 1606167083345, "tmdate": 1606304287854, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "AIwEl3yyx9H", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Response to Reviewer 3 - Part 1", "comment": "We appreciate that the reviewer recoginizes our well-motivated contribution in addressing offline meta-RL problem, as well as clarify in writing.\n\nTo address your questions:\n\n- \"key assumption is infeasible in a practical setting\":\n\nFirst of all, we fully concur that this is a very good point. We realize that in real-world envs, Assumption 1 can almost never be strictly satisfied. **In response, we have added in-depth discussion in 5.1 and 5.2.1, to show that although Assumption seems like a severe constrant to the scope of FOCAL, the experimental evidence demonstrated that the algorithm is robust enough to achieve good performance without strictly satisfying Assumption 1, like on Sparse-Point-Robot for example.** \n\nFormally, we would like to consider Assumption 1 a sufficient condition for the feasibility of our permutation-invariant and deterministic context encoder design. It provides intuitive theoretical insight but is too strong (not necessary) for the algorithm to work well. On the other hand, Assumption is still reasonable for tasks associated with unique reward/transition functions, which covers 4 out of 5 of the meta environments we tested: Half-Cheetah-Vel, Ant-Fwd-Back, Half-Cheetah-Fwd-Back and Walker-2D-Params. Therefore our analyses based on such assumption still has value in its own right.\n\nIf we think FOCAL as one step closer to general/robust model-free OMRL algorithm, then this \"limiting factor\" should be looked more from the bright side. As discussed in the conclusion section, we totally concur that this is exactly the point (or at least one of the most important) where FOCAL can be improved upon in future studies. We consider techniques for more robust task inference, potentially including but not restricted to such as active learning of task labels, attention mechanism of transition sequence or certain stochasticity for quantifying the uncertainty introduced by the confounding samples as possible future directions to improve FOCAL.   \n\n- \"Novelty\":\n\nWe agree that our main framework builds on PEARL by using DML and BRAC, but it is by no means a naive combination of the existing methods. As we stressed in the main text (like last paragraph of section 1, first paragraph of section 4), we consider the novelty/contribution of our method as twofold:\n\n  1. Proposes the first model-free and end-to-end algorithm that successfully address the important and practical (reviewer quote: \"well-motivated\") offline meta-RL problem.\n\n  2. Incorporate several key design choices customized for offline meta-RL setting, all well-motivated and different from SOTA baselines such as PEARL:\n\n    i. Deterministic context encoder instead of a probabilistic variant adopted in SOTA baselines such as PEARL\n\n    ii. Novel Negative power variant of the metric loss instead of common L1, L2 objective\n  \n    iii. Decoupling the task inference training from control policy using the metric loss\n\n- \"More Ablations\":\n\n    1. \"Evaluating how well the context-encoder performs in inferring the meta-test tasks? \"\n    \n    We have provided evaluation of context-encoder for task inference with both visualization and statistics, not specifically but along with ablations. Figure 4(a), 5(a) illustrate the test-performance of clustering of embedding vectors generated by the proposed encoder compared to other design choices. Table 1 shows the test-performance effective separation rate (ESR) across different distance metrics, among which our proposed encoder design performs the best. In our opinion, these are direct evidence of \"how well the context-encoder performs in inferring the meta-test tasks\". We also provided theoretical insight on how effective clustering of embedding vectors enables better approximation of values functions by continuous neural networks in Appendix C. We woud be glad to provide further clarification or evidence if there is any misunderstanding.\n\n    2. \"Ablating the choice of encoder\"\n\n    This is a good point. In fact, we already included the said ablation in Appendix E.2 and figure 6 of the first draft. On sparse-point-robot in particular, the behavior regularization has little effect and FOCAL still outperforms BatchPEARL by a large margin. We are sorry that these were not in the main text since we really struggled to make space for everything within the page constraint. **For rebuttal revision, we have now added such ablation on walker env in 5.2.3, FOCAL still clearly outperforms the baseline.**\n\n- \"Ablations in 5.2.2 (power law of distance metric loss) not well-motivated\":\n\nThe point of 5.2.2 is to demonstrate one of our key design choices or novelty points, that using carefully designed metrics such as \"negative-power\" losses are superior than naive L1, L2 losses (contrastive loss in (12)) for task inference in meta-RL, which are positive powers. As a result of this study, we chose negative-power form in (13) over (12). We don't see why this is not well-motivated but are open to further discussion. \n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "DaTzPKtkzC", "original": null, "number": 2, "cdate": 1603979979043, "ddate": null, "tcdate": 1603979979043, "tmdate": 1606299329621, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Review", "content": {"title": "Review", "review": "Thanks for the feedback. However, I think most of the performance gain came from behavior regularization, which is an already existing technique, rather than the proposed metric learning method (Figure3 and Figure 5). I\u2019ll keep my ratings unchanged.\n\n============\n\nSummary: \nThe paper presents a mode-free, end-to-end offline meta-reinforcement learning algorithm.\nThe authors proposed the context encoder to encode the task from the history of transitions (called the \u201ccontext\u201d) using metric-based approaches. Specifically, they embedded the context to a latent vector z by clustering similar data points (context) taken from the same task while pushing away dissimilar data points taken from different tasks. The latent context variable z is used to condition the actor and the critic.\nExperiments are performed on the MuJoCo simulator and the 2D navigation problem with a sparse reward called Sparse-Point-Robot, using different reward functions depending on the task. The method proposed, FOCAL (Fully-Offline Context-based Actor-critic meta-rL algorithm), outperforms other offline meta-RL algorithms.\nThe main contribution of the paper comes from combining the context-based approach with metric learning.\n \nPros: \n- Overall, the paper is well written. The method of using metric learning has been well-motivated by toy experiments on the context embedding. \n- The paper tackles offline reinforcement learning and meta-learning, which are emerging topics in RL/ML.\n \nCons:\n- Format: The paper violated the paper length limit of 8 pages for the main context. \n- Missing experiments: The paper directly followed the algorithm and the experiment setting of \u201cEfficient Off-Policy Meta-RL\u201d (Rakelly et al., 2019) while omitting experiments on the MuJoCo walker and humanoid environments. Also, for the walker-2d experiment, only the toy experiment on context embedding is available with no return curve.\n- Ablations: The authors compare their method (FOCAL) to Batch PEARL while describing the Batch PEARL as \u201ca vanilla version of FOCAL without behavior regularization or distance metric learning.\u201d It would be interesting to compare FOCAL and Batch PEARL both with the behavior regularization to observe the effect of metric learning in isolation. \n- Novelty: The algorithm in Appendix A seems incremental to the algorithm in \u201cEfficient Off-Policy Meta-RL\u201d (Rakelly et al., 2019). The only difference is that the setting is offline and using metric learning to encode the context instead of KL-divergence. Although the combination of context encoding and metric learning is interesting, the proposed is method is still incremental. \n\n\nMinor comments:\n- There was no source code when I followed the Github link provided in the paper.\n- In the return curves on Half-Cheetah-Vel and Ant-Fwd-Back in Figure 3, there is no value near 1e6 sample steps, where the deterioration of performance begins.\n- I understood that all tasks share the same state and action space, with different transition and reward functions. But all the experiments in the paper use the same transition. I wonder if there is an experiment with a different transition depending on the task.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper405/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143883, "tmdate": 1606915761033, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Review"}}}, {"id": "MHZDz9ArOtZ", "original": null, "number": 5, "cdate": 1606162213626, "ddate": null, "tcdate": 1606162213626, "tmdate": 1606233667212, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment", "content": {"title": "Annoucement To All Reviewers and AC/PCs", "comment": "We really appreciate your efforts in helping us reflect and improve the paper. In light of the reviewers' comments, we made the following major revision based on the previous manuscript (now submitted as the rebuttal version):\n1. Added ablation of training context encoder via DML vs. Bellman gradients (equivalent to FOCAL vs. Batch PEARL with BRAC) in 5.2.3, as a stronger baseline compared to Batch PEARL without BRAC \n2. Added return curve on walker env also in 5.2.3, where tasks have different transition functions rather than reward functions (as in other envs we experimented with)\n2. Added more extensive discussion on the experimental section (section 5), including our insight on some of the failure cases (overfitting , performance deterioration) of FOCAL\n4. Updated return curve on sparse-point-robot in Fig 3 with our latest data obtained this weekend, no significant deteriotation this time. We also connected the missing dots on all return curves in Fig 3, from the data we already had before. Especially for FOCAL and Batch PEARL, now the data points should be complete.\n5. Uploaded source code and part of the data required for reproducing the key results in main text\n6. Attached the appendix to the main text,  should make things easier for readers to read and link relevant content\n\nThank you all again for your time and valuable advice! We will adresss each reviewer's individual question momentarilly. Feel free to let us know if you have any comments/questions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8cpHIfgY4Dj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper405/Authors|ICLR.cc/2021/Conference/Paper405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Comment"}}}, {"id": "Jt42PVBWdKp", "original": null, "number": 1, "cdate": 1603871897363, "ddate": null, "tcdate": 1603871897363, "tmdate": 1605024696674, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Review", "content": {"title": "Review", "review": "This paper tackles the problem of offline meta reinforcement learning, where an agent aims to learn a policy which can adapt to an unseen task (dynamics/reward), but from entirely offline data. As a result of being fully offline, the agent can no longer explore in the new task at test time, but instead receives randomly sampled transitions from the new task, from which it must infer the task. They then propose a method for learning task inference from fully offline data, as well as a policy conditioned on this task encoding from offline data built off behavior regularized actor critic (BRAC). Results indicate that in this outperforms PEARL as well as multi-task offline RL with BCQ.  \n\nPros:\nThe problem this paper tackles is important, and is formulated in a practical way. While in general meta-RL is often closely tied to online exploration to infer what the task is, in practice it seems like having an agent be able to adapt to a new task given just a few transitions from a potentially different policy would be a valuable instantiation of meta-RL. It also integrates nicely with progress in batch RL. \n\nSecond, the idea of decoupling the task inference objective using the metric loss does nicely integrate with the setting of using an offline dataset, and seems to work well as shown by the qualitative examples. While it does depend heavily on having access to the task labels during training, this assumption is used in a lot of meta-RL work. Additionally, the proposed negative power variant of the metric loss does seem to better encourage separation of tasks with different task labels, and seems like a good tool for metric learning in general.\n\nLastly, the experiments do compare against the relevant baselines to the best of my knowledge, and on tasks similar to those used in prior works. The ablations are also interesting, and make a compelling case for why in the offline case deterministic context encoders may be more fitting (when the tasks satisfy Assumption 1).\n\nCons:\nIn general I think the key assumption in this work, that tasks can be inferred from an unordered set of transitions is limiting, especially for tasks with sparse reward, or which involve adapting to changing dynamics. However, this does seem like it is the best that one can do while being totally offline, any more complex tasks would require some form of online exploration.\n\nI'm not convinced of the argument in Appendix C, that the reason why using the control policy to learn the context encoder works poorly is due to neural networks inability to tell apart small differences in the embedding vectors. Rather I think the decoupled training allows for an easier optimization of the task embedding, and then the policy can be learned using good (and stationary) task embeddings throughout all training. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper405/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143883, "tmdate": 1606915761033, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Review"}}}, {"id": "AIwEl3yyx9H", "original": null, "number": 3, "cdate": 1604029033125, "ddate": null, "tcdate": 1604029033125, "tmdate": 1605024696552, "tddate": null, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "invitation": "ICLR.cc/2021/Conference/Paper405/-/Official_Review", "content": {"title": "Proposes novel algorithm for offline meta-reinforcement learning", "review": "Summary : \nThe paper studies meta-reinforcement learning in the fully offline setting, and proposes a novel algorithm 'FOCAL'. Given offline datasets for tasks sampled from some prior, the algorithm learns a context encoder using distance-based metrics. The encoder is used for inferring the task-latent $z$, which is used to condition the policy rollout $\\pi(a | s, z)$. They demonstrate experiments where FOCAL outperforms baselines like PEARL.\n\nReasons for score :\nWhile I concur with the motivations of the paper, I vote for rejecting the paper (marginally below acceptance threshold). My main concern is the limited novelty of the proposed solution and some missing ablations (c.f weaknesses). I encourage the authors to incorporate feedback for the reviewers and work towards a stronger submission.\n\nStrengths:\n+ The paper is well written and quite easy to follow. The problem is sufficiently well motivated, and the algorithm builds on previous approaches, particularly PEARL/BRAC.\n+ The proposed algorithm seems to outperform reasonable baselines on few navigation/Mujoco tasks in the offline setting.\n\nWeaknesses:\n- As the authors identify, the assumption of task-identifiability from any transition (s, a) seems infeasible in a practical setting. This significantly reduces the scope of the algorithm. The novelty of the proposed algorithm is also quite limited it builds on the framework of PEARL by using (i) distance-based metrics for task inference, which have been studied in the multi-task literature (ii) offline policy learning, where it uses off-the-shelf baselines from the offlineRL literature.\n- The authors include few ablations, I believe adding one/more of the following would improve the quality of discussion:\n       (i) Evaluating how well the context-encoder performs in inferring the meta-test tasks? \n       (ii) Ablating the choice of encoder : Comparing the performance of BatchPEARL, where the encoder is still learned with Bellman gradients, while the policy is behaviorally regularized. I consider this to be a stronger baseline than the existing version of BatchPEARL.\n- Experiments pertaining to 5.2.2 in the ablations are not well-motivated. Given the contrastive loss definition in equation(13), it follows that the \u201cinverse-power\u201d losses learn reasonable task embeddings. In my understanding, n>0 here invalidates the choice of \u201clinear\u201d/\u201dsquare\u201d objectives. I\u2019d be happy to be corrected if I misunderstood the setup. \n- The authors are encouraged to add details sufficient to reproduce the results. In particular, the setup of the baseline \u201cContextualBCQ\u201d is scant on details.\n\nMiscellaneous:\n- The paper has several minor grammatical errors which could be corrected in the final draft.\n- I would encourage the authors to add more discussion to the experiments section. The current version reflects the empirical results, but provides little insight into them. Last half of section 5.1 has some mention on the importance of choice of datasets, but doesn\u2019t provide details (until in the Appendix).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper405/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper405/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "distance metric learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "/attachment/bf956de8abf7f9cc2e9a2d2f893c465fca5ff7cb.zip", "pdf": "/pdf/8759f4f04dacf9bc36c44be92d71dd245bc1261c.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nli2021efficient,\ntitle={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8cpHIfgY4Dj", "replyto": "8cpHIfgY4Dj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143883, "tmdate": 1606915761033, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper405/-/Official_Review"}}}], "count": 11}