{"notes": [{"id": "rygkm5F9lN", "original": null, "number": 6, "cdate": 1545406998526, "ddate": null, "tcdate": 1545406998526, "tmdate": 1545406998526, "tddate": null, "forum": "SkNksoRctQ", "replyto": "HyeGmT71eV", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "content": {"title": "Metareply", "comment": "Thank you for accepting the paper! We believe that these points have been fully addressed during the rebuttal period and that the revised paper provides in-depth explanation of assumptions, comes with stronger experimental results, and offers the writing that is hopefully easier to follow than the original."}, "signatures": ["ICLR.cc/2019/Conference/Paper588/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper588/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620581, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNksoRctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper588/Authors|ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620581}}}, {"id": "SkNksoRctQ", "original": "r1lACSrOFQ", "number": 588, "cdate": 1538087831396, "ddate": null, "tcdate": 1538087831396, "tmdate": 1545406348285, "tddate": null, "forum": "SkNksoRctQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HyeGmT71eV", "original": null, "number": 1, "cdate": 1544662298218, "ddate": null, "tcdate": 1544662298218, "tmdate": 1545354504410, "tddate": null, "forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Meta_Review", "content": {"metareview": "The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper588/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper588/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353162336, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper588/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353162336}}}, {"id": "SkxIvGxF6X", "original": null, "number": 3, "cdate": 1542156893679, "ddate": null, "tcdate": 1542156893679, "tmdate": 1542156893679, "tddate": null, "forum": "SkNksoRctQ", "replyto": "rJlcCDd_2m", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "content": {"title": "Equation (6) made more transparent; equation (12) made more explicit; harmonic approximation defined.", "comment": "Thank you very much for your clarifying comments! Please see below for our responses to your comments (and note our responses to other reviewers as well, especially the discussion with the Reviewer 1 which led to additional experiments).\n\n\n(1) We fully agree that the equation (6) was very confusing. Please see the revised version, wherein we have unrolled the derivation more explicitly. In particular, right before the equation (6), we now state how a probability distribution of model parameters evolves in general, which is used to make the second identity easier to follow; the last identity is just an integration over theta against the delta function. Please let us know if the derivation is still unclear.\n\n\n(2) We have added one middle step in the equation (12) so as to make the deduction process more transparent. We also explicitly allude to the facts that we are Taylor-expanding in the learning rate eta and that the equation (4) has been used. Please let us know if it is still not clear.\n\n\n(4) The first sentence of the Section 2.3.1 now explicitly states what we mean by the harmonic approximation and also defines theta^*.\n\n\n(3) The ways in which the second fluctuation-dissipation relation help to determine the properties of loss-function landscape have been stipulated right after the equation (FDR2) and in the second paragraph of Section 3.2 (along with the figure 3). Before explaining them below in a way slightly different from the draft, let us emphasize that properties exposed by (FDR2) are far from constituting the exhaustive list of loss-function landscape's properties which people would care [e.g., low eigenvalues of loss-function landscapes are essentially invisible to (FDR2)].\n\n(3-I) The Hessian H (~curvature) of a local minimum is one property of the loss-function landscape and Tr(H\\tilde{C}) gives one particular slice of this information. Now, computing this quantity naively would entail the computational cost that scales as P^2 where P is a number of model parameters. The left-hand side of (FDR2), in contrast, has the computational cost that scales as P and hence allows the computation of Tr(H\\tilde{C}) for large models that is more efficient than the direct computation.\n\n(3-II) The more nontrivial result is the fact that the nonlinearity of <grad(f)^2> as a function of the learning rate eta tells the breakdown of the idealized assumption on the loss-function landscape (such as the harmonic approximation, the breakdown of which reflects the degree of nonconvexity of the landscape). The inappropriateness (when so) of such idealized assumptions on the landscape is another property that (FDR2) exposes [which we found to be vividly pronounced for CIFAR-10 experiments even at fairly small learning rates: please see the figure 3(b)].\n\n\nWe hope that all the confusing points have now been resolved and that, as a result, the paper is easier to follow than before. We thank you again and would appreciate further clarification if any."}, "signatures": ["ICLR.cc/2019/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620581, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNksoRctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper588/Authors|ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620581}}}, {"id": "BJxciglKpm", "original": null, "number": 2, "cdate": 1542156450154, "ddate": null, "tcdate": 1542156450154, "tmdate": 1542156450154, "tddate": null, "forum": "SkNksoRctQ", "replyto": "SylFlge92Q", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "content": {"title": "Adam+AMSGrad added; further optimization of adaptive scheduling hyperparameters performed; stationarity caveats clarified", "comment": "Thank you so much for your very constructive comments! Please see below for our responses to your comments (and note our responses to other reviewers as well).\n\n\n(1) According to your suggestion, we have carried out two additional sets of experiments.\n\n(1-I) First, we carried out the experiments with Adam/AMSGrad [J. Reddi et al. (2018)] with the default hyperparameters. As we found AMSGrad to be marginally better than Adam for the task at hand (see newly added Appendix C if wished), we included the AMSGrad in the main text. Please see the figures 4(a) and 4(b) in the revised version. For the MNIST classification task with MLP, the proposed adaptive algorithm outperforms AMSGrad in terms of accuracy attained at long time and also has a quick convergence. For the CIFAR-10 classification task with CNN, the proposed adaptive algorithm again outperforms AMSGrad in terms of accuracy attained at long time, but its initial accuracy gain is visibly slower than AMSGrad.\n\n(1-II) Second, given the initial speed lag stipulated above for the CIFAR-10 task, we further carried out an extra hyperparameter search. We in turn found that a combination of the scheduling hyperparameters X and Y that outperforms AMSGrad in terms of long-time accuracy while also competing well in terms of initial accuracy gain (see Appendix C if wished). However, since it would be unfair to claim victory with the \"task-specific-finely-tuned\" adaptive scheduling against the \"out-of-the-box\" preset scheduling and AMSGrad, in the main text we still present the original results (+AMSGrad) with the \"out-of-the-box\" X and Y.\nThese two additional experiments support the advantage of the adaptive method proposed herein, at least for the image classification tasks described in the draft. This is not to claim the universal applicability of the proposed method and further tests on the near state-of-the-art architectures on image classification tasks and also NLP tasks (for which Adam is often reported to be better than scheduled SGD) should be carried out.\n\n\n(2) The question of whether or not the SGD, viewed as a time-homogeneous Markov chain, holds stationary distributions is interesting, especially in the absence of the convexity assumption on loss-function landscapes. Our approach is empirical -- i.e., we prove and experimentally check various necessary conditions for stationarity, including the relation (FDR1) (c.f. Section 3.1) in addition to plateaus of loss and theta^2 (which we checked), and only then use its consequences [e.g. the relation (FDR2) to probe the landscape properties and (FDR1) to adaptively reduce the learning rate]. Figuring out under which conditions such stationary distributions exist is nontrivial. Specifically, if its formulation is too generic so as to include all the network architectures, then we would find a counterexample to such a putative existence statement. The case in point is a model with the cross entropy loss without regularizer, whose model parameters logarithmically diverge and never reach stationarity [please see footnote 2 of Neyshabur et al. (2014), page 4 of Neyshabur et al. (2017), and figure 1(B) of Soudry et al. (2017), which are all cited in the draft now]; there essentially is no local minimum with finite model parameters. That being said, this is the only class of nontrivial counterexamples that comes to our minds, and the said cascade can be avoided by the L^2 regularization.  \n\nGiven the comment, we nonetheless have extensively revised the Section 2.3.4 which can be interpreted as another paraphrased response to your comment.\n\n\nWe believe that both of the two concerns have now been properly addressed, which in turn strengthened our results (especially due to the inclusion of Adam/AMSGrad algorithms). We thank you again for constructive comments and would appreciate further feedback if any."}, "signatures": ["ICLR.cc/2019/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620581, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNksoRctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper588/Authors|ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620581}}}, {"id": "r1eky6kYTX", "original": null, "number": 1, "cdate": 1542155479142, "ddate": null, "tcdate": 1542155479142, "tmdate": 1542155479142, "tddate": null, "forum": "SkNksoRctQ", "replyto": "Bkxa5zr52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "content": {"title": "Figure 2 revised; Mandt et al. (2015) cited.", "comment": "Thank you very much for your generous review! Please see below for our responses to your comments (and note our responses to other reviewers as well, especially the discussion with the Reviewer 1 which led to additional experiments).\n\n(1) The short answer is that \"no, it does not assume that the optimum is at theta=0; please see the comment after the equation (FDR1').\" In more detail: as you note, the expression first looks translationally non-invariant, picking up the origin as a special point. However, due to <grad(f)>=0, the reference point of theta fluctuations can be shifted to anywhere one likes [i.e. replace theta*grad(f)->(theta-theta_{ref})*grad(f) with arbitrary choice of constant theta_{ref}] without affecting the stationary-state average value of O_L, as also mentioned in the footnote 4. (In practice, numerical accuracy/convergence speed to the stationary value can depend on a choice of the reference point.)\n\n(2) We agree that the distinction between solid and dotted curves were not so visible. After playing around with figure parameters to increase the contrast, we decided to reverse dotted and solid (now O_L is solid and O_R is dotted) and further made O_R thicker and colored lighter.\n\n(3) Thank you for bringing up a missing reference by Mandt et al.,  which we have now included along with their longer journal publication in 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620581, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNksoRctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper588/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper588/Authors|ICLR.cc/2019/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers", "ICLR.cc/2019/Conference/Paper588/Authors", "ICLR.cc/2019/Conference/Paper588/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620581}}}, {"id": "Bkxa5zr52Q", "original": null, "number": 3, "cdate": 1541194388792, "ddate": null, "tcdate": 1541194388792, "tmdate": 1541533863158, "tddate": null, "forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "content": {"title": "An innovative paper to assess equilibration in SGD", "review": "The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. \n\nOne of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations.\n\nThis is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments.\n\nI have only a few questions / comments:\n\n1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0?\n2. In Fig. 2, the distinction between solid and dotted curves could be made better visible.\n3. For completeness, it would be good to add the following citation:\nStephan Mandt, Matthew D. Hoffman, and David M. Blei. \"Continuous-time limit of stochastic gradient descent revisited.\"\u00a0NIPS 2015 Workshop on Optimization for Machine Learning.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper588/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "cdate": 1542234425620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335758402, "tmdate": 1552335758402, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylFlge92Q", "original": null, "number": 2, "cdate": 1541173233045, "ddate": null, "tcdate": 1541173233045, "tmdate": 1541533862955, "tddate": null, "forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "content": {"title": "Inspired by statistical mechanics, the authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD. They further use the relations to set training schedule adaptively and analyze the loss-function landscape. However, the analysis about the stationarity assumption is insufficient and some experiments are weak.", "review": "The authors establish a stationary fluctuation-dissipation theorem and derive two specific fluctuation-dissipation relations. The authors use the first relation to check the stationarity and the second relation to delineate the shape of the loss-function landscape.\nTo verify their claim, the authors further use the relations to set the learning-rate schedule adaptively in SGD. \n\nMy major concerns are as follows.\n\n1. The experiments in subsection 3.3 are not convincing. The authors compare the proposed adaptive training schedule with a preset training schedule. However, the improvement by the proposed schedule is insignificant.\nTo make this paper more convincing, the authors may want to compare the proposed adaptive training schedule with other approaches that have dynamic learning rates, such as those mentioned in [1].\n\n2. The derived relations are based on the stationarity assumption. However, there are few discussions on when this assumption will hold. The authors may want to analyze the conditions for the assumption to hold and explain why imposing L^2-regularization can ensure stationarity.\n\nThis paper will be more convincing if the above issues are addressed properly, and I will be happy to raise my score.\n\n[1] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv: 1609.04747, 2017.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper588/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "cdate": 1542234425620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335758402, "tmdate": 1552335758402, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlcCDd_2m", "original": null, "number": 1, "cdate": 1541076946192, "ddate": null, "tcdate": 1541076946192, "tmdate": 1541533862750, "tddate": null, "forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "content": {"title": "A nice attempt to understand the stationary equilibrium of SGD but the paper not easy to follow", "review": "Understanding the stationary equilibrium helps to understand the practical performance of stochastic gradient descent. In this paper, the authors propose two fluctuation-dissipation relation to link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. An advantage over the existing study is that the results here hold for any stationary state and do not need the analogy with continuous-time differential equations. Empirical results are also reported to verify these fluctuation relation.\n\nComments:\n\n(1) I do not quite understand the second identity of (16). In particular, it seems that the authors replace the first two $\\theta(t+1)$ with $\\theta(t)$, and do not use this replacement for the third $\\theta(t+1)$ (this was addressed by (1)).\n\n(2) It would be helpful to the readers if the authors can give the deduction process of (12). It is not easy for me to understand how it holds.\n\n(3) It is not clear to me how the second fluctuation-dissipation relation helps to determine the properties of loss function landscape.\n\n(4) In Section 2.3.1, can you give some explanation for the harmonic approximation. Also the notation $\\theta^*$ seems not to be defined.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper588/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fluctuation-dissipation relations for stochastic gradient descent", "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.", "keywords": ["stochastic gradient descent", "adaptive method", "loss surface", "Hessian"], "authorids": ["shoyaida@fb.com"], "authors": ["Sho Yaida"], "TL;DR": "We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.", "pdf": "/pdf/2b424796c43cea9eee25004714c27e18a6e9fc18.pdf", "paperhash": "yaida|fluctuationdissipation_relations_for_stochastic_gradient_descent", "_bibtex": "@inproceedings{\nyaida2018fluctuationdissipation,\ntitle={Fluctuation-dissipation relations for stochastic gradient descent},\nauthor={Sho Yaida},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNksoRctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper588/Official_Review", "cdate": 1542234425620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNksoRctQ", "replyto": "SkNksoRctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper588/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335758402, "tmdate": 1552335758402, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper588/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}