{"notes": [{"id": "HkElFj0qYQ", "original": "rJev9qcqKQ", "number": 416, "cdate": 1538087800467, "ddate": null, "tcdate": 1538087800467, "tmdate": 1545355376093, "tddate": null, "forum": "HkElFj0qYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 34, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SklNuUfzlE", "original": null, "number": 1, "cdate": 1544853099538, "ddate": null, "tcdate": 1544853099538, "tmdate": 1545354532592, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Meta_Review", "content": {"metareview": "This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written. \n\nHowever, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. \n\nFurthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. \n\nThis concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. \n\nThe paper thus requires significantly stronger baselines and attacks.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "A novel approach but has significant issues "}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper416/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353224339, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353224339}}}, {"id": "Ske-mUxFCX", "original": null, "number": 16, "cdate": 1543206425103, "ddate": null, "tcdate": 1543206425103, "tmdate": 1543206425103, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "r1g-LRjic7", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Re: A couple of concerns", "comment": "Thanks for your detailed comments and sorry for the late reply. We had to perform some experiments to answer your questions.\n\n- The accuracy reported for CIFAR10 is for a simple 3 layer dense network. We agree that this is far from desirable, but we believe that if future work can reach higher accuracy on clean images in the permutation-phase domain with more advanced techniques, this automatically results in higher accuracy on adversarial examples.\n\n- The high accuracy is possibly due to the fact that the attacks do not try to modify the digit, rather try to destruct the pixels. In addition, we do not claim retaining this level of accuracy under any type of attack. The purpose of the distortion measure is to compare attacks based on the same ground. Indeed, based on the facts that you mentioned (average L2 distortion of 7.5), this shows that current attacks are not using the distortion budget efficiently.\n\n- As you mentioned, from figure 6 in the appendix, it can be seen that distortion level of 0.2 is quite large for human eye to correctly classify. The code will be released after the final decision to reproduce the results. I'm not sure if I understood your point on 10,000 different random noise. Do you mean expectation over transformation attack?\n\n- We have tested expectation over transformation (https://arxiv.org/pdf/1707.07397.pdf) by attacking a set of 30 PPD models and then test on ensemble of another 10 models. However, the results showed that no more effect than a single model."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "BJezPa1FCX", "original": null, "number": 15, "cdate": 1543204185741, "ddate": null, "tcdate": 1543204185741, "tmdate": 1543204185741, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "B1xMkk49am", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Re: FFT after performing a permutation?", "comment": "Thanks for your question.\nFFT should be placed second. The role of FFT is to build an image based on the relational positions of the permuted image (while the relational positions are encrypted using the permutation block). If FFT is placed first, the whole intuition behind the defense fails because adversary can also simply first take FFT and then perform the attack. Note that random permutation by itself is not secure (see Section 3.2 and the subsections)."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "B1eu-5JYR7", "original": null, "number": 14, "cdate": 1543203328011, "ddate": null, "tcdate": 1543203328011, "tmdate": 1543203432305, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HJlPpLn_2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Reply: Good idea but has limited use case", "comment": "We thank the reviewer for the detailed feedback and address the comments below:\n\nReviewer comment: Poor clean accuracy makes the technique very impractical.\n\nOur response: The 48% accuracy on CIFAR-10 is for a simple 3 layer dense neural network and our goal was to show that even with such a simple network, SOTA robustness can be achieved. We believe that high accuracy combined with adversarial robustness is possible for CIFAR-10, and transfer learning shows promise in this direction. What we plan to do as future work is to replace the neural network in the PPD pipeline with a pre-trained model on massive datasets such as ImageNet and retrain the final layers to fit the permutation-phase domain.\n\nReviewer comment: Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs.\n\nOur response: Thanks for bringing this attack scenario to our attention. To test PPD against an adversary that tries to learn the transformation, we used Blackbox attack (https://arxiv.org/abs/1602.02697 ). In this attack, adversary probes an ensemble of PPD models as a black box by enough input-output pairs and trains a substitute model. The substitute model is then used to craft adversarial examples. Table 1 is updated with the Blackbox results.\n\nReviewer comment: The adversary may attack an ensemble of PPD models for different random permutations (i.e., expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.\n\nOur response: Per the reviewer's request, we tested PPD against expectation over transformation (EoT) (https://arxiv.org/abs/1707.07397) where the permutation is considered as the transformation. 30 PPD models (with different permutations) are used for EoT. The adversarial examples are then fed to an ensemble of 10 PPD models (with different permutations from the 30 models). Our experiments show that EoT can not decrease accuracy more than an adversary that attacks with a single model. One possible explanation is that EoT is mostly useful in the case that sampling a few transformations provides a good approximation of the expectation over transformation. For example, two scenarios that EoT is shown to be successful are: (1) synthesizing adversarial examples that are robust to camera viewpoint shift and (2) breaking a defense that randomly drops pixels of the image and replaces them with total variance minimization. In both of these two scenarios, sampling a few transformations gives a good idea of the expectation. However, in PPD, each transformation has its own fingerprint which is totally different from others."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "r1eZktyYCX", "original": null, "number": 13, "cdate": 1543203032683, "ddate": null, "tcdate": 1543203032683, "tmdate": 1543203110786, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HJedRiS5hX", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Reply: Intersting approach", "comment": "We thank the reviewer for the positive feedback.\n\nReviewer comment: It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.\n\nOur response: Per the reviewer requested, we tested PPD against expectation over transformation (EoT) attack (https://arxiv.org/abs/1707.07397 ). EoT uses an ensemble of 30 PPD models to make adversarial examples. Our experiments showed that EoT could not degrade the performance more than an adversary that uses a single PPD model. One possible explanation is that each permutation yields a unique domain. In other words, information gained by other domains does not reveal much about the unknown domains.\n\nReviewer comment: It would be great to see this extended to convolutional networks.\n\nOur response: We have observed that PPD shows robustness even in the case that convolutional networks are used. However, the accuracy in the convolutional networks is slightly worse than dense networks. For example, convolutional nets achieved around 97% (rather than 98%) on MNIST dataset and 44% (rather than 48%) on CIFAR-10 dataset. One possible explanation for this observation is that the permutation block breaks the local properties of the images exploited by convolutional networks."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "ByxUfwyKR7", "original": null, "number": 12, "cdate": 1543202574409, "ddate": null, "tcdate": 1543202574409, "tmdate": 1543202574409, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "r1gIr-d0hX", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Reply: No Title", "comment": "We thank the reviewer for the valuable feedback and address the comments in the following:\n\nReviewer comment: The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This makes me question  how favorable the trade-off between robustness vs performance is.\n\nOur response: Training on CIFAR-10 is a much more complicated task compared to MNIST and moving to the permutation-phase domain makes it even more difficult. We don't know at the moment what type of network structure and learning technique results in the best accuracy in the permutation-phase domain, but our experiments demonstrate that even a simple 3 layer dense neural network that achieves 48\\% accuracy on clean test images, provides SOTA robustness. We believe that using techniques such as transfer learning helps to improve accuracy on CIFAR-10 data set, but this idea requires more time and resources to evaluate and we leave it for future work.\n\nReviewer comment: The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\". I am not sure if this is true. One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).\n\nOur response: By better results on clean images, we mean better results on clean \"test\" images. The paper referred by the reviewer states the ability of neural networks to memorize the entire training data set and reaching 100% training accuracy while achieving only random guess on testing data set. This is absolutely correct and in fact we have seen it even in the permutation-phase domain that training accuracy can reach as high as 100% while generalizing poorly on testing data set (although not reported in the paper).  However, our claim states that a PPD model that can reach high clean \"test\" accuracy, will not perform poorly on adversarial examples. This is simply because PPD breaks adversarial perturbation of pixel domain to random noise in the permutation-phase domain. Thus, the only goal left for future work is to increase clean test accuracy."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "B1xMkk49am", "original": null, "number": 16, "cdate": 1542237913541, "ddate": null, "tcdate": 1542237913541, "tmdate": 1542237913541, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "It doesn't make intuitive sense why you would perform an FFT after permuting the pixels. This seems like it would destroy all spatial locality making the FFT useless.\n\nDo you instead mean that you *first* perform a FFT and then *second* permute the pixels?", "title": "FFT after performing a permutation?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "r1gIr-d0hX", "original": null, "number": 3, "cdate": 1541468478423, "ddate": null, "tcdate": 1541468478423, "tmdate": 1541534014178, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "content": {"title": " ", "review": "The Paper is written rather well and addresses relevant research questions.\nIn summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). \n\nMy main points of critique are:\n\n1. The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This \nmakes me question  how favorable the trade-off between robustness vs performance is. \n\n2. The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\"\n\nI am not sure if this is true.   One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).\n\n\n\n[1] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper416/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "cdate": 1542234466330, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335719332, "tmdate": 1552335719332, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJedRiS5hX", "original": null, "number": 2, "cdate": 1541196751897, "ddate": null, "tcdate": 1541196751897, "tmdate": 1541534013968, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "content": {"title": "Interesting approach.", "review": "This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels.\n\nThe idea is clearly presented and evaluated. \n\n*Details to Improve*\nIt would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.\n\nIt would be great to see this extended to convolutional networks.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper416/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "cdate": 1542234466330, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335719332, "tmdate": 1552335719332, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlPpLn_2Q", "original": null, "number": 1, "cdate": 1541093055339, "ddate": null, "tcdate": 1541093055339, "tmdate": 1541534013755, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "content": {"title": "Good idea but has limited use case", "review": "This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.\n\nWhile the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.\n\nPros:\n- Novel defense technique against very challenging white-box attacks.\n- Sound threat model drawn from traditional security.\n- Clearly written.\n\nCons:\n- Poor clean accuracy makes the technique very impractical.\n- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper416/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Review", "cdate": 1542234466330, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335719332, "tmdate": 1552335719332, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gvH3hZim", "original": null, "number": 15, "cdate": 1539587135494, "ddate": null, "tcdate": 1539587135494, "tmdate": 1539587246064, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "ryxazwD0qQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "It has been shown repeatedly that the adversary does not need access to the gradients to construct images. A black-box attack that fools an ensemble of models would definitely be \"adversarial\" to your defense, I think. Worth a shot?\n\nhttps://arxiv.org/abs/1611.02770 may be worth a read if you disagree :)", "title": "\"can adversary really generate adversarial images?\""}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "HyxIf9wC9Q", "original": null, "number": 10, "cdate": 1539369485971, "ddate": null, "tcdate": 1539369485971, "tmdate": 1539369485971, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "rklxM4To9Q", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for your interesting question. So far, we have not been able to come up with an attack that can learn the permutation. We know that there are huge possible permutations (784! > 10^1500) for MNIST for example. This rules out the possibility of random guess, but does not preclude the possibility of leaking information. We don't know at the moment if this kind of attack is possible."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "ryxazwD0qQ", "original": null, "number": 9, "cdate": 1539368725284, "ddate": null, "tcdate": 1539368725284, "tmdate": 1539368725284, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "Hyeb7Rhi57", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Re: I feel that you did not really address my comments", "comment": "Encryption is not happening in the inference stage. It is happening in building the adversarial images. Maybe the word \"encryption\" has been misleading. Sorry for that. Let me explain in other words: \n\nI understand that when you are given an image for inference you do not know whether it is adversarial or clean and you just feed it to the classifier. But the question is can adversary really generate adversarial images? That's where we are stopping adversary not in the inference stage. Adversary requires two elements to generate adversarial examples: (a) input space, and (b) classifier. But how PPD stops adversary? You can think about it in two ways:\n\n1. If you consider the neural network as the classifier, PPD is hiding the input space.\n2. If you consider the whole pipeline as the classifier, PPD is hiding the classifier.\n\nI hope you find this explanation helpful."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "HklMPJPC5X", "original": null, "number": 8, "cdate": 1539366745869, "ddate": null, "tcdate": 1539366745869, "tmdate": 1539366745869, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "SJgdGN2i97", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "An example for clarification", "comment": "Maybe the following example helps to clarify partial vs complete obscurity:\n\nOne method proposed in https://openreview.net/forum?id=SyJ7ClWCb is total variance minimization. This approach randomly selects a \"small\" set of pixels, and reconstructs the simplest image that is consistent with the selected pixels. Why small set of pixels? Because the neural network wants to decide based on the reconstructed image and if a lot of pixels are changed, it will fail to make a good decision. This is what I mean by saying that important parts of the image (or in other words a large portion of the image) are preserved in total variance minimization.\n\nHowever, PPD completely obscures the input space by random permutation + pixel2phase. Why are we saying completely hides important pixels of the image? Consider an MNIST image of digit 2. Clearly, important pixels are the white pixels in the image. If we just use random permutation (which is a partial obscurity), we are changing positions of these white pixels. So, if adversary attacks white pixels in the unpermuted image of 2 (by converting them to grey pixels), it has successfully attacked a permuted image no matter what permutation is used, because grey pixels of unpermuted image remain gray pixels in the permuted domain as well. But adding pixel2phase block after the permutation block solves this issue. Fourier transform captures frequency of change in pixel values. So, instead of looking at what pixels are white in the permuted image of digit 2, it looks at the change in pixel values of neighbors (and this is hidden from adversary due to the random permutation). So severity of attack is mitigated in this way"}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "rklxM4To9Q", "original": null, "number": 14, "cdate": 1539195912427, "ddate": null, "tcdate": 1539195912427, "tmdate": 1539195912427, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "This method develops a fixed permutation to use for all images. The results show that if an attacker learns the permutation, the method is insecure. It is therefore important that it is not possible for the attacker to learn the permutation. Have you thought at all about if this attack might be possible? \n\nIn crypto (which the paper gives as inspiration for a secret key), papers dedicate a significant amount of effort to demonstrating that the secret key is not leaked. Even under a chosen plaintext attack, it should not be possible to learn anything about the key that is being used.", "title": "Can the secret permutation be extracted?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "Hyeb7Rhi57", "original": null, "number": 13, "cdate": 1539194393439, "ddate": null, "tcdate": 1539194393439, "tmdate": 1539194393439, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "ryxP9XMicQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "Thanks for your quick response.  However, can you spend more time to read and think about my comments?  No matter an adversary uses or does not use a similar/same model to generate adversarial images, they don't need to perform permutation and Pixel2Phase by themselves.  Again, the raw inputs to your or any other network could be clean or adversarial images and the fact is unknown (otherwise, no problem needs to be addressed).   The inputs will go through the same pipeline and whatever secret or public permutations that you use.  The attackers don't need to know the secret permutation.  In other words, the \"encryption\" concept is not applicable here.  I won't further comment on your paper unless you or other readers convincingly dispute my comments or point out what I really missed in your paper.", "title": "I feel that you did not really address my comments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "r1xTlHni57", "original": null, "number": 12, "cdate": 1539192052845, "ddate": null, "tcdate": 1539192052845, "tmdate": 1539192052845, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "B1xKozVj9X", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "Nope. I meant an even simpler attacks -- \n\nBlackbox: use FGSM with a large-ish epsilon on a model without any defenses to generate adversarial examples and test your model+defense against that. This is a sure sign that the defense is based on gradient obscuring.\n\nUniversal Adversarial Perturbations: https://arxiv.org/abs/1610.08401\n\nCompare both vs Madry et al. and see if you notice any improvement.", "title": "Simpler attacks can reveal things about your defense"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "SJgdGN2i97", "original": null, "number": 11, "cdate": 1539191824443, "ddate": null, "tcdate": 1539191824443, "tmdate": 1539191824443, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "rJlwltEs9X", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "The defense only works if you obscure how exactly you are changing the input space. What is the important of parts of the input vs the whole input?", "title": "Partial vs Complete Obscurity?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "r1g-LRjic7", "original": null, "number": 10, "cdate": 1539190344918, "ddate": null, "tcdate": 1539190344918, "tmdate": 1539190344918, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "- Your CIFAR-10 accuracy is <45%. This is half (!) of what a SOTA neural network achieves, and is *lower* than the black-box adversarial accuracy of SOTA models.\n\n- With an L2 distortion of 4 on MNIST, it is often possible to actually change the underlying digit to a human, yet the paper claims 96%+ accuracy. What is your accuracy at a L2 distortion of larger values (6? 8?)? This is sufficiently high to modify any digit to any other digit, and should be 0%. In particular, the mean minimum distance between an MNIST digit and a different digit in a different class is about 7.5.\n\n- Similarly, with a CIFAR-10 distortion of 0.2 (that is, 51/255), it is possible to make images completely unrecognizable to humans, yet you still claim only a two percentage point loss in accuracy. This is highly suspicious. For each test sample, it would be useful to try 10,000 different values of uniform random noise with mangitude 0.2 as suggested by Athalye et al. (2018) as a way to verify your defense is not just breaking the optimizers. You do report \"On CIFAR-10, for l_\\infty perturbations less than 0.1, adversarial attacks are not more effective than random noise distortion\" but this does not repeatedly trying random noise for the same input sample, which is often much more effective.\n\n- Have you tried performing an attack using Expectation over Transforms (Athalye et al. 2018) where you take the expectation over the different random seeds? ", "title": "A couple of concerns"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "rJlwltEs9X", "original": null, "number": 7, "cdate": 1539160303136, "ddate": null, "tcdate": 1539160303136, "tmdate": 1539160303136, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "rJl20qQjq7", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "PPD is not just obscurity, it completely changes the input space", "comment": "Randomized perturbation of the input proposed in previous work does not completely change the input space. For example, the methods proposed in this paper https://openreview.net/forum?id=SyJ7ClWCb can be broken because they just change some parts of the input through randomization. In fact, they have to keep a large portion roughly unchanged to retain accuracy. However, PPD completely changes the input space. Note that the role of pixel2phase block is to build the input image of the neural network out of the relational positions of the pixels while the relational positions are hidden using the permutation."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "B1xKozVj9X", "original": null, "number": 6, "cdate": 1539158689202, "ddate": null, "tcdate": 1539158689202, "tmdate": 1539158689202, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "H1eNFs7j97", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Will be tested", "comment": "If I understood correctly, you mean: \n\"attacks that query the true model and construct a substitute model based on the query results without any permutation or pixel2phase block. Then, use the substitute model to craft adversarial examples.\"\nAnother comment also mentioned such a scenario based on this paper:  https://arxiv.org/abs/1602.02697. We agree that this is a valid scenario for adversarial attack. It will be tested and the results will be added to the revision."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "HklbH57i9X", "original": null, "number": 7, "cdate": 1539156536776, "ddate": null, "tcdate": 1539156536776, "tmdate": 1539157183305, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "Hkxk5Fgic7", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "If the permutation is known, the paper itself shows that the accuracy can be brought down. How then does this work as a defense against the \"most powerful attacks\" currently available? \n\nThis argument of hiding the permutation is tantamount to security through obscurity.  Perhaps \"most powerful grey-box attacks\" is the right term, so as to not over-claim the results? \n\nAlso, the comparison with Madry is unfair. Only meaningful attacks are those designed for the specific defense. ", "title": "At best it can be called \"grey box\"?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "H1eNFs7j97", "original": null, "number": 9, "cdate": 1539156860414, "ddate": null, "tcdate": 1539156860414, "tmdate": 1539156860414, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "ryxP9XMicQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "How does the defense fare against blackbox attacks and attacks such as universal adversarial perturbations? It seems to me that attacks that rely on the current model may fail (because of the hidden permutation) but other attacks such as the ones mentioned may succeed (accuracy similar to or less than that of Madry et al.)", "title": "Blackbox attacks, universal adversarial perturbations, etc"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "rJl20qQjq7", "original": null, "number": 8, "cdate": 1539156692396, "ddate": null, "tcdate": 1539156692396, "tmdate": 1539156692396, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "rkemB1Zs57", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "Do you then agree that the defense purely relies on obscurity? It has been shown that some other defenses based on randomized perturbations of the input can be broken by adaptive attacks. ", "title": "Gradient Masking vs Obscurity"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "ryxP9XMicQ", "original": null, "number": 5, "cdate": 1539150735103, "ddate": null, "tcdate": 1539150735103, "tmdate": 1539150735103, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HylHzT-sqX", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Assumption is valid", "comment": "Thanks for your comment. Input image by itself is not enough to craft adversarial images. Adversary requires both input image and a classifier to generate adversarial examples. However, hiding permutation seed stops adversary from using the true model (note that hiding permutation stops adversary's access to the gradient of loss function with respect to the input). So, adversary cannot push the image towards the decision boundaries of the classifier.\n\nAs an alternative, adversary has to use a substitute model to craft adversarial images. Assuming that the permutation is not revealed, adversary may use a similar model trained with a different permutation which is shown to be ineffective."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "HylHzT-sqX", "original": null, "number": 6, "cdate": 1539149069280, "ddate": null, "tcdate": 1539149069280, "tmdate": 1539149069280, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "A neural network is fed with input images that could be clean or adversarial which is unknown.\nInput images should go through the same pipeline.  In your case the pipeline is PPD in Figure 1.  \n\nAdversarial images generated by whatever algorithms in traditional ways (i.e., based on clean images) \nor any benign test images sent to the network will go through the same permutation and Pixel2Phase in your PDD.  \nTherefore, attackers don't need to know your secret permutation at all.\n\nIt seems that your fundamental assumption in section 4.2 (the \"Adversary\u2019s Knowledge\" paragraph) \nthat \"yet a different permutation to craft adversarial examples\" is invalid.  Or did I miss something?\n\n", "title": "It seems to me that your fundamental assumption is invalid.  Or did I miss something?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "BJeQJMZi97", "original": null, "number": 4, "cdate": 1539146202588, "ddate": null, "tcdate": 1539146202588, "tmdate": 1539146202588, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "r1xuz7PZ5X", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Will be tested", "comment": "Thanks for bringing this attack to our attention. We will test and add results to the revision."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "rkemB1Zs57", "original": null, "number": 3, "cdate": 1539145530543, "ddate": null, "tcdate": 1539145530543, "tmdate": 1539145530543, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "SJxT0bPbcm", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "PPD robustness is because of unknown permutation", "comment": "-The blue curves in Figures 4 and 5 show the accuracy for the target model (the one with the known permutation). As seen in these figures, some attacks such as MIM and CW have decreased the accuracy of the target model to below 20% for MNIST. However, they have not been successful on models with hidden permutation (red dashed curves). This shows that robustness is actually achieved by unknown permutation and not gradient masking.\n\n- I'm not sure if I understood your second part of comment. Do you mean the following? \n\"Adversary trains ~1000 PPD models and craft adversarial examples for each of those models. Then, tests adversarial examples of each of those models against the unknown PPD model to see which PPD model out of the 1000 models was the most damaging to the unknown PPD model?\"\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "Hkxk5Fgic7", "original": null, "number": 2, "cdate": 1539144070841, "ddate": null, "tcdate": 1539144070841, "tmdate": 1539144070841, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "BJg9Hyvbqm", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Term \"black box\" usually describes models that hide everything", "comment": "The word \"black box\" is not used because PPD only requires to hide the random permutation seed. Everything else can be revealed. However, the term \"black box\" is usually meant for models that hide structure, parameters, training dataset, etc., and can only be queried."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "rkgx0tkj97", "original": null, "number": 1, "cdate": 1539140039814, "ddate": null, "tcdate": 1539140039814, "tmdate": 1539140039814, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "H1gfa0IZ5Q", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "content": {"title": "Will be fixed in the revision", "comment": "Thanks for pointing that out. It will be fixed in the revision."}, "signatures": ["ICLR.cc/2019/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612228, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkElFj0qYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper416/Authors|ICLR.cc/2019/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612228}}}, {"id": "r1xuz7PZ5X", "original": null, "number": 5, "cdate": 1538515727613, "ddate": null, "tcdate": 1538515727613, "tmdate": 1538515727613, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "I understand that PPD is meant to work when the permutation is kept secret.\nDoes it still work if the attacker gets to send queries to the model and observe the output? Or can the attacker reverse-engineer the function represented by the network with few queries?\n(e.g. would it stand up to this attack? https://arxiv.org/abs/1602.02697 )", "title": "Does PPD work in \"gray box\" settings?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "SJxT0bPbcm", "original": null, "number": 4, "cdate": 1538515412654, "ddate": null, "tcdate": 1538515412654, "tmdate": 1538515412654, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "Many proposed defenses actually achieve only \"gradient masking\": they break the optimizer used for traditional attacks, but don't actually move the decision boundary.\n\n- Have you checked that your attacks reach ~100% success rate against the target model (the one with the known permutation)? If the attacks do not mostly succeed here, this suggests that the attack fails because the new operations make optimization difficult, not because the permutation key is unknown. Sorry if this is in the paper and I've missed it.\n\n- Have you tested your model by running ~1000 noisy perturbations for each example and picking the most damaging noisy permutation? This is essentially random search for adversarial examples and it sometimes works in cases where gradient-based search fails due to gradient masking.", "title": "Checking for gradient masking"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "BJg9Hyvbqm", "original": null, "number": 2, "cdate": 1538514753805, "ddate": null, "tcdate": 1538514753805, "tmdate": 1538514753805, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "The abstract says that the defense provides \"state-of-the-art robustness against the most powerful adversarial attacks currently available.\" This should be revised to say \"the most powerful black box adversarial attacks\" because all of the evaluations are against black box attacks. These are not the most powerful; white box attacks are more powerful.", "title": "Abstract should be revised"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}, {"id": "H1gfa0IZ5Q", "original": null, "number": 1, "cdate": 1538514618411, "ddate": null, "tcdate": 1538514618411, "tmdate": 1538514618411, "tddate": null, "forum": "HkElFj0qYQ", "replyto": "HkElFj0qYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "content": {"comment": "Section 2 title should be \"Preliminaries\"", "title": "Typo"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper416/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "TL;DR": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "pdf": "/pdf/ee899ca90aef6c00531b0cc282070a9322847430.pdf", "paperhash": "jafarniajahromi|ppd_permutation_phase_defense_against_adversarial_examples_in_deep_learning", "_bibtex": "@misc{\njafarnia-jahromi2019ppd,\ntitle={{PPD}: Permutation Phase Defense Against Adversarial Examples in Deep Learning},\nauthor={Mehdi Jafarnia-Jahromi and Tasmin Chowdhury and Hsin-Tai Wu and Sayandev Mukherjee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkElFj0qYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper416/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311845332, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkElFj0qYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper416/Authors", "ICLR.cc/2019/Conference/Paper416/Reviewers", "ICLR.cc/2019/Conference/Paper416/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311845332}}}], "count": 35}