{"notes": [{"id": "MLSvqIHRidA", "original": "f0a13uNQqix", "number": 458, "cdate": 1601308058473, "ddate": null, "tcdate": 1601308058473, "tmdate": 1615836934601, "tddate": null, "forum": "MLSvqIHRidA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YozgKEfNYlM", "original": null, "number": 1, "cdate": 1610040454310, "ddate": null, "tcdate": 1610040454310, "tmdate": 1610474056799, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This paper reveals a novel interpretation of the well-established CD for energy-based model training as an adversarial game through conditional NCE. The paper could be potential impactful for the community of EBMs.\n\nThere are several points should be addressed in final version:\n\n1, Based on such an interpretation, the number of steps becomes a tunable parameters, rather than in vanilla understaning in CD-family (the larger, the better in terms of approximation, by with more computation cost).\n\n2, It is okay to stop the gradient when solving an adversarial game as the paper discussed. However, propagating the gradient through the component is also another choice, which leads to the algorithm proposed in [1].\n\nIt will be interesting to discuss these in the paper.\n\n[1] Sohl-Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese. \"Minimum probability flow learning.\" arXiv preprint arXiv:0906.4779 (2009).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040454296, "tmdate": 1610474056782, "id": "ICLR.cc/2021/Conference/Paper458/-/Decision"}}}, {"id": "8cBLeu0qZC", "original": null, "number": 1, "cdate": 1603627720798, "ddate": null, "tcdate": 1603627720798, "tmdate": 1606307370967, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Review", "content": {"title": "Interesting formulation, practical impact seems limited", "review": "Post-rebuttal update: Thank you for your response.  My concerns are relatively minor and I believe this work is above the acceptance threshold.\n\n### Summary\n\nThis paper formulates CD-k training as an adversarial game, where the EBM parameterizes a discriminator which tries to classify whether a k-step Markov chain is reversed or not.\n\n### Reasons for Score\n\nPros:\n\n+ While there have been analyses of CD training under different restrictive scenarios, to my knowledge this is the first paper whose formulation applies to the CD-k algorithm used in practice.\n\n+ The derivations seem correct.\n\nCons:\n\n- While it is nice to have a justification for CD-k, the manuscript could be improved if there is more discussion about practical implications of this formulation. For example, is it possible to include a discussion on the commonly used tempered Langevin dynamics (Nijkamp et al, 2019, Du & Mordatch, 2019), which seems justifiable with the current formulation?\n\n- The impact of the work is still limited by the fact that it doesn't explains the PCD algorithm, which is more common in large-scale settings.\n\n### Minor Comments\n\n- It is probably better to change the notations to signify the fact that the gradient of $\\tilde{X}$ or $X^{(k)}$ wrt $\\theta$ will not be accounted, e.g., by using $\\partial_\\theta$ instead of $\\nabla_\\theta$, or $\\text{stop\\_gradient}$. \n\n- It is technically incorrect to say that CD's update steps don't correspond to the gradient of any objective. CD-1 with infinitesimal step-size corresponds to various well-defined objectives, see the references below.\n\n- In Eq.(15), the subscript in $E_{q_\\theta}$ should probably be dropped, to be consistent with the rest of the paper.\n\n### References\n\nHyvarinen, Aapo. \"Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables.\" IEEE Transactions on neural networks 18.5 (2007): 1529-1531.\n\nSohl-Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese. \"Minimum probability flow learning.\" arXiv preprint arXiv:0906.4779 (2009).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142715, "tmdate": 1606915787902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper458/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Review"}}}, {"id": "CAMshMPkNiN", "original": null, "number": 9, "cdate": 1605986544191, "ddate": null, "tcdate": 1605986544191, "tmdate": 1605986544191, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "kNyphIrUfyz", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment", "content": {"title": "Thank you for referring us to these papers as they present good examples for using energy based models.", "comment": "Since our discussion is agnostic to the actual implementation of the energy-based model, we did not dive into this topic and have only provided a handful of examples in order to establish the motivation for using such models. Specifically, in the context of neural networks, we have chosen to bring some of the most recent works in the field rather than provide an exhaustive survey. Since [1] presented a breakthrough in using energy-based models with deep nets, we have updated our paper and added this reference at the end of the first paragraph as an additional example.\n\n#### References\n\n[1] A Theory of Generative ConvNet. Jianwen Xie *, Yang Lu *, Song-Chun Zhu, Ying Nian Wu (ICML 2016)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper458/Authors|ICLR.cc/2021/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870782, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment"}}}, {"id": "k-sCWBSmQwx", "original": null, "number": 5, "cdate": 1605896987339, "ddate": null, "tcdate": 1605896987339, "tmdate": 1605899711814, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "8cBLeu0qZC", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment", "content": {"title": "Thanks for your time and feedback.", "comment": "The goal of this paper was to answer a long ongoing debate regarding the validity of the CD algorithm by suggesting an alternative way to view the algorithm\u2019s objective. The immediate impact is indeed limited but we hope that this alternative derivation will improve the intuition regarding the learning process of CD and possibly inspire new variants of the algorithm.\n\n- **CD\u2019s updates are gradients of some objective**: You are indeed correct that in the limit where the step size approaches zero, the update step of CD converges to a gradient of some fixed loss function. This is shown in the paper by Hyvarinen, which you mentioned, and we have therefore corrected the first sentence in the last paragraph of section 2.1 to make the statement more accurate. Note, however, that this is generally incorrect for non-infinitesimal step sizes, as shown in the references we cite. Regarding the paper by Sohl-Dickstein et al., which you mentioned, they don\u2019t show the equivalence of their method to CD. They only show the updates of the two methods are similar (see their Sec. 3.1) but not exactly the same. In any case, we added the first reference to our corrected statement.\n- **The PCD algorithm**: It would certainly be very interesting to analyze the commonly used PCD algorithm. However, this would probably require a quite different approach than what we used here since, as opposed to what its name suggests, this algorithm is not really a variant of CD. PCD is actually an efficient version of the approximated maximum likelihood method, in which the MCMC chain is assumed to converge to the model\u2019s distribution (and it does not impose any restrictions on the chains\u2019 initialization). Since our derivation strongly relies on the Markov chains to be initialized using samples from the true distribution, we did not find a simple way to extend our derivation to include the approximated maximum likelihood algorithm.\n- **Justifying the use of tempered Langin Dynamics in [1,2]**: Again, we share your view that this is an important research direction. But unfortunately, following this last point, since [1] uses PCD and [2] uses a version of approximated maximum likelihood with a limited number of MCMC steps (not initialized from the dataset), we currently can\u2019t use our analysis to justify their use of tempered Langevin dynamics.\n- **Notation highlighting that the derivative according to $\\tilde{X}$ is not accounted for**: This is indeed an important point in our derivation. Following your comment (and remarks by the other reviewers), we added an extra paragraph at the end of Section 3.4 in order to try and make this point clearer. We tend to believe that use of the partial derivative notation will only make the equation more cluttered and will not necessarily make this issue clearer.\n- **Subscript below the expectation value in Eq. (15)**: We assume you\u2019re referring to the subscript $\\theta$ in $q_\\theta$. We removed it, thanks!\n\n  In case you were referring to the entire subscript of the expectation (indicating the distributions of the random variables), our convention was to have it in all expectations except for those with the long chains, which involve a large number of random variables (including it in those expectations would make the equations cluttered).\n\n#### References\n\n[1] Yilun Du and Igor Mordatch.  Implicit generation and generalization in energy-based models.  InAdvances in Neural Information Processing Systems, volume 32, pp. 3608\u20133618, 2019.\n\n[2] Erik  Nijkamp,  Mitch  Hill,  Song-Chun  Zhu,  and  Ying  Nian  Wu.   Learning  non-convergent  non-persistent short-run mcmc toward energy-based model.  InAdvances in Neural Information Pro-cessing Systems, pp. 5232\u20135242, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper458/Authors|ICLR.cc/2021/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870782, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment"}}}, {"id": "QO4Jz7-qeaG", "original": null, "number": 4, "cdate": 1605896871632, "ddate": null, "tcdate": 1605896871632, "tmdate": 1605896871632, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "t6EYOQ2Q-2Y", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment", "content": {"title": "Thank you for your time and your feedback.", "comment": "- **Evaluation**: Kindly note that we have intentionally tried to avoid performing a thorough comparison between CNCE and CD since this is not the goal of this paper. Our main goal was to show that CNCE is a framework, which can be generalized to include CD as a special case. The purpose of the toy model was only to shed light on the importance of allowing the conditional distribution $q$ to adapt to the learned model. The model was therefore explicitly designed to be challenging for the classic CNCE. As for redoing the experiments in the original NCE and CNCE papers, note that they only performed a simple ICA experiment and illustrated results on some very simple 2D toy examples (simpler than our 10D example). We therefore don\u2019t believe that repeating these experiments would benefit this work.\n- **Selection of k**: This is a good point. k was intentionally selected to be small in order to mimic the behavior of the algorithms for complex (possibly high-dimensional) distributions. In such cases, k is usually extremely small relative to the number of steps it takes for the MCMC process to converge to the model's distribution. When increasing k, the performance of all CD based methods improve. It\u2019s important to note however that for a larger k the optimal step size will be different, usually smaller.\n- **Difference between MH and ACD as a function of k**: One noted difference between the adjusted version of CD and the MH corrected version is that for large values of k, the adjusted version has usually resulted in a slightly smaller step size relative to the MH version (the step size is automatically adjusted to keep the average weight value constant). But since this method doesn\u2019t reject any steps from the Markov chain, the overall distances traversed by the chains were in fact larger. In any case, we weren\u2019t able to observe any differences in performance between the two methods on this model or on others.\n- **Fig 3b**: Thanks for pointing this out. We have used the adjusted version of CD in this figure. We have updated the caption to make it clearer. The selection of the adjusted version over the MH version was arbitrary as both methods have produced very similar results."}, "signatures": ["ICLR.cc/2021/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper458/Authors|ICLR.cc/2021/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870782, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment"}}}, {"id": "DV7UzBc8ntW", "original": null, "number": 3, "cdate": 1605896818871, "ddate": null, "tcdate": 1605896818871, "tmdate": 1605896818871, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "yNU4iv7jxr", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment", "content": {"title": "Thanks for the comments and good suggestions.", "comment": "- **Last term in eq. (2)**: We have chosen to write this term in the same way it appeared in the original paper [1]. This term refers to the gradient of the loss only w.r.t. the $\\theta$ that appears within $r_\\theta^k$. Following your comment, we added an explanation with an alternative way to write this term.\n- **Difference between the CNCE and CD-1**: The main difference between the two is that CNCE uses a fixed conditional distribution $q$ (that doesn\u2019t change from one step to the next), while CD-1 uses a MCMC transition rule that depends on the learned model and therefore changes over time. Please see Section 3.2 (From CNCE to CD-1), which describes the transition from the fixed $q$ to the adaptive one, and the illustration of the difference in Figure 2. Following your question, we added an extra sentence at the beginning of section 3.2 to make it clearer that CNCE uses a fixed $q$.\n- **Connection to gradients of the log-likelihood**: It is correct that as k becomes extremely large the CD-k loss converges to the maximum-likelihood loss, but this convergence usually requires extremely large values of k. For practical values of k (tens or hundreds) the two will usually significantly differ from one another. The best way to see this convergence is by looking at the CD-k loss in Eq. (1). Here, as k goes to infinity $r^k_{\\theta}$ convergence to $p_{\\theta}$ and the second term cancels out. This leaves us with the Kullback Leibler divergence as the loss function (which is equivalent to maximizing the likelihood).\n- **Bibliography**: We fixed the errors in the bibliography. Thanks.\n\n#### References\n\n[1] Hinton, Geoffrey E. \"Training products of experts by minimizing contrastive divergence.\" Neural computation 14.8 (2002): 1771-1800."}, "signatures": ["ICLR.cc/2021/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper458/Authors|ICLR.cc/2021/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870782, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment"}}}, {"id": "G9MVk3rbTq", "original": null, "number": 2, "cdate": 1605896702376, "ddate": null, "tcdate": 1605896702376, "tmdate": 1605896702376, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "6FJuWMBxSp", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment", "content": {"title": "Thanks for the good points you raised and the positive feedback.", "comment": "- **Gradient w.r.t. $\\tilde{x}$**: This is an important point. You are indeed correct regarding the fact that the gradients according to $\\tilde{x}$ are not taken into account when updating the probability model. This choice is not made for convenience. It would simply be incorrect to take into account q when updating the probability model. When generating the Markov chains, $q_{\\theta}$ is being used in order to produce chains that appear to be time-reversible. During the update stage, we try to adjust the model so that it will be better in classifying whether the chains were reversed. Therefore we do not want the optimization to affect the generation process. This is just like in GANs, where the generator and discriminator have different objectives, and so when updating the discriminator, one doesn\u2019t take into account how the generator would change in response. In our case, the transition rule $q_{\\theta}$ acts as a generator which generates the contrastive samples, and the probability model $p_\\theta$ is part of the discriminator. We added this discussion to the end of section 3.2 in order to try and make it clearer.\n- **The architecture in the toy example**: Since the experiment itself was not the main focus of the paper, we did not devote a lot of effort to optimizing the architecture and it is very likely that a much smaller one would have been sufficient. We have initially started with the network that was used in [1] as a discriminator of GAN that was used to learn a 2D spiral dataset. This network uses 5 FC+leakyReLU layers of width 512. In our setting, we have encountered stability issues training this network which we believe to have resulted from the fact that we didn\u2019t use any normalization layers. We have found that by replacing each FC layer with a Residual block of 2 FC layers we can make the network significantly more stable. That led us to the resulting architecture. It is also important to keep in mind that our toy model is slightly more challenging than the simple 2D spiral due to its embedding in a 10-dimensional space.\n- **Typos and additional details**: Thanks. We corrected the errors you pointed out and added a few more intermediate steps to the derivation in eqs. (22) and (24).\n\n#### References\n\n[1] Tanaka, Akinori. \"Discriminator optimal transport.\" Advances in Neural Information Processing Systems. 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper458/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper458/Authors|ICLR.cc/2021/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870782, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Comment"}}}, {"id": "kNyphIrUfyz", "original": null, "number": 1, "cdate": 1605349769816, "ddate": null, "tcdate": 1605349769816, "tmdate": 1605349769816, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Public_Comment", "content": {"title": "related works about deep EBMs", "comment": "Dear Authors and Reviewers,\n\nWe found that the current paper missed some important references about pioneering works that are related to energy-based generative models parameterized with deep net energy.\n\nThe first paper that proposes to train an energy-based model parameterized by modern deep neural network and learned it by Langevin based MLE is in (Xie. ICML 2016) [1]. The model is called generative ConvNet, because it can be derived from the discriminative ConvNet. This is also the first paper to formulate modern ConvNet-parametrized EBM as exponential tilting of a reference distribution, and connect it to discriminative ConvNet classifier. That is, EBM is a generative version of a discriminator. (Xie. ICML 2016) [1] originally studied such an EBM model on image generation theoretically and practically in 2016.\n\n(Xie. CVPR 2017) [2] (Xie. PAMI 2019) [3] proposed to use Spatial-Temporal ConvNet as the energy function in EBMs for video generation. The model is called Spatial-Temporal generative ConvNet. \n\n[2] and [3] are the first paper to give adversarial interpretation of maximum likelihood learning of ConvNet-EBM, That is the EBM serves the roles of both the generator (actor) and the discriminator (critic). The MLE learning is self-critic.\n\n(Xie. CVPR 2018) [4] also proposed to use volumetric 3D ConvNet as the energy function for 3D shape pattern generation. It is called 3D descriptor Net.\n\nAlso, the Generative Cooperative Nets (CoopNets) (Xie. PAMI 2018)[5] and (Xie. AAAI 2018) [6], which jointly trains an EBM and a generator network by MCMC teaching.\n\nThose are the more original and earlier papers for deep EBMs with ConvNet as energy function than what you have cited, e.g., [7](Yilun Du and Igor Mordatch, 2019).\n\nReferences:\n\n[1] A Theory of Generative ConvNet. Jianwen Xie *, Yang Lu *, Song-Chun Zhu, Ying Nian Wu (ICML 2016)\n\n[2] Synthesizing Dynamic Pattern by Spatial-Temporal Generative ConvNet Jianwen Xie, Song-Chun Zhu, Ying Nian Wu (CVPR 2017)\n\n[3] Learning Energy-based Spatial-Temporal Generative ConvNet for Dynamic Patterns Jianwen Xie, Song-Chun Zhu, Ying Nian Wu IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2019\n\n[4] Learning Descriptor Networks for 3D Shape Synthesis and Analysis Jianwen Xie *, Zilong Zheng *, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, Ying Nian Wu (CVPR) 2018\n\n[5] Cooperative Training of Descriptor and Generator Networks. Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2018\n\n[6] Cooperative Learning of Energy-Based Model and Latent Variable Model via MCMC Teaching. Jianwen Xie, Yang Lu, Ruiqi Gao, Ying Nian Wu. AAAI 2018.\n\n[7] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603\u20133613, 2019\n\nThank you!"}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jianwen_Xie1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MLSvqIHRidA", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/Authors", "ICLR.cc/2021/Conference/Paper458/Reviewers", "ICLR.cc/2021/Conference/Paper458/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024982079, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper458/-/Public_Comment"}}}, {"id": "t6EYOQ2Q-2Y", "original": null, "number": 2, "cdate": 1603928445586, "ddate": null, "tcdate": 1603928445586, "tmdate": 1605024685176, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Review", "content": {"title": "This paper is acceptable, and the ICLR community may benefit from the contributions this paper brings to light. ", "review": "Summary:\nThis paper presents a way to view contrastive divergence (CD) learning as an adversarial learning procedure where a discriminator is tasked with classifying whether or not a Markov chain, generated from the model, has been time-reversed. Beginning with the classic derivation of CD and its approximate gradient, noting relevant issues regarding this approximation, the authors present a way to view CD as an extension of the conditional noise contrastive estimation (CNCE) method where the contrastive distribution is continually updated to keep the discrimination task difficult. Specifically, when the contrastive distribution is chosen such that the detailed balance property is satisfied, then the CNCE loss becomes exactly proportional the CD-1 update with the derivation further extended to CD-k. Practical concerns regarding lack of detailed balance are mitigated through the use of Metropolis-Hastings rejection or an adaptive weighting that arises when deriving the gradient of their time-reversal classification loss.  A toy example providing empirical support for correcting the lack of detailed balance is included.\n\nStrengths:\nThe paper is well written. From \"first principles\" through the CD-CNCE link, the paper was straightforward to follow without technical issues and with appropriate references. The results of this work appear novel, and proofs seem correct. The ability to use the weighting to address detailed balance in practice is neat, and the experiments, though limited, show promise.\n\nConcerns:\nI understand performance comparisons and experiments were not the focus of the paper. However, considering the newly presented link between CNCE and CD, it would have been exciting to see some evaluation metrics. Perhaps even just a simple experiment from the original NCE or CNCE papers. \nFor the MCMC process, appendix D mentions that 5 steps of Langevin dynamics were used. How was 5 selected? Was there any significant gain or degradation when it varied? More generally, what kind of impact does chain length have on the discriminator\u2019s classification ability? Does chain length affect the behavior of CD with MH correction and adjusted CD similarly?\n\nMinor:\nIn Figure (3b), it is said that CD is based on Langevin dynamics MCMC adjusted with the method of Sec. 3.4 yet both MH and the weight adjustment are included there. Which one is in Fig. (3.4) \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142715, "tmdate": 1606915787902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper458/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Review"}}}, {"id": "yNU4iv7jxr", "original": null, "number": 3, "cdate": 1603959788519, "ddate": null, "tcdate": 1603959788519, "tmdate": 1605024685111, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Review", "content": {"title": "Establishes an interesting theoretical link", "review": "Summary\n\nTo implement the contrastive divergence (CD) algorithm in practice, an intractable term is typically omitted from the gradient. This leads to an approximation. This work shows that the resulting algorithm can in fact be viewed as an exact algorithm targeting a different, adversarial objective. The derivation in this paper also shows how Markov chains which are not reversible w.r.t. the posterior distribution of interest can be employed within the algorithm. Effectively, this assigns an importance weight to each sample which akin to the acceptance ratio which would be needed for a Metropolis--Hastings type correction.\n\n\n\nStrengths and novelty\n\nTo my knowledge, the derivation of the relationship between CD and conditional noise contrastive estimation (CNCE) is novel. Making it clear how these algorithms are related is a contribution worth publishing.\n\n\nWeaknesses\n\nPerhaps having an additional non-toy example would have been a nice illustration. However, since the paper's main focus is on establishing theoretical connections between CD and CNCE, I do not believe that the lack of further numerical examples should preclude publication.\n\n\n\nMinor comments\n\n- A number of entries in the bibliography have typos such as missing capitalisation of proper nouns inconsistent use of capital letters in journal and conference names.\n\n- I don't understand the last term in Eq. 2. This needs to be more rigorously written.\n\n- Section 3.2 extablishes that the CD-1 gradient is a special case of the CNCE gradient. This would mean that both CD-1 and CNCE lead to computationally the same algorithm. However, this appears not to be the case in the toy example. For readers not familiar with CNCE, please make it more clear in what way CD-1 and CNCE differ in practice if both use the same reversible Markov chain.\n\n- In Section 2.1, it is stated that CD does not use log-likelihood loss. However, it seems to me that for $p_\\theta$-reversible Markov chains, if the chain is either fast mixing or the number of iterations $k$ sufficiently large, the gradient in Eq. 3 is proportional to the gradient of the log-likelihood (multiplied by $-1$) in expectation because in this case, the first term has expectation zero.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142715, "tmdate": 1606915787902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper458/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Review"}}}, {"id": "6FJuWMBxSp", "original": null, "number": 4, "cdate": 1603976311652, "ddate": null, "tcdate": 1603976311652, "tmdate": 1605024685042, "tddate": null, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "invitation": "ICLR.cc/2021/Conference/Paper458/-/Official_Review", "content": {"title": "A new theoretical understanding of contrastive divergence as adversarial training", "review": "# General statements\nThis paper has a special flavour, in the sense that it provides new light on a very established training method for energy-based models: contrastive divergence. Its core contribution is to provide a theoretically grounded understanding of CD as it is widely used, avoiding the common assumption that this algorithm stems out of a simplifying assumption.\n\nThis is done through a connection between CD and adversarial training. On their way, the authors show how some minor corrections suggested by their interepretation may dramatically improve performance of CD, at least on their toy example.\n\nSince CD is a widely accepted method, I feel that the deliberate choice of restricting their experiments on toy data is legitimate.\n\n\nAll in all, I would say that the paper is a very nice read, and its english usage is good, as well as the references that are appropriate.\nI think that it is appropriate for presentation at ICLR, since it may stimulate new research on CD.\n\n\n# Detailed comments\nBelow are some minor comments in chronological order\n## Introduction\n* \"Thus, Our\": uppercase\n\n## Toy example\n* In figure 4, you probably mean \"from left to right\"\n* To be extra sure, are you effectively disabling gradient recording when computing \\tilde{x} as I assume you do ? I'm asking because \\tilde{x} actually appears as a function of x, parameterized by \\theta, i.e. as \\tilde{x}_\\theta(x), since it involves the transition kernel q_theta for its computation. As you write below eq. (17), you are considering that the kernel q as kept fixed, explaining such a choice. \nhowever, and if I'm not mistaken, it should not be too difficult with autograd mechanics to include this dependency in the updates. Did you try it ? Did it break the algorithm ? \n* I would appreciate more steps in your derivations (22) and (24): I don't follow easily the transitions to lines 2 and 4 of each.\n* The neural net used for the toy data looks impressively large (8 layers of FC+leakyReLU with 512 hidden size). Was it really necessary ?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper458/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper458/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "authorids": ["~Omer_Yair1", "~Tomer_Michaeli1"], "authors": ["Omer Yair", "Tomer Michaeli"], "keywords": ["Unsupervised learning", "energy based model", "adversarial learning", "contrastive divergence", "noise contrastive estimation"], "abstract": "Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).", "one-sentence_summary": "We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yair|contrastive_divergence_learning_is_a_time_reversal_adversarial_game", "supplementary_material": "/attachment/1124c6c41a5beb640f3b369d8dc9de3ee2d20144.zip", "pdf": "/pdf/03d95d33dbce2d626edf50ba2d01876c374f7049.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyair2021contrastive,\ntitle={Contrastive Divergence Learning is a Time Reversal Adversarial Game},\nauthor={Omer Yair and Tomer Michaeli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MLSvqIHRidA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MLSvqIHRidA", "replyto": "MLSvqIHRidA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142715, "tmdate": 1606915787902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper458/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper458/-/Official_Review"}}}], "count": 12}