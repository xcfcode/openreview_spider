{"notes": [{"id": "HygLm4QeOV", "original": "H1ea8rKuDN", "number": 19, "cdate": 1553114141620, "ddate": null, "tcdate": 1553114141620, "tmdate": 1562082107236, "tddate": null, "forum": "HygLm4QeOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "De-biasing Weakly Supervised Learning by Regularizing Prediction Entropy", "authors": ["Dean Wyatte"], "authorids": ["dean.r.wyatte@gmail.com"], "keywords": [], "abstract": "We explore the effect of regularizing prediction entropy in a weakly supervised setting with inexact class labels. When underlying data distributions are biased toward a specific subclass, we hypothesize that entropy regularization can be used to bootstrap a training set that mitigates this bias. We conduct experiments over multiple datasets under supervision of an oracle and in a semi-supervised setting finding substantial reductions in training set bias capable of decreasing test error rate. These findings suggest entropy regularization as a promising approach to de-biasing weakly supervised learning systems.", "pdf": "/pdf/ed9830a0beacb91edf8a78c25432d64271417fa1.pdf", "paperhash": "wyatte|debiasing_weakly_supervised_learning_by_regularizing_prediction_entropy"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "ryeFpZd6_V", "original": null, "number": 1, "cdate": 1553985985331, "ddate": null, "tcdate": 1553985985331, "tmdate": 1555512029617, "tddate": null, "forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Official_Review", "content": {"title": "Promising use of entropy penalty to de-bias noisy datasets, but context could be made clearer", "review": "Summary: The paper proposes to use the well-known technique of entropy regularization to de-bias the training set. The classes in the training set are assumed to come from biased mixtures of true classes. By penalizing low-entropy prediction of the model, combined with new labels from an oracle or inferred from prediction probabilities, the procedure can reduce the class bias in the noisy dataset. Experiments on MNIST, CIFAR-10 and CIFAR-100 validates the utility of the proposed scheme, showing that the bias decreases after the procedure.\n\nStrengths:\n1. The use of entropy penalty to de-bias noisy dataset is novel to the best of my knowledge.\n2. Experimental results on MNIST, CIFAR-10, and CIFAR-100 are convincing and validate the claim in the paper.\n\nWeaknesses:\n1. Bias is never defined. I take it to mean class imbalance in the mixture class. I recommending elaborating on the set up and the context where this setting makes sense.\n2. The experimental procedure isn't clear. What are mixture classes? Why does it make sense to construct mixture classes as such? What kind of real scenario are these experiments simulating?", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "De-biasing Weakly Supervised Learning by Regularizing Prediction Entropy", "authors": ["Dean Wyatte"], "authorids": ["dean.r.wyatte@gmail.com"], "keywords": [], "abstract": "We explore the effect of regularizing prediction entropy in a weakly supervised setting with inexact class labels. When underlying data distributions are biased toward a specific subclass, we hypothesize that entropy regularization can be used to bootstrap a training set that mitigates this bias. We conduct experiments over multiple datasets under supervision of an oracle and in a semi-supervised setting finding substantial reductions in training set bias capable of decreasing test error rate. These findings suggest entropy regularization as a promising approach to de-biasing weakly supervised learning systems.", "pdf": "/pdf/ed9830a0beacb91edf8a78c25432d64271417fa1.pdf", "paperhash": "wyatte|debiasing_weakly_supervised_learning_by_regularizing_prediction_entropy"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Official_Review", "cdate": 1553713419183, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713419183, "tmdate": 1555511816935, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper19/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rygmm14EYV", "original": null, "number": 2, "cdate": 1554427674523, "ddate": null, "tcdate": 1554427674523, "tmdate": 1555512027227, "tddate": null, "forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Official_Review", "content": {"title": "Review: De-biasing Weakly Supervised Learning by Regularizing Prediction Entropy", "review": "The authors propose using conditional entropy regularization during the training in semi-supervised settings to mitigate the bias of imbalanced data. The idea is elegant and effectively communicated, and the authors demonstrate its effectiveness empirically on MNIST and CIFAR-10/100.\n\nI suggest the committee consider this submission for best paper.\n\nI'm curious how the weight on the regularization term affects the bias mitigation.\n\nMinor grammatical errors: \"care should be taken *into* account\", \"a a substantially more diverse ranking\".\n", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "De-biasing Weakly Supervised Learning by Regularizing Prediction Entropy", "authors": ["Dean Wyatte"], "authorids": ["dean.r.wyatte@gmail.com"], "keywords": [], "abstract": "We explore the effect of regularizing prediction entropy in a weakly supervised setting with inexact class labels. When underlying data distributions are biased toward a specific subclass, we hypothesize that entropy regularization can be used to bootstrap a training set that mitigates this bias. We conduct experiments over multiple datasets under supervision of an oracle and in a semi-supervised setting finding substantial reductions in training set bias capable of decreasing test error rate. These findings suggest entropy regularization as a promising approach to de-biasing weakly supervised learning systems.", "pdf": "/pdf/ed9830a0beacb91edf8a78c25432d64271417fa1.pdf", "paperhash": "wyatte|debiasing_weakly_supervised_learning_by_regularizing_prediction_entropy"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Official_Review", "cdate": 1553713419183, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper19/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713419183, "tmdate": 1555511816935, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper19/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Hklu2xNFKE", "original": null, "number": 1, "cdate": 1554755760113, "ddate": null, "tcdate": 1554755760113, "tmdate": 1555510985549, "tddate": null, "forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "De-biasing Weakly Supervised Learning by Regularizing Prediction Entropy", "authors": ["Dean Wyatte"], "authorids": ["dean.r.wyatte@gmail.com"], "keywords": [], "abstract": "We explore the effect of regularizing prediction entropy in a weakly supervised setting with inexact class labels. When underlying data distributions are biased toward a specific subclass, we hypothesize that entropy regularization can be used to bootstrap a training set that mitigates this bias. We conduct experiments over multiple datasets under supervision of an oracle and in a semi-supervised setting finding substantial reductions in training set bias capable of decreasing test error rate. These findings suggest entropy regularization as a promising approach to de-biasing weakly supervised learning systems.", "pdf": "/pdf/ed9830a0beacb91edf8a78c25432d64271417fa1.pdf", "paperhash": "wyatte|debiasing_weakly_supervised_learning_by_regularizing_prediction_entropy"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper19/Decision", "cdate": 1554736068119, "reply": {"forum": "HygLm4QeOV", "replyto": "HygLm4QeOV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736068119, "tmdate": 1555510970563, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}