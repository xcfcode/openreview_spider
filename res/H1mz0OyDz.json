{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573624691, "tcdate": 1521573624691, "number": 341, "cdate": 1521573624350, "id": "B1bW1y1cz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1mz0OyDz", "replyto": "H1mz0OyDz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning", "abstract": "We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such questions often require reference to multiple plot elements and synthesis of information distributed spatially throughout a figure. To facilitate the training of machine learning systems, the corpus also includes side data that can be used to formulate auxiliary objectives. In particular, we provide the numerical data used to generate each figure as well as bounding-box annotations for all plot elements. We study the proposed visual reasoning task by training several models, including the recently proposed Relation Network as a strong baseline. Preliminary results indicate that the task poses a significant machine learning challenge. We envision FigureQA as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "pdf": "/pdf/bacca0b0c690170537fb4757ada35940cad773d6.pdf", "TL;DR": "We present a question-answering dataset, FigureQA, as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "paperhash": "kahou|figureqa_an_annotated_figure_dataset_for_visual_reasoning", "_bibtex": "@misc{\nebrahimi2018figureqa,\ntitle={Figure{QA}: An Annotated Figure Dataset for Visual Reasoning},\nauthor={Samira Ebrahimi Kahou and Adam Atkinson and Vincent Michalski and \u00c1kos K\u00e1d\u00e1r and Adam Trischler and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=SyunbfbAb},\n}", "keywords": [], "authors": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Adam Atkinson", "\u00c1kos K\u00e1d\u00e1r", "Adam Trischler", "Yoshua Bengio"], "authorids": ["samira.ebrahimi@microsoft.com", "vincent.michalski@umontreal.ca", "adatkins@microsoft.com", "kadar.akos@gmail.com", "adam.trischler@microsoft.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730165397, "tcdate": 1518468619210, "number": 260, "cdate": 1518468619210, "id": "H1mz0OyDz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1mz0OyDz", "original": "SyunbfbAb", "signatures": ["~Adam_Atkinson1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning", "abstract": "We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such questions often require reference to multiple plot elements and synthesis of information distributed spatially throughout a figure. To facilitate the training of machine learning systems, the corpus also includes side data that can be used to formulate auxiliary objectives. In particular, we provide the numerical data used to generate each figure as well as bounding-box annotations for all plot elements. We study the proposed visual reasoning task by training several models, including the recently proposed Relation Network as a strong baseline. Preliminary results indicate that the task poses a significant machine learning challenge. We envision FigureQA as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "pdf": "/pdf/bacca0b0c690170537fb4757ada35940cad773d6.pdf", "TL;DR": "We present a question-answering dataset, FigureQA, as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "paperhash": "kahou|figureqa_an_annotated_figure_dataset_for_visual_reasoning", "_bibtex": "@misc{\nebrahimi2018figureqa,\ntitle={Figure{QA}: An Annotated Figure Dataset for Visual Reasoning},\nauthor={Samira Ebrahimi Kahou and Adam Atkinson and Vincent Michalski and \u00c1kos K\u00e1d\u00e1r and Adam Trischler and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=SyunbfbAb},\n}", "keywords": [], "authors": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Adam Atkinson", "\u00c1kos K\u00e1d\u00e1r", "Adam Trischler", "Yoshua Bengio"], "authorids": ["samira.ebrahimi@microsoft.com", "vincent.michalski@umontreal.ca", "adatkins@microsoft.com", "kadar.akos@gmail.com", "adam.trischler@microsoft.com", "yoshua.bengio@umontreal.ca"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730165397, "tcdate": 1509134767551, "number": 781, "cdate": 1518730165386, "id": "SyunbfbAb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SyunbfbAb", "original": "SyioWfbRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning", "abstract": "We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such questions often require reference to multiple plot elements and synthesis of information distributed spatially throughout a figure. To facilitate the training of machine learning systems, the corpus also includes side data that can be used to formulate auxiliary objectives. In particular, we provide the numerical data used to generate each figure as well as bounding-box annotations for all plot elements. We study the proposed visual reasoning task by training several models, including the recently proposed Relation Network as strong baseline. Preliminary results indicate that the task poses a significant machine learning challenge. We envision FigureQA as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "pdf": "/pdf/914415a31980b392db7609eced0f0a78a1ca537b.pdf", "TL;DR": "We present a question-answering dataset, FigureQA, as a first step towards developing models that can intuitively recognize patterns from visual representations of data.", "paperhash": "kahou|figureqa_an_annotated_figure_dataset_for_visual_reasoning", "_bibtex": "@misc{\nebrahimi2018figureqa,\ntitle={Figure{QA}: An Annotated Figure Dataset for Visual Reasoning},\nauthor={Samira Ebrahimi Kahou and Adam Atkinson and Vincent Michalski and \u00c1kos K\u00e1d\u00e1r and Adam Trischler and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=SyunbfbAb},\n}", "keywords": ["dataset", "computer vision", "deep learning", "visual reasoning", "relational reasoning"], "authors": ["Samira Ebrahimi Kahou", "Adam Atkinson", "Vincent Michalski", "\u00c1kos K\u00e1d\u00e1r", "Adam Trischler", "Yoshua Bengio"], "authorids": ["samira.ebrahimi@microsoft.com", "adatkins@microsoft.com", "vincent.michalski@umontreal.ca", "kadar.akos@gmail.com", "adam.trischler@microsoft.com", "yoshua.bengio@umontreal.ca"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}