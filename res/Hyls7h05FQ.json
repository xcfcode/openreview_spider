{"notes": [{"id": "Hyls7h05FQ", "original": "ryx6bfCcFQ", "number": 1389, "cdate": 1538087970863, "ddate": null, "tcdate": 1538087970863, "tmdate": 1545355387004, "tddate": null, "forum": "Hyls7h05FQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xd0pDxe4", "original": null, "number": 1, "cdate": 1544744399711, "ddate": null, "tcdate": 1544744399711, "tmdate": 1545354523353, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Meta_Review", "content": {"metareview": "\nPros:\n\n*  High quality evaluation across different benchmarks, plus human eval\n\n*  The paper is well written (though one could quibble about the motivation for the method, see Cons)\n\nCons:\n\n*  The approach is incremental, the main contribution is replacing marginalization or RL with G-S. G-S has already been studied in the context of VAEs with categorical latent variables, i.e. very similar models.\n\n*  The main technical novelty is varying amount of added noise (i.e. downscaling Gumbel noise). In principle, the Gumbel relaxation is not needed here as exact marginalization can be done (as) effectively. Unlike the standard strategy used to make discrete r.v. tractable in complex models, samples from G-S are not used in this work to weight input to the 'decoder' (thus avoiding expensive marginalization) but to weight terms corresponding to reconstruction from individual latent states (in constract, e.g., to SkimRNN of Seo et al (ICLR 2018)). Presumably adding noise to softmax helps to force sharpness on the posteriors (~ argmax in previous work) and stochasticity may also help exploration.  \n\n(Given the above, \"to preserve differentiability and circumvent the difficulties in training with reinforcement learning, we apply the reparameterization trick with Gumbel softmax\" seems slightly misleading)\n\n\n*  With contextualized embeddings, which are sense-disambiguated given the context, learning discrete senses (which are anyway only coarse approximations of reality) is less practically important\n\nTwo reviewers are somewhat lukewarm (weak accept) about the paper (limited novelty), whereas one reviewer is considerably more positive. I do not believe that the reviews diverge in any factual information though.\n\n\n\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "word sense induction with Gumbel-Softmax"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1389/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352858765, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352858765}}}, {"id": "SJlEps7zkN", "original": null, "number": 11, "cdate": 1543809980072, "ddate": null, "tcdate": 1543809980072, "tmdate": 1543882797114, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "rygvBxcJyN", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Address the commenter's question 1", "comment": "We thank the commenter for their interest in our paper and the valuable comments! We address the concerns as follows:\n\n1) Model assumptions\n\nIf we understand correctly, the commenter worries that the implied assumption w->s->c (first select a sense s from word w, then generate the context c) of our model is flawed, because they think w and c are not independent given s. They provide an example to show when w and c should not be independent given s. They suggest that we should model \u201cthe collocation of senses\u201d (Qiu et al. 2016) instead of \u201cthe collocation of words and sense\u201d like Li & Jurafsky (2015) to address this concern.  \n\nFirst, we\u2019d like to argue that no model assumption is perfect (https://en.wikipedia.org/wiki/All_models_are_wrong), for example, both Bag-of-Word model and LDA are flawed language model, it doesn\u2019t mean that they\u2019re not useful. Without mentioning what consequences our model assumptions would cause in training, testing, or evaluation, the commenter's concern seems to be purely cognitive/theoretical. Nevertheless, we're happy to discuss with the commenter about it\n\nWe think that the commenter may have misunderstood a few points and the assumption w->s->c is not a concern. The conditional independence of w and c given sense could hold based on our assumption (we discuss using the commenter\u2019s example in 1.2 below). \n\n\nMoreover, the evaluations demonstrate that our model is able to learn distinguishable senses and succeeds in both similarity-based and human evaluations. We agree that the model approach of Qiu et al. (2016) also makes sense, but it is not necessarily better than ours, as shown in our evaluation where ours outperform theirs (Table 1). \n\nIn detail:\n1.1 The commenter may have an inaccurate understanding about our premise.\n\nWe\u2019d like to reassure the commenter that our model does not share senses among words, like *all* multi-prototype models. Also, the premise is not to learn \u201csame\u201d embeddings for words that have the same sense. It is to learn similar embeddings for certain senses of words that are synonyms (share similar contexts). Two words may be synonyms, but they won\u2019t have identical senses. \n\n1.2  The commenter worries \u201cc is not independent of w given s\u201d, if we understand correctly, they think \u201cnot sharing senses\u201d could be a solution but not quite. \n\nHowever, no matter the senses are shared across words or not, we can have c independent of w given s. \n           \nA. When not sharing senses (our case),\n\n         P(w_i | s_i^k) = 1, i.e., given a specific sense of a word, the surface word type is fixed, therefore P(w, c|s) = P(w|s)*p(c|s) = P(c|s).\n\n>>>\nFor example, \"guy\" and \"man\" are synonym, but one is more casual and the other is more formal. Hence, despite the same sense, different realization would cause the contexts to be more (or less) formal.\n<<<\n\nIn this example, \u201cguy\u201d and \u201cman\u201d won\u2019t be associated to the same sense vector. One sense of \u201cguy\u201d s_{guy}^i and one sense of \u201cman\u201d s_{man}^j may be similar but are two separate embeddings, which would have distinct (perhaps close) P(c|s_{guy}^i) and P(c| s_{man}^j). Ideally, P(c|s_{guy}^i) would result in more casual expression while P(c| s_{man}^j) results in formal expression. Therefore, this case won\u2019t result in the commenter\u2019s concern.\n\n     B. When senses are shared among words\n\nIn the commenter's example: it is possible that senses are divided to s_{man_casual}, s_{man_formal} and s_others, once we observe s_{man_casual}, we know that the context should be casual rather than formal no matter it\u2019s generated by \u201cman\u201d or \u201cguy\u201d. \n\nSurely the surface word type eventually affect its context, in the way that \u201cguy\u201d may have a higher probability to generate s_{man_casual} while \u201cman\u201d may have a higher probability to generate s_{man_formal}.\n\nAlthough \u201cman\u201d and \u201cguy\u201d are synonyms, they will have different distribution: P(s_i| guy) and P(s_i|man).\n\nIdeally, \nP(s_{man_casual}|guy) > P(s_{man_formal}|guy),\n and \nP(s_{man_casual}|man) < P(s_{man_formal}|man). \n\nIf they have the same distribution, it means that the usage of \u201cguy\u201d and \u201cman\u201d are identical, which contradicts the commenter\u2019s statement \u201cone is more casual and the other is more formal\u201d. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "S1exdsXGkE", "original": null, "number": 10, "cdate": 1543809895995, "ddate": null, "tcdate": 1543809895995, "tmdate": 1543880619627, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "rygvBxcJyN", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Address the commenter's question 2, 3 and 4", "comment": "\n2) Approximation in Eq. (5)\n\nWe add the approximation notation because we estimate the sense disambiguation distribution with local context instead of using the global word sense distribution (Tian et al. 2014). We assume that we can disambiguate senses based on local context. The same assumption is adopted by Neelakantan et al. (2014), Li and Jurafsky (2015), Lee and Chen (2017), etc. Moreover, if we consider the original Skip-Gram, each word-context pair counted is in a fixed context window, the local context is given, the left side is also implicitly conditioned on the local context within the window. We will revise the notation to make this clear.\n\n\n3) Parameters\n\nThanks for pointing out this. We do not claim in the paper that learning two sets of parameters is our contribution. In fact, several previous works apply the same parameter setting, such as Tian et al. (2014), Neelakantan et al. (2014) [the additional centers are computed with context parameters] and Li and Jurafsky (2015). Despite the same parameter setting, similar to previous work, our novelty lies in *different* mechanisms to disambiguate and select senses. We highlight the # of parameters in responses mainly to differentiate our work from Lee and Chen (2017), mentioning the parameter setting is also the basis in discussing sense disambiguation and sense selection mechanism. \n\n\n4) Analysis of estimator\n\n>>>\nIf the gumbel softmax estimation is an important novelty to this paper, the analysis of the estimator should be shown instead of the end-to-end performance. \n<<<\n\nWe thank the commenter for this insight! We agree that Gumbel Softmax (GS) and RL approach have their advantages/disadvantages. However, the focus of our paper is not a direct comparison of GS and RL but rather to develop an efficient but effective sense embedding model and to answer \u201cwhat are good sense embeddings\u201d. The goal is to learn sense vectors that both capture semantics and are distinguishable to human. Thus, with limited space, following previous work, we focus on evaluating the quality of the learned embeddings and propose a new human evaluation method to exam the ability of the proposed model in learning human distinguishable senses. Moreover, in order to show the benefit of the proposed scaled GS, we perform the ablation study by comparing SASI, GASI and GASI-\\beta. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "rygvBxcJyN", "original": null, "number": 1, "cdate": 1543639103216, "ddate": null, "tcdate": 1543639103216, "tmdate": 1543639103216, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Public_Comment", "content": {"comment": "This is a nice work that extends current works to gumbel softmax and provides new experiment settings. However, I have some concerns based on my understanding. I hope that they can be answered by the author. \n\n1. First of all, I have a methodological concern about the central assumption of the method Eq. (5). If I understand it correctly, the authors do not mention the conditional independence assumption (w -> s -> c) but instead using it directly. It suggests that the word w_i and the context words are conditionally independent given the underlying sense s^i_k. Isn't it true that the concrete realization of a sense (i.e., a word) drastically affect the context words? For example, \"guy\" and \"man\" are synonym, but one is more casual and the other is more formal. Hence, despite the same sense, different realization would cause the contexts to be more (or less) formal. One potential hope for this concern is that the sense of different words are not shared in your models, but given the premise is to learn embeddings of words such that the words with the same sense would have \"the same\" sense embeddings, such modeling framework does not solve this problem. In contrast, the recent trend to model the collocation of senses (Qiu et al., 2016, Lee & Chen, 2017) seems to avoid this problem. Why do the authors pursue an older approach (akin to (Li & Jurafsky, 2015)) that models the collocation of words and sense?\n\n2. Also, the second \"approximation\" in Eq. (5) is very weird, how can p(s^i_k|w_i, \\tilde{c}_i) be similar to p(s^i_k|w_i)? In the non-parametric sense, the author change the conditional table of (# of sense, # of words) for p(s^i_k|w_i) to (# of sense, # of words, # of contexts) for p(s^i_k|w_i, \\tilde{c}_i), while the # of contexts are exponential to the # of words. It suggests a exponential difference in the complexity for the \"approximation\". (Sorry for that I cannot follow the remaining part of the methodology quite well because the remaining methods highly depend on the above assumptions.)\n\n3. Why do the authors think modeling *two* sets of parameters (words and senses) as a novelty? Isn't it simply the conventional design as (Li & Jurafsky, 2015) that is different from more recent approaches that models a purely sense-based framework (Qiu et al., 2016, Lee & Chen, 2017)?\n\n4. The gumbel softmax is a nice estimator that enables differentiability, but suffer from an biased gradient. The RL approach used by (Lee & Chen, 2017) provides unbiased gradient but suffer from large variance. Both approaches have their advantages and disadvantages, so I'm interested in seeing an ablation study that analyze the difference of gradient estimator in this task in terms of the impact of gradient estimation on the learning process. If the gumbel softmax estimation is an important novelty to this paper, the analysis of the estimator should be shown instead of the end-to-end performance. ", "title": "Some questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311608748, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hyls7h05FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311608748}}}, {"id": "ryl5e-HnRX", "original": null, "number": 8, "cdate": 1543422193920, "ddate": null, "tcdate": 1543422193920, "tmdate": 1543422193920, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hkxssn3oRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Will release the code and data soon", "comment": "We thank the reviewer for the new comments! We'll elaborate on the differences between our model with prior works with a little more detail in the next revision after the decision been made. And we'll release our code and data for the human evaluation as soon as possible. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "Hkxssn3oRX", "original": null, "number": 6, "cdate": 1543388322681, "ddate": null, "tcdate": 1543388322681, "tmdate": 1543388349244, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "r1eqzQYw07", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Complete experiments and analysis", "comment": "After seeing the author responses, my score is updated.\n\nThe revised paper includes more experiments (almost all I can think of) and detailed analysis.\nThe difference between the proposed model and the prior work can be better elaborated for explicitly pointing out the novelty.\nAlso, the improved performance seems convincing for the proposed model, and it will be better to see the published code for encouraging researchers to easily follow up this direction.\n\nTo sum up, this is a good paper that can motivate the following research in the related field."}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "BygDQWP537", "original": null, "number": 1, "cdate": 1541202207248, "ddate": null, "tcdate": 1541202207248, "tmdate": 1543387619114, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "content": {"title": "This paper points out an important evaluation perspective, but the model architecture is incremental (limited novelty). ", "review": "This paper proposes GASI to disambiguate different sense identities and learn sense representations given contextual information. \nThe main idea is to use scaled Gumbel softmax as the sense selection method instead of soft or hard attention, which is the novelty and contribution of this paper.\nIn addition, the authors proposed a new evaluation task, contextual word sense selection, which can be used to quantitatively evaluate the semantic meaningfulness of sense embeddings.\nThe proposed model achieves comparable performance on traditional word/sense intrinsic evaluation and word intrusion test as previous models, while it outperforms baselines on the proposed contextual word sense selection task.\n\nWhile the scaled Gumbel softmax is the claimed novelty, it is more like an extension of the original MUSE model (Lee and Chen, 2017), which proposed the sense selection and representation learning modules for learning sense-level embeddings.\nThe only difference between the proposed one and Lee and Chen (2017) is Gumbel softmax instead of reinforcement learning between sense selection and representation learning modules.\nTherefore, the idea from the proposed model is similar to Li and Jurafsky (2015), because the sense selection is not one-hot but a distribution.\nThe novelty of this paper is limited because the model is relatively incremental.\n\nFrom my perspective, the more influential contribution is that this paper points out the importance of evaluating sense selection capability, which is ignored by most prior work.\nTherefore, I expect to see more detailed evaluation on the selection module of the model. \nAlso, because the task of this paper is multi-sense embeddings, the traditional word similarity (without contexts) task seems unnecessary. \nMoreover, there is no error analysis about the result on the proposed contextual word sense selection task, which may shed more light on the strength and weakness of the model. \nFinally, I suggest the authors remove the word-level similarity task and try the recently released Word in Context (WiC) dataset, which is a binary classification task that determines whether the meaning of a word is different given two contexts.\nIt would be better to see that GASI performs well on this task given its better sense selection module.\n\nOverall, the contribution is somewhat incremental and the evaluation/discussion should focus more on the sense selection module. \nConsidering the issues mentioned above, I will expect better quality for an ICLR paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "cdate": 1542234240324, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335936322, "tmdate": 1552335936322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eqzQYw07", "original": null, "number": 3, "cdate": 1543111442119, "ddate": null, "tcdate": 1543111442119, "tmdate": 1543111483896, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "BygDQWP537", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for taking time to read our paper and the useful suggestions! We address the reviewer\u2019s concerns and suggestions as follows:\n\n1) Additional evaluation with recently released WiC dataset\n>>>\nFinally, I suggest the authors remove the word-level similarity task and try the recently released Word in Context (WiC) dataset\n<<<\nWe thank the reviewer for their suggestion! We add an evaluation on the recently released WiC dataset in the revision. We focus the evaluation on the sense selection module of the model and classify the senses in an unsupervised fashion. Our model achieves the highest accuracy among competing models (Table 2), except for DeConf which is a supervised sense model that annotates senses on the same lexical resource (WordNet) that was used to build WiC. \n\nWe believe that the word similarity tasks demonstrate that each sense-specific embedding learned by our model captures good semantics in addition to better sense disambiguation ability. The high quality of each sense embeddings demonstrates the benefits of using Gumbel softmax. Therefore we decide to keep this evaluation in the revision, but it is more meaningful alongside the WiC results.\n\n2) Novelty\n\n>>>\nThe only difference between the proposed one and Lee and Chen (2017) is Gumbel softmax instead of reinforcement learning between sense selection and representation learning modules.\n<<<\nWe appreciate that the reviewer noticed the similarity between our models with MUSE by Lee and Chen (2017), as both try to improve the sense selection module with hard attention. However, the overall structure of our model (Figure 1) is quite different, in addition to using Gumbel Softmax (GS) instead of RL for hard attention, we\u2019d like to explain the differences on two key aspects:\n\nModel structure and parameters: MUSE learns *four* sets of parameters: sense representations for target words U, collocation context representations V, and two additional matrix P, Q to estimate sense selection distribution for both target words and contexts (both target and context have multiple senses). \nIn contrast, ours learns *two* sets of parameters (Section 3.1): sense representations for target words S and global context representations C. We use C to disambiguate senses of target words S instead of using additional parameters like in MUSE, which reduces the number of total parameters in our model. Furthermore, we update S and C in both the sense selection and context prediction modules, as these two modules are \u201csymmetric\u201d (predicting senses by context and predict context by word sense) and both help to capture the semantics in words.  Moreover, similar to Neelakantan et al. (2014), we do not disambiguate senses for context words (one global vector per context word) to further reduce the parameter size.\n\nOptimization function: We use the (scaled) GS instead of straight-through (scaled) GS to have a stronger error signal (update not only the senses that are chosen but also the ones that are not).  To use a distribution instead of a one-hot selection and reduce the computational cost by negative sampling, we optimize the lower bound of the original negative sampling Skip-Gram objective with marginalization and Jensen's Inequality. Using straight-through (scaled) GS learns worse sense embeddings (lower word similarity score and human-model consistency) than (scaled) GS. Due to space limitations, we didn\u2019t include the comparison in the paper. RL methods are similar to ST-GS since they also make a hard selection each time and update the selected senses but not others. \n\n>>>\nthe idea from the proposed model is similar to Li and Jurafsky (2015), because the sense selection is not one-hot but a distribution.\n<<<\nLi and Jurafsky (2015) sample one-hot senses during the training with Chinese Restaurant Process (CRP) and model the CRP with a distribution; while we directly use the distribution and implement the standard skip-gram objective with marginalization over senses. \n\n3) Error analysis \n\n>>>\nMoreover, there is no error analysis about the result on the proposed contextual word sense selection task, which may shed more light on the strength and weakness of the model. \n<<<\nWe appreciate the reviewer\u2019s suggestion! We add the error analysis on the crowdsourced contextual word sense selection task in the revision (Section 6.2 Error Analysis). \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "SkgpFMYv0m", "original": null, "number": 2, "cdate": 1543111301183, "ddate": null, "tcdate": 1543111301183, "tmdate": 1543111301183, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "BkgDZsDThQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for taking time to read our paper and the useful suggestions on improving our writing! We address the specific points from the reviewer as follows:\n\n>>>\nIf \\beta=0, then we get SASI, right? How well does this perform on the non-contextual word similarity task? Also, on the crowdsourced evaluation? \n<<<\nWe thank the reviewer for this suggestion! We\u2019ve added the results in Table 3 in the revision. SASI generally performs poorly on the word similarity tasks, so we focus our comparison between our main model GASI-beta with the baseline models given limited space. \n\n>>>\nThe motivation for the hard attention/Gumbel softmax is to learn sense representations that are distinguishable. But do the experiments test this? \n<<<\n\nOur crowdsourced contextual sense selection task evaluates this property. The raters need to distinguish between the learned senses in order to make a selection (Section 6.2, sense disambiguation and interpretability). We also add more detail to these experiments in the additional error analysis in the revision. \n\n>>>\nThere's something strange about Eq 6. \u2026\u2026,  While the motivation for the right hand side makes sense, the notation could use work.\n<<<\nWe address the notation issue in the revision.\n\n>>>\nThe description of how the number of senses is pruned in section 3.1 seems to be a bit of a non sequitur.\n<<<\nWe thank the reviewer\u2019s suggestion. Since it\u2019s not the focus of our paper, in our revision we move the descriptions of pruning to the appendix.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "r1xGmzKvAQ", "original": null, "number": 1, "cdate": 1543111194497, "ddate": null, "tcdate": 1543111194497, "tmdate": 1543111194497, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "r1xVV1VR2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for taking time to read our paper and the useful suggestions! We address the suggestions from the reviewer as follows:\n\n1\uff09dynamic number of senses and evaluation on downstream applications. \n\nWe thank the reviewer for these two suggestions! The simplest way to model words that have more than 3 senses is to initialize all words with more senses and prune aggressively; we set K=3 mainly for purpose of comparison. We think both implementing a dynamic number of senses (e.g., by setting a threshold to split senses) and evaluating on end tasks are great ideas; given the limited space, we\u2019ll address these in future work.\n\n2\uff09benefits of differentiability\n\nIn addition to updating the sense selection module and context prediction module at the same time, full differentiability allows updates to flow to all senses, not only the ones chosen by the attention, which results in stronger error signals and better sense selection ability. While approximating hard attention still guarantees that the model will focus on specific senses so that each sense captures good semantics (Table 3) and is interpretable to humans (Section 6), the Gumbel-softmax trick helps to guarantee both with the original objective, and we don\u2019t need additional parameters for the policy network in RL.\n\n>>>\nFor example, the contrast with Lee and Chen (2017) seems to be only that of differentiability. \n<<<\nWe also contrast the sense selection module with Lee and Chen (2017) in the related work and Section 3.1. The overall structure of the two models are actually different, Lee and Chen (2017) learn *four* set of parameters while we learn *two*. Given limited space, we don\u2019t elaborate further in our paper, but we discuss this with more details in our response to Reviewer 3. \n\n3) negative sampling and lower bound\n\nThe negative sampling (NS) is for reducing the computational cost. To still optimize our original objective while implementing NS, we deduce the lower bound by Jensen\u2019s Inequality.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613808, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyls7h05FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1389/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1389/Authors|ICLR.cc/2019/Conference/Paper1389/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers", "ICLR.cc/2019/Conference/Paper1389/Authors", "ICLR.cc/2019/Conference/Paper1389/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613808}}}, {"id": "r1xVV1VR2m", "original": null, "number": 3, "cdate": 1541451563936, "ddate": null, "tcdate": 1541451563936, "tmdate": 1541533174167, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "content": {"title": "Neat idea applying Gumbel-softmax to multi sense embeddings", "review": "The paper presents a method for deriving multi sense word embeddings. The key idea behind this method is to learn a sense embedding tensor using a skip-gram style training objective. The objective defines the probability of contexts marginalised over latent sense embeddings. The paper uses Gumbel-softmax reparametrization trick to approximate sampling from the discrete sense distributions. The method also uses a separate hyperparameter to help scale the dot product appropriately. \n\nStrengths:\n\n1. The technique is a well-motivated solution for a hard problem that builds on the skip-gram model for learning word embeddings.\n2. A new manual evaluation approach for comparing sense induction approaches.\n3. The empirical advance while relatively modest appears to be significant since the technique seems to yield better results than multiple baselines across a range of tasks. \n\nSuggestions:\n\n1. The number of senses is fixed to three. This is a bit arbitrary, even though it is following some precedence. I like the information in the appendix that shows how to handle cases when there are duplicate senses induced for words that dont have many senses. It would be useful to know how to handle the cases where a word can have more than three senses. Given that the authors have a way of pruning duplicate senses, it would have been interesting to try a few basic methods that select the number of senses per word dynamically. \n\n2. The evaluation includes word similarity task and crowdsourcing for sense intrusion and sense selection. These provide a measure of intrinsic quality of the sense based embeddings. However, as Li and Jurafsky (2015) point out, typically applications use more powerful models that use a wide context. It is not clear how these improvements to sense embeddings will translate in these settings. It would have been useful to have at least one or two end applications to illustrate this. \n\n\n3. Given that the empirical gains are not quite consistent, I would encourage the authors to specifically argue why this particular method should be favoured over other existing methods. The related work discussion merely highlights methodological differences. For example, the contrast with Lee and Chen (2017) seems to be only that of differentiability. Is the claim that differentiability is desirable because this allows for fine tuning in applications? If this is the case then it will be nice to have this verified. \n\n4. The lower bound on the log likelihood objective is good but what are we supposed to take away from it? Is it that there is an interpretation that allows us to get away with negative sampling? \n\nOverall I like the paper. It presents an application of the Gumbel-softmax trick for sense embeddings induction and shows some empirical evidence for the usefulness of this idea, including some manual evaluation. \n\nI think the evaluation could be strengthened with some end applications and much crisper arguments on why the method is preferable over other methods that achieve comparable performance.\n\nReferences:\n\n[Li and Jurafsky., EMNLP 2015] Do Multi-Sense Embeddings Improve Natural Language Understanding?\n\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "cdate": 1542234240324, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335936322, "tmdate": 1552335936322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgDZsDThQ", "original": null, "number": 2, "cdate": 1541401343481, "ddate": null, "tcdate": 1541401343481, "tmdate": 1541533173927, "tddate": null, "forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "content": {"title": "Interesting paper, promising results", "review": "* Summary\n\n  This paper extends the skipgram model using one vector per sense of a word. Based on this, the paper proposes two models for training sense embeddings: One where the word senses are marginalized out with attention over the senses, and the second where only the sense with highest value of attention contributes to the loss. For the latter case, the paper uses a variant of Gumbel softmax for training. The paper shows evaluations on benchmark datasets that shows that the Gumbel softmax based method is competitive or better than other methods. Via a crowdsourced evaluation, the paper shows that the method also produces human interpretable clusters.\n\n* Review\n  This paper is generally well written and presents a plausible solution for the problem of discovering senses in an unsupervised fashion.\n  \n  If \\beta=0, then we get SASI, right? How well does this perform on the non-contextual word similarity task? Also, on the crowd sourced evaluation? The motivation for the hard attention/Gumbel softmax is to learn sense representations that are distinguishable. But do the experiments test this? \n\n  There's something strange about Eq 6. If I understand this correctly, \\tilde{c_i} is the context and c_j^i is the j^th context word. Then P(c_j^i | w, \\tilde{c_i}) should be 1 because the context is given, right? While the motivation for the right hand side makes sense, the notation could use work.\n  \n  The description of how the number of senses is pruned in section 3.1 seems to be a bit of a non sequitur. It is not clear whether this is used in the experiments and if so, how it compares. The appendix gives more details, but it seems a bit out of place even then because the evaluations don't seem to use it.\n\n\n* Minor comments\n  There are some places where the writing could be cleaned up.\n  - Eq 16 changes the notation for the sense embeddings and the context words from earlier, say Eq 12.\n  - Parenthetical citations would be more appropriate in some places Eg: above Eq 3, in footnote 3\n  - Page 6, above 6.2: Figure-Figure?\n  - Page 9, Agreement paragraph: hight -> highest\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1389/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax", "abstract": "We present a differentiable multi-prototype word representation model that disentangles senses of polysemous words and produces meaningful sense-specific embeddings without external resources. It jointly learns how to disambiguate senses given local context and how to represent senses using hard attention. Unlike previous multi-prototype models, our model approximates discrete sense selection in a differentiable manner via a modified Gumbel softmax. We also propose a novel human evaluation task that quantitatively measures (1) how meaningful the learned sense groups are to humans and (2) how well the model is able to disambiguate senses given a context sentence. Our model outperforms competing approaches on both human evaluations and multiple word similarity tasks.", "keywords": ["unsupervised representation learning", "sense embedding", "word sense disambiguation", "human evaluation"], "authorids": ["fenfeigo@cs.umd.edu", "miyyer@cs.umass.edu", "leahkf@uw.edu", "jbg@umiacs.umd.edu"], "authors": ["Fenfei Guo", "Mohit Iyyer", "Leah Findlater", "Jordan Boyd-Graber"], "TL;DR": "Disambiguate and embed word senses with a differentiable hard-attention model using Scaled Gumbel Softmax", "pdf": "/pdf/be0888bf76fdedb1c5a86f752da632be3b213bb2.pdf", "paperhash": "guo|a_differentiable_selfdisambiguated_sense_embedding_model_via_scaled_gumbel_softmax", "_bibtex": "@misc{\nguo2019a,\ntitle={A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax},\nauthor={Fenfei Guo and Mohit Iyyer and Leah Findlater and Jordan Boyd-Graber},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyls7h05FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1389/Official_Review", "cdate": 1542234240324, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyls7h05FQ", "replyto": "Hyls7h05FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1389/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335936322, "tmdate": 1552335936322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1389/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}