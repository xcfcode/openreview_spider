{"notes": [{"id": "HJlk-eHFwH", "original": "S1lbDAJKDS", "number": 2120, "cdate": 1569439734851, "ddate": null, "tcdate": 1569439734851, "tmdate": 1577168293505, "tddate": null, "forum": "HJlk-eHFwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "orGK3FEZfF", "original": null, "number": 1, "cdate": 1576798741063, "ddate": null, "tcdate": 1576798741063, "tmdate": 1576800895161, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Decision", "content": {"decision": "Reject", "comment": "The paper has major presentation issues. The rebuttal clarified some technical ones, but it is clear that the authors need to improve the reading substantially, ,so the paper is not acceptable in its current form.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714649, "tmdate": 1576800264398, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Decision"}}}, {"id": "SJxDQYG0FS", "original": null, "number": 1, "cdate": 1571854623244, "ddate": null, "tcdate": 1571854623244, "tmdate": 1574200933723, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper presents a voice conversion approach using GANs based on adaptive instance normalization (AdaIN).  The authors give the mathematical formulation of the problem and provide the implementation of the so-called AdaGAN. Experiments are carried out on VCTK and the proposed AdaGAN is compared with StarGAN.  The idea is ok and the concept of using AdaIN for efficient voice conversion is also good.  But the paper has a lot of issues both technically and grammatically, which makes the paper hard to follow.\n\n1. On writing\nThere are glaring grammar errors in numerous places. e.g.\n  -- \"Although, there are few GAN-based systems that produced state-of-the-art results for non-parallel VC. Among these algorithms, even fewer can be applied for many-to-many VC task. At last, there is the only system available for zero-shot VC proposed by Qian et al. (2019).\"   This is hard to parse.\n -- \"helps generator to make ...\"  -> \"helps the generator make ...\"\n --  \"let assume\" -> \"Let's assume\" \n --  \"We know that the idea of transitivity as a way to regularize structured data has a long history.\"   what does it mean?\n --  \"the generator of AdaGAN is consists of Encoder and Decoder.\"  -> \"consist of\"\n -- \"After training of AdaGAN for large number of iteration of $\\tau$ , where theoretically $\\tau \\rightarrow \\infty$.\" where is the second half of the sentence?\n\n2.  On math notation\n The math notation is messy and there are lots of inaccuracies.  e.g.\n  --  $X_{i} \\in p_{X}(\\cdot|Z_{i},U_{i})$ should be $X_{i} \\sim p_{X}(\\cdot|Z_{i},U_{i})$\n  --  \"generate the distribution denoted by $\\hat{X}_{Z_{1}\\rightarrow Z_{2}}$\"  -> why  $\\hat{X}_{Z_{1}\\rightarrow Z_{2}}$ becomes a distribution? \n  --  \"$p_{N}(\\cdot|Z_{1},U_{1})$, $p_{N}(\\cdot|Z_{2},U_{1})$\" in Eq.14,  $N$ should be replaced by the random variable.\n  --  $S'_{X}$ and $S'_{Y}$ should be $S_{X'}$ and $S_{Y'}$ in line 15 in the algorithm\n\n3. On technical details:\n -- In Fig.1 (b), why is there only one input to the discriminator?  How do you inject the adversarial samples and how do you generate adversarial samples? \n-- In section 4.4, \"in encoder and decoder all layers are Linear layers\".  Are you referring to fully-connected layers? Linear layers are usually referred to those with linear activation functions.  \n-- The experiments are claimed to be zero-shot, but 3-5s of speech is required.  can you explain? \n\nAlthough the samples sound OK, given its current form, the paper needs significant re-work. \n\nP.S. rebuttal read.   I will stay with my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221843286, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Reviewers"], "noninvitees": [], "tcdate": 1570237727425, "tmdate": 1576221843300, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review"}}}, {"id": "HklBlV-ssB", "original": null, "number": 6, "cdate": 1573749740986, "ddate": null, "tcdate": 1573749740986, "tmdate": 1573751125142, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "r1xjn_YV5r", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We would like to thank respected Reviewer 1 for their reviews and constructive suggestions. We are glad that the reviewer liked our work. Below, we provide clarification for the reviewer\u2019s queries.\n\nQ 1. Section 4.4 Are all of the utterances the same length? Based on the architecture description, it   appears as though the model generates one output frame for each input frame. This would suggest that for training input and output need to be synchronized. If so, make this explicit and include length parameters in Section 6.1\n\nAns: Thanks for this query. No, the length of utterances is not the same. Based on the length of the utterances, the number of frames will change. Moreover, our model generates one output frame for each input frame. Due to this variable utterances, Dynamic Time Warping (DTW) or any other methodologies are used for synchronization on parallel-data. However, proposed AdaGAN is designed for non-parallel data. Hence, there is no need to do this kind of synchronization. Thank you for your query about length of utterance, we have updated the section 6.1 accordingly to clarify this point.\n\nQ 2. Section 6.2 states \"For statistically significant analysis, the results are shown in different conversion possibilities.\" However, no test of statistical significance is presented. This pointer may be helpful.\n\nAns: Thanks for this suggestion. We have shown the margin of error corresponding to the 95% confidence interval to quote the statistical significance of our results. In addition, we have incorporated p-value for the subjective test to present the statistical significance of the results. In particular, we obtain 0.013 p-values for the subjective test, which is less than 0.05. This indicates the significance of the results. Thanks again! \n\nQ 3. Section 3.1: I would recommend using different subscripts for $Z_i$ and $U_i$, since when indexing $Z$ this implies the $i^{th}$ speaker, and when indexing $U$ it's the $i^{th}$ utterance. The formulas in Section 3.1 imply a single index i for both of these which is clearly not intended.\n\nAns: Thanks for this suggestion. However, we used $i$ as the common index. $Z$ is used for speaker, and $U$ for utterance. Hence, $Z_i$ indicates $i^{th}$ speaker, and $U_i$ indicates $i^{th}$ utterance. For example, the index is the same for the first speaker, i.e., $Z_1$ and first utterance, i.e., $U_1$. Thanks again.\n\nQ 4. Section 4: Consider using the present tense instead of the perfect tense when describing the results. \"...we discuss our proposed AdaGAN architecture... We have shown... We have presented...\" can be \"...we discuss our proposed AdaGAN architecture... We show... We present...\"\n\nAns: Thank you for your guidance. We will be careful from now onwards regarding this. Also, we have made the changes (using present tense instead of the perfect tense) in section 1, 4, and 6 accordingly in the updated manuscript.\n\nQ 5. Section 5.2; Tables 1 and 2: Consider some partition of the FLOPS and Parameters, separation by commas, spaces or even abbreviation e.g. 2952233 -> 2,952,233 or 2 952 233 or 2.9M. This will make this table much easier to read.\n\nAns: Thank you for this valuable suggestion. We have updated the number representations in revised manuscript.\n\nQ 6. Section 6.2; Figures 2-5: MOS scores have a minimum value of 1. This should be the axis of the chart, rather than 0.\n \nAns: Yes, we strongly agree with the respected reviewer. Thanks. We have made changes in range of the y-axis from 1 to 5 in the revised manuscript.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlk-eHFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2120/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2120/Authors|ICLR.cc/2020/Conference/Paper2120/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146033, "tmdate": 1576860529817, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment"}}}, {"id": "BylOnuWiir", "original": null, "number": 8, "cdate": 1573750960352, "ddate": null, "tcdate": 1573750960352, "tmdate": 1573750960352, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "SJxDQYG0FS", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We sincerely thank respected Reviewer 2 for these valuable comments. These comments were highly useful to improve the updated manuscript. We sincerely believe that the reviewer\u2019s concerns addressed additional clarifications in the updated version of the paper. We hope the reviewer will agree. We provide further details below in this regard.\n\nQ 1. On writing There are glaring grammar errors in numerous places. e.g. \n-- \"Although, there are few GAN-based systems that produced state-of-the-art results for non-parallel VC. Among these algorithms, even fewer can be applied for many-to-many VC tasks. At last, there is the only system available for zero-shot VC proposed by Qian et al. (2019).\" This is hard to parse.\n\nAns: Thanks for these suggestions. We have gone through several rounds of reading to take care of the grammatical errors. From the above paragraph, we are trying to convey the message that, \u201cAmong all the methods which exist for many-to-many VC, only a few can work on non-parallel data. Among these non-parallel many-to-many VC approaches, there is only one method proposed by Qian et al. (2019), which work for the zero-shot VC task as well [1].\u201d\n\nQ 2. \"After training of AdaGAN for large number of iterations, where theoretically .\" where is the second half of the sentence?\n\nAns: Thank you very much for pointing out our mistakes, we are very sorry for this. Actually, by mistake, a full stop was added instead of commas. We have updated this mistake in section 5.1 and also gone through the whole paper to make it grammatically correct. Many Thanks!\n\nQ 3. \"We know that the idea of transitivity as a way to regularize structured data has a long history.\" what does it mean?\n\nAns: It was written in different context. We have now removed this sentence. We are very sorry for this.\n\nQ 4. Math notation issue: $X_i \\in p_X(.|Z_i,U_i)$ should be $X_i \\sim p_X(.|Z_i,U_i)$\n\nAns: Yes, it was a mistake from our side. Thank you for bringing out this key issue. We have corrected it.\n\nQ 5. Math notation issue: \"generate the distribution denoted by $\\hat{X}_{Z_1\\rightarrow Z_2}$\" -> why $\\hat{X}_{Z_1\\rightarrow Z_2}$ becomes a distribution? \n\nAns: $\\hat{X}_{Z_1\\rightarrow Z_2}$ is denoting a distribution change from $p_{X}(.|Z_1, U_1)$ to $p_{X}(.|Z_2, U_1)$. This indicates that $\\hat{X}_{Z_1\\rightarrow Z_2}$ is distribution containing linguistic information $U_1$ and speaking style of $Z_2$.\n\nQ 6. Math notation issue: \"$p_{N}(.|Z_1, U_1)$, $p_{N}(.|Z_2, U_1)$\" in Eq.14, $N$ should be replaced by the random variable. \n\nAns: To avoid confusion, now, we have changed the notation to: $p_{IN}(.|Z_1, U_1)$, $p_{IN}(.|Z_2, U_1)$ (in section 5.1, eq. (14)). Here, we use IN to denote that respective distributions are after the normalization during AdaIN step.\n\nQ 7. Math notation issue: $S_X'$ and $S_Y'$ should be $S_{X'}$ and $S_{Y'}$ in line 15 in the algorithm.\n\nAns: Yes, instead of $S_X'$ and $S_Y'$ there should be $S_{X'}$ and $S_{Y'}$. We are sorry for this typo. We have made changes. Thanks this suggestion!\n    \nQ 8. On technical details: -- In Fig.1 (b), why is there only one input to the discriminator? How do you inject the adversarial samples and how do you generate adversarial samples?\n\nAns: We have shown our adversarial training in Eq. (8) and Eq. (9), in that we have clearly shown that discriminator takes both the input generated features as well as original features. To show our training process simple and to make the figure straightforward, we did not indicate original features as input to the discriminator. However, we have made this change in Fig. (1) in the revised manuscript. \n\nQ 9. In section 4.4, \"in encoder and decoder all layers are Linear layers\". Are you referring to fully-connected layers? Linear layers are usually referred to those with linear activation functions.\n\nAns: Yes, we are referring to fully-connected layers. Thanking you for drawing our attention to this point. We have now updated this in section 4.4 of revised manuscript.\n\nQ 10. The experiments are claimed to be zero-shot, but 3-5s of speech is required. can you explain?\n\nAns: Yes, we claim experiments to be zero-shot. We do understand that zero-shot means that we do not require any target speakers' data during training procedure. Here, AdaGAN requires 3-5 seconds from target speakers\u2019 speech during the testing phase only and just as a reference to the target. The carefully designed training procedures allow AdaGAN to generate the latent representation of unknown speakers' speech via some reference utterance of the same speaker. The AutoVC paper which proposed the first attempt to zero-shot VC via encoders and decoders, which requires 20 seconds of target speakers' speech as a reference [1]. \n\n\n\n[1] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. Autovc: Zero-shot voice style transfer with only autoencoder loss. In International Conference on Machine Learning (ICML), pp. 5210\u20135219, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlk-eHFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2120/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2120/Authors|ICLR.cc/2020/Conference/Paper2120/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146033, "tmdate": 1576860529817, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment"}}}, {"id": "rkehj4Zosr", "original": null, "number": 7, "cdate": 1573749923906, "ddate": null, "tcdate": 1573749923906, "tmdate": 1573749923906, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "BkgNckrRtH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for your valuable suggestions. We do believe that the reviewer\u2019s concerns  are taken care through additional clarifications in the updated version of the paper. We hope the reviewer will agree. We provide further details below.\n\nQ 1. There are many typos and wrong notations in the text.\n\nAns: Thank you for pointing out this issue. We have carefully reviewed our paper and made sure that there are no typos and wrong notation in the revised manuscript.\n\nQ 2. A closely related task is voice cloning, which is arguably more challenging than voice conversion, because the synthesis need generalize to arbitrary text. One may properly discuss the recent advances in this community (e.g., Arik et al., 2018; Nachmani et al., 2018; Jia et al., 2018).\n\nAns: Yes, we do agree that voice cloning is more challenging. However, Voice Conversion has its problems that need to be solved. Through this study represented in our manuscript, we proposed a completely new way to approach Voice Conversion using latent representation learning. We have updated the related work according to recent advances in voice cloning, as well as per the reviewer\u2019s suggestion. We sincerely thank respected reviewer.\n\nQ 3. This paper is poorly written and difficult to follow. For example, I could not accurately identify the major contribution & novelty after reading the abstract and introduction. \n\nAns: We welcome this important feedback of language related corrections and we have taken serious note of it. We added a new paragraph at the end of the introduction section, which describes novelty and key contributions of the AdaGAN.\n\nQ 4. As an application paper, the authors may clearly explain the ideas with a few sentences in the most natural way without \"heavy notations\", e.g., Eq. (5)(6)(7).\n\nAns: Yes, we do understand. However, we believe that by adding mathematical formulas, it will be helpful to future readers to implement and manage data processing for various applications of voice conversion.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlk-eHFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2120/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2120/Authors|ICLR.cc/2020/Conference/Paper2120/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146033, "tmdate": 1576860529817, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment"}}}, {"id": "BkgNckrRtH", "original": null, "number": 2, "cdate": 1571864460218, "ddate": null, "tcdate": 1571864460218, "tmdate": 1572972380517, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper tackles many-to-many voice conversion task using GAN for style transfer between different speakers. The core idea is adaptive instance normalization (Huang & Belongie, 2017). \n\nDetailed comments:\n\nThere are many typos and wrong notations in the text. Here is an incomplete list:\n- \"it were spoken by target speaker\", should be \"was\". \n- In Section 3.1, \"Here, U_1 and U_2 are spoken by Z_i and Z_2\" should Z_1. Overall, the descriptions in this subsection is confusing. For example, it seems utterance U_i is from speaker Z_i in the dataset, but there are n speakers and m utterances.\n\n- A closely related task is voice cloning, which is arguably more challenging than voice conversion, because the synthesis need generalize to arbitrary text. One may properly discuss the recent advances in this community (e.g., Arik et al., 2018; Nachmani et al., 2018; Jia et al., 2018).\n\nPros:\nThe empirical improvement seems meaningful.\n\nCons:\nThis paper is poorly written and difficult to follow. For example, I could not accurately identify the major contribution & novelty after reading the abstract and introduction. As an application paper, the authors may clearly explain the ideas with a few sentences in the most natural way without \"heavy notations\", e.g., Eq. (5)(6)(7)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221843286, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Reviewers"], "noninvitees": [], "tcdate": 1570237727425, "tmdate": 1576221843300, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review"}}}, {"id": "r1xjn_YV5r", "original": null, "number": 3, "cdate": 1572276402743, "ddate": null, "tcdate": 1572276402743, "tmdate": 1572972380466, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work describes an efficient voice conversion system that can operate on non-parallel samples and convert from and to multiple voices.  The central element of the methodology is the AdaIn modification.  This is an efficient speaker adaptive technique where features are re-normalized to a particular speaker's domain.  The rest of the machinery is well motivated and well executed, but less novel.  This addition enables the voice conversion between speakers.\n\nSection 4.4 Are all of the utterances the same length?  Based on the architecture description, it appears as though the model generates one output frame for each input frame.  This would suggest that for training input and output need to be synchronized.  If so, make this explicit and include length parameters in Section 6.1 \n\nSection 6.2 states \"For statistically significant analysis, results are shown in different conversion possibilities.\" However, no test of statistical significance is presented.  This pointer may be helpful (https://pdfs.semanticscholar.org/b2b1/d01336323f3794f54de26567335aa0bcac46.pdf)\n\nPresentation Comments:\n\nSection 3.1: I would recommend using different subscripts for Z_i and U_i, since when indexing Z this implies the i-th speaker, and when indexing U it's the i-th utterance.  The formulas in Section 3.1 imply a single index i for both of these which is clearly not intended.\n\nSection 4: Consider using the present tense instead of the perfect tense when describing the results.  \"...we discuss our proposed AdaGAN architecture... We have shown... We have presented...\" can be \"...we discuss our proposed AdaGAN architecture... We show... We present...\"\n\nSection 5.2; Tables 1 and 2: Consider some partition of the FLOPS and Parameters, separation by commas, spaces or even abbreviation e.g. 2952233 -> 2,952,233 or 2 952 233 or 2.9M.  This will make this table much easier to read.\n\nSection 6.2; Figures 2-5: MOS scores have a minimum value of 1.  This should be the axis of the chart, rather than 0.  \n\nIt's pretty bold to star by contextualizing the work with the sentence \"Language is the core of civilization and speech is the most powerful and natural form of communication.\"  :-)"}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576221843286, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Reviewers"], "noninvitees": [], "tcdate": 1570237727425, "tmdate": 1576221843300, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Review"}}}, {"id": "SkgCMSDftB", "original": null, "number": 1, "cdate": 1571087637798, "ddate": null, "tcdate": 1571087637798, "tmdate": 1571087637798, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "r1lkU8ByuH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment", "content": {"comment": "Hello,\nAgain, thank you for reading our research work and drawing our attention to your nice piece of work. \nWe will definitely look into this.", "title": "Thank you for sharing your work! "}, "signatures": ["ICLR.cc/2020/Conference/Paper2120/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlk-eHFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2120/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2120/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2120/Authors|ICLR.cc/2020/Conference/Paper2120/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146033, "tmdate": 1576860529817, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Official_Comment"}}}, {"id": "r1lkU8ByuH", "original": null, "number": 1, "cdate": 1569834567405, "ddate": null, "tcdate": 1569834567405, "tmdate": 1569837735900, "tddate": null, "forum": "HJlk-eHFwH", "replyto": "HJlk-eHFwH", "invitation": "ICLR.cc/2020/Conference/Paper2120/-/Public_Comment", "content": {"comment": "Hi,\nThank you for interesting work.\nI am the author of this paper: https://arxiv.org/abs/1904.05742\nI found that we adopted similar idea (adaIN) to the task of VC. \nI believe that including my work in your paper can make your work more thorough. ", "title": "About related work "}, "signatures": ["~Ju-Chieh_Chou1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ju-Chieh_Chou1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maitreya_patel@daiict.ac.in", "purohit_mirali@daiict.ac.in", "mihirparmar@asu.edu", "nirmesh88_shah@daiict.ac.in", "hemant_patil@daiict.ac.in"], "title": "AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion", "authors": ["Maitreya Patel", "Mirali Purohit", "Mihir Parmar", "Nirmesh J. Shah", "Hemant A. Patil"], "pdf": "/pdf/b3900224fc95bb09431a9b70f1845bdeb8de5094.pdf", "TL;DR": "Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. ", "abstract": "Voice Conversion (VC) is a task of converting perceived speaker identity from a source speaker to a particular target speaker. Earlier approaches in the literature primarily find a mapping between the given source-target speaker-pairs. Developing mapping techniques for many-to-many VC using non-parallel data, including zero-shot learning remains less explored areas in VC. Most of the many-to-many VC architectures require training data from all the target speakers for whom we want to convert the voices. In this paper, we propose a novel style transfer architecture, which can also be extended to generate voices even for target speakers whose data were not used in the training (i.e., case of zero-shot learning). In particular, propose Adaptive Generative Adversarial Network (AdaGAN), new architectural training procedure help in learning normalized speaker-independent latent representation, which will be used to generate speech with different speaking styles in the context of VC. We compare our results with the state-of-the-art StarGAN-VC architecture. In particular, the AdaGAN achieves 31.73%, and 10.37% relative improvement compared to the StarGAN in MOS tests for speech quality and speaker similarity, respectively. The key strength of the proposed architectures is that it yields these results with less computational complexity. AdaGAN is 88.6% less complex than StarGAN-VC in terms of FLoating Operation Per Second (FLOPS), and 85.46% less complex in terms of trainable parameters.  ", "keywords": ["Voice Conversion", "Deep Learning", "Non parallel", "GAN", "AdaGAN", "AdaIN"], "paperhash": "patel|adagan_adaptive_gan_for_manytomany_nonparallel_voice_conversion", "original_pdf": "/attachment/340c7c6e1d67d1c66c09386e15ef4da092cfd7e4.pdf", "_bibtex": "@misc{\npatel2020adagan,\ntitle={Ada{\\{}GAN{\\}}: Adaptive {\\{}GAN{\\}} for Many-to-Many Non-Parallel Voice Conversion},\nauthor={Maitreya Patel and Mirali Purohit and Mihir Parmar and Nirmesh J. Shah and Hemant A. Patil},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlk-eHFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlk-eHFwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504184840, "tmdate": 1576860563541, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2120/Authors", "ICLR.cc/2020/Conference/Paper2120/Reviewers", "ICLR.cc/2020/Conference/Paper2120/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2120/-/Public_Comment"}}}], "count": 10}