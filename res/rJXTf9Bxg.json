{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396312928, "tcdate": 1486396312928, "number": 1, "id": "Hk-ssfI_l", "invitation": "ICLR.cc/2017/conference/-/paper27/acceptance", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Ratings summary:\n 3: Clear rejection\n 6: Marginally above acceptance threshold\n 6: Marginally above acceptance threshold\n \n Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The author\u00d5s point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.\n \n Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396313502, "id": "ICLR.cc/2017/conference/-/paper27/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396313502}}}, {"tddate": null, "tmdate": 1484852838271, "tcdate": 1484852838271, "number": 8, "id": "S1AP0YC8l", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "r1H_OmC8x", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Response to Reviewer 2", "comment": "Hi, thanks for the response.\n\nMost importantly:\n\nA. This response only addresses claims we made about MS-SSIM, and only some of those claims.\nThe reviewer also made claims about the discriminability metric\nthat we feel have been thoroughly rebutted.\n \nB. In our opinion, using pairwise MS-SSIM to evaluate variability is - at best - the \nthird most important contribution of the paper (see list in global response).\nEven if one were to accept (we do not) the reviewer's claims about MS-SSIM,\nit ought not to affect one's evaluation of the paper that greatly.\n\nHaving said that, we address specific claims made here:\n\n1. > Sorry, I should have clarified that I am referring to Section 3.4 of (Theis et al, 2016) \n> on Evaluation based on Nearest Neighbours. \n> I think one can criticise this method along the same lines as mentioned there.\n\nWe assume that the reviewer is referring to the third paragraph of section 3.4,\nsince the first is an introductory paragraph and the second is about euclidean distance.\nThis paragraph is a criticism of using nearest neighbors in the training set.\nWe are not searching for nearest neighbors (we are choosing random pairs of samples)\nand we are not using the training set.\n\nIn particular, the paragraph says: \"Even when overfitting, most models will not reproduce\nperfect or trivially transformed copies of the training data.\n In this case, no distance metric will find a close match in the training set.\"\n\nThis argument doesn't apply in our case.\n\nThe paragraph also says: \"A model which overfits might still never generate a plausible\nimage or might only be able to generate a small fraction of all plausible images\n (e.g., a model as in Equation 10 where instead of training images we store several\n transformed versions of the training images, or a model which only describes data \nin a lower-dimensional subspace).\"\n\nFirst: our model generates plausible images, so that particular concern is not relevant.\nSecond: it's true that a model that could somehow only produce 10% of all natural images\nwould achieve a high diversity score using our method, but we don't claim that our method\nis a good proxy for the entropy of the generator's distribution over pixel space - we \nclaim that it's a useful measure of the perceptual diversity of outputs. \n\n2. The reviewer seems to make the claim that, since MS-SSIM was developed with the \nintention of being used on pairs of images that are already quite similar, it \nis dangerous to try and use it on pairs of images that are not that similar.\nWe already took several steps in the paper to address this concern.\n\nFigure 4 shows various MS-SSIM values for both training data and samples.\nTo our eye, the values do seem to correspond to perceptual diversity.\n\nWe also restricted pairwise evaluations to within-a-class precisely for this reason.\n\nMoreover, the fact that the MS-SSIM values for samples relate to MS-SSIM classes\nfor training data in the way that they do is itself evidence - in the bayesian sense - \nthat the score is working \"as you'd expect\" in this regime.\n\n3. > For point c. I still think that Section 3.4 of (Theis et al, 2016) is relevant.\n\nAgain, that section claims that using nearest neighbors in the training data \nis only good for ruling out the worst forms of overfitting. \nThis is explicitly what we claim to use it for in the above point c.\nWe also use latent space interpolations, which are not addressed.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1484826733221, "tcdate": 1484826733221, "number": 1, "id": "r1H_OmC8x", "invitation": "ICLR.cc/2017/conference/-/paper27/official/comment", "forum": "rJXTf9Bxg", "replyto": "B1UyEGPHe", "signatures": ["ICLR.cc/2017/conference/paper27/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper27/AnonReviewer2"], "content": {"title": "Thank you for the response", "comment": "Thank you for your detailed response. Here is my response to a few points about SSIM\n\n1. Sorry, I should have clarified that I am referring to Section 3.4 of (Theis et al, 2016) on Evaluation based on Nearest Neighbours. I think one can criticise this method along the same lines as mentioned there.\n\n2. I agree that the way I worded my criticism of SSIM and MS-SSIM is not accurate, and does not actually say what I wanted to say.\n\nI am aware of the literature you cited on SSIM and related perceptual metrics. However, it is important that in all of these studies SSIM is used as a full reference image quality measure, and it is indeed designed to serve this purpose. This means that, it measures similarity 'locally' with respect to a reference image: how similar is a blurred version of an image to the original reference image? It is not meant to measure image similarity in any semantic sense of the word, such as 'this llama picture is more similar to that other llama picture, than that ostrich image'. This is what I meant when I wrote it is not that different from Euclidean measure: it doesn't capture very high level image content, it mostly works at the level of local illumination patterns. Of course, I do agree with the authors that SSIM and Euclidean distance work very differently locally - it's is not locally isometric, and it will be more sensitive to certain types of local variation. But do not beleive there is experimental evidence that it should be substantially better at measuring similarity between unrelated images than Euclidean distance.\n\n3. On SSIM being in the range of [0,1] - I'm sorry I'm familiar with a definition where it ranges between -1 and 1, but of course it can be normalised and centred differently. For point c. I still think that Section 3.4 of (Theis et al, 2016) is relevant."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759420, "id": "ICLR.cc/2017/conference/-/paper27/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper27/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper27/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759420}}}, {"tddate": null, "tmdate": 1483314142237, "tcdate": 1483314142237, "number": 7, "id": "B1UyEGPHe", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "S1pqbrY4x", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Response to Reviewer 2", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\nAs mentioned in the global response, we believe that the review misstates several key points in our paper.  We have revised our paper for clarity and we respond to each point in detail below.\n\n======================\nVARIABILITY AND MS-SSIM\n======================\n\n1. \u201cThe authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation\u2026). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1.\u201d\n\nThe main claim of Theis et al (2015) is that log-likelihood might not correspond to sample quality in a generative model. Furthermore, training a model based on one objective will not guarantee good performance under another objective. We view these points as orthogonal to our evaluation framework. If the reviewer views this differently, we would be interested to hear their perspective. That said, Theis et al (2015) provide an explicit motivation for us to build an evaluation method that is independent of the training objective.\n\n2. In regards to the statement that \u2018MS-SSIM is not that dissimilar from Euclidean distance\u2019, we strongly disagree. We believe that this statement is not supported by the literature.\n\nTo start with, SSIM is a highly nonlinear metric that has been constructed to reflect human perceptual judgements [*]. The original SSIM paper as well as follow up works on MS-SSIM have demonstrated that SSIM-based metrics provide substantially improved quantitative estimates of human perceptual judgements compared to simple Euclidean distance measures (and many other quantitative measures of perceptual similarity) [1, 2, 3; and references therein].\n\nA simple intuition for why SSIM is perceptually superior to Euclidean distance can be gained through looking at examples. The SSIM web page (http://www.cns.nyu.edu/~lcv/ssim/ ) shows that images which are drastically different perceptually can have the same MSE and quite different SSIM scores.\n\n[1] Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. \"Multiscale structural similarity for image quality assessment.\"\n[2] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity\"\n[3] Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, and Lei Zhang. Group mad competition - a new methodology to compare objective image quality models.\n[*] Note that our MS-SSIM implementation ranges from [0, 1] and not from [-1, 1]. All values on our graphs should be interpreted accordingly.\n \n3. \u201cEvaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself.\u201d\n\nWe claim that this example is seriously flawed, for the following reasons:\n\na. The range of MS-SSIM is [0, 1] (see above), so the average would be 0.5, which corresponds to half as much diversity as in the least diverse ImageNet class. \nb. This type of memorization would manifest as high variance of the mean MS-SSIM, but we report lower variance for samples than for training data (In other words, we explicitly account for this in the metric itself). One could also compute higher moments or look at a histogram of pairwise MS-SSIM values, etc.\nc. We have also ruled out memorization explicitly by examining both latent space interpolations and nearest neighbors (both by L1 and MS-SSIM).\nd. We have provided 10,000 sample images, which do not show this type of memorization.\ne. It\u2019s not even clear how well other existing methods of measuring diversity would perform in this case.\n\n4. The reviewer suggests that in an ideal world we would compute the entropy of the generator distribution. We argue that -- even if this were feasible -- we would still want to use perceptually calibrated metrics such as MS-SSIM. Namely, we might want to ignore variability due to contrast or pixel intensity in favor of diversity of image content.\n\n5. \u201cConversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.\u201d\n\nWe think that this is a good thing (see above).\nThis is a sense in which Euclidean distance measures are very different than MS-SSIM, which directly contradicts the reviewer\u2019s earlier point.\n\n=============\nDISCRIMINABILITY\n===============\n\n1. \u201cDiscriminability: Figure 3 doesn\u2019t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.\u201d\n\nWe feel that there was a large misunderstanding about our methods, so we present a quick summary of what was done to measure discriminability:\n\nOur goal in this analysis was to measure how much of the output resolution is actually used by the image synthesis model. In other words, does a 128x128 model just produce 32x32 images that are naively resized to 128x128? This is the goal of the \u2018blurring\u2019 analysis and a question that has not been addressed in the literature. If a sample were a naive resizing, a blurring would not reduce its discriminability.\n\nFor each image analyzed, and for each resolution in [16, 32, 64, 128, 256], we iteratively resized the sample from its original size (using bilinear interpolation) to the resolution in question. We then passed the image to a pretrained Inception model, which resizes all inputs to 299x299 before processing. We took note of whether Inception correctly classified the input, and then we reported these results in the lower left hand corner of figure 3. We have revised the manuscript to better describe these methods.\n\n2. \u201cIt is found - not surprisingly - that higher resolution improves discriminability (because more information is present).\u201d\n\nIf the reviewer means that simply increasing the resolution should result in higher discriminability: We did explicitly test for this by upsampling images to confirm that simply upsampling would not increase discriminability. See Figure 3 and Section 4.1.\n\nIf the reviewer means that it is unsurprising that the model is successfully making use of its available output resolution: Naive resizing is not an idle concern. We tested another model that actually failed to meaningfully increase discriminability score from 64x64 to 128x128.\n\n3.\"[the authors do not retrain] all the models to work on low resolution in the first place\". \n\nWe did retrain models at a lower output resolution. We found that samples from these models are about half as discriminable at the 128x128 resolution as samples from the 128x128 model. The results of this experiment correspond to the blue curve in the lower left of Figure 3. This procedure was also described in section 4.1.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1483313776948, "tcdate": 1483313776948, "number": 6, "id": "S1KOffPHl", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "H1qHMAWVe", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Response to Reviewer 3", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\n> - Diversity metric is of limited use for training non class-conditional GANs.\n\nWe note that the MS-SSIM metric may be used for measuring \u201ccollapsing behavior\u201d in GANs regardless of whether the GAN is class-conditional.\n\nIn regards to employing MS-SSIM for measuring diversity *across* classes, we would like to clarify that this should be doable; it would just require re-fitting the MS-SSIM parameters.\n\n> - No experimental comparison of AC-GAN to other class-conditional models.\n\nWe have not been able make the original conditional GAN [1] train successfully on the full ImageNet dataset - we claim that a primary contribution of this paper is to provide the first image synthesis model to do so.\n\nFrom a model performance perspective, we would argue that the relevant architectural choice is whether label information is used. We do compare to the most favorable quantitative assessment of a GAN that has used label information [2] and show substantially improved results.\n\n> Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you\n> extend  MS-SSIM to color images in your work? Were they computed channel-wise across\n> R,G, and B?\n\nWe calculate the mean across all color channels using an open-source implementation:\nhttps://github.com/tensorflow/models/blob/master/compression/msssim.py#L120\n\n> Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or \n> one for each group\u2026\n\nTo the best of our knowledge, we are performing an analysis identical to that in [2]. (Note that they also perform the splitting of the samples in order to compute score variance.) We have added information to Table 2 of Appendix A to highlight this.\n\n> * Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.\n\nOur empirical observation (as some others have seen) is that once mode-collapse has occurred, the GAN can not recover. \n\n[1] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014\n\n[2] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1483313628309, "tcdate": 1483313628309, "number": 5, "id": "ryNkfzPHg", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "rJPBH1MEg", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Response to Reviewer 1", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\n> The overall novelty of this approach is somewhat lacking in that previous methods have\n> proposed training a classifier head on the discriminator and the discriminability metric \n> proposed is simply the inception score of [1] except with class information. \n\nThe novelty of our discriminability analysis is mostly that we measure the extent to which the model is making use of its given output resolution. Our work is the first to attempt this measurement. Note that this also allows us to single out classes for which high-frequency information is important (e.g. zebra - see the green dot in Figure 3).\n\n> Why do you think splitting the imagenet training into 100 different models improves \n> performance? Is the issue with the representation of the class?\n\nWe stumbled upon this result when debugging previous GAN models. We do not have an explanation for this behavior beyond our empirical results.\n\nWe also claim that this work highlights that the challenge of ImageNet is mainly the large number of classes.  This result is consistent with [1] in which the image synthesis model is restricted to visually similar classes (i.e. birds). We do not know whether this is an issue of representation or learning, but we think it is a fascinating question and one which we hope to address in future work. We have added some comments to the discussion accordingly.\n\n> But can you explain why you would want the discriminator to also maximize the classification \n> accuracy of generated samples? \n\nThe short answer is that it makes the results better. Our suspicion is that this is due to a sort of feedback loop: more training data for the discriminator makes the discriminator better, which is then reflected in the generator.\n\n[1] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1483313291578, "tcdate": 1483313291578, "number": 4, "id": "SJE9xfvrl", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Global Response to Reviewers", "comment": "Thank you for your time and for the reviews. We will respond to the details of each review individually, but we wish to highlight some larger points for all reviewers. \n\n+ Novelty: All of the reviewers highlighted some concerns about novelty. We claim that, apart from the model architecture, there is a lot of novelty spread across both the experimental results and the quantitative metrics. This paper is the first work to:\n\n1. Demonstrate an image synthesis model for all 1000 ImageNet classes at a 128x128 spatial resolution (or any spatial resolution) [Section 3].\n2. Measure how much an image synthesis model actually uses its output resolution [Section 4.1].\n3. Measure variability and 'collapsing' behavior in a GAN with a fast, easy-to-compute metric [Section 4.2].\n4. Highlight that a high number of classes is what makes ImageNet synthesis difficult for GANs and provide an explicit solution [Section 2, Appendix D].\n5. Demonstrate experimentally that GANs that perform well perceptually are not those that memorize a small number of examples [Section 4.3].\n6. Achieve state of the art on the Inception score metric when trained on CIFAR-10 without using any of the tricks from \"Improved Techniques for Training GANs\" [Section 4.4].\n\nWe also summarize our rebuttal to 'AnonReviewer2'. Her/his criticisms were particularly sharp and we believe that they are the result of misunderstandings about several key concepts:\n\n+ Diversity: We disagree with the reviewer's statement that 'MS-SSIM is not that dissimilar from Euclidean distance'. The entire purpose of SSIM/MS-SSIM is to be distinct from Euclidean distance. MS-SSIM is highly nonlinear and has been demonstrated in the literature to be quantitatively superior for predicting human perceptual similarity judgments [1,2,3]. We also object strongly to the toy example given in the review and we do not agree that Theis et al (2015) supports the conclusions drawn by the reviewer about our work.\n\n+ Discriminability: The review suggests that higher resolution samples will necessarily lead to more discriminability. This is not correct. In fact, we test this precisely and we demonstrate the opposite. The review also claims that we did not perform an analysis (retraining AC-GAN to output 64x64 images) that we actually did perform.\n\nWe have revised the manuscript to emphasize these points.\n\n[1] Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. \"Multiscale structural similarity for image quality assessment.\"\n[2] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity\"\n[3] Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, and Lei Zhang. Group mad competition - a new methodology to compare objective image quality models.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1482408341497, "tcdate": 1482408341497, "number": 3, "id": "S1pqbrY4x", "invitation": "ICLR.cc/2017/conference/-/paper27/official/review", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["ICLR.cc/2017/conference/paper27/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper27/AnonReviewer2"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "Apologies for the late review.\n\nThis submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.\n\nFigure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).\n\nThe authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).\n\nDiscriminability: Figure 3 doesn\u2019t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.\n\nDiversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation\u2026). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.\n\nOverall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723153, "id": "ICLR.cc/2017/conference/-/paper27/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper27/AnonReviewer3", "ICLR.cc/2017/conference/paper27/AnonReviewer1", "ICLR.cc/2017/conference/paper27/AnonReviewer2"], "reply": {"forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723153}}}, {"tddate": null, "tmdate": 1481926172060, "tcdate": 1481925951385, "number": 2, "id": "rJPBH1MEg", "invitation": "ICLR.cc/2017/conference/-/paper27/official/review", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["ICLR.cc/2017/conference/paper27/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper27/AnonReviewer1"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:\n- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.\n- Training different models on different subsets of imagenet classes improves performance.\n- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)\n- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .\n\nThe overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. \n\nQuestions for the authors: \n(1) Why do you think splitting the imagenet training into 100 different models improves performance?  Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. \n(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize  log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. \n\nOverall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. \n\n[1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498)\n[2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)\n[3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723153, "id": "ICLR.cc/2017/conference/-/paper27/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper27/AnonReviewer3", "ICLR.cc/2017/conference/paper27/AnonReviewer1", "ICLR.cc/2017/conference/paper27/AnonReviewer2"], "reply": {"forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723153}}}, {"tddate": null, "tmdate": 1481921090153, "tcdate": 1481921090153, "number": 1, "id": "H1qHMAWVe", "invitation": "ICLR.cc/2017/conference/-/paper27/official/review", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["ICLR.cc/2017/conference/paper27/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper27/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.\n\nPros:\n+ The paper is clear and well-written.\n+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.\n+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.\n\nCons:\n- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.\n- Diversity metric is of limited use for training non class-conditional GANs.\n- No experimental comparison of AC-GAN to other class-conditional models.\n\nTo my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.\n\n* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?\n* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.\n* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.\n\n[1] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723153, "id": "ICLR.cc/2017/conference/-/paper27/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper27/AnonReviewer3", "ICLR.cc/2017/conference/paper27/AnonReviewer1", "ICLR.cc/2017/conference/paper27/AnonReviewer2"], "reply": {"forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723153}}}, {"tddate": null, "tmdate": 1481487167562, "tcdate": 1481487167554, "number": 3, "id": "r1Or7ViXx", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "HkcU0jxml", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Answers to pre-review questions", "comment": "Thanks for the questions and sorry for the slow response!\nIt's difficult to get things done during NIPS.\n\n1. We weren't able to get normal conditional GANs to work for ImageNet \n(The auxiliary classifier seems to actually make training easier),\n so a direct quantitative comparison of that sort is tricky. \nWe would have liked to perform quantitative comparisons with e.g. PixelCNN,\nbut sampling from these autoregressive models is too expensive for that to be feasible.\n\nQualitative comparisons are easier: we have included a comparison of our samples with \nthose from \"Improved Techniques for Training GANs\" in the appendix.\nThe comparison is somewhat limited, since prior papers on image synthesis did\nnot include many samples.\n\n2. The MS-SSIM score as employed here would probably not work 'out-of-the-box' to measure\n inter-class diversity (as opposed to intra-class diversity, which we use it for). \nThe score uses several hyper-parameters (e.g. weight variances across spatial scales)\n that would need to be retuned for that use-case.\nIn this work, we used the default settings for MS-SSIM provided by \"MULTI-SCALE STRUCTURAL SIMILARITY \nFOR IMAGE QUALITY ASSESSMENT\".\n\n3. This is a very sensible suggestion!\nWe actually considered doing it originally and went with L1 because it was more standard.\nWe have included an additional analysis in the revised paper, which we have already submitted to arXiv.\n\nBroadly speaking, the neighbors seem somewhat more structurally similar than when L1 is used,\nbut there is still no real sign of memorization. \nThis is in agreement with the results from the ICLR submission \n\"On the Quantitative Analysis of Decoder-Based Generative Models\",\nwhich finds that GANs do not overfit as measured by train/test log-likelihood differences.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1480797778532, "tcdate": 1480797778528, "number": 1, "id": "HkcU0jxml", "invitation": "ICLR.cc/2017/conference/-/paper27/pre-review/question", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["ICLR.cc/2017/conference/paper27/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper27/AnonReviewer3"], "content": {"title": "Pre-review questions", "question": "\n1. Did you try a comparison of AC-GAN to other class-conditional models? Specifically, plotting Inception accuracy vs. MS-SSIM for AC-GAN compared to e.g. conditional GANs would be insightful. A qualitative comparison of AC-GAN samples vs. other models would be helpful as well.\n2. Do you think that the MS-SSIM diversity score is useful for unlabeled datasets, or do you feel its primary use is for class-labeled datasets?\n3. In Figure 7, did you try performing the nearest neighbor search using MS-SSIM instead of L1?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959504003, "id": "ICLR.cc/2017/conference/-/paper27/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper27/AnonReviewer3"], "reply": {"forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper27/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959504003}}}, {"tddate": null, "tmdate": 1480351175540, "tcdate": 1480351175535, "number": 2, "id": "ryJRTAtfg", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "Sy9OStwGx", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "writers": ["~Augustus_Odena1"], "content": {"title": "Re: Typo in Appendix A", "comment": "Thanks for the comments.\n\nYou're correct about the typo.\n\nWe'll include the CIFAR-10 hyperparameters in the appendix when we make a revision.\nIn the meantime, I can tell you that they are as follows:\n\nThe discriminator is the same as that from the ImageNet experiments.\nThe generator is composed of one linear projection layer and 3 layers of fractionally strided convolution.\nThe layers have feature map counts of 384, 192, 96, and 3. \n\nThe hyperparameters that were searched over were \"generator_learning_rate\", \"discriminator_learning_rate\", and \"discriminator_activation_noise_standard_deviation\", for which we used values of [0, 0.0001, 0.0002], [0, 0.0001, 0.0002] and [0, 0.1, 0.2] respectively."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "tmdate": 1480197490382, "tcdate": 1480197490378, "number": 1, "id": "Sy9OStwGx", "invitation": "ICLR.cc/2017/conference/-/paper27/public/comment", "forum": "rJXTf9Bxg", "replyto": "rJXTf9Bxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Typo in Appendix A", "comment": "In the hyperparameter tables it's shown that D(x) receives 128 \u00d7 3 \u00d7 3 input. I assume this is a typo and the input size should be 128 x 128 x 3.\n\nAlso, is it possible to also append the hyperparameters for CIFAR-10 experiments?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287759563, "id": "ICLR.cc/2017/conference/-/paper27/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJXTf9Bxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper27/reviewers", "ICLR.cc/2017/conference/paper27/areachairs"], "cdate": 1485287759563}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1477972667027, "tcdate": 1477972667018, "number": 27, "id": "rJXTf9Bxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJXTf9Bxg", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "content": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "pdf": "https://arxiv.org/pdf/1610.09585v1.pdf", "TL;DR": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "paperhash": "odena|conditional_image_synthesis_with_auxiliary_classifier_gans", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": ["BkDDM04Ke"], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 15}