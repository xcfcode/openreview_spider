{"notes": [{"id": "HklJQ1JEDE", "original": "H1lAGyJEDN", "number": 8, "cdate": 1552310038712, "ddate": null, "tcdate": 1552310038712, "tmdate": 1562082109386, "tddate": null, "forum": "HklJQ1JEDE", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Regularity Normalization: Constraining Implicit Space with Minimum Description Length", "authors": ["Baihan Lin"], "authorids": ["doerlbh@gmail.com"], "keywords": ["MDL", "Universal code", "LLD", "Normalization", "Biological plausibility", "Unsupervised attention", "Imbalanced data"], "TL;DR": "Considering neural network optimization process as a model selection problem, we introduce a biological plausible normalization method that extracts statistical regularity under MDL principle to tackle imbalanced and limited data issue.", "abstract": "Inspired by the adaptation phenomenon of biological neuronal firing, we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space. By considering the neural network optimization process as a model selection problem, the implicit space is constrained by the normalizing factor, the minimum description length of the optimal universal code. We introduce an incremental version of computing this universal code as normalized maximum likelihood and demonstrated its flexibility to include data prior such as top-down attention and other oracle information and its compatibility to be incorporated into batch normalization and layer normalization. The preliminary results showed that the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task. As an unsupervised attention mechanism given input data, this biologically plausible normalization has the potential to deal with other complicated real-world scenarios as well as reinforcement learning setting where the rewards are sparse and non-uniform. Further research is proposed to discover these scenarios and explore the behaviors among different variants.", "pdf": "/pdf/8ccb0550dd14d532fa099e80104224635ea2e3cc.pdf", "paperhash": "lin|regularity_normalization_constraining_implicit_space_with_minimum_description_length"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "ICLR.cc/2019/Workshop/LLD"}, {"id": "H1gUBpm8KV", "original": null, "number": 1, "cdate": 1554558270040, "ddate": null, "tcdate": 1554558270040, "tmdate": 1555512024197, "tddate": null, "forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Official_Review", "content": {"title": "Good introduction but unclear theoretical framework and inconclusive experimental evidence", "review": "This paper provides an adaptation of the minimum description length (MDL) principle in computational neuroscience to propose a new attention mechanism for deep neural networks (DNN), whether in a supervised, unsupervised, or reinforcement learning setting. The authors name this attention mechanism \"regularity normalization\". \n\nAs of now, the manuscript suffers from many imprecisions and inaccuracies, thus hindering its eloquence.\n\nFirst of all, the authors equate MDL with negative log-likelihood (NLL), but nowhere in the text do they make this connection explicit. Yet, there is a considerable body of literature in minimizing negative log-likelihood for unsupervised learning applications. Maybe i am missing an important distinction between MDL and NLL, but even then, the authors should clarify that distinction.\n\nThe definition of the universal code \\bar{P}(x) is unclear, and there is no justification of the existence of such universal code.\n\nThe following sentence is quite mysterious: \"the compressibility of the model will be unattainable for multiple inputs, as the probability distributions are different\". It is impossible to know what these \"multiple inputs\" and \"different probability distributions\" are.\n\nMany of the claims in the paper lack a proper definition of their terms. An example is: \"The normalized maximum likelihood minimizes the worst case regret with the minimax optimal solution\". This sentence is very opaque given that the authors haven't explained how they compute regret, what \"worst-case\" means, and thus what \"minimax\" corresponds to in their specific setting. The authors should not leave to reader's guesswork the understanding of these technical terms.\n\nThe introduction of the paper is well written. The discussion of prior literature, especially in the field of neuroscience, is solid. However, the presentation of the new contribution is too vague to redeem the lack of convincing experimental evidence for its success.\n\nFor example, Algorithm 1 (Regularity Normalization) relies heavily on the computation of complexity (COMP in Equation 2), but this equation is not self-contained. On the left-hand side, there is a model class \\Theta. On the right-hand side, there are samples x. How does x relate to \\Theta? Is the equation integrated over the probability distribution X of the data? None of these questions are properly answered in the text. Therefore, the algorithm is not reproducible. This problem is not limited to Equation (2). Equation (3) is also very difficult to understand, with a dummy variable x_j in the denominator summing up to \"0 ... i\".\n\nAlthough this paper exhibits a relatively original and interesting idea for training deep neural networks, i do not recommend it for publication, because it is still in an immature stage. I would recommend the authors to carry out further research, both theoretical and experimental, on regularity normalization, and submit an updated version of this paper in the near future.", "rating": "1: Strong rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularity Normalization: Constraining Implicit Space with Minimum Description Length", "authors": ["Baihan Lin"], "authorids": ["doerlbh@gmail.com"], "keywords": ["MDL", "Universal code", "LLD", "Normalization", "Biological plausibility", "Unsupervised attention", "Imbalanced data"], "TL;DR": "Considering neural network optimization process as a model selection problem, we introduce a biological plausible normalization method that extracts statistical regularity under MDL principle to tackle imbalanced and limited data issue.", "abstract": "Inspired by the adaptation phenomenon of biological neuronal firing, we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space. By considering the neural network optimization process as a model selection problem, the implicit space is constrained by the normalizing factor, the minimum description length of the optimal universal code. We introduce an incremental version of computing this universal code as normalized maximum likelihood and demonstrated its flexibility to include data prior such as top-down attention and other oracle information and its compatibility to be incorporated into batch normalization and layer normalization. The preliminary results showed that the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task. As an unsupervised attention mechanism given input data, this biologically plausible normalization has the potential to deal with other complicated real-world scenarios as well as reinforcement learning setting where the rewards are sparse and non-uniform. Further research is proposed to discover these scenarios and explore the behaviors among different variants.", "pdf": "/pdf/8ccb0550dd14d532fa099e80104224635ea2e3cc.pdf", "paperhash": "lin|regularity_normalization_constraining_implicit_space_with_minimum_description_length"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Official_Review", "cdate": 1553713421028, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421028, "tmdate": 1555511818853, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper8/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SkxLzVYy5E", "original": null, "number": 2, "cdate": 1555170317986, "ddate": null, "tcdate": 1555170317986, "tmdate": 1555511874540, "tddate": null, "forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Official_Review", "content": {"title": "Review of \"Regularity Normalization: Constraining Implicit Space with Minimum Description Length\"", "review": "The authors present a novel normalization procedure based on weighted maximum likelihood. They extend the idea to make it works for feedforward neural networks.\n\nGlobally, the paper is clear and well written, and the contribution interesting.\n\nThe algorithm is well-motivated, and the related work is explained clearly.  I like the flexibility of the algorithm, highlighted in section 3.3. However, the step \"increment\" in Algorithm 1 could be explained more clearly. I think it is worth to include a quick discussion on the complexity of one iteration, and compare it to other regularisation procedure.\n\nThe numerical experiments seem promising, and the combination of the regularisation procedure with layernorm gives impressive results when classes are imbalanced.\n\nIn conclusion, I recommend acceptance of this paper.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularity Normalization: Constraining Implicit Space with Minimum Description Length", "authors": ["Baihan Lin"], "authorids": ["doerlbh@gmail.com"], "keywords": ["MDL", "Universal code", "LLD", "Normalization", "Biological plausibility", "Unsupervised attention", "Imbalanced data"], "TL;DR": "Considering neural network optimization process as a model selection problem, we introduce a biological plausible normalization method that extracts statistical regularity under MDL principle to tackle imbalanced and limited data issue.", "abstract": "Inspired by the adaptation phenomenon of biological neuronal firing, we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space. By considering the neural network optimization process as a model selection problem, the implicit space is constrained by the normalizing factor, the minimum description length of the optimal universal code. We introduce an incremental version of computing this universal code as normalized maximum likelihood and demonstrated its flexibility to include data prior such as top-down attention and other oracle information and its compatibility to be incorporated into batch normalization and layer normalization. The preliminary results showed that the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task. As an unsupervised attention mechanism given input data, this biologically plausible normalization has the potential to deal with other complicated real-world scenarios as well as reinforcement learning setting where the rewards are sparse and non-uniform. Further research is proposed to discover these scenarios and explore the behaviors among different variants.", "pdf": "/pdf/8ccb0550dd14d532fa099e80104224635ea2e3cc.pdf", "paperhash": "lin|regularity_normalization_constraining_implicit_space_with_minimum_description_length"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Official_Review", "cdate": 1553713421028, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper8/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421028, "tmdate": 1555511818853, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper8/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rJgrqWNnYN", "original": null, "number": 1, "cdate": 1554952589225, "ddate": null, "tcdate": 1554952589225, "tmdate": 1555510983397, "tddate": null, "forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularity Normalization: Constraining Implicit Space with Minimum Description Length", "authors": ["Baihan Lin"], "authorids": ["doerlbh@gmail.com"], "keywords": ["MDL", "Universal code", "LLD", "Normalization", "Biological plausibility", "Unsupervised attention", "Imbalanced data"], "TL;DR": "Considering neural network optimization process as a model selection problem, we introduce a biological plausible normalization method that extracts statistical regularity under MDL principle to tackle imbalanced and limited data issue.", "abstract": "Inspired by the adaptation phenomenon of biological neuronal firing, we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space. By considering the neural network optimization process as a model selection problem, the implicit space is constrained by the normalizing factor, the minimum description length of the optimal universal code. We introduce an incremental version of computing this universal code as normalized maximum likelihood and demonstrated its flexibility to include data prior such as top-down attention and other oracle information and its compatibility to be incorporated into batch normalization and layer normalization. The preliminary results showed that the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task. As an unsupervised attention mechanism given input data, this biologically plausible normalization has the potential to deal with other complicated real-world scenarios as well as reinforcement learning setting where the rewards are sparse and non-uniform. Further research is proposed to discover these scenarios and explore the behaviors among different variants.", "pdf": "/pdf/8ccb0550dd14d532fa099e80104224635ea2e3cc.pdf", "paperhash": "lin|regularity_normalization_constraining_implicit_space_with_minimum_description_length"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper8/Decision", "cdate": 1554736070021, "reply": {"forum": "HklJQ1JEDE", "replyto": "HklJQ1JEDE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736070021, "tmdate": 1555510968723, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}