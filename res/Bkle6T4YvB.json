{"notes": [{"id": "Bkle6T4YvB", "original": "BJxbM_xODr", "number": 805, "cdate": 1569439159893, "ddate": null, "tcdate": 1569439159893, "tmdate": 1577168255912, "tddate": null, "forum": "Bkle6T4YvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZVPCbueQK_", "original": null, "number": 10, "cdate": 1576964840733, "ddate": null, "tcdate": 1576964840733, "tmdate": 1576964840733, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "4fMleifPuQ", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment", "content": {"title": "bilingual BERT baseline", "comment": "I thank AC for their meta-review. However, I feel that the main point of the paper is missed by reviewers. The purpose of the proposed transfer approach is to obtain a good bilingual model under a limited computational budget (1 day for BERT based model). The extra experiments show that even with 4 days of training, the resulting bilingual BERT is still heavily underperformed the RAMEN model. If an unlimited computational resource is given, bilingual-BERT perhaps will match and outperform RAMEN. But this is not the point of the paper as stated in the abstract \"While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper805/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkle6T4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper805/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper805/Authors|ICLR.cc/2020/Conference/Paper805/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165950, "tmdate": 1576860544977, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment"}}}, {"id": "4fMleifPuQ", "original": null, "number": 1, "cdate": 1576798706652, "ddate": null, "tcdate": 1576798706652, "tmdate": 1576800929593, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a method to transfer a pretrained language model in one language (English) to a new language. The method first learns word embeddings for the new language while keeping the the body of the English model fixed, and further refines it in a fine-tuning procedure as a bilingual model. Experiments on XNLI and dependency parsing demonstrate the benefit of the proposed approach.\n\nR3 pointed out that the paper is missing an important baseline, which is a bilingual BERT model. The authors acknowledged this in their rebuttal and ran a preliminary experiment to obtain a first set of results. However, since the main claim of the paper depends on this new experiment, which was not finished by the end of the rebuttal period, it is difficult to accept the paper in its current state. In an internal discussion, R1 also agreed that this baseline is critical to support the paper.\n\nAs a result, I recommend to reject this paper for ICLR. I encourage the authors to update their paper with the new experiment for submission to future conferences (given consistent results).", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707340, "tmdate": 1576800255523, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper805/-/Decision"}}}, {"id": "r1lhNKE0Yr", "original": null, "number": 2, "cdate": 1571862835526, "ddate": null, "tcdate": 1571862835526, "tmdate": 1574436837714, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "This paper presents a method to efficiently transfer pre-trained english language model to bilingual language model. The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state-of-the-art performances.\n\n\nPros:\n\n- Experiments clearly show that, using the proposed method, stronger pre-trained English embedding leads to stronger bilingual language model and thus to better performances for downstream foreign tasks.\n\nCons: \n\nWhile it is generally  intelligible, some structural modifications could be done to  improved the clarity of the paper. For instance, the method used to align foreign word vectors with English word vectors, when no aligned corpus is available, should appear sooner. It is described in 3.1 but should probably appear in 2.1 subsection Learning from Monolingual Corpus.\n\nMinor issues:\n\n- in section 3: RoBERA -> RoBERTa\n- in section 5.1: the third sentence is syntactically incorrect\n- in Conclusion: our approach produces better than -> our approach performs better than\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575056005285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper805/Reviewers"], "noninvitees": [], "tcdate": 1570237746747, "tmdate": 1575056005297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Review"}}}, {"id": "H1xVY-9Dsr", "original": null, "number": 2, "cdate": 1573523836288, "ddate": null, "tcdate": 1573523836288, "tmdate": 1573523836288, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "rylWllu1qB", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment", "content": {"title": "comparison to Bilingual BERT", "comment": "We thank reviewer for your valuable feedback. We agree with your point about the lack of comparision with a Bilingual BERT trained on two languages from scratch. We run an additional experiment to train a bilingual BERT-base from scratch for each language pair. For each language pair, we learn a join 60k bpe code and follow the same hyper-parameters setup described in section 3.3. In total, we train 6 bilingual BERT-base models. We evaluate these models at two checkpoints: (1) at 175,000 updates (the same number of updates with RAMEN), and (2) at 1,000,000 updates (~4 days) when we train these models longer.\n\nThe results of XNLI (accuracy) and UD (Labeled Attachment Score) are listed bellow. We see that RAMEN outperform Bilingual BERT evaluated at the two checkpoints. Hoverever, we note that if the Bilingual BERT is trained on much longer time, perhap it will eventually match or surpass RAMEN performance.\n\nXNLI @ 175K updates\nfr: 60.8 | ru: 46.6 | ar: 48.7 | hi: 44.4 | vi: 52.1 | zh: 58.2\nXNLI  @ 1000K updates\nfr: 69.7 | ru: 55.5 | ar: 59.1 | hi: 46.4 | vi: 58.3 | zh: 66.0\nRAMEN - XNLI (copy from the paper for reference)\nfr: 75.2 | ru: 69.4 | ar: 68.2 | hi: 62.2| vi: 71.0 | zh: 71.7\n\n\nUD @ 175K updates\nfr: 61.1 | ru: 30.7 | ar: 14.7 | hi: 15.3 | vi: 24.5 | zh: 19.7\nUD  @ 1000K updates\nfr: 69.2 | ru: 41.0 | ar: 17.8 | hi: 14.9 | vi: 26.3 | zh: 22.6\nRAMEN - UD (copy from the paper for reference)\nfr: 76.8| ru: 66.1 | ar: 32.9 | hi: 33.0| vi: 36.8 | zh:  29.7\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper805/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkle6T4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper805/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper805/Authors|ICLR.cc/2020/Conference/Paper805/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165950, "tmdate": 1576860544977, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment"}}}, {"id": "H1e8totDsB", "original": null, "number": 1, "cdate": 1573522301638, "ddate": null, "tcdate": 1573522301638, "tmdate": 1573522301638, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "SyxkmrNAFH", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank you for your feedback. We find the idea of transferring entire pretrained model is quite exiciting. When we started working on this idea, we try the most simple approach (presented in the paper) and to our surprise, it works quite well in comparision to multilingual BERT. We also observed that there is a large space for improvement in zero-shot transfer of dependency parsing. It seems that transferring syntax is harder than transferring semantics. We are looking forward to work on more exicting modeling approach to improve zero-shot dependency parsing."}, "signatures": ["ICLR.cc/2020/Conference/Paper805/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkle6T4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper805/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper805/Authors|ICLR.cc/2020/Conference/Paper805/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165950, "tmdate": 1576860544977, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper805/Authors", "ICLR.cc/2020/Conference/Paper805/Reviewers", "ICLR.cc/2020/Conference/Paper805/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Comment"}}}, {"id": "SyxkmrNAFH", "original": null, "number": 1, "cdate": 1571861783271, "ddate": null, "tcdate": 1571861783271, "tmdate": 1572972550120, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to adapt a pretrained BERT model from English to another languages with a limited time/GPU budget. Evaluation on 6 target languages shows good performance for natural language inference and dependency parsing. \n\nConcretely, the proposed approach consists of, starting from a pretrained English language model, first training language-specific embeddings and then fine-tuning the entire pretrained model on English *and* the target language, using those embeddings. The language-specific embeddings are initialized based on the English embeddings (the authors propose two different ways for doing that).\n\nI like about the paper that the approach is simple and fast. The experiments seem reasonable, too. The only minor negative point is that the approach is not particularly exciting."}, "signatures": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575056005285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper805/Reviewers"], "noninvitees": [], "tcdate": 1570237746747, "tmdate": 1575056005297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Review"}}}, {"id": "rylWllu1qB", "original": null, "number": 3, "cdate": 1571942376770, "ddate": null, "tcdate": 1571942376770, "tmdate": 1572972550033, "tddate": null, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "invitation": "ICLR.cc/2020/Conference/Paper805/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, the authors propose a way to transfer a pre-trained English BERT model to a new language within a short amount of time. The key insight is to map English embeddings to the foreign language and have separate embeddings for both English and the foreign language. The resulting bilingual LM is evaluated for zero-shot transfer learning on two tasks: XNLI and dependency parsing.\n\nPros:\n- The authors provide good details into their hyperparameter settings and about how the obtain the foreign language word embeddings.\n- By leveraging existing pre-trained models, they\u2019re able to do pre-training for their bilingual LM within 2 days.\n\nCons:\nI find that a key comparison point in this paper is missing, which is Bilingual BERT trained on just the two languages that are being considered for their RAMEN system. This is not a fair comparison while mBERT which is trained on 100+ languages is not.\nAll comparisons are not fair since a simple baseline of just training mBERT on two languages with monolingual data and with a shared WPM is not evaluated here.\nThe proposed system has an unfair advantage over mBERT since it\u2019s initialized from BERT/RoBERTA and fine-tuned only on two languages. Hence most of the parameters are used for just the two languages while mBERT uses the parameters for 104 languages.\nGiven this unfair comparison, I\u2019m not sure if we can draw a meaningful conclusion from all the experiments. \n\nRating justification:\nGiven the lack a fair comparison between the bilingual and multilingual BERT models, I don't think the conclusions are insightful.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper805/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "authors": ["Ke Tran"], "authorids": ["ketranmanh@gmail.com"], "keywords": ["pretrained language model", "zero-shot transfer", "parsing", "natural language inference"], "TL;DR": "How to train non-English BERT within one day on using a single GPU", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "pdf": "/pdf/e8d3036658a184a016b5b819a825f13c93e7c56f.pdf", "paperhash": "tran|from_english_to_foreign_languages_transferring_pretrained_language_models", "original_pdf": "/attachment/6fea8da76644f19eec927fc3616f0a9f3759b02f.pdf", "_bibtex": "@misc{\ntran2020from,\ntitle={From English to Foreign Languages: Transferring Pre-trained Language Models},\nauthor={Ke Tran},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkle6T4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkle6T4YvB", "replyto": "Bkle6T4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575056005285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper805/Reviewers"], "noninvitees": [], "tcdate": 1570237746747, "tmdate": 1575056005297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper805/-/Official_Review"}}}], "count": 8}