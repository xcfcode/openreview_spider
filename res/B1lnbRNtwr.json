{"notes": [{"id": "B1lnbRNtwr", "original": "Hkl-2P4dDH", "number": 981, "cdate": 1569439236409, "ddate": null, "tcdate": 1569439236409, "tmdate": 1583912042647, "tddate": null, "forum": "B1lnbRNtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "6ygjR6QOg", "original": null, "number": 1, "cdate": 1576798711391, "ddate": null, "tcdate": 1576798711391, "tmdate": 1576800924976, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper investigates hybrid NN architectures to represent programs, involving both local (RNN, Transformer) and global (Gated Graph NN) structures, with the goal of exploiting the program structure while permitting the fast flow of information through the whole program.\n\nThe proof of concept for the quality of the representation is the performance on the VarMisuse task (identifying where a variable was replaced by another one, and which variable was the correct one). Other criteria regard the computational cost of training and number of parameters.\n\nVaried architectures, involving fast and local transmission with and without attention mechanisms, are investigated, comparing full graphs and compressed (leaves-only) graphs. The lessons learned concern the trade-off between the architecture of the model, the computational time and the learning curve. It is suggested that the Transformer learns from scratch to connect the tokens as appropriate; and that interleaving RNN and GNN allows for more effective processing, with less message passes and less parameters with improved accuracy.\n\nA first issue raised by the reviewers concerns the computational time (ca 100 hours on P100 GPUs); the authors focus on the performance gain w.r.t. GGNN in terms of computational time (significant) and in terms of epochs. Another concern raised by the reviewers is the moderate originality of the proposed architecture. I strongly recommend that the authors make their architecture public; this is imo the best way to evidence the originality of the proposed solution. \n\nThe authors did a good job in answering the other concerns, in particular concerning the computational time and the choice of the samples. I thus recommend acceptance. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716806, "tmdate": 1576800267020, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper981/-/Decision"}}}, {"id": "SJgoCH9TKB", "original": null, "number": 3, "cdate": 1571821010666, "ddate": null, "tcdate": 1571821010666, "tmdate": 1574081118447, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper proposes improvements on existing probabilistic models for code that predicts and repairs variable misuses. This is a variant of the task, proposed by Vasic et al. The task takes a dataset of python functions, introduces errors in these functions and makes a classifier that would identify what errors were introduced and effectively reconstruct the original code.\n\nThe paper claims to improve state-of-the-art results published by Vasic et al for this task, however the RNN model by Vasic et al was known to be far from optimal when the work was published. Furthermore, that task was evaluated on artificially introduced changes (the original code could contain an error), but it is not clear that the improvements would have any practical effect. In fact, I conjecture that the bug-detector is in fact worse, because the entire dataset is not sufficiently large for millions of parameters and it is not clear that bugs that were originally the dataset were not learned by the better model, making it worse at spotting them. Given the relatively thinner contribution on the rest of the paper, I think this would be a valid question to be addressed to show the effectiveness of the model beyond accuracy on the artificial task.\n\nThe paper does a number of contributions to the neural architecture. The most important change precision-wise is to use transformer model instead of RNN (the model used by Vasic et al).  This change is also what makes the work perform as well or better than GGNN-based approaches. The paper then proposes to improve the transformer model by modifying the attention where there are edges. The rest of the contributions seem to be addressing the problem of aster convergence speed.  The other contribution of the paper is by selecting which edges to include and it is also shown to improve convergence speed.\n\nGiven that most of the work talks about performance, it would also help if the authors clarify what kind of hardware was used and which optimizer.\n\nQ: Why a larger transformer model was not evaluated?\n\nMore minor issues:\n\u201cWe conjecture that the Transformer learns to infer many of the same connections\u201d. There is no confirmation for this besides similar accuracy, but if this is the case, why would I change the architecture and not just try with initializing the vectors to values corresponding to this knowledge and get faster convergence?\npage 3, \u201cwhere q and k correspond to the query and key vectors as described above,\u201d. It seems it is q_i and k_j?\n\nUpdate after the rebuttal:\n\n - I thank the authors for running additional experiments on a short notice. I have some reservations about their correctness though (we still do not know if there were bugs fixed in these commits, their number is very low and the authors seem to have cherrypicked the numbers to show - their first updated revision had recall at 20%, then decided to show it at 10%, the RNN baseline is actually having the highest recall of all although it is not highlighted). I am not sure that data cleaning of this small evaluation sample will not show a different picture.\n\n - I actually increase my score a bit (I was torn in the beginning), because this is one of the first papers to run transformer model on code. I still think the actual contributions of the paper are minor.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575822810938, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper981/Reviewers"], "noninvitees": [], "tcdate": 1570237744132, "tmdate": 1575822810951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Review"}}}, {"id": "H1gGWvQ5oB", "original": null, "number": 10, "cdate": 1573693177947, "ddate": null, "tcdate": 1573693177947, "tmdate": 1573693233398, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "HkxIsMCKir", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Response to new questions", "comment": "Thanks for your suggestion. We had earlier performed deduplication from Allamanis et al. (SPLASH\u201919)  in the train and test splits for our synthetic data results, but we constructed our real-world Github bug dataset independent of that. We have now additionally deduplicated our Github data w.r.t. our synthetic training data, which resulted in discovering a small amount of overlap -- 9 out of 170 bugs had similar functions in the training data. We have updated the results in the paper on this dataset with 161 bugs. As may be expected, all models\u2019 performances dropped slightly, but the overall result remains the same; our models outperform prior models including GGNNs and sequence based models:\n\n|Models| \t\t|Real-world GitHub Bug localization & repair Precision|\nRNN                 \t\t\t13.3%\nTransformer\t\t\t\t15.8%\nGGNN\t\t\t\t\t17.7%\nRNN Sandwich\t\t\t28.6%\nTransformer Sandwich\t21.5%\nGREAT \t\t\t\t\t23.7%\n\nTo clarify the text on commits: we did not specifically limit our data to entire commits that only change one token; instead, we considered only functions changed in a commit if the commit changes a single token in that function, and that single-token change is an update to the name of a variable that is both declared and read in that function. Ensuring that the change only affects one of its usages, and that the variable is not declared out of the scope of that function avoids the risk of e.g. a global variable name change outside the function creating an apparent variable misuse. Additionally, since we are limiting to only a single token change in a function in which the same variable is used elsewhere at least once, this also removes cases in which there is e.g. a variable rename refactoring.\n\nThe Adverse Effects of Code Duplication in Machine Learning Models of Code\nM. Allamanis. SPLASH Onward! 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "HkxIsMCKir", "original": null, "number": 9, "cdate": 1573671582156, "ddate": null, "tcdate": 1573671582156, "tmdate": 1573671582156, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "ryg-TanKsS", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Questions", "comment": "Indeed, these new results are quite interesting.\n\nCould you confirm if the 170 commits do or do not have overlapping methods with the training data?\n\nText clarifications:\nIt is not clear from the text if the commits only change one token or there is only token change in the specific function (that just matches something changed elsewhere and was not a misuse).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "ryg-TanKsS", "original": null, "number": 8, "cdate": 1573666233342, "ddate": null, "tcdate": 1573666233342, "tmdate": 1573666233342, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "SJgK1zcQir", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Follow-up to AnonReviewer2", "comment": "Please let us know if our response helped clarify your concerns regarding the synthetic training data's generalization, and helped elucidate our performance objectives. Also, please let us know about any additional clarifications or questions that may help improve your assessment of our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "ryxhyq3tjB", "original": null, "number": 7, "cdate": 1573665252431, "ddate": null, "tcdate": 1573665252431, "tmdate": 1573665496371, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "r1xSTbc7sS", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Follow-up to AnonReviewer1", "comment": "Please let us know if our response helped clarify your concerns regarding the novelty of our sandwich and GREAT models, and correctness of the claims. Also, please let us know about any additional clarifications or questions that may help improve your assessment of our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "Bkg3QfqQoS", "original": null, "number": 5, "cdate": 1573261860316, "ddate": null, "tcdate": 1573261860316, "tmdate": 1573261860316, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "New results on real-world bugs", "comment": "Several reviewers asked if our new models yield improved performance at detecting real world bugs. To explore this, we collected a set of 170 real-world variable misuse bugs from Github projects. The dataset contains both the original version of the code with a var misuse bug, and the fixed version. \n\nWe find that both of our new model classes, GREAT and sandwich, show a large practical improvement over any of the baseline models. See Table 2 in revised paper. For example, for classification accuracy (bug versus no-bug), we find:\n\nRNN: 57.1% \nTransformer: 62.9% \nGGNN: 67.1% \nRNN Sandwich: 67.7% \nTrans. Sandwich: 76.5% \nGREAT: 71.2%\n\nAdditionally, at localizing and repairing the bugs correctly, this is a much more difficult problem, but we similarly observe a large increase in performance:\n\nRNN: 18.2%\nTransformer: 22.8%\nGGNN: 20.6%\nRNN Sandwich: 31.6%\nTransformer Sandwich: 27.1%\nGREAT: 27.7%\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "HygHbzqXiH", "original": null, "number": 4, "cdate": 1573261821052, "ddate": null, "tcdate": 1573261821052, "tmdate": 1573261821052, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "HklypX-iYH", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for their constructive comments. With regards to the detailed comments:\n\n1. We have updated the paper with the now-completed experimental results, which are largely in line with was submitted, but mainly demonstrate that our GREAT model outcompetes the Sandwich models in some metrics in the long term.\n\n2. The difference in curvature is due to the LOESS smoothing picking up smaller variations on this substantially zoomed in region of the data (the Y-axis is zoomed in the region of 70-85% in Fig. 2 compared to unzoomed y-axis in Fig. 1); the underlying data points (marked with \u2018x\u2019) are the same. The cause for this transition is the underlying GGNN; at this inflection point, the 12-layer model first starts to out-compete the 8-layer architectures, setting a series of new highs in quick succession. A similar, if more nuanced inflection point can be seen at around 100h, where the 16-layer GGNN overtakes it in turn.\n\n3. There are several perspectives on this. From a performance point of view, the GGNN in source code settings is somewhat hindered by the many distinct types of edges that are present there (more than 20 in our work), which roughly linearly increase modeling time. We note that this is not out of line with prior work on modeling source code, which has invested considerable efforts in making these models scale to moderate-sized training sets (see Allamanis et al., ICLR\u201918 & Balog et al., ICLR\u201919). More generally, however, we stress that the speed of the models is not our main target here; even if we compare the performance of these models w.r.t. the number of samples seen, regardless of how long it took to process these, we still see very similar training curves with the same ultimate outcomes. We recognize that our plots\u2019 emphasis of time (on the x-axis) may have created the wrong impression and have therefore added another, time-agnostic comparisons to Figure 1 in the paper, in which we instead compare the number of samples seen to training accuracy. Here too, both in the limit and throughout the training process, our models substantially outperform the prior state-of-the-art.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "SJgK1zcQir", "original": null, "number": 3, "cdate": 1573261792762, "ddate": null, "tcdate": 1573261792762, "tmdate": 1573261792762, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "SJgoCH9TKB", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for their extensive feedback. Principally, we want to emphasize that the goal of our work is neither to 1) outperform Vasic et al.\u2019s proposed models, nor 2) propose models that train very quickly, although both of these are of course useful outcomes of our proposed techniques.\n\nThe main contribution of our paper is the conceptual combination of structural features and global models. We leverage this idea to present two new models to embed code -- Sandwich models and GREAT -- which we compare against the current state-of-the-art in representation learning for code: GGNNs (Allamanis et al.). We show that our models perform significantly better not just in terms of convergence speed, but also in terms of final convergence accuracies (irrespective of the time the models take). Note that this improved performance is not just a product of switching from RNNs to (edge-biased) Transformers; the family of Sandwich models provides valuable insights into alternative forms of mixing message passing networks and traditional sequence-based models (RNN, Transformers) and are substantially more accurate with relatively few training samples. Indeed, some of our best results are achieved with RNN-style Sandwich models.\n\nPerformance-wise, our proposed models do not just train faster, but are substantially more accurate regardless of training speed, both in terms of ultimate convergence, but especially also in terms of data efficiency. We recognize that our plots\u2019 use of time on the x-axis may have given the impression that we were optimizing for efficiency given some time budget; so, we added an equivalent plot to Figure 1 that instead plots performance against the number of samples seen (conf. epochs), demonstrating that the various models\u2019 curves are very similar regardless of time, and the convergent outcomes the same. This is also why we do not discuss our hardware (P100 GPUS, we will add this to the paper); our results are independent of time or implementation.\n\nYour comment regarding the risk of learning on synthetic data is well taken. Therefore, in addition to evaluating the models on synthetically generated bugs, we have also added an evaluation on variable-misuse errors mined from commits in a real-world GitHub dataset, in Section 5.5. The dataset was obtained by collecting one million commits to GitHub Python projects and extracting variable misuses using the same criteria we used to introduce them (i.e., a commit changing a single variable usage in a function, from one variable in scope to another), which we also used to introduce them synthetically. As can be observed from the table below, the GREAT and Sandwich models outperform GGNNs, Transformer, and BiRNN models by 7% or more.\n\n|Models| \t\t|Real-world GitHub Bug identification Accuracy|\nRNN                 \t\t18.2%\nTransformer\t\t\t22.8%\nGGNN\t\t\t\t20.6%\nRNN Sandwich\t\t31.6%\nTransformer Sandwich\t27.1%\nGREAT \t\t\t27.7%\n\nPlease find more detailed results on real-world GitHub bugs for different recall percentages in Table 2 in the updated draft.\n\nRegarding larger Transformer models: we considered transformers with up to 10 layers and 1024 attention dimension (substantially larger than those originally proposed by Vaswani et al.) and found that the relative difference in performance with our proposed model was consistent across architecture sizes (in terms of number of parameters). Furthermore, all our models in our various comparisons are matched across comparable ranges of parameters, regardless of the underlying architectures.\n\nIt may indeed be beneficial to initialize a Transformer to incorporate edge-related bias, though we are not aware of any work that has proposed to do so. We suspect, however, that this will be less effective than incorporating the bias directly into the architecture, as we do. These two forms of inductive bias may also be complementary, which is worth investigating further.\n\nYour comment regarding \u2018q\u2019 and \u2018k\u2019 on page 3 is correct; we will improve this notation in the paper.\n\nWe hope this helps clarify the novelty and significant improvements of our sandwich and GREAT models (in terms of final accuracy) over the previous state-of-the-art GGNN models. Please let us know if there are any other questions or comments.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "r1xSTbc7sS", "original": null, "number": 2, "cdate": 1573261756707, "ddate": null, "tcdate": 1573261756707, "tmdate": 1573261756707, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "HJelNGkaKr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We appreciate your comments and are glad that you found the paper easy to follow and the results to be noteworthy.\n\nFirst of all, we would like to emphasize that the main novel contribution of our paper is to present two new code-embedding models (Sandwich and GREAT), which achieve significantly better performance and accuracies than GGNNs (Allamanis et al. 2018), which is considered to be the current state-of-the-art for modeling code.\n\nSecondly, this performance follows from our key conceptual contribution, which is the insight that, while GNNs succeed by capturing information that is not _lexically_ local, the choice of edge types and message passing still confines it to a _topologically_ local space. That is, although a GNN can reach information from far away, it is heavily biased towards information that is reachable via relatively few \u2018hops\u2019 in the graph for entirely practical reasons (i.e., because it\u2019s very expensive to run more than, say, 12 message-passing steps). This is in sharp contrast to e.g. Transformer models, which are \u2018global\u2019 in that they can choose to attend to any information at any point (even across tokens that may be far more than, say, 12 hops away along any program-graph edge paths). We demonstrate through our various proposed hybrid models that although this structural bias does give the GNN an initial edge over unstructured counter-parts, its lack of truly global information is also a limitation that can be complemented, and the model greatly improved, by combination with other, global models. This ties in closely to the claim (in the abstract) of graph-based models being \u2018local\u2019; this is by no means a mistake, but rather an essential insight to understand our contribution and proposed models. We will clarify this in the paper.\n\nWe appreciate your reference to the related work on \u201cContextualized Non-local Neural Networks for Sequence Learning\u201d; we read the paper with interest. The two methods are similar in spirit, but are architecturally rather different. Our GREAT model is a direct extension to the Transformer architecture, whereas the previous work makes a number of architectural changes (such as a different similarity measure from Transformers, explicit layers for learning graph structure and performing message passing, concatenating edge features, updating node states via gating, etc.), whose effect could be challenging to disentangle. Another key difference is that in our domain, programs have much more explicit and deterministic structure, such as control flow, data flow, compared to natural language. This leads to a much larger set of edge types than used in that work.\n\nAdditionally, our sandwich models present a very different point in the architectural-design space, where the graph structure is fixed a-priori from the program structure and allows for additional layer alterations between the Graph and RNN layers, whereas this work only applies a single LSTM (or CNN) before any message passing, much like Fernandes et al. (ICLR\u201919). We will add this discussion to the related work section.\n\nPlease let us know if this clarifies your concerns.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "HklypX-iYH", "original": null, "number": 1, "cdate": 1571652534639, "ddate": null, "tcdate": 1571652534639, "tmdate": 1572972527637, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors proposed a new method to model the source code for the bug repairing task. Traditional methods use either a global sequence based model or a local graph based model. The authors proposed a new sandwich model like [RNN GNN RNN]. The experiments show that such simple combination of models significantly improve the localization and repair accuracy. \n\nThe idea is simple, so the technical contribution is a bit low. But the message is clear. The sandwich model can benefit from GNN model that can achieve a higher accuracy at the beginning of training where transformer did a poor job. At the end of training, the sandwich model outperforms both kinds of models.\n\nHere are some detailed comments:\n1. It would be interesting to have the complete result from Transformer in Figure 1 and Figure 2 which is missing.\n2. The results from GGNN (smaller model) in Figure 1 and Figure 2 seems to be not the same.\n3. One major benefit of GNN is its efficient local computation. Some industrial applications have also used GNN for recommendation that can be trained very fast. Why GGNN is so slow in this paper? Is this because of implementation? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575822810938, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper981/Reviewers"], "noninvitees": [], "tcdate": 1570237744132, "tmdate": 1575822810951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Review"}}}, {"id": "HJelNGkaKr", "original": null, "number": 2, "cdate": 1571775015606, "ddate": null, "tcdate": 1571775015606, "tmdate": 1572972527603, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Strength:\n-- Interesting problem\n--The paper is well written and easy to follow\n-- The proposed approach seems very effective\n\nWeakness:\n-- the novelty of the proposed is marginal\n-- Some of the claims are not right in the paper\n\nThis paper studied learning the representations of source codes by combining sequential-based approaches (RNN, Transformers) and graph neural network to model both the local and global dependency between the tokens. Experimental results on both synthetic and real-world datasets prove the effectiveness of the proposed approach.\n\nOverall, the paper is well written and easy to follow. However, the novelty of the proposed technique seems to be marginal to me. Some of the claims in the paper are not right. In the abstract, the authors said the graph neural network is more local-based while the transformer is more global-based. The essential difference between the two approaches lie in the way of constructing the graphs since transformer used the fully-connected graph (more local dependency) while graph neural networks usually capture the long-range dependency. \n\nAnd there are actually some existing work that have already explored this idea in the context of natural language understanding, e.g.,\nContextualized Non-local Neural Networks for Sequence Learning. https://arxiv.org/abs/1811.08600\nThe authors should clarify the difference between these approaches.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575822810938, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper981/Reviewers"], "noninvitees": [], "tcdate": 1570237744132, "tmdate": 1575822810951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Review"}}}, {"id": "BygeU4qC_S", "original": null, "number": 1, "cdate": 1570837575714, "ddate": null, "tcdate": 1570837575714, "tmdate": 1570837575714, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "SJeTdt32OS", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment", "content": {"comment": "Thank you for your interest in our work.\n(1) That is correct, these can be pre-computed per edge type; our implementation does in fact do this. We will improve the description in the paper.\n\n(2) Thank you for pointing out this work. The key difference in the model is that they introduce relations only after the usual MHA computations, on the resulting \u2018values\u2019, whereas our work directly alters the attention weights. As such, their approach does not include an explicit relational term, and does not explicitly take a sparse graph as input --- unlike our method. These two approaches seem quite orthogonal and could have complementary benefits; ours more closely follows the approach by Shaw et al. (2018).\n\n(3) First, we want to point out that both the GREAT and Sandwich models not only improve the training speed, but also significantly improve the final accuracy of the models. The GGNN models trained for significantly longer still perform significantly worse (>10% worse accuracy).\nSecond, the dataset in Allamanis et al., ICLR 2018, is significantly smaller than ours. Our Python dataset contains more than 3 million samples. Prior work on this same Python dataset also found continued improvements over several days of training, albeit with RNNs (Vasic et al., ICLR 2019).\nFinally, our experience is similar to yours in the sense that we are able to get reasonable performance after 13 hours of training, but given our larger Python training set, we find that accuracy keeps increasing with much longer training times. Our reported time-to-convergence numbers are based on the time to best possible accuracy on a validation set.\nOur implementation gets similar throughput to what you describe. Our 8-layer GGNN\u2019s with a 50K node batch-size trained at ~75 \u2018full\u2019 graphs/second, which increases to 150+ on graphs without AST nodes; these numbers become 50 & 100 for 16-layer GGNNs. However, the best-performing instances used a batch size of 12.5K, which train ca. 3x slower. We also note that we trained on P100 GPUs.\n\nWe note that we could not evaluate our models on the dataset from Allamanis et al. (ICLR\u201918) because the dataset has incomplete sequence to graph node mapping information; the graphs in that work only included nodes that were reachable within 8 steps of a selected node, whereas we require the combined full graph (in alignment with the function body) for our hybrid models.", "title": "Response to Marc Brockschmidt's questions"}, "signatures": ["ICLR.cc/2020/Conference/Paper981/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper981/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper981/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper981/Authors|ICLR.cc/2020/Conference/Paper981/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163127, "tmdate": 1576860542093, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Official_Comment"}}}, {"id": "SJeTdt32OS", "original": null, "number": 1, "cdate": 1570716020789, "ddate": null, "tcdate": 1570716020789, "tmdate": 1570717545569, "tddate": null, "forum": "B1lnbRNtwr", "replyto": "B1lnbRNtwr", "invitation": "ICLR.cc/2020/Conference/Paper981/-/Public_Comment", "content": {"comment": "Thank you for this paper. I was left with three questions:\n\n(1) In the definition of your GREAT model, you define $b_{ij} = W_e^T \\mathbf{e} + b_e$, where e is the type of the edge connecting $i$ and $j$. I don't understand the affine transformation here: Its result is fixed per edge-type, and hence could be \"pre-computed\" in the embedding of the edge type?\n\n(2) There is concurrent work on improving transformers by making learned relations explicit submitted to ICLR (https://openreview.net/forum?id=B1xfElrKPr ). Could you comment on how their approach (which is an elementwise modulation of the output of the attention layer, based on the input) relates to your model?\n\n(3) I am somewhat confused by the speed that you reported for your GNN implementation in section 5. Concretely, I've recently run _a lot_ of experiments on our publicly released VarMisuse dataset, with a number of different architectures. I can report that on a V100, a GGNN with 10 propagation steps and a hidden size of 128 converges in about 6 hours (processing about 120 graphs per second during training). While I understand that you use 16 rather than 10 propagation steps, I'm somewhat surprised that your models need 40x time to converge. Do you have intuition what causes this slowdown?\n\nThanks,\nMarc", "title": "Questions"}, "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Marc_Brockschmidt1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vjhellendoorn@gmail.com", "charlessutton@google.com", "rising@google.com", "maniatis@google.com", "dbieber@google.com"], "title": "Global Relational Models of Source Code", "authors": ["Vincent J. Hellendoorn", "Charles Sutton", "Rishabh Singh", "Petros Maniatis", "David Bieber"], "pdf": "/pdf/1a70bffc358e61847e6c4b29f824a9204f8ce4c3.pdf", "TL;DR": "Models of source code that combine global and structural features learn more powerful representations of programs.", "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "keywords": ["Models of Source Code", "Graph Neural Networks", "Structured Learning"], "paperhash": "hellendoorn|global_relational_models_of_source_code", "_bibtex": "@inproceedings{\nHellendoorn2020Global,\ntitle={Global Relational Models of Source Code},\nauthor={Vincent J. Hellendoorn and Charles Sutton and Rishabh Singh and Petros Maniatis and David Bieber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lnbRNtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/eeffeebe48ecb72497dbbe5e2d5b5f373fe10737.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lnbRNtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201343, "tmdate": 1576860575514, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper981/Authors", "ICLR.cc/2020/Conference/Paper981/Reviewers", "ICLR.cc/2020/Conference/Paper981/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper981/-/Public_Comment"}}}], "count": 15}