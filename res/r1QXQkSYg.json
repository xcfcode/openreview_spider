{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028621029, "tcdate": 1490028621029, "number": 1, "id": "SySL_Kpjl", "invitation": "ICLR.cc/2017/workshop/-/paper134/acceptance", "forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028621530, "id": "ICLR.cc/2017/workshop/-/paper134/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028621530}}}, {"tddate": null, "tmdate": 1489416494485, "tcdate": 1489416494485, "number": 2, "id": "HJIEbENog", "invitation": "ICLR.cc/2017/workshop/-/paper134/public/comment", "forum": "r1QXQkSYg", "replyto": "rJf7vaeie", "signatures": ["~mehdi_cherti1"], "readers": ["everyone"], "writers": ["~mehdi_cherti1"], "content": {"title": "Answer", "comment": "Thank you for your comments and suggestions. \nWe are working on a more detailed analysis to understand under \nwhich conditions we obtain a model that generates novelty."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487364891877, "tcdate": 1487364891877, "id": "ICLR.cc/2017/workshop/-/paper134/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper134/reviewers"], "reply": {"forum": "r1QXQkSYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487364891877}}}, {"tddate": null, "tmdate": 1489416446938, "tcdate": 1489416446938, "number": 1, "id": "H1w-b4Esg", "invitation": "ICLR.cc/2017/workshop/-/paper134/public/comment", "forum": "r1QXQkSYg", "replyto": "SkUcM-bjl", "signatures": ["~mehdi_cherti1"], "readers": ["everyone"], "writers": ["~mehdi_cherti1"], "content": {"title": "Answer", "comment": "Thank you for your comments and suggestions. We definitely want\nto redo the same experiments and analysis on other settings or \ndatasets like Omniglot for which the availability of a large\nnumber of classes will be helpful.\nRegarding your question about how the pangrams were generated,\nwe took the set of images generated by a given model, then\nwe selected manually one character from the top 16 in every letter, \nwhere the top 16 was selected automatically according to the predicted\nprobability of the letter according to the discriminator which was \ntrained on digits and letters."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487364891877, "tcdate": 1487364891877, "id": "ICLR.cc/2017/workshop/-/paper134/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper134/reviewers"], "reply": {"forum": "r1QXQkSYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487364891877}}}, {"tddate": null, "tmdate": 1489207949881, "tcdate": 1489207949881, "number": 2, "id": "SkUcM-bjl", "invitation": "ICLR.cc/2017/workshop/-/paper134/official/review", "forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "signatures": ["ICLR.cc/2017/workshop/paper134/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper134/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper attempts to formalize a notion of creativity in generative models. The idea is to see if a generative model trained on one dataset can be used to generate novel samples that resemble elements of another dataset. In this case, it is examined whether a generative model trained on digits could be used to generate samples that look like alphabetical characters. Several metrics for determining the alphabetical nature of the generated samples are given; this is used as a proxy for novelty. It is shown that these can be useful in choosing models that generate novel samples outside of the classes the model was initially trained on.\n\nI can agree with the premise that when it comes to out-of-class novelty, likelihood is probably not a good measure since it will penalize models that generate samples that are too far outside of the data distribution. However, I'm not  yet convinced that the conclusions drawn here would generalize beyond the specific examples given in the paper. It would be good in a future iteration to see this same analysis on another dataset, or perhaps even to reverse the existing experiment (train on alphabetical characters, evaluate on digits). Another possibility would be to test on several different alphabets, like those found in Omniglot.\n\nAlthough I think this particular analysis is limited (it is a workshop submission), I do think it proposes an interesting direction for measuring the novelty of samples from a generative model. I could see this being a potentially useful direction for measuring interesting properties of generative models in terms of creativity.\n\nHow are the panagrams (a)-(d) generated? Are letters chosen based on Euclidean distance to some reference characters?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489207950828, "id": "ICLR.cc/2017/workshop/-/paper134/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper134/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper134/AnonReviewer2", "ICLR.cc/2017/workshop/paper134/AnonReviewer1"], "reply": {"forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper134/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489207950828}}}, {"tddate": null, "tmdate": 1489192729762, "tcdate": 1489192729762, "number": 1, "id": "rJf7vaeie", "invitation": "ICLR.cc/2017/workshop/-/paper134/official/review", "forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "signatures": ["ICLR.cc/2017/workshop/paper134/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper134/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "\nThis paper attempts to formalize the notion of 'computational creativity' from a machine learning perspective, in order for machine learning researchers to make better progress on this problem. In particular, the authors propose measuring the 'computational creativity' of a model by several metrics intending to capture whether the model can generate new objects from classes unseen during training.\n\nI think this is an interesting paper and a good first step in this area. Indeed, absent proper definitions and metrics for vague concepts such as 'creativity', it is difficult to make progress on related computational problems. While the proposed metrics are not perfect,* they seem reasonable enough to warrant future investigation, and thus I think this paper is worthy of acceptance as an ICLR workshop paper.\n\n*Further thoughts: I'm not convinced that these metrics are selecting for the \"right\" models from a creativity point of view. If Figure 1 is really a random sample of digits generated by one of the 'most creative' models according to the proposed metrics, it seems like it is mostly just good at capturing lower-level correlations in the data, while generating random high-level details. Thus it seems like a 'creative' model is one that has been artificially limited in order to poorly model high-level features of the data. This seems intuitively to contrast with creativity as we perceive it in humans -- creative humans are still capable of modeling the world around them, they are just able to combine what they've learned in new and interesting ways. Perhaps 'true creativity' is out of the reach of current generative models? (Or, perhaps the word 'creativity' is not really meaningful from a computational perspective?)  However, I'm not an expert in this area, and I still think the idea is worthwhile presenting, as it may generate interesting discussions. In future work, I'd like to see a more thorough analysis of what model settings lead to  the most 'creative behaviour' according to these metrics.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489207950828, "id": "ICLR.cc/2017/workshop/-/paper134/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper134/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper134/AnonReviewer2", "ICLR.cc/2017/workshop/paper134/AnonReviewer1"], "reply": {"forum": "r1QXQkSYg", "replyto": "r1QXQkSYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper134/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489207950828}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487487761409, "tcdate": 1487364891253, "number": 134, "id": "r1QXQkSYg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1QXQkSYg", "original": "ByEPMj5el", "signatures": ["~mehdi_cherti1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.", "pdf": "/pdf/eeb028184f0138482f8c900a366c6b11d9dd69bb.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazs.kegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1483650019405, "tcdate": 1478304348397, "number": 536, "id": "ByEPMj5el", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByEPMj5el", "signatures": ["~mehdi_cherti1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Out-of-class novelty generation: an experimental foundation", "abstract": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.\n", "pdf": "/pdf/caeae8b5f18946c9886063a7aad979ad7df862ba.pdf", "paperhash": "cherti|outofclass_novelty_generation_an_experimental_foundation", "conflicts": ["lri.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Mehdi Cherti", "Bal\u00e1zs K\u00e9gl", "Ak\u0131n Kazak\u00e7\u0131"], "authorids": ["mehdicherti@gmail.com", "balazskegl@gmail.com", "akin.kazakci@mines-paristech.fr"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}