{"notes": [{"id": "y2I4gyAGlCB", "original": "foYUUePqug3", "number": 1628, "cdate": 1601308180479, "ddate": null, "tcdate": 1601308180479, "tmdate": 1614985634691, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "stwBeEViugK", "original": null, "number": 1, "cdate": 1610040526435, "ddate": null, "tcdate": 1610040526435, "tmdate": 1610474135598, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a method for tool synthesis by jointly training a generative model over meshes and a task success predictor. Gradient-based planning is then used to find a latent space tool representation which maximizes task success, given a starting tool and an input scene. The results indicate that this method can successfully generate simple tools, and that it performs better than either a random baseline or a version where the generative model and success predictor are trained independently.\n\nThe reviewers unanimously felt that this paper was not quite ready for publication at ICLR. While I'm a strong believer that unique and creative papers which tackle understudied problems---such as this one---ought to be encouraged, and that the authors' rebuttal satisfactorily addressed most of the reviewers' concerns, there was one major point that was not. In particular, all reviewers noted that the paper lacks comparison to convincing baselines and/or sufficiently extensive experiments. While I do not think baselines are necessary per se, especially in such a unconventional setting such as this, I believe what the reviewers are getting at (and I agree) is that the results as presented don't really help the reader understand the contours of the method and/or problem space, and as a result, the contributions of the paper feel thin. For example, here are some questions that the reviewers raised, which I do not feel were adequately addressed:\n\n- R3: What are the failure cases of the model?\n- R2: How important is the particular representation of the task and tool (i.e., visual for the task, meshes for the tool)?\n- R4: How do the imagined tool trajectories compare between the task-aware and task-unaware cases?\n- R4: Is the success classifier trained to the same level of performance in both task-aware and task-unaware settings? (In general, it would be helpful to include learning curves in the appendix.)\n- R1: How important is the choice of the particular planning/optimization method (i.e. gradient descent)?\n- R1: What is the generalization performance of the model along affordance directions (e.g. needing to synthesize longer/shorter tools than seen during training)?\n\nTaken individually, such questions might not be an issue, but together they illustrate a larger concern that the paper has not done a thorough enough job of analyzing and evaluating the proposed method. Therefore, at this stage I recommend rejection. I think that by fleshing the paper out with some answers to the above questions, this could make an excellent submission to a future conference."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040526422, "tmdate": 1610474135583, "id": "ICLR.cc/2021/Conference/Paper1628/-/Decision"}}}, {"id": "QnqwFETJPw", "original": null, "number": 1, "cdate": 1603839782506, "ddate": null, "tcdate": 1603839782506, "tmdate": 1607410261785, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review", "content": {"title": "Initial review", "review": "Summary\n-------\nBy combining a task-success classifier with the latent space of a tool-shape generative model, this paper shows that an activation-maximization approach can generate tool shapes which can succeed at particular tasks.  \n\nPositives\n---------\nThe paper addresses the interesting topic of affordances, and how latent representations of tool shapes might be structured.\n\nThe methods and experiments are clean and well motivated, and the writing is clear.\n\nNegatives\n---------\nThe specific contribution of the paper is difficult to see at first glance.  The architecture is taken from previous work, as well as the method for maximizing activations.\n\nAlthough the task-unaware baseline is a reasonable one to compare against, I would like to see other comparisons to approaches present in the literature.  For example, how do the supervised affordance learning approaches perform in the tool imagination task when combined with task-success prediction?  Additionally, are other methods for latent disentangling sufficent to observe similar behavior?\n\nIn the tool imagination task, it would be useful to see a couple of things from the task-unaware approach to make sure the conclusions are sound.  First, how well does the activation-maximization step work?  Is the model reaching the same level of feasibility for task aware and unaware versions?  Second, what do the imagined tool trajectories look like for the task unaware approach?  If they are also smooth and encode length, width, etc., it would be hard to claim that task-aware approaches are required for that type of encoding.\n\nReasons for score\n-----------------\nThe topic explored is interesting, and the experiments are simple and illustrative, but there are remaining questions about the baseline comparisons required to make strong conclusions.\n\nPost-rebuttal response\n-----------\nThank you for the detailed rebuttal responses.  The rebuttal suggests that there are not many other baselines against which to compare.  If that is true, I would want to see much more detailed analysis of the comparisons offered in the paper.  However, I am still missing details of the latent space generated by the task-unaware approach.  I will leave my score as it is.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114359, "tmdate": 1606915807530, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1628/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review"}}}, {"id": "AnA218efMpU", "original": null, "number": 3, "cdate": 1603935656581, "ddate": null, "tcdate": 1603935656581, "tmdate": 1606804511866, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review", "content": {"title": "Is imagination necessary here?", "review": "This paper presents a method with two goals: (1) estimate if a given tool can solve a given task, and (2) generate a tool that can solve a given task. One encoder maps a tool image and silhouette into a latent code, and a decoder maps this code into a mesh; another encoder maps a task image into a latent code; this code is concatenated with the tool code, then mapped to a task success probability. The networks are trained together, so that task success probability has some impact on the tool encoder's latent space. Results show that the model is largely successful in both tasks, but no challenging baselines are here. \n\nI think the idea overall is creative and well-executed, and the paper is well-written. \n\nI was happy to see that the supplemental includes information about how feasibility labels are generated. I think this should be in the main paper.   \n\nWhat is the geometry loss L_g? I looked in the cited paper and saw that details are bare there too -- it only says \"regularizes the Laplacian of both shape and color predictions\". What does this mean exactly, and why does it help?\n\nWhy is the success signal \\rho called \"sparse\" (top of page 5)? My understanding is that this label is available on every training example. \n\nWhen I think of \"imagination\", I think of interpolating or extrapolating into space that was not quite seen at training time. This leads to two ideas: \n\n1. The task is simple enough that imagination may not be necessary. How about a nearest neighbors baseline? Classify the task using nearest neighbor image retrieval (searching in the training set, using maybe the task encoding z_G), and use the tool\u00a0that was successful in that training example. \n\n2. How about creating a setup where imagination is definitely necessary? e.g., generate a scenario that requires a tool in-between one of the tools seen at training time. Maybe train with long sticks and short L's, and create a task that demands a long L. This would show that the model has a useful latent space and that the imagination (i.e., latent space traversal) is beneficial. The paper notes a novel \"T-shape tool\" but I am not sure this counts, since in Figure 4 the T is only some intermediate stage of the optimization, and in Figure 5, a T is a strange choice for the task shown (in row 3), so it seems like traversal simply stopped when the horizontal part shifted far enough, and there is no purpose to the \"T\" tool per se.\n\n\n-------------\nPost-rebuttal: After reading the rebuttal and the other reviews, I have decided to lower my rating. The rebuttal appears to state that all baselines and evaluations suggested by the reviewers are irrelevant to the goals of the paper. I see the two main hypotheses clarified in the rebuttal, but I do not see a convincing argument for avoiding comparisons to related approaches to the task at hand. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114359, "tmdate": 1606915807530, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1628/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review"}}}, {"id": "yfS-VAfmelL", "original": null, "number": 2, "cdate": 1603877781512, "ddate": null, "tcdate": 1603877781512, "tmdate": 1606794515491, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review", "content": {"title": "Review", "review": "The authors try to tackle the problem of tool synthesis by using a classifier to guide the learning and exploitation of a generative model through activation maximization. Experiments are conducted on the proposed simulated reaching dataset on tool selection and tool imagination.  While the idea of synthesizing tools step by step continuously is interesting, the technical and experimental design make the problem over-simplified and thus raise some concerns. \n\nThe overall concern for the paper is two-fold: 1) Some overclaims. Affordance is often used to describe the potential action that can be applied to certain objects. In this case only the shape of tool is changing however the concept of `\"emergent affordances\" is hardly touched. Also current proposed dataset only tackle reaching task in a static way. So the problem degenerates to a geometric constraint satisfaction problem. It would be interesting to add more different tasks and potentially extend the current binary label for task success.  2) The experiment design is weak.  For the tool selection task, the task-unaware and task-driven model achieve similar result, which means information from task predictor is not helpful in this case. In tool imagination task, the gradient doesn't flow back to the learned representation in the task-unaware case so suboptimal performance understandable. It would be better if the test setting is more diversified with more degree of freedom in the simulation dataset to showcase the capability of proposed model.\n\nThe paper is easy to follow despite some typos, e.g. the bold numbers in Table1. \n\nSome detail questions:\nWhat are the differences for 5 types of the scenarios in detail? \nHow many images of tools are used to train 3D reconstruction model?\nIs the position/orientation of the tool fixed in the tool selection and tool imagination evaluation? Is it possible that a tool that cannot finish the reaching task at one location can reach the target when positioned at another location/direction?\nAre there any failure cases in tool imagination and what might be the reason?\nIn tool selection task, how are the tools sampled apart from the ground truth?\n\n------\nPost rebuttal:\nAfter reading the author's response as well as the opinions from other reviewers, I will stick to my original rating. Although the authors resolve some of the concerns in the rebuttal, there are still limitations in the method and task design.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114359, "tmdate": 1606915807530, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1628/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review"}}}, {"id": "eBaRxERB7QI", "original": null, "number": 3, "cdate": 1605542824506, "ddate": null, "tcdate": 1605542824506, "tmdate": 1605542824506, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Comment", "content": {"title": "Rebuttal Part II", "comment": "As noted by several reviewers, our direction of travel is interesting and creative. It is novel in the context of representation learning for object affordances. We are unaware of any other works exploring similar research hypotheses. Consequently, we were unable to identify suitable baselines. \n\n- What are the differences between the five scenario types? Can a tool that does not succeed in one place succeed in another? Generate scenarios where tools in-between are required? [AR1, AR3]\nBeyond tool synthesis, the agent is free to choose tool position and orientation for all scenarios - so yes, a tool that does not succeed in one place can succeed in another. The only constraints are that the agent itself is not allowed to enter the red work space. The scenarios are designed such that they encourage the emergence of different aspects of tool synthesis. [A] is entirely unconstrained (position, length), [B] does not allow direct access to the target but is otherwise unconstrained (length and tool geometry), [C] requires a bottleneck to be traversed before accessing the target (encourages a change in length and width), [D] contains a target off-centre beyond a bottleneck (so encourages a change length, width and tool geometry), [E] provides multiple bottlenecks (at this point all aspects investigated are required to vary). One might argue that any tool synthesised here is an \u201cin-between\u201d tool in terms of the affordances investigated. What we do not show here explicitly is tool extrapolation (e.g. to shapes not previously seen). This is on our roadmap for future work.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y2I4gyAGlCB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1628/Authors|ICLR.cc/2021/Conference/Paper1628/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857567, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Comment"}}}, {"id": "K5UxaHGNQq", "original": null, "number": 2, "cdate": 1605542765631, "ddate": null, "tcdate": 1605542765631, "tmdate": 1605542765631, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Comment", "content": {"title": "Rebuttal Part I", "comment": "We thank the reviewers for their time, effort and  constructive comments and are delighted about the general enthusiasm the reviews exhibit for the idea we are pursuing. The principal criticisms offered by the reviews refer to (i) overly simplistic experimental setups and (ii) a lack of baseline comparisons. We will tackle these as well as additional points made by individual reviewers below. \n\nOur submission investigates two succinct and intimately related hypotheses: (a) object affordances are implicitly captured in structured latent spaces trained based on experience (task success) alone; and (b) they are specifically addressable in a similarly task-driven way. We are not aware of any other works either postulating these hypotheses or offering any experimental insight into whether they have merit. And we interpret all reviews to agree that this direction of investigation is unusual, novel and interesting.  \n\n- Overly simplistic experimental setup; Degenerates to geometric constraint satisfaction.  [AR2, AR3]\nOur experiments are designed to confirm or reject our research hypotheses. Rather than addressing all possible object affordances we select a subset relating to object geometry: length, width and configuration. This is done precisely because it lends itself to simple, easily reproducible experimental setups. The \u201cdegeneration\u201d to geometric constraint satisfaction, therefore, is a deliberate design decision to allow for a clear and targeted proof of concept. There is, however, nothing in our architecture that limits our approach to these particular affordances. \n\n- Tool synthesis requires an understanding of physics and causality [AR2].\nIt is tempting to agree with this. However, the comment suggests that meaningful progress in representing objects and their affordances can only be made once understanding physics and causality have been solved. We do not believe that this is the case - and we offer our submission as evidence to the contrary. Not all scenarios requiring tool synthesis require physical and/or causal understanding. The foundational interest in similar reaching tasks in the experimental psychology literature is a case in point that such a task simplification can be instructive when it comes to understanding spatial reasoning - not despite the simplified setup, but because of it. \n\n- What about actions? What about trajectories? [AR2] \nAgents and their actions are intricately interwoven with the notion of object affordances. However, in order to disambiguate the role of various agents (which rightly introduce additional complexity) from the effect of task success or failure on object representations we control for the set of agents by choosing only a single one, which is able to act as required to manipulate the objects considered. We agree that this is a significant simplification of the problem space. And we argue that this is exactly what is required in the context of our research hypotheses. By considering agent experience (via task success) our framework seamlessly extends to cases of different agents. However, in the context of our submission we believe such an investigation distracts from the specific hypotheses we have set out to investigate. \n\n- Task-driven becomes a simple label [AR2].\nCorrect. This is, in fact, one of the merits of our approach. Task success is an easily quantifiable measure. It is ubiquitous in the robotics literature. The fact that we observe meaningful changes of tool configuration based simply on a representation learned via the high-level signal of task success is, we believe, not just interesting but noteworthy.\n\n- No challenging baselines; What about other methods of latent disentanglement?\n [AR1, AR4]\nWe thank the reviewers for the suggestions regarding baselines: (a) nearest neighbour classification of successful tools; (b) supervised affordance learning combined with success predictors; and (c) other methods of latent disentanglement. However, we posit that (a) and (b) do not add value to our investigation into the ability of structured representations learned with minimal supervision to implicitly encode specific, task-relevant object affordances. In this context both (a) and (b) simply amount to supervised tool selection, which is interesting but largely orthogonal to our research direction. In our work we employ tool selection only as a means of validating parts of our architecture, which is otherwise designed to effect tool synthesis. (c) is an interesting proposition and would add value if our principal claim was that our way of achieving disentanglement was superior to those already proposed in the literature. We do not claim this. Instead we propose an architecture which naturally accomplishes disentanglement in the same task-driven framework used for representation learning."}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y2I4gyAGlCB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1628/Authors|ICLR.cc/2021/Conference/Paper1628/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857567, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Comment"}}}, {"id": "wD0oamMkqKR", "original": null, "number": 4, "cdate": 1603946926368, "ddate": null, "tcdate": 1603946926368, "tmdate": 1605024397401, "tddate": null, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "invitation": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review", "content": {"title": "Interesting problem, but some key ingredients are missing. Experimental results are insufficient.", "review": "This paper explores the idea of tool synthesis in an unsupervised generative learning setting.\n\nPros:\n\n+ Tool use is one of the crucial aspects to demonstrate human-like intelligence. The authors' proposal to synthesize the tool's shapes for a given task is a fundamental research problem. This is different from most prior work that focuses on ranking/selection or simply categorization. Tool synthesis is particularly difficult; only one paper was published recently on this particular topic [1]. It has not yet passed the peer-review, but I recommend the authors consider citing it for future readers for completeness purposes.\n\n+ Different from classic object recognition, the tool use is intrinsically a task-oriented concept. This paper did a few things right: emphasis on the task-oriented view, no affordance label, etc. The authors are definitely heading in the right direction.\n\nCons:\n\n- What is the representation of the task or the tool? This is probably the most significant difference that could tell the present work from the prior work, especially if the results are promising. However, the authors only spend three short paragraphs in the paper (section 3.1), which does not really touch this point, despite the section title is called \"representing tasks and tools.\" It seems to be a simple, off-the-shelf encoder that has been published. Representing tools is significant as making/synthesizing tools requires a deep understanding of physics and causality (for instance, see the cited Zhu et al 2015 and Toussint et al 2018). Without a proper representation of these concepts or a clear disentanglement of the model, one simply cannot justify their model is learning how to synthesize tools. Instead, the model is more likely to learn an association between the given task and a trained shape (or 3D shape space if learned better). In this view, it really does not touch the core of the computational problem of tool-use: The problem of tool synthesis becomes 3D shape synthesis without really understanding why an object becomes a tool in that context, and the crucial concept of \"task-driven\" becomes a simple label.\n\n- What about action? A tool-use or task-oriented view cannot be separated from the actual action. This is one of the key ideas behind affordance in Gibson's theory. Given different embodiments of the agent with various capabilities of actions, the synthesized tool would be dramatically different. This is a very challenging problem in tool manipulation in robotics (e.g., see [2]).\n\n- Furthermore, what about trajectories? Even if an algorithm can synthesize the tool and chooses an action, one still needs to properly manipulate tools with the planned trajectory to complete the task. This seems to be completely left out in the paper.\n\n- Why not directly use 3D meshes as input for the algorithm, instead of using two views to reconstruct? The reconstructed one does not bring in any benefit for the overall goal of the paper.\n\n- The experiments are far too simple. The authors do have extra space on page 8, but did not include additional results. This makes me wonder how robust the algorithm is.\n\n[1] Tool MacGyvering: A Novel Framework for Combining Tool Substitution and Construction\n[2] Mirroring without Overimitation: Learning Functionally Equivalent Manipulation Actions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1628/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1628/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authorids": ["~Yizhe_Wu1", "~Sudhanshu_Kasewa1", "~Oliver_Groth1", "~Sasha_Salter1", "~Kevin_Li_Sun1", "~Oiwi_Parker_Jones1", "~Ingmar_Posner1"], "authors": ["Yizhe Wu", "Sudhanshu Kasewa", "Oliver Groth", "Sasha Salter", "Kevin Li Sun", "Oiwi Parker Jones", "Ingmar Posner"], "keywords": ["Affordance Learning", "Imagination", "Generative Models", "Activation Maximisation"], "abstract": "In this paper we explore the richness of information captured by the latent space of a vision-based generative model.  The model combines unsupervised generative learning with a task-based performance predictor to learn and to exploit task-relevant object affordances given visual observations from a reaching task, involving a scenario and a stick-like tool.  While the learned embedding of the generative model captures factors of variation in 3D tool geometry (e.g. length, width, and shape), the performance predictor identifies sub-manifolds of the embedding that correlate with task success. Within a variety of scenarios, we demonstrate that traversing the latent space via backpropagation from the performance predictor allows us to imagine tools appropriate for the task at hand.  Our results indicate that affordances \u2013 like the utility for reaching \u2013 are encoded along smooth trajectories in latent space. Accessing these emergent affordances by considering only high-level performance criteria (such as task success) enables an agent to manipulate tool geometries in a targeted and deliberate way.", "one-sentence_summary": "We demonstrate that a task-driven traversal of a learned latent space leads to object affordances emerging naturally as smooth trajectories in this space accessible via the optimisation of high-level performance criteria.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|imagine_that_leveraging_emergent_affordances_for_3d_tool_synthesis", "supplementary_material": "/attachment/f0c0bff0725fd33217bce61cfce73dc11aa2c64a.zip", "pdf": "/pdf/607bcedcf30e811e9d04833db98a7d9ceae41560.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zHcaWgUocH", "_bibtex": "@misc{\nwu2021imagine,\ntitle={Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis},\nauthor={Yizhe Wu and Sudhanshu Kasewa and Oliver Groth and Sasha Salter and Kevin Li Sun and Oiwi Parker Jones and Ingmar Posner},\nyear={2021},\nurl={https://openreview.net/forum?id=y2I4gyAGlCB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y2I4gyAGlCB", "replyto": "y2I4gyAGlCB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114359, "tmdate": 1606915807530, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1628/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1628/-/Official_Review"}}}], "count": 8}