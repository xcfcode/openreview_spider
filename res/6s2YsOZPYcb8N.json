{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363702380000, "tcdate": 1363702380000, "number": 7, "id": "Od6cRb72yhb2P", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["Christian Scheible"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks everyone for your comments! I would like to address some of the points made across various comments.\r\n\r\nI would like to point out to reviewer 'Anonymous 5b0f' that, in the experiment 'noembed', while the embeddings are not used in the classifier, they are still learned during RAE training . Thus, to train the RAE, we do indeed need 50x100 + 50x10,000 parameters, thus making RAE training more complicated than using embeddings only. Training the RAE without any embeddings produces results similar to 'noembed' line 1.\r\n\r\nRegarding the tree structures, we found that they do no influence the results too much. We achieve around 74% accuracy by simply enforcing iterative combinations from left to right using a one-sided recursion rule.\r\n\r\nI agree with the point that a binary classification task less complicated than for example a structured-prediction task and thus is too simple to show an improvement with a structural model. I find the result interesting nevertheless, structural understanding should help in sentiment analysis -- at least from a linguistic point of view. However, the RAE model does not seem to capture these properties very well. Socher et al. presented a matrix-vector-based approach at EMNLP 2012 which addresses this problem and is more suitable for modeling compositionality.\r\n\r\nIt is true that the human evaluation is rather small-scale. We intended this analysis to illustrate the point. Regarding the point about one of the examples, I see that 'not bad' is in itself not too positive, but I (and our annotators) would think that 'not bad at all' is positive."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362455040000, "tcdate": 1362455040000, "number": 6, "id": "XHzDeHdtlbXIc", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["Arun Tejasvi Chaganty"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The paper presents a very interesting error analysis of recursive autoencoder\r\ntrees. However, I would wish the following aspects of the evaluation were\r\naddressed.\r\n\r\na) In the qualitative analysis (Section 5), only 10 samples out of a corpus of\r\nover 10,000 were studied. This is too small to make any statistically\r\nsignificant statements.\r\n\r\nb) When describing the behaviour on sentences with reversing constructions, it\r\nis not clear how the RAE trees actually predicted the sentence; were the three\r\ncorrect instances those with reversed sentiment, suggesting that the RAE trees\r\nalways reverse the sentiment when a reverser appeared?\r\n\r\nc) Looking at the results from the quantitative analysis, the fact that the RAE\r\ntrees predict with a full 77.5% accuracy despite random feature embeddings\r\nseems to be a strong signal that the parse structure is playing a very\r\nimportant role. This conflicts with the qualitative analysis that\r\ncompositionality is not well modelled by RAEs. I feel more space should be\r\ndedicated to discussing this result. If the real compositionality modelled by\r\nthe RAE trees is for intensifying constructors, it should be possible to\r\nevaluate intensifying constructions by comparing the softmax classification\r\nweights for a sentiment."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362361260000, "tcdate": 1362361260000, "number": 2, "id": "SPfmPG0ry9nrB", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["anonymous reviewer 2611"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Cutting Recursive Autoencoder Trees", "review": "This research analyses the Semi Supervised Recurive Autoencoder (RAE) of Socher et al., obtained with the NLP task of sentiment classification from sentences of movie reviews.\r\n \r\nA first qualitative analysis conducted wth th help of human annotators, reveals that the syntactic and semantic role of reversers ('not') is not modeled well in many cases.\r\n\r\nThen a systematic quantitative analysis is conducted, using the representation of a sentence as the average of the representation output at each node of the tree to train a classifier of sentiment, and analysing what is lost or gained by using only specific subsets of the tree nodes. These results clearly indicate that intermediate nodes bring no additional value for classfication performance compared to using only the word embeddings learned at the leaf nodes. \r\nThe full depth of the tree appears to extract no more useful information than the leaf nodes only.\r\n\r\nPros:\r\nI believe that this paper's analysis is a significant contribution with an important message. It is well conducted and properly questions and sheds light on the meaning and usefulness of the tree *structure* learned by RAEs, showing that drastic structure simplifications yield the same state-of-the-art performance on the considered classification task. It has the potential to start a healthy controversy, that will surely seed interest into further investigation of this important point.\r\n\r\nCons: \r\nThe message would carry much more weight if a similar analysis of RAEs could be conducted also on several other (possibly more challenging) NLP tasks than movie sentiment classification and pointed towards similar conclusions."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362181620000, "tcdate": 1362181620000, "number": 1, "id": "vDY7MvZACzMTc", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["Sam Bowman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I was very impressed by some of these results\u2014especially those for the noembed models\u2014and this does seem to provide evidence that the high performance of RAEs on sentence-level binary sentiment classification need not reflect a breakthrough due to the use of tree structures.\r\n\r\nThere were a couple of points that I would like to see brought up:\r\n\r\nThere appears to have been follow up work by some of the same authors on RAE models for other related tasks, and it seems somewhat unfair to claim that 'the trees and the embeddings model the same phenomena,' when using one particularly uncomplicated domain of phenomena (binary sentiment) as a case study.\r\n\r\nLess critically, I would like to see some discussion of (and further investigation into) the extremely poor performances seen with the sub and win models. If I understand correctly that the best-class baseline should achieve at least 50% accuracy, achieving a result substantially worse than seems to reflect a robust and potentially interesting result about the role of strongly positive and strongly negative words."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362170160000, "tcdate": 1362170160000, "number": 5, "id": "KB-5ppfbu7pwL", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["anonymous reviewer 5a71"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Cutting Recursive Autoencoder Trees", "review": "The paper considers the compositional model of Socher et al. (EMNLP 2011) for predicting sentence opinion polarity. The authors define several model simplification types (e.g., reducing the maximal number of levels) and study how these changes affect sentiment prediction performance. They also study how well the induced structures agree with human judgement, i.e. how linguistically plausible (both from syntactic and semantic point of view) the structures are. \r\n\r\nI find this work quite interesting and the (main?) result somewhat surprising. What it basically shows is that the compositional part does not seem to benefit  the sentence-level sentiment performance.  In other words, a bag-of-words model with distributed word representations performs as well (or better).  An additional, though somewhat small-scale, annotation study show that the model is not particularly accurate in representing opinion shifters (e.g., 'not' does not seem to reverse polarity reliably). \r\n\r\nThough some of the choices of model simplification seem relatively arbitrary (e.g., why choosing a single subtree, rather than, e.g., dropping several subtrees within some budget?) and the human evaluation is somewhat small scale (and, consequently, not entirely convincing), I found the above observation interesting and important.\r\n\r\nIt would also be interesting to see if the compositional model appears to be more important when an actual syntactic tree (as in Socher et al (NIPS 2011) for paraphrase detection) is used instead of automatically inducing the structure.   \r\n\r\nOne point which might be a little worrying is that the same parameters are used across different learning architectures, though one may expect that different regularizations and training regimes might be needed. However, the full model is estimated with the parameters chosen by the model designers on the same datasets, so it should not affect the above conclusion.\r\n\r\nPros:\r\n-- It provides interesting analysis of the influential model of Socher et al (2011)\r\n-- Both analysis of linguistic plausibility are provided and analysis of the effect of model components on sentiment prediction performance.  Though, the original publication (Socher et al, EMNLP 2011) contained the BOW baseline it was not exactly comparable, the flat model studied here seems a more natural baseline.\r\n\r\nCons:\r\n-- Semantic and syntactic coherence analysis may be too small scale to be seriously considered (2 human experts on a couple dozens of examples)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362043620000, "tcdate": 1362043620000, "number": 4, "id": "9IkTIwySTQw0C", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["Sida Wang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I've also done some (unpublished) analysis with using random and degenerate tree structures and found that it did not matter very much under the RAE framework. I just have a short comment for the results table.\r\n\r\nGiven that most of the different schemes eventually got us roughly identical results near including the original RAE, 77.6% and knowing that Naive Bayes/Logistic Regression get an accuracy of around 78% with bag-of-words features and one can get over 79% with bag-of-bigrams features [Wang and Manning, Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation, ACL 2012]. \r\n\r\nIt seems to me that one conclusion from these results is that many schemes will perform similarly once enough information is preserved in the training features. If around 80% accuracy is what a fairly general purpose machine learning algorithm can possibly be expected to do on this dataset without outside information, then one does not have to be very clever with a correct discriminative method to do just slightly worse than Naive Bayes/Logistic Regression.\r\n\r\nYour results do suggest that the particular structure does not matter very here, neither does the embedding. But I think to really determine if the structure is doing anything, one should repeat this analysis in a place where the model with the structure is way better than the generic-linear-model-with-moderately-informative-features-benchmark, preferably without using extra knowledge."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362019200000, "tcdate": 1362019200000, "number": 3, "id": "fvJTwf6BDQvYu", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6s2YsOZPYcb8N", "replyto": "6s2YsOZPYcb8N", "signatures": ["anonymous reviewer 5b0f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Cutting Recursive Autoencoder Trees", "review": "This paper analyzes recursive autoencoders for a binary sentiment analysis task. \r\nThe authors include two types of analyses: looking at example trees for syntactic and semantic structure and analyzing performance when the induced tree structures are cut at various levels.\r\nMore in depth analysis of these new models is definitely an interesting task. Unfortunately, the presented analyses and conclusions are incomplete or flawed.\r\n\r\nTree Cutting Analysis:\r\nThis experiment explores the interesting question of how important the word vectors and tree structures are for the RAE.\r\nThe authors incorrectly conclude that 'the strength of the RAE lies in the embeddings, not in the induced tree structure'.\r\nThis conclusion is reached by comparing the following two models (among others):\r\n1) A word vector average model with no tree structures that uses about 50x10,000 parameters (50 dimensional word vectors and a vocabulary of around 10,000 words) and reaches 77.67% accuracy.\r\n2) A RAE model with random word vectors that uses 50 x 100 parameters and gets 77.49% accuracy.\r\n\r\nAn accuracy difference of 0.18% on a test set of ~1000 trees means that ~2 sentences are classified differently and is not statistically significant. So the results of both models are the same.\r\nThat means that the RAE trees achieved the same performance with 1/100 of the parameters of the word vectors. So, the tree structures seem to work pretty well, even on top of random word vectors.\r\n\r\nA good comparison would be between models with the same number of parameters. Both models could easily be increased or decreased in size.\r\n\r\nOne possible take away message could have been that the benefits of RAE tree structures and word embeddings are equal but performance does not increase when both are combined in a task that only has a single label for a full sentence.\r\nBut even that one is difficult:\r\nAll columns of the main results table (cutting trees) have the same top performance when it comes to statistical significance, so it would have also been good to look at another dataset.\r\nAnother problem is that the RAE induced vectors are only used by averaging all vectors in the tree.  \r\n\r\nMore important analyses into the model could explore what the higher node vectors are capturing by themselves instead of only in an average with all lower vectors.\r\n\r\n\r\nTree Structure Analysis:\r\nThe first analysis is about the induced tree structures and finds that that they do not follow traditional parsing trees. \r\nThis was already pointed out in the original RAE paper and they show several examples of phrases that are cut off.\r\nAn interesting comparison here would have been to apply the algorithm on correct parse trees and analyze if it makes a difference in performance.\r\n\r\n\r\nThe second analysis is about sentiment reversal, such as 'not bad'. \r\nUnfortunately, the given binary examples are hard to interpret. \r\nAre phrases like 'not bad' positive in the original training data? It's not clear to me that 'not bad' is a very positive phrase.\r\nDo the probabilities change in the right direction? When does it work and when does it not work? Is the sentiment of the negated phrase wrong or is the negation pushing in the wrong direction?\r\nIn order to understand what the model should learn, it would have been interesting to see if the effects are even in the training dataset.\r\nAnother interesting analysis would be to construct some simple examples where the reversal is much clearer like 'really not good'.\r\n\r\n\r\nThe paper is well written.\r\nOnly one typo: E_cE and E_eC both used."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358718300000, "tcdate": 1358718300000, "number": 52, "id": "6s2YsOZPYcb8N", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "6s2YsOZPYcb8N", "signatures": ["christian.scheible@gmail.com"], "readers": ["everyone"], "content": {"title": "Cutting Recursive Autoencoder Trees", "decision": "conferencePoster-iclr2013-conference", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. We therefore have to rely on empirical tests to see whether a particular structure makes sense. In this paper, we present an analysis of a well-received model that produces structural representations of text: the Semi-Supervised Recursive Autoencoder. We show that for certain tasks, the structure of the autoencoder may be significantly reduced and we evaluate the produced structures through human judgment.", "pdf": "https://arxiv.org/abs/1301.2811", "paperhash": "scheible|cutting_recursive_autoencoder_trees", "keywords": [], "conflicts": [], "authors": ["Christian Scheible", "Hinrich Schuetze"], "authorids": ["christian.scheible@gmail.com", "hinrichwork@googlemail.com"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 8}