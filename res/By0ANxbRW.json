{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730172586, "tcdate": 1509127382443, "number": 577, "cdate": 1518730172576, "id": "By0ANxbRW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "By0ANxbRW", "original": "H1jANgZRZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "DNN Model Compression Under Accuracy Constraints", "abstract": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "pdf": "/pdf/2dffd9a96b38af32dd4556e9166a0b4bb73917fd.pdf", "TL;DR": "Compressing trained DNN models by minimizing their complexity while constraining their loss.", "paperhash": "khoram|dnn_model_compression_under_accuracy_constraints", "_bibtex": "@misc{\nkhoram2018dnn,\ntitle={{DNN} Model Compression Under Accuracy Constraints},\nauthor={Soroosh Khoram and Jing Li},\nyear={2018},\nurl={https://openreview.net/forum?id=By0ANxbRW},\n}", "keywords": ["DNN Compression", "Weigh-sharing", "Model Compression"], "authors": ["Soroosh Khoram", "Jing Li"], "authorids": ["khoram@wisc.edu", "jli@ece.wisc.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260083685, "tcdate": 1517249941650, "number": 633, "cdate": 1517249941639, "id": "HyCcrJTHf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "By0ANxbRW", "replyto": "By0ANxbRW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state-of-the-art.\nPaper presentation quality also needs to be improved. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DNN Model Compression Under Accuracy Constraints", "abstract": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "pdf": "/pdf/2dffd9a96b38af32dd4556e9166a0b4bb73917fd.pdf", "TL;DR": "Compressing trained DNN models by minimizing their complexity while constraining their loss.", "paperhash": "khoram|dnn_model_compression_under_accuracy_constraints", "_bibtex": "@misc{\nkhoram2018dnn,\ntitle={{DNN} Model Compression Under Accuracy Constraints},\nauthor={Soroosh Khoram and Jing Li},\nyear={2018},\nurl={https://openreview.net/forum?id=By0ANxbRW},\n}", "keywords": ["DNN Compression", "Weigh-sharing", "Model Compression"], "authors": ["Soroosh Khoram", "Jing Li"], "authorids": ["khoram@wisc.edu", "jli@ece.wisc.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642472292, "tcdate": 1511651365204, "number": 1, "cdate": 1511651365204, "id": "rk6muOPxG", "invitation": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "forum": "By0ANxbRW", "replyto": "By0ANxbRW", "signatures": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Promising Idea, Confusing Writing, Key Experiment Missing", "rating": "4: Ok but not good enough - rejection", "review": "1. Summary\n\nThis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change.\n\n\n2. High level paper\n\n- I believe the writing is a bit sloppy. For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below). This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models.\n\n\n3. High level technical\n\n- I'm confused at the first and second lines of equation (19). In the first line, shouldn't the first term not contain \\Delta W ? In the second line, shouldn't the first term be \\tilde{\\mathcal{L}}(W_0 + \\Delta W) ?\n- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data. My worry is if you're compressing these networks with your method are the weights not treated as binary anymore? Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright. But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply.\n- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354). What is your compression ratio for 0 accuracy loss? I think this is a key experiment that should be run as this result would be much easier to compare with the other methods.\n- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers?\n- The first paper to propose weight sharing was not Han et al., 2015, it was actually:\nChen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. \"Compressing Neural Networks with the Hashing Trick\" ICML 2015\nAlthough they did not learn the weight sharing function, but use random hash functions.\n\n\n4. Low level technical\n\n- The end of Section 2 has an extra 'p' character\n- Section 3.1: \"Here, X and y define a set of samples and ideal output distributions we use for training\" this sentence is a bit confusing. Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3.\n- Section 3.1: \"W is the learnt model...\\hat{W} is the final, trained model\" This is unclear: W and \\hat{W} seem to describe the same thing. I would just remove \"is the learnt model and\"\n\n\n5. Review summary\n\nWhile the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read. This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DNN Model Compression Under Accuracy Constraints", "abstract": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "pdf": "/pdf/2dffd9a96b38af32dd4556e9166a0b4bb73917fd.pdf", "TL;DR": "Compressing trained DNN models by minimizing their complexity while constraining their loss.", "paperhash": "khoram|dnn_model_compression_under_accuracy_constraints", "_bibtex": "@misc{\nkhoram2018dnn,\ntitle={{DNN} Model Compression Under Accuracy Constraints},\nauthor={Soroosh Khoram and Jing Li},\nyear={2018},\nurl={https://openreview.net/forum?id=By0ANxbRW},\n}", "keywords": ["DNN Compression", "Weigh-sharing", "Model Compression"], "authors": ["Soroosh Khoram", "Jing Li"], "authorids": ["khoram@wisc.edu", "jli@ece.wisc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642472202, "id": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper577/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer2", "ICLR.cc/2018/Conference/Paper577/AnonReviewer3", "ICLR.cc/2018/Conference/Paper577/AnonReviewer1"], "reply": {"forum": "By0ANxbRW", "replyto": "By0ANxbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642472202}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642472256, "tcdate": 1511675013118, "number": 2, "cdate": 1511675013118, "id": "Hk6K4Rwlf", "invitation": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "forum": "By0ANxbRW", "replyto": "By0ANxbRW", "signatures": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The paper is clearly unqualified for publication in the current stage.", "rating": "3: Clear rejection", "review": "The paper addresses an interesting problem of DNN model compression. The main idea is to combine the approaches in (Han et al., 2015) and (Ullrich et al., 2017) to get a loss value constrained k-means encoding method for network compression. An iterative algorithm is developed for model optimization. Experimental results on MNIST, CIFAR-10 and SVHN are reported to show the compression performance. \n\nThe reviewer would expect papers submitted for review to be of publishable quality. However, this manuscript is not polished enough for publication: it has too many language errors and imprecisions which make the paper hard to follow. In particular, there is no clear definition of problem formulation, and the algorithms are poorly presented and elaborated in the context. \n\nPros: \n\n- The network compression problem is of general interest to ICLR audience. \n\nCons:\n\n- The proposed approach follows largely the existing work and thus its technical novelty is weak. \n\n- Paper presentation quality is clearly below the standard. \n\n- Empirical results do not clearly show the advantage of the proposed method over state-of-the-arts. \n\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DNN Model Compression Under Accuracy Constraints", "abstract": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "pdf": "/pdf/2dffd9a96b38af32dd4556e9166a0b4bb73917fd.pdf", "TL;DR": "Compressing trained DNN models by minimizing their complexity while constraining their loss.", "paperhash": "khoram|dnn_model_compression_under_accuracy_constraints", "_bibtex": "@misc{\nkhoram2018dnn,\ntitle={{DNN} Model Compression Under Accuracy Constraints},\nauthor={Soroosh Khoram and Jing Li},\nyear={2018},\nurl={https://openreview.net/forum?id=By0ANxbRW},\n}", "keywords": ["DNN Compression", "Weigh-sharing", "Model Compression"], "authors": ["Soroosh Khoram", "Jing Li"], "authorids": ["khoram@wisc.edu", "jli@ece.wisc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642472202, "id": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper577/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer2", "ICLR.cc/2018/Conference/Paper577/AnonReviewer3", "ICLR.cc/2018/Conference/Paper577/AnonReviewer1"], "reply": {"forum": "By0ANxbRW", "replyto": "By0ANxbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642472202}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642472218, "tcdate": 1511713592238, "number": 3, "cdate": 1511713592238, "id": "HJlSjw_ez", "invitation": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "forum": "By0ANxbRW", "replyto": "By0ANxbRW", "signatures": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The manuscript presents a compression method for DNN, but I cannot find significant differences from and advantages over existing deep compression approaches. Besides, the experiments are not persuasive.", "rating": "3: Clear rejection", "review": "1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter. However, all compression methods such as pruning and quantization also have this concern. For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved. Therefore, the novelty of the proposed method is somewhat weak.\n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3]. However, the paper only did a very simple investigation on related works.\n[r1] CNNpack: packing convolutional neural networks in the frequency domain.\n[r2] LCNN: Lookup-based Convolutional Neural Network.\n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks.\n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10. It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DNN Model Compression Under Accuracy Constraints", "abstract": "The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms. Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation. This is achieved by either eliminating components from the model, or penalizing complexity during training. While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions. In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function. In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it. We will show that using this technique, we can achieve competitive results.", "pdf": "/pdf/2dffd9a96b38af32dd4556e9166a0b4bb73917fd.pdf", "TL;DR": "Compressing trained DNN models by minimizing their complexity while constraining their loss.", "paperhash": "khoram|dnn_model_compression_under_accuracy_constraints", "_bibtex": "@misc{\nkhoram2018dnn,\ntitle={{DNN} Model Compression Under Accuracy Constraints},\nauthor={Soroosh Khoram and Jing Li},\nyear={2018},\nurl={https://openreview.net/forum?id=By0ANxbRW},\n}", "keywords": ["DNN Compression", "Weigh-sharing", "Model Compression"], "authors": ["Soroosh Khoram", "Jing Li"], "authorids": ["khoram@wisc.edu", "jli@ece.wisc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642472202, "id": "ICLR.cc/2018/Conference/-/Paper577/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper577/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper577/AnonReviewer2", "ICLR.cc/2018/Conference/Paper577/AnonReviewer3", "ICLR.cc/2018/Conference/Paper577/AnonReviewer1"], "reply": {"forum": "By0ANxbRW", "replyto": "By0ANxbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642472202}}}], "count": 5}