{"notes": [{"id": "SHvF5xaueVn", "original": "aovGaClOrvF", "number": 464, "cdate": 1601308059147, "ddate": null, "tcdate": 1601308059147, "tmdate": 1616151412214, "tddate": null, "forum": "SHvF5xaueVn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8nH6Cxet0gC", "original": null, "number": 1, "cdate": 1610040355416, "ddate": null, "tcdate": 1610040355416, "tmdate": 1610473944869, "tddate": null, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "invitation": "ICLR.cc/2021/Conference/Paper464/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Summary of discussion: Three reviewers rated the paper Good (7) while Reviewer2 disagreed. R2's criticism was focussed on how this work is placed within existing/related literature, and no technical problem was identified. The authors have addressed some of R2's comments/concerns, R2 has not participated in the discussion.\n\nNovelty and contributions: Overall the reviews seem consistent with an incremental paper which is technically valid, improves the state of the art on a reasonably difficult task. However, it does not appear from the reviews that the paper substantially advances our understanding of machine learning more broadly beyond this specific application.\n\nExperiments: There is some disagreement among reviewers on the adequacy of the experiments, with at least two reviewers calling for experiments involving 'natural photos'. I believe the author's responses adequately address these concerns: they pointed out that the key selling point of their paper is the ability to model structured noise which is less relevant in natural photos.\n\nOn the balance of things, I think this paper should be accepted, but I wouldn't argue if it did not make the cut due to its narrow scope. For this reason, I recommended poster presentation."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040355398, "tmdate": 1610473944849, "id": "ICLR.cc/2021/Conference/Paper464/-/Decision"}}}, {"id": "SFdfblBvjs4", "original": null, "number": 1, "cdate": 1602929667765, "ddate": null, "tcdate": 1602929667765, "tmdate": 1605024683795, "tddate": null, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "invitation": "ICLR.cc/2021/Conference/Paper464/-/Official_Review", "content": {"title": "The idea looks reasonable, but I have several concerns.", "review": "This paper proposes a framework to train a network to remove noises, which are zero-mean, additive and independent of the clean image, with only noisy images and without knowing the noise statistics. They mathematically prove that a network, which is trained from pairs of images generated by adding simulated noises into the noisy image, can remove noises from the input noisy image. The proposed framework can remove the noises from the input noisy image. Then, it adds simulated noises into the denoised image to generate a pair of images and train the next network to further remove noises and it does this process iteratively. The proposed framework follows GCBD and utilizes flat textureless regions to train a network to simulate noises and proposes a wavelet based method to effectively distinguish flat regions from the ones that contain high-frequency repeating patterns. The experimental results show that the proposed method has good performance under simulated Gaussian noises as well as WT and CT datasets.\n\nI have several concerns and suggestions for this paper:\n\n(1) One assumption of the proposed method is the noises and signal are independent which is not true for most of the scenarios. The raw images captured by the camera contain both Poisson noises. I am wondering if the proposed method can deal with Poisson noises or not. Also, the final displayed images processed by ISP contain signal dependent noises with spatial correlation. Can the proposed framework remove this kind of noises?\n\n(2) In Sec. 3.3, why does the framework add simulated noises into the network 'ground truth' Z_j1^i. Will it be better if we directly treat X_phi_{j-1}(Z^i) in Eq. (10) as the 'ground truth'?\n\n(3) As claimed above Theorem 1, f_Noisy N2N (Z, y) gives a better estimate of X than X' for a sufficiently large \u03c3_0^2. What the results will be if \u03c3_0^2 is not large enough? That is to say, can the proposed framework remove small noises from the input images?\n\n(4) Similarly, according to Theorem 1, the iterative process in Sec. 3.3 will converge if y_0<y<1.  Can the network still effectively remove noises if y<y_0 which means the residual noises are not significant in the images?\n\n(5) g_theta1 generates noises with given random vector r. How does g_theta3 not need a random vector to add simulated noises into g_theta2(Z)?\n\n(6) More analyses are needed in Sec. 4.4 for ablation study. Especially, why is sigmoid so important?\n\nSome descriptions are not clear and confusing:\n\n(7) I cannot understand the relationship between N2C with Eq. 4. \n\n(8) What is Unif[\u2212s, s] in Sec. 4.2?\n\n(9) It is not clear the described dataset for Table 3 for training or testing.\n\n(10) For a fair comparison,  do the two N2C networks have the same network structure?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper464/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper464/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142585, "tmdate": 1606915759915, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper464/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper464/-/Official_Review"}}}, {"id": "Zev9jC3gzOZ", "original": null, "number": 2, "cdate": 1603616981959, "ddate": null, "tcdate": 1603616981959, "tmdate": 1605024683732, "tddate": null, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "invitation": "ICLR.cc/2021/Conference/Paper464/-/Official_Review", "content": {"title": "This paper proposes a blind image denoising method based on a self-supervised learning approach. ", "review": "The proposed method uses the generative learning method to simulate the noise and synthesize noisy image pairs to train the proposed network. Experimental results show the effectiveness of the proposed method. \n\nLearning denoised network from noisy images has been developed by Krull et al., 2019; Batson\n& Royer, 2019; Laine et al., 2019. The main difference is the use of generative learning. One possible clarification is whether the aforementioned methods using generative learning can generate comparable results or not. If so, the contribution of the paper is limited.\n\nThe motivation of using the proposed network design is not clear. In addition, using more generators will lead to larger capacity models than existing methods. It is not clear whether the performance gains are due to use such larger capacity models or not. \n\nFor the real noise images, why do the authors evaluate the proposed method on the microscopy and medical images? How about the results on the real-world natural noisy images? In addition, the proposed results still contain significant noise residual as shown in Figure 4.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper464/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper464/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142585, "tmdate": 1606915759915, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper464/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper464/-/Official_Review"}}}, {"id": "H-ADO2pY3JN", "original": null, "number": 3, "cdate": 1603750712146, "ddate": null, "tcdate": 1603750712146, "tmdate": 1605024683661, "tddate": null, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "invitation": "ICLR.cc/2021/Conference/Paper464/-/Official_Review", "content": {"title": "Solid work with sound technical approach, but better with analysis on other well known approaches.", "review": "This paper addresses a challenging task of blind image denoising where a single noisy image is provided with assumption that it is zero mean, additive and independent from the original image content. This is mostly the real-world scenario. Different from the recent N2N training, the authors propose a GAN2GAN based method since this blind setting cannot be trained by N2N. N2N or deterministic training needs explicit or implicit knowledge of clean image in order to be trained whereas the GAN2GAN method does not, leading to more realistic and efficient training. This method first attempts to simulate noise given the noisy image, generate rough and noisy estimates of the clean image, and iteratively train a denoiser with synthetic noisy pairs from the generator. For blind denoising, this work produces impressive results for synthetic and real world blind denoising. \n\nThis work provides a sound approach to denoising given only the noisy image.\nThe experiments were done with extensive datasets and baselines.\nAlthough the paper is solid, I am not sure how this work compares to well known algorithms for blind image denoising:\n- Residual Dense Network for Image Restoration\n- High-Quality Self-Supervised Deep Image Denoising\n- CycleISP: Real Image Restoration via Improved Data Synthesis\n- Real Image Denoising with Feature Attention\njust to name a few.\nIf the authors can provide comparisons or discussion, analysis on these algorithms and comparison, it would be helpful. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper464/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper464/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142585, "tmdate": 1606915759915, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper464/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper464/-/Official_Review"}}}, {"id": "DQhR6ol6g8J", "original": null, "number": 4, "cdate": 1603934475665, "ddate": null, "tcdate": 1603934475665, "tmdate": 1605024683571, "tddate": null, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "invitation": "ICLR.cc/2021/Conference/Paper464/-/Official_Review", "content": {"title": "interesting idea of iterative denoiser training", "review": "\nThis paper proposed a new method for blind image denoising using a \"GAN2GAN\" network. Different from previous work noise2noise, the proposed method only needs single noisy images to train the network, without noisy pairs. Given a noisy dataset, a GAN generator is trained using the real noisy but smooth patches. With the noise generator, a pair of generated noise samples are added on the noisy image content to train a denoiser. This is trained iteratively, so that the image content is better and better after cleaned by the trained denoiser. The final results on synthetic and real data showed improvements over the baselines.\n\nOverall, this paper is very interesting to read. It solves a harder problem than noise2noise when the pairs are not available. The idea of refining the noisy image iteratively is reasonable, which makes the image content for training progressively better. The results showed significant improvements over its competitor noise2void.\n\nThere are still some areas that it could improve:\n\n1. It is unclear the purpose of the cycle loss. Although it improves the PSNR, the motivation is not clear.\n\n2. The idea of generating synthetic noise for image denoising has been studied already. This should be discussed.\n\nAbdelhamed, Abdelrahman, Marcus A. Brubaker, and Michael S. Brown. \"Noise flow: Noise modeling with conditional normalizing flows.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n\n3. It would be great to have natural images in the experiments to validate the idea. In particular, showing that the proposed method can reduce and noise and reveal more image content and texture.\n\nAbdelhamed, Abdelrahman, Stephen Lin, and Michael S. Brown. \"A high-quality denoising dataset for smartphone cameras.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\nPlotz, Tobias, and Stefan Roth. \"Benchmarking denoising algorithms with real photographs.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n4. It is good to have a table or flowchart about the iteratively training, to make the paper easier to read.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper464/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper464/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images", "authorids": ["~Sungmin_Cha1", "pte1236@skku.edu", "bjkim2006@naver.com", "jongdukbaek@yonsei.ac.kr", "~Taesup_Moon1"], "authors": ["Sungmin Cha", "Taeeon Park", "Byeongjoon Kim", "Jongduk Baek", "Taesup Moon"], "keywords": ["blind denoising", "unsupervised learning", "iterative training", "generative learning"], "abstract": "We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. ", "one-sentence_summary": "We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cha|gan2gan_generative_noise_learning_for_blind_denoising_with_single_noisy_images", "supplementary_material": "/attachment/57f3c946bea20085e94c939919507c57b3c66f2f.zip", "pdf": "/pdf/327657a649de624ba01604896357cfd80b6dadcc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncha2021gangan,\ntitle={{\\{}GAN{\\}}2{\\{}GAN{\\}}: Generative Noise Learning for Blind Denoising with Single Noisy Images},\nauthor={Sungmin Cha and Taeeon Park and Byeongjoon Kim and Jongduk Baek and Taesup Moon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SHvF5xaueVn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "SHvF5xaueVn", "replyto": "SHvF5xaueVn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142585, "tmdate": 1606915759915, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper464/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper464/-/Official_Review"}}}], "count": 6}