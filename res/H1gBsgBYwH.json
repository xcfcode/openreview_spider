{"notes": [{"id": "H1gBsgBYwH", "original": "H1gO9y-Yvr", "number": 2506, "cdate": 1569439900530, "ddate": null, "tcdate": 1569439900530, "tmdate": 1583912053163, "tddate": null, "forum": "H1gBsgBYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "e6Mdhsu7aF", "original": null, "number": 1, "cdate": 1576798750680, "ddate": null, "tcdate": 1576798750680, "tmdate": 1576800885073, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper focuses on studying the double descent phenomenon in a one layer neural network training in an asymptotic regime where various dimensions go to infinity together with fixed ratios. The authors provide precise asymptotic characterization of the risk and use it to study various phenomena. In particular they characterize the role of various scales of the initialization and their effects. The reviewers all agree that this is an interesting paper with nice contributions. I concur with this assessment.  I think this is a solid paper with very precise and concise theory. I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711181, "tmdate": 1576800260335, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Decision"}}}, {"id": "Bke6usYzKr", "original": null, "number": 1, "cdate": 1571097461417, "ddate": null, "tcdate": 1571097461417, "tmdate": 1574722886080, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The authors study the generalization error of two-layer neural nets, where an asymptotic point of view is taken. Their main results can be summarized as follows.\n1. If only the second layer is optimized, they observe the double-descent phenomenon.\n2. However, if only the first layer is optimized, the double-descent is not observed.\nThis shows that recent results for certain linear models (e.g. Song, Montanari 2019) do not directly transfer to neural networks. As the authors point out, however, if a different scaling is used in the asymptotics, double descent might still be observed.\n\nI see the following strengths of the paper. \n-This is a very well-written paper with a clear message.\n-The result is important and gives new insights into the generalization properties of neural networks.\n\nIn my view, this is an interesting contribution, which should be accepted. \n\n---------\n\nThank you for your response. I will leave the rating unchanged.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661122739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Reviewers"], "noninvitees": [], "tcdate": 1570237720433, "tmdate": 1575661122755, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review"}}}, {"id": "Bklc6FAsoH", "original": null, "number": 3, "cdate": 1573804482282, "ddate": null, "tcdate": 1573804482282, "tmdate": 1573849942338, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "ryxeE1cjYH", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment", "content": {"title": "Reply to Reviewer 3", "comment": "Thank you for the comments and suggestions. The technical comments are addressed below:\n\nExtending result to other target functions:\nWe agree that the problem might be significantly more difficult for different target functions, and would like to make the following remarks:\n1. Note that in our bias-variance decomposition, only the bias term depends on the target function. In other words, our result on the variance (including Theorem 4) would still be valid for other targets, such as two-layer neural network. One caveat is that for general target function, the output needs to be properly scaled since our current analysis in Section 5 relies on linearizing the network.\n2. When the target function is a multiple-neuron neural network, deriving the bias term can be challenging. However, we note that under the same setup, the bias may be obtained when the teacher is a slightly more general single-index model, i.e. $y=\\psi(\\beta^\\top x)$ with Lipschitz link function $\\psi$, equivalent to a single-neuron network. For instance, the bias under vanishing initialization is the same as that of least squares regression on the input, which can be solved under isotropic prior on $\\beta$ via decomposing the activation function similar to Appendix C.5.\n\nParameter count: \nTo clarify our statement in the discussion section, our current result requires $n,d,h$ to grow at the same rate, and thus $n = O(dh)$ is beyond the regime we consider. This is also true for previous works on double-descent in random feature model [Hastie et al. (2019)][Mei and Montanari (2019)].  When $h \\ll n$, it is not clear if the same analysis still applies (for instance approximating the network with a kernel model), and thus the instability of the inverse may not be the complete explanation of double-descent (if it appears). Characterizing the generalization in this regime would be an interesting direction.\n\nTraining both layers: \nThank you for the suggestion; we have included training both layers simultaneously as a future direction. We would like to briefly mention that under certain model parameterization and initialization, gradient flow on both layers may reduce to one of the three models we analyzed (see [Williams et al. (2019)]). More generally, our current result may be extended to cases where the dynamics of training both layers can be linearized (for instance initialization in the \"kernel regime\"), for which the learned model can be written down in closed-form. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gBsgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2506/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2506/Authors|ICLR.cc/2020/Conference/Paper2506/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140343, "tmdate": 1576860553535, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment"}}}, {"id": "B1xu2HCssS", "original": null, "number": 2, "cdate": 1573803440215, "ddate": null, "tcdate": 1573803440215, "tmdate": 1573803440215, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "Bke6usYzKr", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment", "content": {"title": "Reply to Reviewer 2", "comment": "Thank you for the comments and suggestions. We agree that characterizing the generalization properties of neural network under different scalings is an important future direction. \n\nWe have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gBsgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2506/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2506/Authors|ICLR.cc/2020/Conference/Paper2506/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140343, "tmdate": 1576860553535, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment"}}}, {"id": "Hkee-HRoiH", "original": null, "number": 1, "cdate": 1573803255578, "ddate": null, "tcdate": 1573803255578, "tmdate": 1573803255578, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "rylsXynatr", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for the comments and suggestions. As you pointed out, our current result in Section 5 does not apply to non-smooth activations -- understanding the generalization of ReLU networks would be interesting future work. \n\nWe have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gBsgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2506/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2506/Authors|ICLR.cc/2020/Conference/Paper2506/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140343, "tmdate": 1576860553535, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Authors", "ICLR.cc/2020/Conference/Paper2506/Reviewers", "ICLR.cc/2020/Conference/Paper2506/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Comment"}}}, {"id": "ryxeE1cjYH", "original": null, "number": 2, "cdate": 1571688231934, "ddate": null, "tcdate": 1571688231934, "tmdate": 1572972316410, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper provides exact bounds on the risk when training a two-layer neural network in an asymptotic regime. Namely, the paper considers training under the square-loss objective, a two-layer neural network with $h$ hidden units on inputs of dimension $d$ and training on $n$ samples. The asymptotic regime is considered by making all of $d$, $h$, $n$ go to $\\infty$, in a way that the ratio $d/n$ approaches $\\gamma_1$ and the ratio $h/n$ approaches $\\gamma_2$.\n\nThis paper considers the following scenarios of training described below, where the data is generated from a linear model on Gaussian inputs and with a zero-mean noise. The emphasis of the results is on understanding when a \"double descent\" type phenomenon occurs (\"Double descent\" is a recently coined phenomenon in literature where the risk, as a function of the \"complexity of the model\", initially has a classical U-shape behavior, but eventually decreases again once the complexity of the model exceeds the number of training points.)\n\n1. Training only the second layer: The risk is first decomposed into a bias and a variance term. An exact bound on the variance term of the risk is obtained. While the exact nature of the bound is rather complex to parse, the takeaway is that a double descent phenomenon is observed in terms of $\\gamma_2$, namely, the risk blows up when $h \\approx n$, but decreases as $h$ is increased beyond $n$.\n\n2. Training only the first layer: Two different regimes are considered here, depending on the scale of initialization, called \"vanishing\" and \"non-vanishing\" initializations. In both regimes, the risk is independent of $\\gamma_2$, that is, the risk does not depend on number of hidden units (although the risk bounds are different and there is an additional assumption in the case of non-vanishing initialization to ensure that the initialized network computes the zero function). In other words, a \"double descent\" phenomenon is not observed in this setting.\n\nRecommendation:\nI recommend \"weak acceptance\". The paper extends prior works that obtain asymptotic risk bounds on linear models to the setting of two-layer neural networks (where only one layer is trained).  However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.\n\nTechnical Comments:\n- I felt that while it is valuable to have exact bounds on the risk, the form of the bounds are quite complex and hard to parse (especially in Thm 4, case of training only the second layer). Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically. So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.\n- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement \"the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.\"\n- Another future direction that could be included in discussions is the setting where both layers are trained simultaneously."}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661122739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Reviewers"], "noninvitees": [], "tcdate": 1570237720433, "tmdate": 1575661122755, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review"}}}, {"id": "rylsXynatr", "original": null, "number": 3, "cdate": 1571827490913, "ddate": null, "tcdate": 1571827490913, "tmdate": 1572972316370, "tddate": null, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview: This work is an interesting work to understand the generalization capabilities of a two layered neural network in a high dimensional setting (samples, features and neurons tend to infinity). It studies the conditions under which the \"double descent phenomenon\" may be observed.\n\nSummary: The work shows that in two layered neural networks with non-linearity\n1) the double descent phenomenon of the bias-variance decomposition may be observed when the second layer weights are optimized assuming that the first layer weights are constant.\n2) the bias-variance decomposition does not exhibit double descent when optimizing only the first layer with both vanishing and non-vanishing initialization of weights.\n3) For vanishing initalization of weights for the first layer with non-linear activation , the gradient flow solution is asymptotically close to a two layered linear network. It is independent of overparametrization. However, the condition for this is smooth activation and the result does not hold for ReLU activation.\n4) For non-vanishing initilization of the weights for the first layer with non-linear activation, the gradient flow solution is well approximated by a kernel model. However, the risk is independent of overparametrization.\n\nI believe this is an interesting work that needs to be accepted."}, "signatures": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2506/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jba@cs.toronto.edu", "erdogdu@cs.toronto.edu", "taiji@mist.i.u-tokyo.ac.jp", "dennywu@cs.toronto.edu", "ztz16@mails.tsinghua.edu.cn"], "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "authors": ["Jimmy Ba", "Murat Erdogdu", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "pdf": "/pdf/f00f191828c2fb2bfaedb094247dab585d9b6b7f.pdf", "TL;DR": "Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of  \"double descent\".", "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups.  When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks.", "keywords": ["Neural Networks", "Generalization", "High-dimensional Statistics"], "paperhash": "ba|generalization_of_twolayer_neural_networks_an_asymptotic_viewpoint", "_bibtex": "@inproceedings{\nBa2020Generalization,\ntitle={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},\nauthor={Jimmy Ba and Murat Erdogdu and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gBsgBYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/414bf03d13704ae0089a886abcb075b6eafa33b4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gBsgBYwH", "replyto": "H1gBsgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661122739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2506/Reviewers"], "noninvitees": [], "tcdate": 1570237720433, "tmdate": 1575661122755, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2506/-/Official_Review"}}}], "count": 8}