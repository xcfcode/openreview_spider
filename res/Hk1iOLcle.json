{"notes": [{"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1528908097367, "tcdate": 1478285463347, "number": 305, "id": "Hk1iOLcle", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk1iOLcle", "signatures": ["~saurabh_tiwary1"], "readers": ["everyone"], "content": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396494458, "tcdate": 1486396494458, "number": 1, "id": "BJ8LhMU_g", "invitation": "ICLR.cc/2017/conference/-/paper305/acceptance", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Though the dataset is likely to have a large impact on the community, there is a general consensus among the reviewers that the authors could have done a better job characterizing the dataset (lack discussion on answer types, careful comparison with previous datasets, human performance on the dataset). So sadly, though the dataset is potentially very important, the paper does not quite cut it.\n \n Positive:\n -- an important dataset\n -- a good job with establishing baselines\n \n Negative\n -- analysis and discussions are very limited"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396494938, "id": "ICLR.cc/2017/conference/-/paper305/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396494938}}}, {"tddate": null, "tmdate": 1483559538693, "tcdate": 1483559538693, "number": 5, "id": "HJiuMA5rl", "invitation": "ICLR.cc/2017/conference/-/paper305/public/comment", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "More details needed for reproducibility", "comment": "While this dataset would undoubtedly benefit the community, the paper itself lacks sufficient details to to establish reproducible baselines, which is a key part of a dataset submission in my opinion.\n\nFirstly, the model descriptions are lacking:\n\n1. What exactly is the \"best passage\" model?\n2. What is \"a DSSM-alike passage ranking model\"?\n3. For each model, how does the model handle each case where there are multiple selected passages, a single selected passage, and zero selected passage?\n\nSecondly, the evaluation set on which the metrics are computed are not clearly defined.\n\n1. The authors refer to \"a subset of MS MARCO\" in all evaluation tables (5, 6, and 7). What examples do these subsets contain? Without this information non of the evaluations are reproducible.\n2. How does the author evaluate the case where there are no answers or there are several answers? It is ambiguous how one computes the BLEU or ROUGE scores in these cases.\n\nLastly, there are no human baselines on this dataset, making the upper bounds unclear (unlike some of the other datasets released)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287629340, "id": "ICLR.cc/2017/conference/-/paper305/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk1iOLcle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper305/reviewers", "ICLR.cc/2017/conference/paper305/areachairs"], "cdate": 1485287629340}}}, {"tddate": null, "tmdate": 1483558615912, "tcdate": 1483558615912, "number": 4, "id": "HJeJJA9Bl", "invitation": "ICLR.cc/2017/conference/-/paper305/public/comment", "forum": "Hk1iOLcle", "replyto": "SJFC0BBXx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Clarifications on baselines", "comment": "I'm still unsure about what the best passage baseline constitutes. I see from the dataset that each example consists of multiple passages, each of which is tagged with a \"is_selected\" field.\n\n1. How is the best passage baseline Rouge obtained in the event that only one passage is selected?\n2. How is the best passage baseline Rouge obtained in the event that no passage is selected?\n3. How is the best passage baseline Rouge obtained in the event that 2 passages are selected?\n\nMoreover, what exactly is a \"DSSM-alike passage ranking model\"? It would be nice if the authors provided system descriptions for each of the models implemented in Table 5."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287629340, "id": "ICLR.cc/2017/conference/-/paper305/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk1iOLcle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper305/reviewers", "ICLR.cc/2017/conference/paper305/areachairs"], "cdate": 1485287629340}}}, {"tddate": null, "tmdate": 1482363571466, "tcdate": 1482363571466, "number": 3, "id": "rJjnf5_Eg", "invitation": "ICLR.cc/2017/conference/-/paper305/official/review", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Paper Summary: \nThis paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset.\n\nPaper Strengths: \n-- The questions in the dataset are real queries from users instead of humans writing questions given some context.\n-- Context passages are extracted from real web documents which are used by search engines to find answers to the given query.\n-- Answers are generated by humans instead of being spans in context.\n-- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries.\n\nPaper Weaknesses: \n-- The authors say, \"We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\", but the statement is not backed up with any study.\n-- The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets. \n-- The paper should talk about what challenges are involved in obtaining a good performance on this dataset.\n-- What are the human performances as compared to the models presented in the paper?\n-- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset?\n-- What is DSSM mentioned in row 2, Table 5?\n-- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.\n-- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there?\n-- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.\n\nPreliminary Evaluation: \nThe proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512629004, "id": "ICLR.cc/2017/conference/-/paper305/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512629004}}}, {"tddate": null, "tmdate": 1482020522704, "tcdate": 1482020522704, "number": 2, "id": "Hy7hLLmEl", "invitation": "ICLR.cc/2017/conference/-/paper305/official/review", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This is a dataset paper that brings unique values over existing reading comprehension challenges. Unlike others, MS MARCO is derived from query logs, thus represents real questions that people ask, rather than solicited questions that might be rather artificial in practical settings.\n\nThere are potential downsides of using query logs however. It may be that people adapt their language and questions for search engines such that users ask questions that they know current search engines can reasonably answer. Thus, it may be that people limit the complexity of questions or language or both. I think authors could have addressed this concern by being more selective about the query logs, by down-sampling on simple questions that can be easily answered by keyword matching without any sophisticated reading comprehension, and up-sampling more complex questions that require at least paraphrasing and ideally synthesis of information taken from more than one sentences.\n\nIt\u2019s great that there are several new efforts to construct large-scale reading comprehension challenges, but my main concern is whether the majority of the questions can be answered through relatively easy text matching without intelligent reading or reasoning.\n\nAlso, the paper reads like the authors were running out of time before the deadline. I would appreciate more analytic and quantitative comparisons against other existing datasets, and more insights on the degree of challenges required to handle QAs in MS MARCO. For example, the authors could collect statistics on QAs: (1) exact match exists in the text snippet, (2) paraphrasing is required but otherwise the relevant answer is directly available in the text snippet, (3) requires synthesizing information taken from more than one sentences, (4) requires external knowledge. The author response mentions that (4) is unlikely, but a more formal and complete analysis would be helpful.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512629004, "id": "ICLR.cc/2017/conference/-/paper305/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512629004}}}, {"tddate": null, "tmdate": 1481860742662, "tcdate": 1481860639506, "number": 1, "id": "S1P7LJZNe", "invitation": "ICLR.cc/2017/conference/-/paper305/official/review", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer1"], "content": {"title": "Need human performance, comparison of distribution of questions / answer-types, comment on automatic metrics", "rating": "6: Marginally above acceptance threshold", "review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512629004, "id": "ICLR.cc/2017/conference/-/paper305/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512629004}}}, {"tddate": null, "tmdate": 1481101008884, "tcdate": 1481101008877, "number": 3, "id": "SJFC0BBXx", "invitation": "ICLR.cc/2017/conference/-/paper305/public/comment", "forum": "Hk1iOLcle", "replyto": "rJA1XaJml", "signatures": ["~Mir_Rosenberg1"], "readers": ["everyone"], "writers": ["~Mir_Rosenberg1"], "content": {"title": "Best passage baseline details and inter-human agreement for answers", "comment": "Thanks for your interest in our MS MARCO our paper and for your question!\n\n1. Could you provide more details about the Best Passage baseline?\n\nDuring the judgment process, the judges are asked not only to write the best answer but also identify passages from which the answers have been synthesized or inferred. These passages identified by the judges constitute the best passage baseline. Hence, this baseline is the upper bound performance of the passage ranking methods.  \n\n2. Could you please report inter-human agreement for the collected answers?\n\nWhereas we do not have a precise number for human error, we have done several rounds of detailed data analysis amongst the authors and the rest of our team. Due to the nature of the dataset, there will be some variance in the exact answers generated by different human judges in terms of phrasing and presentation of the answers (and we observed the same during our analysis). We have seen most of the variance along this dimension. The judges have generally been consistent in the factual content of the answers. We also plan to significantly increase the number of questions with multiple answers in the next iteration of this dataset and to measure algorithms across these multiple answers to limit the variance in accuracy scores for the different algorithms. The metric pa-BLEU, that we propose to use as a metric in the paper, also aims to address the issues arising due to intra-human variance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287629340, "id": "ICLR.cc/2017/conference/-/paper305/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk1iOLcle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper305/reviewers", "ICLR.cc/2017/conference/paper305/areachairs"], "cdate": 1485287629340}}}, {"tddate": null, "tmdate": 1481021990899, "tcdate": 1481021990892, "number": 2, "id": "r1kVcM47x", "invitation": "ICLR.cc/2017/conference/-/paper305/public/comment", "forum": "Hk1iOLcle", "replyto": "SJn5xcRzl", "signatures": ["~Mir_Rosenberg1"], "readers": ["everyone"], "writers": ["~Mir_Rosenberg1"], "content": {"title": "Comprehension/inference statistics for MS MARCO is in progress", "comment": "Thanks for your interest in our paper and for this question! I am sorry for the delay in our reply as I was flying to NIPS at the time and did not check email frequently, unfortunately. \n\nWhereas we have not collected and measured this exact information accurately yet, we agree this would be a useful addition to the paper. Categories 1, 2, and 3 mentioned in your question should be reasonably represented in the MS MARCO dataset with 4 (requires external knowledge) being close to zero. The latter is because of the way the human judgment task was set up. Upon your request, we can definitely measure this accurately and include this information in a revised version of the paper. Please advise."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287629340, "id": "ICLR.cc/2017/conference/-/paper305/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk1iOLcle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper305/reviewers", "ICLR.cc/2017/conference/paper305/areachairs"], "cdate": 1485287629340}}}, {"tddate": null, "tmdate": 1481021297753, "tcdate": 1481021297745, "number": 1, "id": "Skq_wf4Qe", "invitation": "ICLR.cc/2017/conference/-/paper305/public/comment", "forum": "Hk1iOLcle", "replyto": "SJVAhPXQl", "signatures": ["~Mir_Rosenberg1"], "readers": ["everyone"], "writers": ["~Mir_Rosenberg1"], "content": {"title": "Cloze-style model experiment details for MS MARCO dataset and key takeaway messages", "comment": "Thanks for these insightful questions and for taking time to review our paper!\n\nCan the authors please provide more details about the Cloze-style model experiments on their MS-MARCO dataset? How did they create the missing symbols and the context texts using the queries, passages and answers present in their dataset?\n\nTo start, we\u2019d like to cite the last paragraph of section 1 in the original paper:\n\n\u201cAs we summarized in the last paragraph of Section 1, compared to other publicly available datasets, including cloze datasets, MS MARCO is unique in that (a) all questions are real user queries, (b) the context passages, which answers are derived from, are extracted from real web documents, (c) all the answers to the questions are human generated, (d) a subset of these questions has multiple answers, (e) a subset of these questions has no answers.\u201d\n\nOn a high level, the MS MARCO set differs from commonly used cloze test datasets such as CNN/Daily Mail, etc. in a few ways. We performed a cloze-style model experiment by looking into each of them:\n\n1) In MS MARCO we have free style human-generated answers instead of a set of pre-determined answer candidates. As mentioned in the paper, in order to apply cloze-style models to our dataset, we filtered the dataset to only the numeric category, where the labeled answer is a single identifiable number that has appeared in the passages. This gives us a subset of MS MARCO which was shown as test data in the table.  Accordingly, for any query the candidate answer set is composed of all identified numbers in all the given passages. Instead of generating an answer, the model\u2019s goal is to find the most probable answer among the candidates. This makes the dataset applicable for discriminative models like a cloze test model.\n\n2) Our queries are mostly real user queries in natural language, which are not in the form of sentences with missing symbols. This is less of a problem as the models under test (AS Reader and ReasoNet) take the query as a sequence input of RNN where the missing symbols are indifferent like other vocabulary words. In these models, the concatenated final RNN states were taken as query presentation, regardless of whether the query has missing-symbol format or not.\n\n3) For a given query, we have multiple context passages instead of just one. In the experiments we ask the model to take the concatenated passages as the context input.\n\nAs for the second comment: what are the take away messages from those experiments?\n\nThese experimental results are designed to showcase characteristics of the MS MARCO dataset described in Section 3. In particular, we show that real user queries are of high diversity. In order to measure the effectiveness of RC models, different evaluation metrics need to be used for different categories of queries. MS MARCO provides a much more comprehensive benchmark than other (cloze) RC datasets. Additionally, the accuracy numbers seemed to suggest that the MS MARCO numeric segment is harder to solve than CNN test data, but also sensitive to model advancement, and is directionally consistent with CNN test data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287629340, "id": "ICLR.cc/2017/conference/-/paper305/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk1iOLcle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper305/reviewers", "ICLR.cc/2017/conference/paper305/areachairs"], "cdate": 1485287629340}}}, {"tddate": null, "tmdate": 1480977612061, "tcdate": 1480977612057, "number": 3, "id": "SJVAhPXQl", "invitation": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer3"], "content": {"title": "Cloze-style model experiments", "question": "Can the authors please provide more details about the Cloze-style model experiments on their MS-MARCO dataset? How did they create the missing symbols and the context texts using the queries, passages and answers present in their dataset? What are the take away messages from those experiments?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480977612689, "id": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480977612689}}}, {"tddate": null, "tmdate": 1480737510165, "tcdate": 1480737510161, "number": 2, "id": "rJA1XaJml", "invitation": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer1"], "content": {"title": "Clarification Question and Inter-human Agreement", "question": "Dear Authors,\n\n1. Could you provide more details about the Best Passage baseline?\n\n2. Could you please report inter-human agreement for the collected answers?\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480977612689, "id": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480977612689}}}, {"tddate": null, "tmdate": 1480659091780, "tcdate": 1480659091774, "number": 1, "id": "SJn5xcRzl", "invitation": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "signatures": ["ICLR.cc/2017/conference/paper305/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper305/AnonReviewer2"], "content": {"title": "type of comprehention / inference", "question": "Do you have statistics on the type of comprehension/inference required for QAs? E.g., (1) exact match exists in the text snippet, (2) paraphrasing is required but otherwise the relevant answer is directly available in the text snippet, (3) requires synthesizing information taken from more than one sentences, (4) requires external knowledge, ..."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "pdf": "/pdf/3ce987396a42538babbc77ac4afbf1e44815be13.pdf", "TL;DR": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "conflicts": ["microsoft.com"], "keywords": [], "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "paperhash": "nguyen|ms_marco_a_humangenerated_machine_reading_comprehension_dataset"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480977612689, "id": "ICLR.cc/2017/conference/-/paper305/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper305/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper305/AnonReviewer2", "ICLR.cc/2017/conference/paper305/AnonReviewer1", "ICLR.cc/2017/conference/paper305/AnonReviewer3"], "reply": {"forum": "Hk1iOLcle", "replyto": "Hk1iOLcle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480977612689}}}], "count": 13}