{"notes": [{"id": "Yj4mmVB_l6", "original": "nfDhDnbIcKq", "number": 3446, "cdate": 1601308382450, "ddate": null, "tcdate": 1601308382450, "tmdate": 1614985653194, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xJguOR9Hum", "original": null, "number": 1, "cdate": 1610040509838, "ddate": null, "tcdate": 1610040509838, "tmdate": 1610474117488, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper provides a unified view of some known methods for monotone operator inclusion problems like Forward-Backward-Forward (FBF) and OGDA, and provides new convergence results for the stochastic version of a variant of FBF called FBFp. All reviewers initially recommended rejection. The rebuttal and the manuscript update addressed several concerns from the reviewers, though the general consensus after rebuttal was still that the paper lacked in significance for the ICLR community. The AC thinks that the paper could make an interesting overview paper in a more optimization / theoretically minded venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040509825, "tmdate": 1610474117472, "id": "ICLR.cc/2021/Conference/Paper3446/-/Decision"}}}, {"id": "XWHQfvu5_6G", "original": null, "number": 1, "cdate": 1602568759768, "ddate": null, "tcdate": 1602568759768, "tmdate": 1606803035085, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review", "content": {"title": "Unifying Optimistic Gradient Descent Ascent and Forward Backward Forward For Convex-Concave Optimization", "review": "The authors in this paper, inspired by the applications of min-max optimization in GANs, study the problem of min-max optimization for convex-concave functions. The main contribution of the paper is proving novel convergence results for Forward-Backward-Forward (FBF) algorithms as well as Optimistic Gradient Descent Ascent (OGDA) based on tools from monotone inclusion problems. Their convergence results cover both deterministic and stochastic settings and the rates of convergence for suitably chosen gap function are non-asymptotic. Finally, they apply their algorithms both on toy problems but also on training GANs on CIFAR-10.\n\nPros:\n1) To the best of my knowledge, the connection between OGDA and monotone inclusion problems is new.\n2) Convergence results are non asymptotic for the specified gap function.\n\nCons:\n1) The connection of this work with GANs is a bit tenuous because, as the authors also acknowledge, training GANs is a non-convex non-concave min-max problem. The authors should try to express why they believe their  unification result informs practical applications.\n2) This lack of connection is reflected in the experimental section as well. Most experiments re-establish that optimism, extragradient updates or regularization are beneficial for min-max optimization, observations that are already widely known. Again, any experimental insight that is particular to this work, would go a long way towards closing this gap.\n3) It is not clear that the connection of OGDA to FBF and monotone inclusion provides any new insights about the convergence properties of either method.  It would be very helpful, if the authors provided any additional intuition why their result could be used to answer open questions related to OGDA or FBF.  For example, last iterate convergence of OGDA is still an open problem even in convex-concave problems.\n4) While the gap function used allows the authors to provide non asymptotic guarantees, the intuition behind this gap function when its value is non-zero is unclear. Does this gap function have any game theoretic interpretation?\n\nFor now, I am assigning a weak reject score mainly because it is unclear to me if there are significant implications of this unification result either in theory or practice. I am willing to increase my score substantially if the authors provide additional details that address my concerns outlined above.    \n\n---------------------------------\nPost-Rebuttal evaluation.\n\nI would like to thank the authors for their detailed answers, especially regarding the interpretation of the gap function.\nBased on their answers, I decided to increase my score to a 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075712, "tmdate": 1606915802892, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3446/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review"}}}, {"id": "9qbmDI2d5L-", "original": null, "number": 3, "cdate": 1603848615039, "ddate": null, "tcdate": 1603848615039, "tmdate": 1606802935282, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review", "content": {"title": "Contributions seem incremental", "review": "Summary: This work studies minimax optimization (a.k.a saddle-point problems) with nonsmooth regularizers. By leveraging the monotone operator theory, the authors propose to use the forward-backward-forward method so as to avoids the notorious limit cycling problem. The classical FBF method requires two gradient evaluations per step, the authors introduce a new algorithm which reuses the past gradient in the same way as OGDA. In the setting of convex-concave minimax optimization, the authors claim to prove novel convergence rates for both methods. \n\nReview: This paper is well-written, though I think the difference with extra-gradient can be made much clearer. For example, the authors could compare FBF with EG for a toy example like Eqn. (13). As far as I can tell, the main difference with extra-gradient is just the regularization term. From this standpoint, the authors should clarify the motivations and importance of adding regularizers. In the current version of the paper, section 2.4 is vague and doesn't explain the role of regularizers well in GAN training.\nBesides, a similar convergence analysis has been done for EG and OGDA (the authors don't even cite properly), so I believe the convergence analysis of this paper is not novel. Given these reasons, I suggest the rejection of this paper and give a score of 4.\n\nComments:\n- It was shown in [1] that the ergodic (averaged) iterates of extra-gradient converge with a rate of O(1/k) which is the same as FBF. Could the authors clarify the novelty/difference of your proof technique for FBF?\n\n- It was shown in [2] that extra-gradient is NOT robust to gradient noise in convex-concave minimax optimization. Could the authors comment on that and explain why FBF could achieve the rate of O(1/sqrt(k)) in the stochastic setting (while EG fails)?\n\n- In GAN training, we typically care more about the last iterate since averaging could actually hurt the performance when the loss surface is highly nonconvex. Is it possible to derive the last iterate convergence rate? \n\n\nI'm willing to increase my rating if the authors could resolve some of my concerns, especially my concern on the novelty of the analysis.\n\n\n-------------\n**I've read the authors' response. I'm still concerned with the novelty of the paper given there are similar results for EG/OGDA. Therefore, I stick to my original rating.**\n\nReferences:\n\n[1] Convergence Rate of O(1/k) for Optimistic Gradient and Extra-gradient Methods in Smooth Convex-Concave Saddle Point Problems, 2019.\n\n[2] Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling, 2020.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075712, "tmdate": 1606915802892, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3446/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review"}}}, {"id": "tMiFhXZIAO9", "original": null, "number": 7, "cdate": 1605353591715, "ddate": null, "tcdate": 1605353591715, "tmdate": 1605353591715, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "VDWmwqsTZP", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "we meant distance to solution", "comment": "Thank you for pointing out the article. We thought 'distance of the last iterate to the solution set' was meant, while the mentioned paper uses 'norm of the gradient' and 'gap'. Unfortunately we do not know whether this analysis can be adapted to the regularized setting, even under the stronger assumption of Lipschitz second derivatives."}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "VDWmwqsTZP", "original": null, "number": 6, "cdate": 1605295537875, "ddate": null, "tcdate": 1605295537875, "tmdate": 1605295537875, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "209qADXJiAy", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "Last iterate convergence rates", "comment": "Please see the following paper for the last iterate analysis: \n\n- Last Iterate is Slower than Averaged Iterate in Smooth Convex-Concave Saddle Point Problems"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "_Nyy-fdto7a", "original": null, "number": 5, "cdate": 1605282106163, "ddate": null, "tcdate": 1605282106163, "tmdate": 1605282106163, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "XWHQfvu5_6G", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Dear reviewer, thank you for your time and remarks. For your convenience we highlighted all the changes in the pdf in red.\n\nSince some of the points raised in Cons. 1-3 are connected we try and respond to them at the same time.\n\n> The authors should try to express why they believe their unification result informs practical applications.\n> It is not clear that the connection of OGDA to FBF and monotone inclusion provides any new insights about the convergence properties of either method.\n> [...] any experimental insight that is particular to this work\n\nThe unification result itself may not provide much practical benefit although it sheds some light on the stepsize requirements of the different methods (see the fourth comment to R3). The benefit of our work for practical applications lies more in the treatment of regularizers and the fact that FBF requires less evaluations (see the first reply to R3) and is thus able to save computation time (compared to EG) without sacrificing performance.\n\n> last iterate convergence of OGDA is still an open problem even in convex-concave problems\n\nAs far as we know asymptotic convergence of the iterates is known for OGDA.\nRegarding convergence *rates* for the (last) iterate(s) we don't think it is possible to obtain such a statement without the use of more restrictive assumptions (see last comment to R3). If talking about rates for the gap function but in terms of the last iterate (instead of the ergodic), this constitutes indeed an interesting and possibly obtainable (albeit more difficult) statement, but was outside the scope of our work.\n\n> the intuition behind this gap function [...] is unclear\n\nThank you this very valid comment. We added a clarifying remark and intuition in the updated version of our manuscript. Please see also the reply to R1 were we give a short summery."}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "xF3tJX6PtL", "original": null, "number": 4, "cdate": 1605282060051, "ddate": null, "tcdate": 1605282060051, "tmdate": 1605282060051, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "lVwYVd9lbOV", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Dear reviewer, thank you for your time and interest. For your convenience we highlighted all the changes in the pdf in red.\n\n> The paper takes a long time until it becomes clear what actually the monotone inclusion looks like.\n\nSince monotone inclusions are not a well known object in the ML community we wanted to make sure that the concept is properly motivated.\n\n> preceded by a long and unnecessary discussion about existing solvers\n\nDue to the similarities of the existing methods we think that this exposition is necessary in order to not create confusion. First of all it seems very natural to us to compare FBF with EG since the two methods are so closely related. Secondly, by recycling previous gradients we provide a novel intuition for OGDA/FRB, which we think is relevant and requires us to mention said methods.\n\n> p.3 claim that FBF has not been rigorously analyzed for saddle point problems. This is of course not true.\n\nWe meant, but did not write explicitly, this statement to be in terms of function values / gap. We added this clarifying specification in the pdf on page 3.\nAt the same time we would like to point out that in the contribution paragraph and in the conclusion this was already stated more precisely before.\n\n> The stochastic FBF has been studied in [...]\n\nWe added the reference and clarified the differences with the mentioned paper. In particular, the cited article does not deal with the minimax setting specifically and is therefore not able to make statements about the gap function.\n\n> For SFBF we know non-asymptotic convergence rates of the last iterate. This is not mentioned at all.\n\nThank you for pointing this out. We mentioned this in the updated version of our preprint and clarified that these rates are in terms of fixed point residual, which is a rather general notion and does not exploit the special structure of the problem. Another key difference between our work and the mentioned paper is that we do not rely on minibatch sizes that go to infinity.\n\n> if the aim is to solve the VI over an unconstrained domain, then FBF coincides with EG [...]\nIn my opinion it would therefore be cleaner to assume at the outset that the domain of $F+\\partial r$ is bounded\n\nWe do not see a convincing reason to assume that the domain of the problem is bounded. In fact there are many interesting regularizers which do not have a bounded domain. Note further that EG and FBF are not the same method in this regularized (but unconstrained) setting.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "209qADXJiAy", "original": null, "number": 3, "cdate": 1605282007860, "ddate": null, "tcdate": 1605282007860, "tmdate": 1605282007860, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "9qbmDI2d5L-", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Dear reviewer, thank you for your interest and remarks. For your convenience we highlighted all the changes in the pdf in red.\n\n> the authors could compare FBF with EG for a toy example like Eqn. (13)\n\nThe two methods are compared for this problem in Figure 1. The difference between the two methods is not so much the regularization term (which is part of the objective function) but how it is treated: per iteration EG requires two evaluations of the projection (prox) corresponding to the regularizer, while FBF only needs one.\nNote that in the absence of regularizers or constraints (in the problem formulation) EG and FBF reduce to the same method.\n\n> the authors should clarify the motivations and importance of adding regularizers. In the current version of the paper, section 2.4 is vague and doesn't explain the role of regularizers well in GAN training.\n\nWe added some remarks about regularizers used for GAN training in Section 2.4 mentioning the box constraints in the original WGAN formulation and spectral normalization for SN-GAN.\n\n> It was shown in [1] that the ergodic (averaged) iterates of extra-gradient converge with a rate of O(1/k)\n\nThank you for pointing this article out, which we were not aware of. We added the reference and adapted the text accordingly. Note however that our result still provides a generalization in the sense that we also cover regularizers which encompass, but are not limited to constraints.\n\n> a similar convergence analysis has been done for EG and OGDA.\n> Could the authors clarify the novelty/difference of your proof technique for FBF?\n\nSince OGDA, EG and FBF are similar they naturally rely on similar proof techniques. Through our unified approach it becomes easier to draw a distinction between and highlight the similarities of OGDA and FBF. Because of this we are able to pinpoint where the different stepsize requirements arise (OGDA only allows for half as big of a stepsize compared to FBF, and thus possibly negate every saving stemming from the reduced amount of gradient evaluations..). In particular, it indicates why OGDA tends to requires a smaller stepsize than FBF/EG in applications.\n\n> It was shown in [2] that extra-gradient is NOT robust to gradient noise in convex-concave minimax optimization.\nCould the authors comment on that and explain why FBF could achieve the rate of $O(1/sqrt(k))$ in the stochastic\nsetting (while EG fails)?\n\nReference [2] itself mentions that EG exhibits the same $O(1/\\sqrt{k})$ rate, see Table 1 in that paper. These two seemingly conflicting statements stem in our opinion from the fact that one is with respect to the sequence of (last) iterates itself and the other one is in terms of the ergodic (averaged) ones. The mentioned issue is of course relevant and a version of FBF which is robust to noise would be interesting.\n\n> Is it possible to derive the last iterate convergence rate?\n\nThis is unknown and even unlikely as convergence rates for the iterates can typically only be obtained under more restrictive assumptions as strong convexity/monotonicity (or error bounds).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "_ZHcJQBhyxz", "original": null, "number": 2, "cdate": 1605281964492, "ddate": null, "tcdate": 1605281964492, "tmdate": 1605281964492, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "CvBWcXq43u_", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Dear reviewer, thank you for your interest and the comments. For your convenience we highlighted all the changes in the pdf in red.\n\n> Why do you introduce h(y) here?\n\nThe functions $h$ and $f$ here act as regularizers. These could reflect the box constraints in the case of weight clipping or the spectral normalization in the case of SN-GANs. We added an explanatory remark in Section 2.4 of the manuscript. These functions are mentioned explicitly and treated separately in the algorithms because of their potential nonsmoothness.\n\n> Later the author restrict Equation(1) to a deterministic version\n\nWe restrict to the deterministic version only in the introductory Section 2. We try to differentiate between the stochastic and the deterministic setting not via the expectation in the objective function but whether stochastic or batch gradient updates are used. This was done purely for ease of notation in order to not having to write the expected value whenever the objective function appears. We tried to clarify this in the manuscript right below eqn (1).\n\n> I cannot understand the first sentence of Section 2.2.\n> I don't know why the problem (1) can be written as Equation (8)\n\nWhat we meant was that the monotone inclusion corresponds to the first order optimality condition of our main problem.\nIn general finding a first order critical point stipulates a necessary condition for being a local solution. Due to the assumed convexity of the problem every critical point is indeed a solution.\nThus, solving the (monotone) inclusion is equivalent to solving the minimax problem and finding a saddle point. We clarified this point in the updated version.\n\n> the authors provided [...] lots of iterative methods. It is difficult for me to distinguish\nwhich are completely new\n\nWe rephrased this section slightly in order to highlight the origin of the different methods.\n\n> could you explain more about why $G_B (w)$ in Equation (10) is defined as this form?\n\nWe added a game theoretic interpretation of the minimax gap. In short it corresponds to the payoff each player can achieve by choosing the best response given the (suboptimal) strategy of their opponent. A set of strategies thus corresponds to a small gap if only a small payoff can be achieved for either player by playing the best strategy given the current one.\n\n> In Section 3.2, the authors provided a generalized FBF algorithm. Isn't this Algorithm 3.1 a combined re-written\nversion of Equation (4) and Equation (5)?\n\nIndeed. It is a template in order to reconstruct these two methods and thus highlights how they are connected.\n\n> There is a big gap between Equation (9) [...] and the experimental results\n\nIn Section 2.2 we tried to make the connection between monotone inclusions and minimax problems clear. If it helps we can explicitly write down our methods for saddle point problems (which includes the training of GANs) in the Appendix.\n\n> there are no open source codes provided\n\nThese should be present in the supplementary material. If there is a problem with the zip file or the download, please let us know.\n\n> Although, the authors stated that \"Due to the theoretical nature of this work [...]\"\n\nWe rephrased the beginning of the experiment section in order to highlight our practical contributions.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Yj4mmVB_l6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3446/Authors|ICLR.cc/2021/Conference/Paper3446/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Comment"}}}, {"id": "lVwYVd9lbOV", "original": null, "number": 2, "cdate": 1603137865534, "ddate": null, "tcdate": 1603137865534, "tmdate": 1605023999608, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review", "content": {"title": "Report on \"Two steps at a time --- taking GAN training in stride with Tseng's method\" ", "review": "Summary: This paper introduces the forward-backward-forward splitting for variational inequalities. The main results are an asymptotic convergence results and a non-asymptotic convergence results using a restricted merit function. A new method, FBFp, is introduced and studied. Complete proofs are given. Preliminary numerical results obtained by training GANs are reported. \n\nPros:\n+ complete proofs \n+ A new stochastic operator splitting method based on Tseng's FBF is introduced in which the operator needs to be evaluated once per iteration. This splitting, called FBFp, is indeed new, and has the potentially of being of practical relevance.\n+ Preliminary numerical results on standard GAN architectures. \n\nCons: \n- The paper takes a long time until it becomes clear what actually the monotone inclusion looks like. It seems that the problem of interest is formulation in eq. (9), preceded by a long and unnecessary discussion about existing solvers. It would have been much more accurate to simply start with the problem formulation, then propose your solution method, followed by a critical explanation of the contribution. \n- p.3 claim that FBF has not been rigorously analyzed for saddle point problems. This is of course not true. Even the original paper by Tseng (A MODIFIED FORWARD-BACKWARD SPLITTING METHOD FOR MAXIMAL MONOTONE MAPPINGS, SICON 2000) discusses the application to saddle point problems. See Example 5 in that paper.  \n- The stochastic FBF has been studied in Bot et al. Mini-batch Forward-Backward-Forward Methods for solving Stochastic Variational inequalities, forthcoming in Stochastic Systems. Note that the Arxive version of that paper is available since 2019. Overall, the paper contains only marginal contributions to the state-of-the-art. \n- Only convergence rates for the ergodic average is provided. It is known that the ergodic average might destroy important features of the true solution, such as sparsity. For SFBF we know non-asymptotic convergence rates of the last iterate. This is not mentioned at all. \n- I have some doubts that the restricted merit function is the appropriate one here. Note if the aim is to solve the VI over an unconstrained domain, then FBF coincides with EG, and there is nothing to be analyzed. The interesting case is thus only the constrained case. These constraints are usually encoded in the non-smooth part of of eq. (8), so there is no need to write this explicitly. In my opinion it would therefore be cleaner to assume at the outset that the domain of $F+\\partial r$ is bounded. The gap function used can in fact be traced back to Facchinei & Pang (2003) and is most likely even longer in use than that. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075712, "tmdate": 1606915802892, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3446/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review"}}}, {"id": "CvBWcXq43u_", "original": null, "number": 4, "cdate": 1603908921873, "ddate": null, "tcdate": 1603908921873, "tmdate": 1605023999548, "tddate": null, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "invitation": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review", "content": {"title": "Many unclear or doubting points", "review": "In this paper,\nthe authors first formulate the optimization problem of GANs as an abstract minimax problem (Equation(1)).\nAs compared to the original optimization objective of Goodfellow's GANs,\nthere is an additional term $h(y)$.\nWhy do you introduce $h(y)$ here? Just for facilitating the adoption of the MONOTONE INCLUSIONS on it?\nThe authors should provide a clear explanation about this point.\n\nLater the author restrict Equation(1) to a deterministic version,\nwhich means that the noise input of GANs will no longer be considered.\nThe noise input is an important ingredient of GANs.\nDespite many new variants of the GANs,\nat least, the noise input is import for the Goodfellow's GANs, which is adopted in this submission as a staring point.\nIt is very hard for me to decide whether this simplification is appropriate.\nExcept Algorithm 3.2 and Theorem 3.2 which suddenly provide stochastic versions,\nall the following results are on this deterministic version.\nIn my opinion, the authors should provide more explanations on this point.\n\nI cannot understand the first sentence of Section 2.2, after read it many times.\nWhat is the exact necessary and sufficient optimality condition for the coupling function being convex-concave and differentiable?\nBefore Equation (2), the authors didn't explain what is monotone and what is monotone inclusion.\nI don't think these concepts are very famous in machine learning community.\n\nIn Section 2.3,\nthe authors provided the introduction of lots of iterative methods.\nIt is difficult for me to distinguish which are completely new findings for solving their inclusion problem and which are the existing results.\nThe authors completed a literature review here.\n\nIn Section 2.4,\nI don't know why the problem (1) can be written as Equation (8).\nCould you provide more explanations about this point?\nLimited space of one submission should be on important points.\n\nIn Section 3,\ncould you explain more about why $G_B(w)$ in Equation (10) is defined as this form?\nJust for facilitating the proof of Theorem 3.1 and Theorem 3.2?\n\nIn Section 3.2,\nthe authors provided a generalized FBF algorithm.\nIsn't this Algorithm 3.1 a combined re-written version of Equation (4) and Equation (5)?\n\nThere is a big gap between Equation (9), Theorem 3.1 and Theorem 3.2 and the experimental results shown in Section 4.\nBesides, there are no open source codes provided.\nIt is very hard for me to figure out the details of the experiments and meantime to check the reproducibility of this paper.\n\nAlthough, the authors stated that \"Due to the theoretical nature of this work, the aim of this section is not to achieve new state-of-the-art\nresults.\"\nI don't think optimization is a theoretical branch of our machine learning community.\nIf a proposed optimization method cannot be proved to be very useful in certain ares or specific tasks,\nit will be very doubting.\nIf we intend to do theoretical contributions,\nwe should try to prove the theoretical properties or convergence bounds for the existing useful optimization methods.\n\nSince ICLR is a highly selective conference,\nthe originality and significance of one submission will always be in the first priority.\nI cannot accept this paper in current state.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3446/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3446/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Two steps at a time --- taking GAN training in stride with Tseng's method", "authorids": ["~Axel_B\u00f6hm1", "~Michael_Sedlmayer1", "~Ern\u00f6_Robert_Csetnek1", "radu.bot@univie.ac.at"], "authors": ["Axel B\u00f6hm", "Michael Sedlmayer", "Ern\u00f6 Robert Csetnek", "Radu Ioan Bot"], "keywords": [], "abstract": "Motivated by the training of Generative Adversarial Networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers.\nWe do so by employing \\emph{monotone operator} theory, in particular the \\emph{Forward-Backward-Forward (FBF)} method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation.\nFurthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost.\nIn doing so we rediscover a known method, related to \\emph{Optimistic Gradient Descent Ascent (OGDA)}.\nFor both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates.\nFor the deterministic and the stochastic problem we show a convergence rate of $\\mathcal{O}(\\nicefrac{1}{k})$ and $\\mathcal{O}(\\nicefrac{1}{\\sqrt{k}})$, respectively.\nWe complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "b\u00f6hm|two_steps_at_a_time_taking_gan_training_in_stride_with_tsengs_method", "supplementary_material": "/attachment/1bcd5f908564faca2f81b6a15b084649a8623db5.zip", "pdf": "/pdf/aa426a1a631cad4c4a07c1ef515c2ae6fe1799d8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BPxIwPTPp-", "_bibtex": "@misc{\nb{\\\"o}hm2021two,\ntitle={Two steps at a time --- taking {\\{}GAN{\\}} training in stride with Tseng's method},\nauthor={Axel B{\\\"o}hm and Michael Sedlmayer and Ern{\\\"o} Robert Csetnek and Radu Ioan Bot},\nyear={2021},\nurl={https://openreview.net/forum?id=Yj4mmVB_l6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Yj4mmVB_l6", "replyto": "Yj4mmVB_l6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075712, "tmdate": 1606915802892, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3446/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3446/-/Official_Review"}}}], "count": 12}