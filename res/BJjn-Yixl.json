{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396678064, "tcdate": 1486396678064, "number": 1, "id": "H1RWTfUdl", "invitation": "ICLR.cc/2017/conference/-/paper562/acceptance", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper shows some strong performance numbers, but I agree with the reviewers that it requires more analysis of where those gains come from. The model is very simple, which is a positive, but more studies such as ablation studies and other examples would help a lot."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396678533, "id": "ICLR.cc/2017/conference/-/paper562/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396678533}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482968557177, "tcdate": 1478361523440, "number": 562, "id": "BJjn-Yixl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJjn-Yixl", "signatures": ["~Pranav_Shyam1"], "readers": ["everyone"], "content": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481941048699, "tcdate": 1481941048699, "number": 3, "id": "HJWrxQM4x", "invitation": "ICLR.cc/2017/conference/-/paper562/official/review", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer2"], "content": {"title": "experimental section improved but still very weak on analysis and insight", "rating": "4: Ok but not good enough - rejection", "review": "This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input.\n\nThe paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character?  Although it is zooming in, shouldn\u2019t it attend to relevant parts of the character?  Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from.\n\nWhile the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512542212, "id": "ICLR.cc/2017/conference/-/paper562/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper562/AnonReviewer3", "ICLR.cc/2017/conference/paper562/AnonReviewer1", "ICLR.cc/2017/conference/paper562/AnonReviewer2"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512542212}}}, {"tddate": null, "tmdate": 1481902383787, "tcdate": 1481902383787, "number": 2, "id": "rJu4Ftb4x", "invitation": "ICLR.cc/2017/conference/-/paper562/official/review", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer1"], "content": {"title": "Strong experimental results, but somewhat unclear where the improvements are coming from", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512542212, "id": "ICLR.cc/2017/conference/-/paper562/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper562/AnonReviewer3", "ICLR.cc/2017/conference/paper562/AnonReviewer1", "ICLR.cc/2017/conference/paper562/AnonReviewer2"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512542212}}}, {"tddate": null, "tmdate": 1481709769966, "tcdate": 1481709769960, "number": 1, "id": "HJfCu9Amg", "invitation": "ICLR.cc/2017/conference/-/paper562/official/review", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer3"], "content": {"title": "The paper need more improvements to be accepted", "rating": "3: Clear rejection", "review": "This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human's behavior. However, there are several cons of the paper:\n\n1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version.\n\n2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)\n\n3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512542212, "id": "ICLR.cc/2017/conference/-/paper562/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper562/AnonReviewer3", "ICLR.cc/2017/conference/paper562/AnonReviewer1", "ICLR.cc/2017/conference/paper562/AnonReviewer2"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512542212}}}, {"tddate": null, "tmdate": 1481708646945, "tcdate": 1481708646940, "number": 1, "id": "ryy_Vc0Ql", "invitation": "ICLR.cc/2017/conference/-/paper562/official/comment", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer3"], "content": {"title": "what is the results on LFW dataset?", "comment": "It is weird that the paper describe LFW dataset but do not provides the results on it."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287520185, "id": "ICLR.cc/2017/conference/-/paper562/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers", "ICLR.cc/2017/conference/paper562/areachairs"], "cdate": 1485287520185}}}, {"tddate": null, "tmdate": 1480809573737, "tcdate": 1480809573733, "number": 2, "id": "rJ0vnCeXg", "invitation": "ICLR.cc/2017/conference/-/paper562/pre-review/question", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer2"], "content": {"title": "discussion of results", "question": "The results for ARC in the current version of the paper do not outperform some baselines.  It seems from the author comment that new results which do outperform will be updated soon.  Could you also provide insight and interpretation (e.g. qualitative examples) into where and how the proposed method outperforms the baselines?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959213794, "id": "ICLR.cc/2017/conference/-/paper562/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper562/AnonReviewer1", "ICLR.cc/2017/conference/paper562/AnonReviewer2"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959213794}}}, {"tddate": null, "tmdate": 1480691504219, "tcdate": 1480691504214, "number": 1, "id": "rJdEJGyml", "invitation": "ICLR.cc/2017/conference/-/paper562/pre-review/question", "forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "signatures": ["ICLR.cc/2017/conference/paper562/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper562/AnonReviewer1"], "content": {"title": "missing results", "question": "In your paper, you write that you are missing results that will be reported.\n\nDo you have a newer version of your paper? In the current version your model doesn't perform as well as other methods. Could you report your \"full context arc\" results? Adding other more qualitative contributions or explaining why comparison is that not important would be nice too. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Recurrent Comparators", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "pdf": "/pdf/5e34c70e4e5709fc4182c94dc10eb16ff020faa6.pdf", "TL;DR": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "paperhash": "shyam|attentive_recurrent_comparators", "conflicts": ["csa.iisc.ernet.in"], "keywords": ["Deep learning", "Computer vision"], "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959213794, "id": "ICLR.cc/2017/conference/-/paper562/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper562/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper562/AnonReviewer1", "ICLR.cc/2017/conference/paper562/AnonReviewer2"], "reply": {"forum": "BJjn-Yixl", "replyto": "BJjn-Yixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959213794}}}], "count": 8}