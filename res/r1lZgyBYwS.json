{"notes": [{"id": "r1lZgyBYwS", "original": "Syg8MGs_vS", "number": 1496, "cdate": 1569439465437, "ddate": null, "tcdate": 1569439465437, "tmdate": 1583912026552, "tddate": null, "forum": "r1lZgyBYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "P-F2reYdG", "original": null, "number": 1, "cdate": 1576798724765, "ddate": null, "tcdate": 1576798724765, "tmdate": 1576800911729, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a lossless image compression consisting of a hierarchical VAE and using a bits-back version of ANS. Compared to previous work, the paper (i) improves the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS (ii) increases compression speed by implementing a vectorized version of ANS (iii) shows that a model trained on a low-resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution.\n\nThe authors addressed properly reviewers' concerns. Main critics which remain are (i) the method is not practical yet (long compression time) (ii) results are not state of the art - but the contribution is nevertheless solid.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718364, "tmdate": 1576800268834, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Decision"}}}, {"id": "B1lCFME2iH", "original": null, "number": 4, "cdate": 1573827206450, "ddate": null, "tcdate": 1573827206450, "tmdate": 1573827206450, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "r1lq-uOFiB", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "I am satisfied with the author's rebuttal, and will keep my rating at \"Accept\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lZgyBYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1496/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1496/Authors|ICLR.cc/2020/Conference/Paper1496/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155156, "tmdate": 1576860531526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment"}}}, {"id": "r1lq-uOFiB", "original": null, "number": 3, "cdate": 1573648386251, "ddate": null, "tcdate": 1573648386251, "tmdate": 1573648386251, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "BygIsY5aKr", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your review. To address the points you raised:\n\n>I would like the authors to revise their statement of state of the art compression performance on page 7 directly below table 2. ... It would be beneficial for the author to include the scores of [IDF generalization].\n\nWe have revised the statement below Table 2 to more accurately reflect the data in the table. We have also added the results for IDF generalization to the table.\n\n>Because of the buffer of initial bits required by bit-back coding, the compression/decompression of several data points has to be sequential if one wants to amortize this cost over several data points. Compression methods that don\u2019t rely on bits-back coding, such as IDF [3], do not have this issue and can compress/decompress data points in parallel. Since this influences the practical usability of the model, it would be transparent to mention this.\n\nWe have added mention of these models to the last paragraph of the Discussion section, where we felt this point fitted.\n\n>My final main question is on the equivalence of evaluation methods of Bit-Swap and Hilloc on imagenet. The Bit-Swap paper states: \u201cFor MNIST, CIFAR-10 and Imagenet (32 \u00d7 32) we report the bitrates, shown in Table 5, as a result of compressing 100 datapoints in sequence (averaged over 100 experiments)...\u201d. This means that Bit-Swap is not evaluated on the full test set of Imagenet 32 (as this contains 50000 images), as opposed to Hilloc. Do the authors think this is a problem?\n\nThis implies that the Bit-Swap results may be noisier than ours. They also give \u2018average net bitrate\u2019 values in tables 2-4, which are close to the values in their table 5. We presume that the error bounds that they give are 2 standard deviations, from the empirical distribution of the \u2018100 experiments\u2019 they ran. We think it\u2019s likely that the figures they give are accurate enough to reasonably compare to ours.\n\n>Furthermore, in the case of \u201cfull\u201d Imagenet, Bit-swap uses a subset of 100 images for evaluation and crops them to a multiple of 32 pixels in height and width, so that bit-swap can compress patches and the result is the average of patches for on image. Hilloc appears to take 500 random images and does not state anything about cropping. Could the authors comment on this?\n\nWe have updated our results after benchmarking on a larger subset of 2000 (not 500) images, and have updated the paper to reflect this. The Bit-Swap result here may be affected by noise due to the smaller scale of their experiment, however again we have assumed that it is accurate enough for comparison. The BitSwap images are indeed cropped so that the side lengths are multiples of 32. For HiLLoC this was not necessary. We think the comparison still makes sense even with this slight difference, and we have added a footnote to explain the difference between our full size ImageNet experiment and the one in Bit-Swap."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lZgyBYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1496/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1496/Authors|ICLR.cc/2020/Conference/Paper1496/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155156, "tmdate": 1576860531526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment"}}}, {"id": "SJx8SmutiS", "original": null, "number": 2, "cdate": 1573647166401, "ddate": null, "tcdate": 1573647166401, "tmdate": 1573647852505, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "BJeuOnp19S", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your review. To address the points you raised:\n\n> ... put a full description of the neural network used\n\nWe have now added a detailed description of the VAE that we used, in Appendix E.\n\n> the authors also need to disclose how long it took to compress an average ImageNet image\n\nWe\u2019ve found the encode/decode times are roughly linear in the number of pixels, and you can extrapolate from the graph. To demonstrate this point, we\u2019ve timed compressing ImageNet images with dimension 500x374, which is slightly over the average size. The compression takes 29s. We agree that it's important to disclose this and we\u2019ve added this information to the paper near the end of Section 4. We also agree that these times are slow, and mean that the method is not yet practical. However, we have improved significantly over existing work, and we see plenty of scope for further optimizing the runtime of the algorithm. In particular, quite a lot of code is still running in the Python interpreter, which could be written in another, compiled language. Also the hierarchical VAE that we used was mainly chosen to demonstrate the scalability of the method and to ensure an excellent compression rate, and not for its practicality. A smaller model would almost certainly be more appropriate in the long run, and distillation could be used to minimize runtime whilst maintaining similar compression performance."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lZgyBYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1496/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1496/Authors|ICLR.cc/2020/Conference/Paper1496/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155156, "tmdate": 1576860531526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment"}}}, {"id": "HJl-XGOYir", "original": null, "number": 1, "cdate": 1573646872918, "ddate": null, "tcdate": 1573646872918, "tmdate": 1573646872918, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "BygJ9qoy9S", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your review. We address the following point:\n\n> However, I have to say that the part describing the vectorized implementation of their method was rather confusing and the paper could benefit a lot from clarifying this part.\n\nIt\u2019s difficult to give a proper description of this without going into a lot more detail about ANS implementation. To aid readers who are confused and/or curious, we\u2019ve added a recommendation, in the second paragraph of Section 3.2, to refer to our code and to Giesen (2015) for more detail."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lZgyBYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1496/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1496/Authors|ICLR.cc/2020/Conference/Paper1496/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155156, "tmdate": 1576860531526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Authors", "ICLR.cc/2020/Conference/Paper1496/Reviewers", "ICLR.cc/2020/Conference/Paper1496/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Comment"}}}, {"id": "BygIsY5aKr", "original": null, "number": 1, "cdate": 1571821981713, "ddate": null, "tcdate": 1571821981713, "tmdate": 1572972460991, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper focuses on lossless source compression with bits back coding for hierarchical fully convolutional VAEs. The focus/contribution is three-fold: 1. Improve the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS. The newly proposed discretization scheme allows for a dependency structure that is not restricted to a Markov chain structure in the encoder model q(z|x) and in the generative part of the model p(x,z). This is in contrast with bit-swap[1], which requires a markov chain structure. The dependency structure that is allowed in the proposed method is widely known to perform better than a markov chain structure, which can explain why it improves significantly over Bit-swap [1] (another hierarchical VAE compression algorithm that uses bits back coding.) 2. Increasing compression speed by implementing a vectorized version of ANS, and heaving an ANS head in the shape of a pair of arrays matching that of the latent variable and the observed variable. The latter allows for simultaneous encoding of the latent with the prior distribution and the image with the decoder distribution. 3. Showing that a model trained on a low-resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution datasets with convincing results. \n\nDecision: Accept.\nThis paper is clearly written, makes clear claims and supports these claims with convincing experiments. The contributions are of practical use and I expect future work to benefit from this paper. \n\n\nSupporting arguments for decision:\nThe paper is well motivated; off the shelf compression algorithms such as PNG are also not trained on every dataset separately, and cross-dataset generalization is important if this model should be used in practice for many different images from different datasets and of different resolutions.  \n\nThe paper clearly supports the main claims. It improves upon the previous bits-back coding-based hierarchical VAE [1]. The only hypothesis that is not checked is the one that hypothesizes that the lower bpd for higher resolution images is due to the lower ratio of edge pixels versus non-edge pixels, but this is not a dealbreaker from my point of view. \n\nI would like the authors to revise their statement of state of the art compression performance on page 7 directly below table 2. \u201cThe fact that HiLLoC achieves state of the art compression rates relative to the baselines even under a change of distribution is striking, and provides strong evidence of its efficacy as a general method for lossless compression of natural images\u201d. This is sentence should be made more nuanced as the proposed model only improves on Bit-Swap, but is still significantly outperformed by Local bits back coding (LBB [2]), and in the case of cifar-10 also by integer discrete flows (IDF [3]). On the other hand, it would be useful to still state that LBB is trained on every dataset separately, as well as IDF. Note also that in [3], a model that is trained on Imagenet32 and evaluated on the other datasets is also reported (see table 1 in [3]). It would be beneficial for the author to include the scores of this model, as the proposed method seems to perform slightly better at generalizing to new datasets.\n\nBecause of the buffer of initial bits required by bit-back coding, the compression/decompression of several data points has to be sequential if one wants to amortize this cost over several data points. Compression methods that don\u2019t rely on bits-back coding, such as IDF [3], do not have this issue and can compress/decompress data points in parallel. Since this influences the practical usability of the model, it would be transparent to mention this. \n\nMy final main question is on the equivalence of evaluation methods of Bit-Swap and Hilloc on imagenet. The Bit-Swap paper states: \u201cFor MNIST, CIFAR-10 and Imagenet (32 \u00d7 32) we report the bitrates, shown in Table 5, as a result of compressing 100 datapoints in sequence (averaged over 100 experiments)...\u201d. This means that Bit-Swap is not evaluated on the full test set of Imagenet 32 (as this contains 50000 images), as opposed to Hilloc. Do the authors think this is a problem? \nFurthermore, in the case of \u201cfull\u201d Imagenet, Bit-swap uses a subset of 100 images for evaluation and crops them to a multiple of 32 pixels in height and width, so that bit-swap can compress patches and the result is the average of patches for on image. Hilloc appears to take 500 random images and does not state anything about cropping. Could the authors comment on this?\n\n\n\nAdditional feedback to improve paper (not part of decision assessment):\n- In the introduction, first paragraph: \u201c the method can achieve an expected message length equal to the variational free energy, often referred to as the evidence lower bound (ELBO) of the model. \u201c \u2192 \u201c the method can achieve an expected message length equal to the variational free energy, often referred to as the negative evidence lower bound (ELBO) of the model. \u201c\n- Section 3.2, last paragraph: It is not clear if in practice the latent and image are actually encoded in parallel as the author states that this is \u201cin theory\u201d possible. \n- Page 4: \u201c... we found that most of the compute time for our compression was spent in neural net inference, \u2026\u201d I assume you mean \u201cinference\u201d in any part of the encoder or decoder, and not specifically approximate inference of the encoder network. Perhaps clarify this to avoid confusion?\n- Section 4: When referring to the ResnetVAE by Kingma et al, it would be appropriate to also cite [4], as this is very similar to resnetVAE\u2019s and was released earlier.\n\n\n\n[1] F. H. Kingma, P. Abbeel, and J. Ho. Bit-Swap: recursive bits-back coding for lossless compression with hierarchical latent variables. In International Conference on Machine Learning (ICML), 2019.\n[2] Jonathan Ho, Evan Lohn, and Pieter Abbeel. Compression with Flows via Local Bits-Back Coding. arXiv e-prints, 2019.\n[3] Emiel Hoogeboom, Jorn W. T. Peters, Rianne van den Berg, and Max Welling. Integer Discrete Flows and Lossless Compression. arXiv e-prints, 2019.\n[4] C. K. S\u00f8nderby, T. Raiko, L. Maal\u00f8e, S. K. S\u00f8nderby, and O. Winther. Ladder variational autoencoders. In Advances in Neural Information Processing Systems (NIPS), 2016.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575765326007, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Reviewers"], "noninvitees": [], "tcdate": 1570237736532, "tmdate": 1575765326020, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review"}}}, {"id": "BygJ9qoy9S", "original": null, "number": 2, "cdate": 1571957383243, "ddate": null, "tcdate": 1571957383243, "tmdate": 1572972460946, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method for lossless image compression based on using\nfully convolutional VAE models. These models are shown to generalize well when\nthey are trained on small images (e.g. 32x32 and 64x64) and then applied to\nmuch larger images. The method is based on a fully vectorized implementation of\nbits back with asymetric numeral systems coding which is much faster than\nprevious non-vertorized implementations. An improvement with respect to similar\nmethods is to use a dynamic discretization of the latent variables which avoids\nhaving to callibrate a static discretization (as in previous methods).\nFinally, the authors initialize the bis back process with information about a\nfew initial images which are coded using a different codec.  The experiments\nperformed illustrate the gains of the method in terms of compression ratio and\nspeed.\n\nClarity:\n\nThe paper is extremelly well writen and it is very easy to read. The athors\nindicate that they will release open-source code to implement all their\nresults, which is very wellcome to improve reproducibility. However, I have to\nsay that the part describing the vectorized implementation of their method was\nrather confusing and the paper could benefit a lot from clarifying this part.\n\nQuality:\n\nThe experiments performed are sound and illustrate the gains produced by their\nmethod (although they do not achieve state of the art results). In particular,\nthe experiments show the speed up gain by the proposed vectorization and the gains\nproduced by the dynamic discretization. The experiments also show how the methods\ntrained on smaller images generalize well to larger images.\n\nNovelty:\n\nThe proposed approach is novel up to my knowledge. Although the methodological\ninnovations are not that advanced, the vectorization in the specific\napplication considered is novel, as well as the dynamic discretization.\n\nSignificance:\n\nThe proposed contributions are significant in my opinion. The vectorization\napproach can be very useful in practice and the dynamic discretization can also\nbe useful as shown by the experiments. One criticism could be that the authors\ndo not achieve state of the art results, but I consider this a minor thing."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575765326007, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Reviewers"], "noninvitees": [], "tcdate": 1570237736532, "tmdate": 1575765326020, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review"}}}, {"id": "BJeuOnp19S", "original": null, "number": 3, "cdate": 1571966064069, "ddate": null, "tcdate": 1571966064069, "tmdate": 1572972460900, "tddate": null, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "invitation": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a method for lossless image compression consisting of a VAE and using a bits-back version of ANS. The results are very impressive on a ImageNet (but maybe not so impressive on the other benchmarks). The authors also discuss how to speed up inference and present some frightening runtime numbers for the serial method, and some better numbers for the vectorized version, though they're nowhere close to being practical.\n\nI think this paper should be accepted. It has a better description of the BB ANS algorithm than I have read before, and it's a truly interesting direction for the field, despite the lack of immediate applicability.\n\nIf we are to accept this paper, I suggest the authors put a full description of the neural network used (it's barely mentioned). I think the authors also need to disclose how long it took to compress an average imagenet image (looking at the runtime numbers for 128x128 pixels is scary, but at least we'd get a better picture on the feasability).\n\nOverall, due to the fact that the authors pledge to open source the framework, I think some of the details will be found in the code, once released. I think this is an important step because there are so many details in this paper that one cannot reasonably reproduce the work by simply reading the text of this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1496/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HiLLoC: lossless image compression with hierarchical latent variable models", "authors": ["James Townsend", "Thomas Bird", "Julius Kunze", "David Barber"], "authorids": ["james.townsend@cs.ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "julius.kunze@cs.ucl.ac.uk", "david.barber@ucl.ac.uk"], "keywords": ["compression", "variational inference", "lossless compression", "deep latent variable models"], "TL;DR": "We scale up lossless compression with latent variables, achieving state of the art on full-size ImageNet images.", "abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.", "pdf": "/pdf/34119517f309200cc11db1cbe6d3f127b858fbcf.pdf", "code": "https://github.com/hilloc-submission/hilloc", "paperhash": "townsend|hilloc_lossless_image_compression_with_hierarchical_latent_variable_models", "_bibtex": "@inproceedings{\nTownsend2020HiLLoC:,\ntitle={HiLLoC: lossless image compression with hierarchical latent variable models},\nauthor={James Townsend and Thomas Bird and Julius Kunze and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lZgyBYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/37b2463721c41e5390ce12ed6a24d81bc1a567c1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lZgyBYwS", "replyto": "r1lZgyBYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1496/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575765326007, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1496/Reviewers"], "noninvitees": [], "tcdate": 1570237736532, "tmdate": 1575765326020, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1496/-/Official_Review"}}}], "count": 9}